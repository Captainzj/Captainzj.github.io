<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译 | Go Further | Stay Hungry, Stay Foolish</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#142421">
    
    
    <meta name="keywords" content="translation">
    <meta name="description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta name="keywords" content="translation">
<meta property="og:type" content="article">
<meta property="og:title" content="Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译">
<meta property="og:url" content="http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/index.html">
<meta property="og:site_name" content="Go Further">
<meta property="og:description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-07-29T04:00:23.997Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译">
<meta name="twitter:description" content="【阅读时间】XXX min XXX words【阅读内容】……">
    
        <link rel="alternate" type="application/atom+xml" title="Go Further" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">CaptainSE</h5>
          <a href="mailto:841145636@qq.com" title="841145636@qq.com" class="mail">841145636@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/Captainzj" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="検索">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-07-19T03:39:20.000Z" itemprop="datePublished" class="page-time">
  2019-07-19
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Paper/">Paper</a><ul class="article-category-list-child"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Paper/ADNI/">ADNI</a></li></ul></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#ABSTRACT"><span class="post-toc-number">1.</span> <span class="post-toc-text">ABSTRACT</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-Introduction"><span class="post-toc-number">2.</span> <span class="post-toc-text">1. Introduction</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-Materials"><span class="post-toc-number">3.</span> <span class="post-toc-text">2. Materials</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-1-Datasets"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">2.1. Datasets</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-2-Participants"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">2.2. Participants</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-1-ADNI"><span class="post-toc-number">3.2.1.</span> <span class="post-toc-text">2.2.1. ADNI</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-2-AIBL"><span class="post-toc-number">3.2.2.</span> <span class="post-toc-text">2.2.2. AIBL</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-3-OASIS"><span class="post-toc-number">3.2.3.</span> <span class="post-toc-text">2.2.3. OASIS</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-3-Imaging-data"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">2.3. Imaging data</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-3-1-ADNI"><span class="post-toc-number">3.3.1.</span> <span class="post-toc-text">2.3.1. ADNI</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-3-2-AIBL"><span class="post-toc-number">3.3.2.</span> <span class="post-toc-text">2.3.2. AIBL</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-3-3-OASIS"><span class="post-toc-number">3.3.3.</span> <span class="post-toc-text">2.3.3. OASIS</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-Methods"><span class="post-toc-number">4.</span> <span class="post-toc-text">3. Methods</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-1-Converting-datasets-to-a-standardized-data-structure"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">3.1. Converting datasets to a standardized data structure</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-1-1-Conversion-of-the-ADNI-dataset-to-BIDS"><span class="post-toc-number">4.1.1.</span> <span class="post-toc-text">3.1.1. Conversion of the ADNI dataset to BIDS</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-1-2-Conversion-of-the-AIBL-dataset-to-BIDS"><span class="post-toc-number">4.1.2.</span> <span class="post-toc-text">3.1.2. Conversion of the AIBL dataset to BIDS</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-1-3-Conversion-of-the-OASIS-dataset-to-BIDS"><span class="post-toc-number">4.1.3.</span> <span class="post-toc-text">3.1.3. Conversion of the OASIS dataset to BIDS</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-2-Preprocessing-pipelines"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">3.2. Preprocessing pipelines</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-2-1-Preprocessing-of-T1-weighted-MR-images"><span class="post-toc-number">4.2.1.</span> <span class="post-toc-text">3.2.1. Preprocessing of T1-weighted MR images</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-2-2-Preprocessing-of-PET-images"><span class="post-toc-number">4.2.2.</span> <span class="post-toc-text">3.2.2. Preprocessing of PET images</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-3-Feature-extraction"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">3.3. Feature extraction</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-4-Classification-models"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">3.4. Classification models</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-4-1-Linear-SVM"><span class="post-toc-number">4.4.1.</span> <span class="post-toc-text">3.4.1. Linear SVM</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-4-2-Logistic-regression-with-L2-regularization"><span class="post-toc-number">4.4.2.</span> <span class="post-toc-text">3.4.2. Logistic regression with L2 regularization</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-4-3-Random-forest"><span class="post-toc-number">4.4.3.</span> <span class="post-toc-text">3.4.3. Random forest</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-5-Evaluation-strategy"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">3.5. Evaluation strategy</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-5-1-Cross-validation"><span class="post-toc-number">4.5.1.</span> <span class="post-toc-text">3.5.1. Cross-validation</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-5-2-Metrics"><span class="post-toc-number">4.5.2.</span> <span class="post-toc-text">3.5.2. Metrics</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-6-Classification-experiments"><span class="post-toc-number">4.6.</span> <span class="post-toc-text">3.6. Classification experiments</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-Results"><span class="post-toc-number">5.</span> <span class="post-toc-text">4. Results</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-1-Influence-of-the-atlas"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">4.1. Influence of the atlas</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-2-Influence-of-the-smoothing"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">4.2. Influence of the smoothing</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-3-Influence-of-the-type-of-features"><span class="post-toc-number">5.3.</span> <span class="post-toc-text">4.3. Influence of the type of features</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-4-Influence-of-the-classification-method"><span class="post-toc-number">5.4.</span> <span class="post-toc-text">4.4. Influence of the classification method</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-5-Influence-of-the-partial-volume-correction-of-PET-images"><span class="post-toc-number">5.5.</span> <span class="post-toc-text">4.5. Influence of the partial volume correction of PET images</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-6-Influence-of-the-magnetic-field-strength"><span class="post-toc-number">5.6.</span> <span class="post-toc-text">4.6. Influence of the magnetic field strength</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-7-Influence-of-class-imbalance"><span class="post-toc-number">5.7.</span> <span class="post-toc-text">4.7. Influence of class imbalance</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-8-Influence-of-the-dataset"><span class="post-toc-number">5.8.</span> <span class="post-toc-text">4.8. Influence of the dataset</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-9-Influence-of-the-training-dataset-size"><span class="post-toc-number">5.9.</span> <span class="post-toc-text">4.9. Influence of the training dataset size</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-10-Influence-of-the-diagnostic-criteria"><span class="post-toc-number">5.10.</span> <span class="post-toc-text">4.10. Influence of the diagnostic criteria</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-11-Computation-time"><span class="post-toc-number">5.11.</span> <span class="post-toc-text">4.11. Computation time</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-Discussion"><span class="post-toc-number">6.</span> <span class="post-toc-text">5. Discussion</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-Conclusions"><span class="post-toc-number">7.</span> <span class="post-toc-text">6. Conclusions</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Acknowledgments"><span class="post-toc-number">7.1.</span> <span class="post-toc-text">Acknowledgments</span></a></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-07-19 11:39:20" datetime="2019-07-19T03:39:20.000Z"  itemprop="datePublished">2019-07-19</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Paper/">Paper</a><ul class="article-category-list-child"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Paper/ADNI/">ADNI</a></li></ul></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p>
<a id="more"></a>
<p><a href="https://arxiv.org/abs/1808.06452" target="_blank" rel="noopener">《Reproducible evaluation of classification methods in Alzheimer’s disease: Framework and application to MRI and PET data》</a>  阿尔茨海默病分类方法的可行性评估：MRI和PET数据的框架和应用</p>
<h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>大量论文引入了新颖的机器学习和自动特征提取方法<br>阿尔茨海默病（AD）的分类。然而，虽然绝大多数这些作品使用公共数据集ADNI进行评估，但它们很难再现，因为验证的不同关键组件通常不易获得。这些组件包括选定的参与者和输入数据，图像预处理和交叉验证程序。不同方法的表现也难以客观地比较。特别地，通常难以评估方法的哪个部分（例如，预处理，特征提取或分类算法）提供真正的改进（如果有的话）。在本文中，我们使用三个公开可用的数据集（ADNI，AIBL和OASIS）提出了AD中可重复和客观分类实验的框架。该框架包括：i）将三个数据集自动转换为标准格式（BIDS）; ii）模块化的预处理流水线，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准。我们使用T1 MRI和FDG PET数据证明该框架用于对1960名参与者进行大规模评估。在此评估中，我们评估不同模态，预处理，要素类型（基于区域或体素的特征），分类器，训练集大小和数据集的影响。表演符合最新技术水平。对于所有分类任务，FDG PET优于T1 MRI。使用不同的图册，图像平滑，FDG PET图像的部分体积校正或特征类型没有发现性能差异。线性SVM和L2逻辑回归导致相似的性能，并且都优于随机森林。分类性能随着用于训练的受试者数量而增加。在ADNI上训练的分类器很好地适用于AIBL和OASIS。框架和实验的所有代码都是公开的：通用工具已集成到Clinica软件（<a href="http://www.clinica.run）中，特定于纸张的代码可从以下网址获得：https：//gitlab.icm-institute" target="_blank" rel="noopener">www.clinica.run）中，特定于纸张的代码可从以下网址获得：https：//gitlab.icm-institute</a> .ORG / aramislab / AD-ML。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>阿尔茨海默病（AD）影响全世界超过2000万人。早期识别AD对于充分护理患者和测试新疗法非常重要。神经影像学提供了识别AD的有用信息（Ewers等，2011）：由于灰质丢失引起的萎缩，解剖学磁共振成像（MRI），18F-氟脱氧葡萄糖正电子发射断层扫描（FDG PET）的低代谢，淀粉样蛋白的积累 - β蛋白与淀粉样蛋白PET成像。然后，主要关注的是分析这些标志物以在早期识别AD。特别是，机器学习方法有助于通过学习神经影像数据的判别模式来帮助识别AD患者。<br>已经提出了大量的机器学习方法来分类和预测AD阶段（参见（Falahati等人，2014; Haller等人，2011; Rathore等人，2017）以进行评论）。他们中的一些使用单一成像模式（通常是解剖学MRI）（Cuingnet等人，2011; Fan等人，2008; Klo€ppel等人，2008; Liu等人，2012; Tong等人。 （2014）和其他人提出结合多种方式（MRI和PET图像，流体生物标记物）（Gray等，2013; Jie等，2015; Teipel等，2015; Young等，2013; Yun et al。，2015; Zhang et al。，2011）。验证和比较这些方法需要随着时间的推移跟踪大量患者。大量已发表的作品使用了公开的阿尔茨海默病神经影像学倡议（ADNI）数据集。然而，他们的结果之间的客观比较几乎是不可能的，因为它们在以下方面不同：i）患者的子集（选择标准的规范不清楚）; ii）图像预处理管道（因此不清楚优越性能是来自分类还是预处理）; iii）特征提取和选择; iv）机器学习算法; v）交叉验证程序和vi）报告的评估指标。由于这些差异，很难得出哪些方法表现最佳，甚至给定模态是否提供有用的附加信息。结果，这些作品的实际影响仍然非常有限。此外，绝大多数这些作品使用ADNI数据集（ADNI1用于早期论文，最常见的是ADNI1，ADNI-GO和ADNI2的组合用于更近期的工作）。因此，即使存在其他公开可用的数据集，例如澳大利亚成像生物标记物和生活方式研究（AIBL）以及开放获取系列成像研究（OASIS），也很少对其他数据集的泛化进行评估。<br>比较论文（Cuingnet et al。，2011; Sabuncu et al。，2015）和挑战（Allen et al。，2016; Bron et al。，2015）通过允许基准测试，成为客观评估机器学习方法的重要一步在同一数据集上使用相同的预处理方法。然而，这些研究提供了方法的“静态”评估。评估数据集在研究时以其当前状态使用，而新患者不断包括在ADNI等研究中。同样，它们仅限于研究时使用的分类和预处理方法。因此很难用新方法补充它们。<br>在本文中，我们提出了一个可重复评估AD中机器学习算法的框架，并展示了它在从三个公开可用的数据集中获得的PET和MRI数据的分类中的用途：ADNI，AIBL和OASIS。具体来说，我们的贡献是三方面的：</p>
<p>i）管理公开数据集的框架及其与新主题的持续更新，特别是全脑自动转换为脑成像数据结构4（BIDS）格式的工具（Gorgolewski等，2016）;<br>ii）一组模块化的预处理管道，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准;<br>iii）对来自三个公开可用的神经成像数据集（ADNI，AIBL和OASIS）的T1 MRI和PET数据进行大规模评估。</p>
<p>我们使用这个框架来证明从三个数据集（ADNI，AIBL和OASIS）获得的T1 MRI和PET数据进行自动分类。 我们评估各种成分对分类性能的影响：模态（T1 MRI或PET），特征类型（体素或区域特征），预处理，诊断标准（标准NINCDS / ADRDA标准或淀粉样蛋白精制标准），分类算法。 首先在ADNI，AIBL和OASIS数据集上独立进行实验，并通过将对ADNI训练的分类器应用于AIBL和OASIS数据来评估结果的推广。<br>框架和实验的所有代码都是公开的：通用工具已经集成到Clinica5（Routier等，2018），这是一个开源软件平台，我们开发用于处理来自神经影像学研究的数据，以及 特定于纸张的代码可从以下网址获得：https：//gitlab.icm-institute.org/aramislab/AD-ML。</p>
<h2 id="2-Materials"><a href="#2-Materials" class="headerlink" title="2. Materials"></a>2. Materials</h2><h3 id="2-1-Datasets"><a href="#2-1-Datasets" class="headerlink" title="2.1. Datasets"></a>2.1. Datasets</h3><p>用于制备本文的部分数据来自阿尔茨海默病神经影像学倡议数据库（adni.loni.usc.edu）。 ADNI于2003年作为公私合作伙伴关系启动，由首席研究员Michael W. Weiner博士领导。 ADNI的主要目标是测试是否可以将连续MRI，PET，其他生物标志物以及临床和神经心理学评估结合起来，以测量轻度认知障碍（MCI）和早期AD的进展。在研究的三个阶段（ADNI1，ADNI GO和ADNI2），北美地区招募了超过1650名参与者。大约400名参与者被诊断患有AD，900名患有MCI，350名被诊断为对照受试者。使用三个主要标准对受试者进行分类（Petersen等，2010）。正常受试者没有记忆投诉，而患有MCI和AD的受试者都必须进行投诉。 CN和MCI受试者的迷你精神状态检查（MMSE）评分在24到30之间（包括在内），AD受试者在20到26之间（包括在内）。 CN受试者的临床痴呆评分（CDR）评分为0，MCI受试者为0.5，强制要求记忆盒评分为0.5或更高，AD受试者为0.5或1.其他标准可见于（Petersen等，2010）。<br>我们还使用了AIBL研究组收集的数据。与ADNI类似，澳大利亚影像，生物标志物和生活方式老龄化旗舰研究旨在发现哪些生物标志物，认知特征以及健康和生活方式因素决定了AD的发展。 AIBL已招募1100名参与者并收集了超过4。5年的纵向数据：211名AD患者，133名MCI患者和768名可比较的健康对照。之前已经报道了AIBL研究方法（Ellis等，2010,2009）。简而言之，MCI诊断是根据基于（Winblad等人，2004）标准的方案和NINCDS-ADRDA标准的AD诊断（McKhann等人，1984）进行的。请注意，大约一半被诊断为健康对照的受试者报告存在记忆（Ellis等，2010,2009）。<br>最后，我们使用了开放获取系列成像研究项目的数据，该项目的目的是使科学界免费获得大脑的MRI数据集。我们专注于“年轻，中年，非痴呆和痴呆老年人的横断面MRI数据”（Marcus等，2007），其中包括416名年龄在18到96.100之间的受试者的横断面收集。 60岁以上的受试者已经临床诊断为非常轻度至中度的AD。用于评估诊断的标准是CDR评分。 CDR大于0的所有参与者被诊断为可能的AD。请注意，OASIS中没有MCI主题。</p>
<h3 id="2-2-Participants"><a href="#2-2-Participants" class="headerlink" title="2.2. Participants"></a>2.2. Participants</h3><h4 id="2-2-1-ADNI"><a href="#2-2-1-ADNI" class="headerlink" title="2.2.1. ADNI"></a>2.2.1. ADNI</h4><p>从ADNI数据集创建了三个子集：ADNIT1w，ADNICLASS和ADNICLASS，Aß。 ADNIT1w包括所有参与者（N = 1628），在基线时T1加权（T1w）MR图像可用。 ADNICLASS包括1159名参与者，他们在基线时可获得具有已知有效分辨率的T1w MR图像和FDG PET扫描。 ADNICLASS，Aß是ADNICLASS的一个子集，包括已知的淀粉样蛋白状态的918名参与者，这些参与者分别使用1.47和1.10作为截止值从PiB或AV45 PET扫描确定（Landau等，2013）。 对于每个ADNI子集，考虑了五个诊断组：</p>
<ul>
<li>CN：在基线时被诊断为CN的受试者;</li>
<li>AD：在基线时被诊断为AD的受试者;</li>
<li>MCI：在基线时被诊断为MCI，EMCI或LMCI的受试者;</li>
<li>pMCI：在基线时被诊断为MCI，EMCI或LMCI的受试者在至少36个月内进行随访，并在第一次就诊和36个月就诊之间进展至AD;</li>
<li>sMCI：在基线时被诊断为MCI，EMCI或LMCI的受试者在至少36个月内被跟踪并且未进展至AD<br>他们的第一次访问和36个月的访问之间。</li>
</ul>
<p>当然，pMCI和sMCI小组的所有参与者也都在<br>MCI集团。 请注意，反过来是错误的，因为一些MCI受试者没有转换为AD，但没有足够长时间地说明他们是sMCI还是pMCI。 我们没有考虑具有显着记忆问题（SMC）的受试者，因为此类别仅存在于ADNI 2中。<br>表1-3总结了参与者组成的人口统计学，MMSE和全球CDR分数 $ADNI_{T1w}$, $ADNI_{CLASS}$ and $ADNI_{CLASS, Aß}$.</p>
<h4 id="2-2-2-AIBL"><a href="#2-2-2-AIBL" class="headerlink" title="2.2.2. AIBL"></a>2.2.2. AIBL</h4><p>在这项工作中考虑的AIBL数据集由608名参与者组成，他们在基线时可获得T1加权MR图像。 用于创建诊断组的标准与用于ADNI的标准相同。 表4总结了AIBL参与者的人口统计学，MMSE和全球CDR得分。</p>
<h4 id="2-2-3-OASIS"><a href="#2-2-3-OASIS" class="headerlink" title="2.2.3. OASIS"></a>2.2.3. OASIS</h4><p>在这项工作中考虑的OASIS数据集由193名年龄在61岁或以上的参与者组成（参与者被诊断为AD的最低年龄）。 表5总结了OASIS参与者的人口统计学，MMSE和全球CDR分数。</p>
<h3 id="2-3-Imaging-data"><a href="#2-3-Imaging-data" class="headerlink" title="2.3. Imaging data"></a>2.3. Imaging data</h3><h4 id="2-3-1-ADNI"><a href="#2-3-1-ADNI" class="headerlink" title="2.3.1. ADNI"></a>2.3.1. ADNI</h4><p><strong>2.3.1.1. T1加权MRI.</strong> 3D T1w图像的采集协议可以在ADNI 1的（Jack等人，2008）和ADNI GO / 2的（Jack等人，2010a）中找到。 图像可以在获取时或在经历多个预处理校正步骤之后下载，其中包括校正由于梯度非线性（gradwarp）引起的图像几何失真，校正在执行RF传输时发生的图像强度不均匀性 具有更均匀的体线圈，同时接收使用不均匀的头部线圈（B1不均匀性），并且由于3T处的波或介电效应或1.5的残余强度不均匀性导致的强度不均匀性降低 T扫描（N3）（Jack等，2010a，2008）。</p>
<p><strong>2.3.1.2. PET.</strong> ADNI FDG PET方案包括动态采集6个5分钟帧（ADNI 1）或4个5分钟帧（ADNI GO / 2），注射后30-60分钟（Jagust等，2015,2010）。 可以下载预处理的不同阶段的图像（帧平均，空间对齐，插值到标准体素大小，以及平滑到8 mm全宽半高的公共分辨率）。 即使没有在实验中使用，也可以获得用于ADNI 1的11C-Pittsburgh化合物B（PIB）和用于ADNI 1 / GO / 2的18F-Florbetapir，也称为AV45，以成像脑中淀粉样蛋白的沉积。。 该方案包括在注射后50至70分钟动态采集4个5分钟的帧（Jagust等，2015,2010）。 至于FDG PET，可以下载预处理的不同阶段的图像。</p>
<h4 id="2-3-2-AIBL"><a href="#2-3-2-AIBL" class="headerlink" title="2.3.2. AIBL"></a>2.3.2. AIBL</h4><p>用于AIBL受试者的T1w MR图像使用ADNI 3D T1w序列采集，with 1 ✕ 1 mm in-plane resolution，切片厚度为1.2mm，TR/TE/TI=2300/2.98/900，翻转角度为9 和视野240鉁 256和160片（Ellis等，2010）。 尽管他们没有在实验中使用，但也获得了Florbetapir，PiB和Flutemeta-mol PET数据。</p>
<h4 id="2-3-3-OASIS"><a href="#2-3-3-OASIS" class="headerlink" title="2.3.3. OASIS"></a>2.3.3. OASIS</h4><p>对于每个OASIS主题，三个或四个T1w图像，with 1 ✕ 1 mm in-plane resolution ，切片厚度为1.25 mm，TR/TE/TI = 9.7/4.0/20，翻转角10，视野256 在单次成像会话中，在1.5T扫描仪上获得了鉁256和128个切片（Marcus等，2007）。 对于每个受试者，还可以下载重新采样到1mm各向同性体素的运动校正的共同配准图像的平均值，以及空间标准化图像。</p>
<h2 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3. Methods"></a>3. Methods</h2><p>我们开发了一套统一的工具，用于数据管理，图像预处理，特征提取，分类和评估。 这些工具已集成到我们开发的Clinica6（Routier等，2018），这是一个开源软件平台。 转换工具允许在新主题可用时轻松更新数据集。 不同的组件以模块化方式设计：使用Nipype处理管道（Gorgolewski等，2011），以及使用scikit-learn7库的分类和评估工具（Pedregosa等，2011）。 这允许开发和测试其他方法作为给定步骤的替代，并且客观测量每个组分对结果的影响。 提供了一个简单的命令行界面，代码也可以用作Python库。</p>
<h3 id="3-1-Converting-datasets-to-a-standardized-data-structure"><a href="#3-1-Converting-datasets-to-a-standardized-data-structure" class="headerlink" title="3.1. Converting datasets to a standardized data structure"></a>3.1. Converting datasets to a standardized data structure</h3><p>尽管公共数据集非常有价值，但这些研究的一个重要困难在于临床和成像数据的组织。例如，ADNI和AIBL成像数据在下载状态下不依赖于社区标准来进行数据组织和缺乏明确的结构。对于参与者的给定访问存在多个图像采集，并且补充图像信息包含在许多csv文件中，使得对数据库和主题选择的探索非常复杂。为了组织数据，我们选择了BIDS格式（Gorgolewski等，2016），这是一种能够存储多种神经影像模型的社区标准。 BIDS基于文件层次结构而不是数据库管理系统，可以在任何环境中轻松部署。非常重要的是，我们提供的代码可以在下载到BIDS组织版本时自动执行数据转换，用于所有使用的数据集：ADNI，AIBL和OASIS。这允许其他组直接再现，而不必重新分配数据集，这在ADNI和AIBL的情况下是不允许的。我们还根据所需的成像模式，随访持续时间和诊断提供了用于选择受试者的工具，这使得可以使用具有尽可能多的研究对象的相同组。最后，我们提出了一个BIDS启发的标准化结构，用于实验的所有输出。</p>
<h4 id="3-1-1-Conversion-of-the-ADNI-dataset-to-BIDS"><a href="#3-1-1-Conversion-of-the-ADNI-dataset-to-BIDS" class="headerlink" title="3.1.1. Conversion of the ADNI dataset to BIDS"></a>3.1.1. Conversion of the ADNI dataset to BIDS</h4><p>ADNI到BIDS转换器要求用户下载所有ADNI研究数据（csv格式的表格数据）和感兴趣的成像数据。请注意，下载的文件必须与下载文件完全一致。以下步骤由自动转换器执行（无需用户干预）。要将成像数据转换为BIDS，首先从ADNIMERGE电子表格中获取其会话的主题列表。该列表针对每种感兴趣的模态与可用的扫描列表进行比较，如模态特定的csv文件（例如MRILIST.csv）所提供的。如果针对特定的一对主题会话获取模态，并且几个扫描和/或预处理的图像可用，则仅转换一个。关于T1扫描，当几个可用于单个会话时，选择首选扫描（如MAYOADIRL_MRI_IMAGEQC_12_08_15.csv中所标识）。如果未指定首选扫描，则选择更高质量的扫描（如MRIQUALI-TY.csv中所定义）。如果未找到质量控制，则我们选择第一次扫描。当可用时选择Gradwarp和B1不均匀校正图像，因为这些校正可以在临床环境中进行，否则选择原始图像。对于ADNI 1，1.5T图像是优选的，因为它们可用于更多患者。关于FDG PET扫描，选择在时间帧上共同登记和平均的图像。扫描失败的质量控制（如果在PETQC.csv中指定）将被丢弃。请注意，AV45 PET扫描也会被转换，但不会在实验中使用。一旦选择了感兴趣的图像并识别出图像文件的路径，就可以将成像数据转换为BIDS。当采用dicom格式时，首先使用dcm2niix工具将图像转换为nifti，或者在dcm2nii工具出现错误的情况下（Li et al。，2016）。通过为每个主题创建子文件夹来生成BIDS文件夹结构。在每个主题子文件夹中创建会话文件夹，并在每个会话子文件夹内创建一个模态文件夹。最后，将nifti中的每个图像复制到相应的文件夹并重命名以遵循BIDS规范。临床数据也转换为BIDS。不随时间变化的数据，例如受试者的性别，教育水平或基线诊断，可从ADNIMERGE电子表格中获取，并收集在位于BIDS文件夹层次结构顶部的participant.tsv文件中。依赖于会话的数据，例如临床分数，从特定的csv文件（例如MMSE.csv）获得，并收集在每个参与者子文件夹中的<subjectid> _session.tsv文件中。转换的临床数据在电子表格（clin-ical_specifications_adni.xlsx）中定义，该电子表格可与转换器的代码一起使用。如果用户想要转换其他临床数据，则可以轻松修改该文件。</subjectid></p>
<h4 id="3-1-2-Conversion-of-the-AIBL-dataset-to-BIDS"><a href="#3-1-2-Conversion-of-the-AIBL-dataset-to-BIDS" class="headerlink" title="3.1.2. Conversion of the AIBL dataset to BIDS"></a>3.1.2. Conversion of the AIBL dataset to BIDS</h4><p>AIBL到BIDS转换器要求用户下载AIBL非成像数据（csv格式的表格数据）和感兴趣的成像数据。将成像数据转换为BIDS依赖于特定于模态的csv文件，这些文件提供可用的扫描列表。对于每个AIBL参与者，每个会话可用的唯一T1w MR图像被转换。请注意，即使它们未用于此项工作，我们也会转换Florbetapir，PiB和Flutemetamol PET图像（每个会话只有一个图像可用于每个会话）。一旦选择了感兴趣的图像并识别出图像文件的路径，就按照上一节中描述的相同步骤将成像数据转换为BIDS。临床数据的转换依赖于在成像数据转换之后和包含非成像数据的csv文件之后获得的受试者和会话的列表。不随时间变化的数据收集在位于BIDS文件夹层次结构顶部的participant.tsv文件中，而会话相关数据收集在每个参与者子文件夹中的<subjectid> _session.tsv文件中。对于ADNI转换器，转换的临床数据在电子表格（clinical_specifications.xlsx）中定义，该电子表格可与转换器的代码一起使用，用户可以修改该代码。</subjectid></p>
<h4 id="3-1-3-Conversion-of-the-OASIS-dataset-to-BIDS"><a href="#3-1-3-Conversion-of-the-OASIS-dataset-to-BIDS" class="headerlink" title="3.1.3. Conversion of the OASIS dataset to BIDS"></a>3.1.3. Conversion of the OASIS dataset to BIDS</h4><p>OASIS到BIDS转换器要求用户下载OASIS-1成像数据和相关的csv文件。 要将成像数据转换为BIDS，可从下载的文件夹中获取主题列表。 对于每个受试者，在可用的多个T1w MR图像中，我们选择重新采样为1mm各向同性体素的运动校正的共同配准的单个图像的平均值，位于SUBJ_111子文件夹中。 确定图像文件的路径后，使用FreeSurfer的mri_convert工具（Fischl，2012）将Analyze格式的图像转换为nifti，创建BIDS文件夹层次结构，并将图像复制到相应的文件夹 并重命名。 使用成像数据转换后获得的受试者列表和包含非成像数据的csv文件转换临床数据，如上一节所述。</p>
<h3 id="3-2-Preprocessing-pipelines"><a href="#3-2-Preprocessing-pipelines" class="headerlink" title="3.2. Preprocessing pipelines"></a>3.2. Preprocessing pipelines</h3><p>开发了两条管道来预处理解剖学T1w MRI和PET图像。 这些管道具有基于Nipype的模块化结构，允许用户容易地连接和/或替换组件，并且依赖于使用公开可用的标准图像处理工具的完善的程序。 这些管道在Clinica中以名称t1-volume- *和pet-volume提供。</p>
<h4 id="3-2-1-Preprocessing-of-T1-weighted-MR-images"><a href="#3-2-1-Preprocessing-of-T1-weighted-MR-images" class="headerlink" title="3.2.1. Preprocessing of T1-weighted MR images"></a>3.2.1. Preprocessing of T1-weighted MR images</h4><p>对于解剖学T1w MRI，预处理管道基于SPM128。首先，统一分割程序（Ashburner和Friston，2005）用于同时执行输入图像的组织分割，偏差校正和空间归一化。接下来，使用DARTEL创建一个组模板，DARTEL是一种用于微分图像配准的算法（Ashburner，2007），来自受试者在原生空间的组织概率图，通常是GM，WM和CSF组织，在之前获得步。这里，不仅获得了组模板，还获得了每个主体的原生空间到DARTEL模板空间的变形字段。最后，应用DARTEL到MNI方法（Ashburner，2007），提供将原生空间图像注册到MNI空间：对于给定主题，其进入DARTEL模板的流场与DARTEL模板到MNI的转换相结合。空间，并将得到的变换应用于对象的不同组织图。结果，所有图像都在共同的空间中，提供跨主体的体素方式对应。</p>
<h4 id="3-2-2-Preprocessing-of-PET-images"><a href="#3-2-2-Preprocessing-of-PET-images" class="headerlink" title="3.2.2. Preprocessing of PET images"></a>3.2.2. Preprocessing of PET images</h4><p>PET预处理管道依赖于SPM12和PETPVC9工具进行部分体积校正（PVC）（Thomas等，2016）。我们假设每个PET图像具有使用上述管道预处理的相应T1w图像。第一步是使用SPM的Co寄存器方法将PET图像配准到原生空间中的相应T1w图像（Friston等，1995）。可以使用来自原生空间中的T1w的不同组织图作为输入区域来执行具有基于区域体素的（RBV）方法（Thomas等人，2011）的可选PVC步骤。然后，使用与对应的T1w相同的变换将PET图像登记到MNI空间中（使用DARTEL到MNI方法）。然后根据参考区域（FDG PET的侵蚀脑桥）对MNI空间中的PET图像进行强度归一化，并且我们获得标准化的摄取值比（SUVR）图。最后，我们使用二进制掩码来掩蔽非脑区域，所述二进制掩模是通过对MNI空间中的受试者的GM，WM和CSF组织概率图的总和进行阈值处理而得到的。由此产生的蒙版SUVR图像也位于一个公共空间中，并提供跨主体的体素相关性。</p>
<h3 id="3-3-Feature-extraction"><a href="#3-3-Feature-extraction" class="headerlink" title="3.3. Feature extraction"></a>3.3. Feature extraction</h3><p>从成像数据中提取了两种类型的特征：体素和区域特征。 在预处理之后，T1w MRI和FDG PET图像都在MNI空间中。 对于每个图像，第一类特征简单地对应于大脑中的所有体素。 从T1w MR图像获得的信号是灰质密度，从FDG PET图像获得的信号是SUVR。<br>区域特征对应于在不同地图集中获得的一组感兴趣区域（ROI）中计算的平均信号（灰质密度或SUVR），也在MNI空间中。 选择的五个地图集包含皮质和皮质下区域，并覆盖受AD影响的大脑区域。 它们描述如下：</p>
<ul>
<li>AAL2（Tzourio-Mazoyer等，2002）是基于单个受试者的解剖图谱。它是AAL的更新版本，可能是神经影像学文献中使用最广泛的分割图。它是在MNI空间中对空间标准化的单一主体高分辨率T1体积使用手动追踪建立的（Holmes等，1998）。它由覆盖整个皮层的120个区域以及主要的皮质下结构组成。</li>
<li>AICHA（Joliot等，2015）是基于多个主题的功能性地图集。它是使用由281名健康受试者的静息状态fMRI数据计算的组级功能连接概况的分组构建的。它由345个区域组成，覆盖整体皮质以及主要的皮质下结构。</li>
<li>Hammers（Gousias等，2008; Hammers等，2003）是一个基于多个科目的解剖图谱。它是使用30名健康受试者的解剖MRI进行手动追踪而建立的。然后将个体受试者分组登记到MNI空间以生成概率图谱以及最大概率图。后者用于目前的工作。它由覆盖整个皮层的69个区域以及主要的皮质下结构组成。</li>
<li>LPBA40（Shattuck等，2008）是基于多个受试者的解剖图谱。它是使用40名健康受试者的解剖MRI进行手动追踪而建立的。然后将各个主题分组登记到MNI空间以生成最大概率图。它由覆盖整个皮层的56个区域以及主要的皮质下结构组成。</li>
<li>Neuromorphometrics10是基于多个科目的解剖图谱。它是使用30名健康受试者的解剖MRI进行手动追踪而建立的。然后将各个主题分组登记到MNI空间以生成最大概率图。它由覆盖整个皮层的140个区域以及主要的皮质下结构组成。数据可用于“MIC-CAI 2012大挑战和多图谱标签研讨会”。</li>
</ul>
<p>LBPA40，Hammers和Neuro-morphometrics地图集之间的主要区别在于解剖学分割的细节程度（即区域的数量）。</p>
<h3 id="3-4-Classification-models"><a href="#3-4-Classification-models" class="headerlink" title="3.4. Classification models"></a>3.4. Classification models</h3><p>我们考虑了三种不同的分类器：线性SVM，具有L2正则化的逻辑回归和随机森林，所有这些都可以在Clinica中获得。线性SVM与体素和区域特征一起使用，因为其计算复杂性仅取决于使用其双重形式时的主体数量。另一方面，使用L2正则化和随机森林模型的逻辑回归仅用于基于区域的分析，因为它们的复杂性取决于特征的数量，这对于包含大约100万个体素的图像变得不可行。我们使用了scikit-learn库的实现（Pedregosa等，2011）。<br>对于执行的每个任务，我们获得描述给定特征对当前分类任务的重要性的特征权重。这些权重存储为分类输出的一部分，重建分类器的信息也是如此，如找到的最佳参数。对于每个分类，我们可以获得具有跨脑体素或区域的权重表示的图像。</p>
<h4 id="3-4-1-Linear-SVM"><a href="#3-4-1-Linear-SVM" class="headerlink" title="3.4.1. Linear SVM"></a>3.4.1. Linear SVM</h4><p>第一种方法是线性SVM。 为了减少计算负荷，使用线性核k对每对图像（xi，xj）（使用区域或体素特征）预先计算Gram矩阵K =（k（xi，xj））i，j 提供科目。 该Gram矩阵用作通用SVM的输入。 我们选择优化误差项的惩罚参数C. SVM的一个优点是，当使用预先计算的Gram矩阵（双SVM）时，计算时间取决于主题的数量，而不取决于特征的数量。 鉴于其简单性，线性SVM可用作比较不同方法的性能的基线。</p>
<h4 id="3-4-2-Logistic-regression-with-L2-regularization"><a href="#3-4-2-Logistic-regression-with-L2-regularization" class="headerlink" title="3.4.2. Logistic regression with L2 regularization"></a>3.4.2. Logistic regression with L2 regularization</h4><p>第二种方法是使用L2正则化的逻辑回归（通常用于减少过度拟合）。 对于线性SVM，我们优化了误差项的惩罚参数C. 具有L2正则化的逻辑回归直接优化每个特征的权重，并且特征的数量影响训练时间。 这就是我们仅将其用于区域特征的原因。</p>
<h4 id="3-4-3-Random-forest"><a href="#3-4-3-Random-forest" class="headerlink" title="3.4.3. Random forest"></a>3.4.3. Random forest</h4><p>使用的第三个分类器是随机森林。 与线性SVM和逻辑回归不同，随机森林是一种集合方法，它适合数据集的各个子样本上的许多决策树。 组合估计器可防止过度拟合并提高预测精度。 基于scikit-learn库（Pedregosa等，2011）提供的实现，可以优化大量参数。 在评估哪个影响较大的初步实验后，我们选择了以下两个超参数来优化：i）森林中的树木数量; ii）寻找最佳分割时要考虑的特征数量。 由于计算成本高，随机森林仅用于区域特征而不是体素特征。</p>
<h3 id="3-5-Evaluation-strategy"><a href="#3-5-Evaluation-strategy" class="headerlink" title="3.5. Evaluation strategy"></a>3.5. Evaluation strategy</h3><h4 id="3-5-1-Cross-validation"><a href="#3-5-1-Cross-validation" class="headerlink" title="3.5.1. Cross-validation"></a>3.5.1. Cross-validation</h4><p>分类性能的评估主要遵循最近的指南（Varoquaux等，2017）。进行交叉验证（CV），维持列车组（用于拟合模型）和测试集（用于评估性能）的独立性的经典策略。 CV过程包括两个嵌套循环：评估分类性能的外循环和用于优化模型超参数的内循环（C用于SVM和L2逻辑回归，树的数量和随机森林的分割特征） 。应当注意，在优化超参数时，使用CV的内环对于避免向上偏置性能是重要的。这一步并不总是在文献中得到适当的执行（Querbes等，2009; Wolz等，2011），导致过度乐观的结果，如（Eskildsen等，2013; Maggi- pinto等。 ，2017）。<br>我们在Clinica实施了三种不同的外部CV方法：k-fold，重复k-fold和重复随机分裂（所有这些分层），使用基于scikit-learn的工具（Pedregosa等，2011）。方法的选择取决于手头的计算资源。但是，只要有可能，建议使用具有大量重复次数的重复随机分组，以获得更稳定的性能估计。因此，我们用于每个实验250次迭代的随机分裂。我们报告评估指标的完整分布以及平均值和经验标准偏差，如（Raamana和Strother，2017）使用神经预测（Raamana，2017）。然而应该指出的是，交叉验证没有无偏差的方差估计（Bengio和Grandvalet，2004; Nadeau和Bengio，2003），经验方差在很大程度上低估了真实的方差。在解释经验方差值时应牢记这一点。此外，我们选择不对不同分类器的性能进行统计测试。这是一个复杂的问题，没有通用的解决方案。在许多出版物中，使用了交叉验证结果的标准t检验。然而，如Nadeau和Bengio（2003）所示，这种方法过于宽松，不应该应用。已经提出了更好的表现方法，例如保守的Z或校正的重采样t检验（Nadeau和Bengio，2003）。但是，必须谨慎使用此类方法，因为它们的行为取决于数据和交叉验证设置。因此，我们选择避免在本文中使用统计检验，以免误导读者。相反，我们报告了指标的完整分布。<br>对于超参数优化，我们实现了内部k折叠。对于每个分割，选择具有最高平衡精度的模型，然后将这些选定的模型在分割中平均以获得模型平均的利润，这应该具有稳定效果。在本文中，对于内环，用k 1/4 10进行实验。</p>
<h4 id="3-5-2-Metrics"><a href="#3-5-2-Metrics" class="headerlink" title="3.5.2. Metrics"></a>3.5.2. Metrics</h4><p>作为分类的输出，我们报告平衡准确度，ROC曲线下面积（AUC），准确度，灵敏度，特异性，以及每个受试者的预测类别，因此用户可以使用此信息计算其他所需指标。、</p>
<h3 id="3-6-Classification-experiments"><a href="#3-6-Classification-experiments" class="headerlink" title="3.6. Classification experiments"></a>3.6. Classification experiments</h3><p>在表6中详细列出了由数据可用性驱动的每个数据集的分析中考虑的不同分类任务。有关组成分的详细信息可在表2-5中找到。一般而言，我们执行临床诊断分类任务，或MCI受试者进化的“预测”任务。请注意，由于sMCI和pMCI类别中的参与者数量较少，因此未对AIBL执行涉及从MCI进展到AD的任务。然而，当更多进化的MCI受试者在AIBL中公开可用时，该框架将允许非常容易地进行这些实验。<br>根据特征的类型，测试了几个具有不同参数的分类器的性能。对于体素特征，唯一的分类器是线性SVM。使用高斯核对图像应用四种不同的平滑水平，从无平滑到半高全宽12mm（FWHM）。对于基于区域的分类实验，测试了三种分类器：线性SVM，逻辑回归和随机森林。使用五个地图集提取特征：AAL2，AICHA，Hammers，LPBA40和Neuromorphometrics。表7总结了这些信息。<br>对于所研究的数据集，可获得不同的成像模式：虽然T1W MRI和FDG PET图像均可用于ADNI参与者，但只有T1w MRI可用于AIBL和OASIS参与者。对于所考虑的每种模态，使用表7中详述的不同参数提取体素和区域特征。在该工作中测试的所有分类实验总结在表8中。如果没有另外说明，则从图像中提取FDG PET特征。没有经过PVC的。</p>
<h2 id="4-Results"><a href="#4-Results" class="headerlink" title="4. Results"></a>4. Results</h2><p>在这里，我们提供了一些我们认为最有价值的结果。 所有实验（包括其他任务，预处理参数，特征或分类器）的完整结果可在补充材料以及包含所有代码和实验的存储库中找到（<a href="https://gitlab.icm-institute.org/aramislab" target="_blank" rel="noopener">https://gitlab.icm-institute.org/aramislab</a> / AD- ML）。 在以下小节中，我们使用平衡准确度作为性能指标来呈现结果，但结果中提供了所有其他指标。</p>
<h3 id="4-1-Influence-of-the-atlas"><a href="#4-1-Influence-of-the-atlas" class="headerlink" title="4.1. Influence of the atlas"></a>4.1. Influence of the atlas</h3><p>为了评估选择地图集对分类精度的影响并潜在地确定首选地图集，选择了使用区域特征的线性SVM分类器。使用五种不同的图谱提取来自ADNI参与者的T1w MRI和FDG PET图像的特征：AAL2，AICHA，Hammers，LPBA40和Neuro-morphometrics。研究了三个分类任务：CN与AD，CN与pMCI和sMCI与pMCI。<br>如图1所示，没有特定的图集为所有任务提供最高的分类准确度。例如，Neuromorphometrics和AICHA在T1w和FDG PET图像上为CN与AD提供了更好的结果，对于T1w提供了LBPA40，而AAL2在两种成像模式下为CN与pMCI和sMCI与pMCI提供了最高的平衡准确度。对AIBL受试者进行了相同的分析（仅限T1w MR图像），同样地，没有任何图集在任务中始终比其他图谱更好地执行。对于以下基于区域的实验，选择AAL2图谱作为参考图谱，因为它具有良好的分类准确度，并广泛用于神经影像学界。同样，存储库中提供了所有其他结果。</p>
<h3 id="4-2-Influence-of-the-smoothing"><a href="#4-2-Influence-of-the-smoothing" class="headerlink" title="4.2. Influence of the smoothing"></a>4.2. Influence of the smoothing</h3><p>使用具有4mm，8mm和12mm的FWHM的高斯核不对T1w MRI和FDG PET图像进行平滑或平滑。为了确定不同平滑度对分类精度的影响，选择了使用体素特征的线性SVM分类器。研究了三个分类任务：CN与AD，CN与pMCI和sMCI与pMCI。图2中的结果表明，对于大多数分类任务，平衡精度在很大程度上不随光滑内核大小而变化。当从T1w MR图像中提取特征时，观察到CN与pMCI和sMCI与pMCI任务的唯一变化：平衡精度随着核尺寸略微增加。使用来自AIBL数据集的T1w MR图像进行相同的分析。平均精度也随着核尺寸略有增加，但平衡精度的标准偏差大于ADNI。由于平滑程度对分类性能没有明显影响，我们选择提供与基于体素的分类相关的后续结果，参考平滑度为4 mm。</p>
<h3 id="4-3-Influence-of-the-type-of-features"><a href="#4-3-Influence-of-the-type-of-features" class="headerlink" title="4.3. Influence of the type of features"></a>4.3. Influence of the type of features</h3><p>我们将使用线性SVM分类器将体素特征的平衡精度与参考平滑（4 mm FWHM的高斯核）与参考图谱（AAL2）的区域特征所获得的平衡精度进行比较。 这些特征是从ADNI参与者的T1w MRI和FDG PET图像中提取的。 评估了与以前相同的三个分类任务。<br>表9中显示的结果未显示使用体素或区域特征获得的平均平衡精度之间的显着差异。 在AIBL数据集的情况下，基于区域的分类的平衡准确度更高（对于AD与CN：基于体素的0.79 [0.059]，基于区域的0.86 [0.042]），但我们可以观察到相应的标准 偏差很高。</p>
<h3 id="4-4-Influence-of-the-classification-method"><a href="#4-4-Influence-of-the-classification-method" class="headerlink" title="4.4. Influence of the classification method"></a>4.4. Influence of the classification method</h3><p>使用三种不同的分类器进行基于区域的实验，以评估取决于所选分类器的平衡精度是否存在变化。 使用来自T1w MRI的参考AAL2图谱和ADNI参与者的FDG PET图像提取区域特征。 执行了三个先前定义的分类任务。<br>图3中显示的结果显示，具有L2正则化模型的线性SVM和逻辑回归导致类似的平衡精度，始终高于用随机森林获得的所有任务和测试的成像模态。</p>
<h3 id="4-5-Influence-of-the-partial-volume-correction-of-PET-images"><a href="#4-5-Influence-of-the-partial-volume-correction-of-PET-images" class="headerlink" title="4.5. Influence of the partial volume correction of PET images"></a>4.5. Influence of the partial volume correction of PET images</h3><p>使用线性SVM分类器进行基于区域和体素的分析，以评估用于部分体积效应的校正PET图像是否对分类准确性有影响。 具有和不具有PVC的ADNI参与者的FDG PET图像用于这些实验。<br>图4中显示的结果表明，使用和不使用PVC获得的平衡精度之间几乎没有差异。 使用体素功能时，无论是否存在PVC，平均平衡精度几乎相同。 使用区域特征时，当FDG PET图像未针对部分体积效应进行校正时，平均平衡精度的增加非常小。</p>
<h3 id="4-6-Influence-of-the-magnetic-field-strength"><a href="#4-6-Influence-of-the-magnetic-field-strength" class="headerlink" title="4.6. Influence of the magnetic field strength"></a>4.6. Influence of the magnetic field strength</h3><p>ADNI1参与者的大多数T1w扫描是在1.5T扫描仪上获得的，而3T扫描仪用于获取ADNIGO / 2参与者的MR图像。 为了评估场强的差异是否对分类性能产生影响，我们分别计算了1.5T和3T扫描的受试者的平衡准确度。 结果显示在表10中。我们观察到，无论实验如何，与1.5T扫描子集相比，3T扫描子集的平衡精度总是更高，这并不奇怪，因为3T图像应该具有更好的信号 噪声比。</p>
<h3 id="4-7-Influence-of-class-imbalance"><a href="#4-7-Influence-of-class-imbalance" class="headerlink" title="4.7. Influence of class imbalance"></a>4.7. Influence of class imbalance</h3><p>我们执行的任务是使用不平衡的类完成的。这种类别不平衡的范围从非常温和（对于ADNI的CN是CN的1.2倍）到中等（比pMCI多1.7倍的CN和比ADNI的pMCI多2倍的sMCI）到非常强（在AIBL中比AD多6.1倍的CN）。<br>我们的目的是评估这种阶级不平衡是否会影响绩效。为此目的，我们随机抽样亚组并进行实验，其中237 CN对237 AD，167 pMCI vs 167 CN和167 pMCI vs 167 pMCI用于ADNI和72 CN和72 AD用于AIBL。我们确保平衡子集的人口统计学和临床特征与原始子集没有差异。结果如图5所示。对于ADNI，其表现与完整人群的表现相似。对于AIBL，基于体素的功能的平衡组的性能要高得多。因此，似乎非常强的阶级不平衡（如AIBL中比例为6比1的情况）导致较低的性能，但适度的阶级不平衡（ADNI中高达2比1）得到充分处理。</p>
<h3 id="4-8-Influence-of-the-dataset"><a href="#4-8-Influence-of-the-dataset" class="headerlink" title="4.8. Influence of the dataset"></a>4.8. Influence of the dataset</h3><p>我们还想知道数据集的结果是否一致，因此我们比较了从ADNI，AIBL和OASIS获得的分类性能，用于区分控制对象与阿尔茨海默病患者的任务。从T1w MR图像中提取体素（4mm平滑）和区域（AAL2图谱）特征并与线性SVM分类器一起使用。我们测试了两种配置：在同一数据集上训练和测试分类器，在ADNI上训练分类器并在AIBL和OASIS上进行测试。<br>结果显示在表11中。在ADNI和AIBL上获得的性能是可比的并且远高于在OASIS上获得的性能。在ADNI培训和AIBL或OASIS测试时，平衡准确度至少与AIBL或OASIS上的训练和测试时一样高，这表明在ADNI上训练的分类器能够很好地推广到其他数据集。特别是，ADNI培训大大提高了OASIS的分类性能。我们的目的是评估这是否是由于ADNI中的大量受试者。为此目的，我们进行了相同的实验，但每个数据集的参与者子集大小相同。我们从每个数据集中随机抽取70名AD患者和70名CN参与者的人群，确保子群体的人口统计学和临床特征与原始人群没有差异。从表11中可以看出，使用该子集，基于体素的改进消失了，但仍然保留了区域特征。</p>
<h3 id="4-9-Influence-of-the-training-dataset-size"><a href="#4-9-Influence-of-the-training-dataset-size" class="headerlink" title="4.9. Influence of the training dataset size"></a>4.9. Influence of the training dataset size</h3><p>计算学习曲线以评估线性SVM分类器的性能如何根据训练数据集的大小而变化。仅使用ADNI参与者，我们测试了四种情景：从T1w MRI和FDG PET图像中提取的体素和区域特征。作为交叉验证，运行250次迭代，其中数据集被随机分成测试数据集（30％的样本）和训练数据集（70％的样本）。对于CN和AD，用于训练和测试每项不同任务的最大受试者数量为362，CN与pMCI为313，sMCI与pMCI为355。对于每次运行，使用10％到所有训练集（从7％到高达70％的样本）在相同的测试集上训练和评估10个分类器，每个样本使用的样本数量增加10％步。因此，用于培训的参与者数量从CN到20到197，sMCI为24到239，pMCI为12到117，AD为17到166。我们可以从图6中的学习曲线中观察到，正如预期的那样，平衡精度随着训练样本的数量而增加。<br>当使用通过组合来自ADNI和AIBL（由72个CN受试者和72个AD受试者组成的平衡子集）和来自ADNI，AIBL和OASIS的参与者获得的较大数据集时，还计算CN与AD任务的学习曲线。结果显示在图7中。我们观察到，对于相同数量的受试者，组合ADNI和AIBL或仅使用ADNI导致类似的平衡准确度。对于区域特征，与仅使用ADNI相比，ADNI和AIBL组合时的性能略高，但差异主要在标准偏差范围内。当结合ADNI和AIBL时，更多的受试者被用于训练时，平衡的准确性会略微增加。然而，当结合ADNI，AIBL和OASIS时，无论是多少科目，性能都比仅使用ADNI或组合ADNI和AIBL时更差。这可能是由于ADNI和AIBL遵循相同的诊断和采集协议，这与OASIS不同。</p>
<h3 id="4-10-Influence-of-the-diagnostic-criteria"><a href="#4-10-Influence-of-the-diagnostic-criteria" class="headerlink" title="4.10. Influence of the diagnostic criteria"></a>4.10. Influence of the diagnostic criteria</h3><p>我们通过使用关于每个受试者的淀粉样蛋白状态的信息（如果可用）来改进先前使用的诊断标准来定义新的分类任务。 从图8中可以看出，当将这些任务的性能与不使用淀粉样蛋白状态的相关任务进行比较时，对于所有新定义的任务，平均平衡准确度更高或至少相同。 我们必须注意，尽管所有受试者都不知道淀粉样蛋白状态，但尽管计数较少，但仍达到了这一性能。</p>
<h3 id="4-11-Computation-time"><a href="#4-11-Computation-time" class="headerlink" title="4.11. Computation time"></a>4.11. Computation time</h3><p>总的来说，我们使用SVM分类器进行了279次实验，使用逻辑回归分类器进行了155次实验，使用随机森林分类器进行了26次实验（有关任务和参数的详细信息，请参阅表6-8）。 使用具有72个核心（Xeon E5-2699 @ 2.30 GHz）和256 GB RAM的机器，运行434个SVMþ逻辑回归实验需要6天，运行26个随机森林实验需要8天。</p>
<h2 id="5-Discussion"><a href="#5-Discussion" class="headerlink" title="5. Discussion"></a>5. Discussion</h2><p>我们提出了一个开源框架，用于可重复评估AD分类方法，其中包含以下组件：i）转换器将三个公开可用的数据集标准化为BIDS; ii）用于T1w MRI和PET的标准化预处理和特征提取管道; iii）标准分类算法; iv）遵循最近的最佳实践的交叉验证程序。我们展示了它用于评估三个公共数据集上的不同成像模态，预处理选项，特征和分类器。<br>在这项工作中，我们首先致力于在AD中评估机器学习方法：i）更可重复; ii）更客观。再现性是基于相同数据和实验程序再现结果的能力。已经在不同领域提出了提高再现性的呼吁，包括神经成像（Poldrack等，2017）和机器学习（Ke等，2017）。再现性与复制不同，复制是在独立数据上确认结果的能力。可重复研究的关键要素包括：数据共享，使用社区标准存储数据，全自动数据处理，代码共享。我们的工作有助于通过不同方面提高AD ML研究的可重复性。第一个组件是将三个公共数据集全自动转换为社区标准BIDS。实际上，ADNI和AIBL无法重新分配。通过这些工具，我们希望能够轻松地基于这些数据集重现实验，而无需重新分发它们。特别是，与简单地公开使用的主题列表相比，我们为用户节省了大量时间。对于复杂的多模态数据集（例如ADNI（具有大量不完整数据，给定模态的多个实例和复杂元数据）尤其如此。第二个关键组件是公开可用的代码，用于预处理，特征提取和分类。这些贡献收集在Clinica11中，这是一个免费提供的临床神经科学研究软件平台。除了提高可重复性之外，我们希望这些工具还能使研究人员的工作变得更加容易。<br>我们也希望为更客观的评估做出贡献。对新方法（分类算法，特征提取方法或其他方法）的客观评估需要在不改变其他组件的情况下测试该特定组件。我们的框架包括用于T1加权MRI和FDG PET数据的预处理和特征提取的标准方法，以及标准分类工具。这些构成了一套基线方法，可以很容易地比较新方法。然后，研究新方法的研究人员可以直接用自己的解决方案替换管道的给定部分（例如特征提取，分类），并评估该特定新组件相对于所提供的基线方法的附加值。我们还提出了严格验证的工具，主要基于最近的指南（Varoquaux等，2017），并基于标准软件scikit-learn（Pedregosa等，2011）实施。其中包括：i）大量重复随机分割，以广泛评估性能的变化; ii）报告准确度和标准偏差的完整分布，而不仅仅是平均准确度; iii）用于超参数调整的足够嵌套CV。</p>
<p>然后，我们基于T1 MRI和FDG PET数据证明了该框架在不同分类任务中的应用。通过这一点，我们的目标是提供一个基准性能，可以比较先进的机器学习和特征提取方法。这些基线性能符合最先进的结果，这些结果已在（Arbabshirani等，2017; Falahati等，2014; Rathore）中进行了总结。<br>et al。，2017），其中CN与AD的分类准确度通常为80％至95％，sMCI与pMCI的分类准确度为60％至80％。例如，使用线性SVM，区域特征（AAL2）和FDG PET数据，我们报告CN与AD相比为88％，CN与pMCI为80％，sMCI与pMCI为73％。<br>ADNI中使用的诊断标准来自NINCDS-ADRDA（McKhann等，1984），其仅依赖于患者的症状和认知状态。然而，AD的明确诊断只能在尸检时进行，并且在很大比例的病例中发现临床诊断是错误的（Knopman等，2001）。在过去的十年中，AD的诊断取得了实质性进展。特别是，有人建议不仅要依靠临床和认知评估，还要依赖成像和脑脊液（CSF）生物标志物。这导致了新的诊断标准。尽管黄金标准仍然是尸检，但这导致在患者生命期间更准确地诊断AD。特别是，IWG（Dubois等，2007），IWG-2（Dubois等，2014）和NIA-AA（Albert等，2011）提出了β-淀粉样蛋白和/或tau蛋白的存在。 ; Jack等人，2011; McKhann等人，2011; Sperling等人，2011）。在这项工作中，我们评估了使用淀粉样蛋白精制诊断组是否改善了表现。从每个参与者的淀粉样蛋白PET扫描（PiB或AV45）确定淀粉样蛋白状态。我们发现了<br>使用淀粉样蛋白精制诊断的分类总是比使用NINCDS-ADRDA诊断的相关任务更好或至少类似，即使训练集包含更少的个体。<br>与T1w MRI相比，FDG PET的分类在任务，特征和分类方法上的表现始终更好。一些研究支持我们的发现（Dukart等，2011,2011; Gray等，2013; Ota等，2015; Young等，2013），而其他研究没有发现性能上的差异（Hinrichs等。 ，2009; Zhang等，2011; Zhu等，2014）。鉴于我们的研究样本量较大且严格的评估设计，我们认为FDG PET与MRI相比具有优越的性能，这是一个很好的发现。这可能是由于在萎缩之前可以在疾病过程中更早地检测到代谢减退（Jack等，2010b）。</p>
<p>多种参数和选项用于AD机器学习研究中的预处理和特征提取。它们对分类性能的影响尚不清楚，并且构成了分类方法可比性的问题。我们评估了图谱选择，平滑程度，PET图像校正部分体积效应以及特征类型（区域或体素）的影响。我们没有发现这些不同组分中的每一种对性能的系统影响。一些研究发现地图集对分类性能的影响（Ota等，2015; 2014）。然而，该研究中的受试者数量很少。在（Chu等人，2012）中，与使用所有体素相比，使用少量ROI的组合时发现了3％的改善。在我们的研究中，使用了更多的受试者和严格的验证过程。<br>我们比较了三种广泛使用的分类方法：SVM，Logistic回归与L2正则化和随机森林。我们的主要发现是后者的表现不佳。这可能是由包含相对同质值的大脑成像数据的性质引起的，并且应该显示体素或大脑区域的依赖性。数据的这些特征可以解释为什么试图找到平滑特征组合的技术（例如使用L2正则化的特征）更适合于单一模态分类问题。另一方面，当组合来自不同模态的特征（例如图像，临床数据和认知评分）时，随机森林或其他集合方法可能是有用的，如（Moradi等，2015;Sørensen等，2018）所述。在比较几种标准分类算法（如SVM，LDA或Naive Bayes）的其他论文中（Aguilar等，2013; Cabral等，2015; Sabuncu等，2015），结果未显示方法之间的差异。</p>
<p>我们还评估了类别不平衡的影响，在我们的数据集中，这种影响范围从非常温和（AD比AD对ADNI多1.2倍）到中度（比pMCI多1.7倍的CN和比ADNI的pMCI多2倍的sMCI）到非常强（在AIBL中，CN比AD多6.1倍。在基于体素的特征的情况下，我们发现非常强的类不平衡（如AIBL中比例为6比1的情况）导致性能降低但是适度的类不平衡（ADNI中高达2比1）得到妥善处理。另一方面，阶级不平衡对区域特征没有影响。这突出表明，当存在非常强的类不平衡和使用非常高维度的特征时，使用平衡组进行训练可能是有益的。<br>我们评估了各种成分对分类性能的影响：模态（T1w MRI与PET），特征类型，寰椎的选择，PVC，平滑，分类器。其他研究评估了其他成分的影响：不同类型的解剖学特征，包括体积，皮质厚度和其他表面特征（Go？mez-Sancho等，2018; Schwarz等，2016; Westman等， 2013），特征选择技术（Tohka等，2016），颅内体积归一化（Voevodskaya等，2014; Westman等，2013）。此外（Tohka等，2016），将LASSO和弹性网与SVM进行了比较，发现前一种方法提供了更高的性能。使用我们的框架也可以评估这些不同组件的影响。在本文中，我们将框架的应用限制为由于以下原因而选择的一组组件。基于体素和区域特征都包括在内，因为它们被广泛使用。另一方面，由于其计算成本，不包括基于Freesurfer的皮质测量。 PVC是一种非常常见的PET数据预处理方法。平滑被广泛用于神经影像学社区中基于体素的分析，并且评估其影响似乎是有用的。然而，在这样的选择中总是存在一些任意性，用框架研究其他组件会很有趣。<br>在这项工作中，我们使用了预定义的功能（在区域或体素级别）。应该提到的另一类方法是直接从数据中学习特征的方法。基于补丁的方法旨在自动学习主体和训练集之间的非局部相似性（Coup？et al。，2015,2012）。此外，深度学习方法可以自动学习多个尺度的相关特征，并且最近在AD的自动分类中变得流行（B€ackstro€et al。，2018; Liu et al。，2018; Lu et al。，2018; Suk等，2017）。两种类型的方法都产生了有希望的结果（例如pMCI与sMCI的比例为73％至83％）。此外，各种工作已经提出使用不同类型的数据驱动特征选择（例如单变量统计检验，多变量方法）（Chu等人，2012; Tohka等人，2016; Vemuri等人，2008）和维数降低（例如主成分分析，流形学习）（Beheshti等，2015; Guerrero等，2014; Liu等，2015; Salvatore等，2015）。这些方法具有改善性能的潜力，但需要使用严格的交叉验证程序进行验证（Eskildsen等，2013; Maggipinto等，2017）。可以使用我们的框架评估所有这些方法的附加值。这超出了本文的范围，留待将来工作。</p>
<p>使用多个数据集对于评估在不同条件下获得的不同群体的表现是否稳健非常重要。第一个组件包括在不同的数据集上执行相同的实验。我们发现ADNI和AIBL数据集的分类结果相似，但OASIS的分类结果要低得多。 OASIS的较低性能可能是由于诊断标准较不严格（在<br>OASIS，CDR&gt; 0的所有参与者被认为是AD）。了解分类器在一个数据集上进行训练并在另一个数据集上进行测试时，它也是有价值的。对ADNI数据进行过培训的分类器很好地适用于AIBL和OASIS。有趣的是，对于OASIS而言，与在OASIS培训时相比，ADNI培训时的表现大大增加。这种改善可能源于几个因素：更大的训练集大小，更高的图像质量或更严格的诊断标准。当使用相同大小的子集时，基于体素的特征所获得的改进消失，这表明增加的训练集大小很重要，特别是在使用非常高的维度特征时。另一方面，对于区域特征，与OASIS子集的训练相比，ADNI子集的训练改善了性能，表明其他因素（图像质量，更严格的诊断标准）有助于改善。一般来说，我们可以说分类器能够在不同的数据集中进行推广，如Dukart等，2013; Sabuncu等，2015中所述，特别是如果它们是使用具有严格诊断标准的大型多中心数据集获得的，就像ADNI的情况一样。<br>不出所料，增加训练集的规模可以提高分类表现。在其他研究中也发现了取决于训练集大小的结果的改进（Abdulkadir等，2011; Chu等，2012; Franke等，2010）。可以注意到，当组合多个数据集时，性能也随着训练集大小而增加。但是，当将OASIS与ADNI和AIBL结合使用时，性能低于仅使用AIBL和ADNI时的性能。这与OASIS的性能系统性低于ADNI和AIBL的性能一致。同样，这可能是由于诊断标准在OASIS中不太严格。有趣的是，根据目前可用的样本数量，尚未达到结果停止改善的程度。分类器的性能取决于为训练提供的图像数量所施加的限制，这意味着需要更多数据才能找到分类器的最佳性能。这些结果强调了对更多可公开获得的数据集的需求，该领域目前的大部分研究都依赖于这些数据集。</p>
<h2 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6. Conclusions"></a>6. Conclusions</h2><p>我们的可重复分类实验框架旨在解决基于机器学习的AD分类领域当前面临的问题，例如结果的可比性和可重复性。 它在T1w MRI和FDG PET数据中的应用允许广泛评估成像模态，预处理选项，特征和算法对性能的影响。 这些结果提供了基准性能，可以与其他方法进行比较。 我们希望框架和实验结果对AD领域的研究人员都有用。</p>
<h3 id="Acknowledgments"><a href="#Acknowledgments" class="headerlink" title="Acknowledgments"></a>Acknowledgments</h3><p>导致这些结果的研究得到了“Investissements d’avenir”ANR-10-IAIHU-06（Agence Natio- nale de la Recherche-10-IA Agence Institut Hospitalo-Universitaire-6）计划的资助，ANR-11- IDEX-004（Agence Nationale de la Recherche-11- Initiative d’Excellence-004，项目LearnPETMR编号SU-16-R-EMR-16），来自欧盟H2020计划（项目EuroPOND，拨款号666992，项目HBP SGA1授权号720270），来自ICM大脑理论计划（项目DYNAMO），来自欧洲研究理事会（Durrleman博士项目LEASP，授权号678304），来自NSF / NIH / ANR联合计划“计算机协作研究”神经科学“（项目HIPLAY7，授权号ANR-16-NEUC- 0001-01）和来自巴塞罗那协会公共部门的”Contrat d’Interface Local“计划（致Col-liot博士）（AP-HP） 。注：根据REA拨款协议no，从欧洲联盟第七框架计划（FP7 / 2007-2013）的人民计划（Marie Curie Actions）获得资金。 PCOFUND-GA-2013-609102，通过由法国校园协调的PRESTIGE计划。<br>该项目的数据收集和共享由Alz-资助 Heimer’s疾病神经影像学倡议（ADNI）（美国国立卫生研究院资助U01 AG024904）和DOD ADNI（国防部奖项编号W81XWH-12-2-0012）。 ADNI由国家老龄化研究所，国家生物医学成像和生物工程研究所资助，并通过以下方面的慷慨捐助：AbbVie，Alzheimer’s Association;阿尔茨海默氏症的药物发现基金会; Araclon Biotech; BioClinica，Inc。;生物遗传; Bristol-Myers Squibb Company; CereSpir，Inc。; Cogstate; Eisai Inc。; Elan Pharmaceuticals，Inc。;礼来公司;欧蒙; F. Hoffmann-La Roche Ltd及其附属公司Genentech，Inc。; Fujirebio公司; GE Healthcare; IXICO有限公司; Janssen Alzheimer Immunotherapy Research＆Development，LLC。;强生药业研发有限责任公司; Lumosity; Lundbeck公司; Merck＆Co.，Inc。; Meso Scale Diagnostics，LLC。; NeuroRx研究; Neurotrack技术;诺华制药公司;辉瑞公司; Piramal成像;施维雅;武田制药公司;和过渡治疗学。加拿大卫生研究院正在提供资金支持加拿大的ADNI临床站点。国家卫生研究院基金会（<a href="http://www.fnih.org）为私营部门的捐助提供了便利。受助组织是北加州研究和教育研究所，该研究由南加州大学阿尔茨海默氏症治疗研究所协调。" target="_blank" rel="noopener">www.fnih.org）为私营部门的捐助提供了便利。受助组织是北加州研究和教育研究所，该研究由南加州大学阿尔茨海默氏症治疗研究所协调。</a> ADNI数据由南加州大学的神经成像实验室传播。<br>OASIS项目得到以下资助：P50 AG05681，P01 AG03991，R01 AG021910，P20 MH071616和U24 RR021382。</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最終更新：<time datetime="2019-07-29T04:00:23.997Z" itemprop="dateUpdated">2019-07-29 12:00:23</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="CaptainSE">
            CaptainSE
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/translation/">translation</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/&title=《Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/&title=《Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译》 — Go Further&source=【阅读时间】XXX min XXX words【阅读内容】……" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译》 — Go Further&url=http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/07/19/Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease-翻译/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease-翻译</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/07/18/Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer-Disease-翻译/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer-Disease-翻译</h4>
      </a>
    </div>
  
</nav>



    

















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢老板~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>このブログの内容物は<a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja">クリエイティブ・コモンズ 表示 - 非営利 - 継承 4.0 国際ライセンスの下に提供されています</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>CaptainSE &copy; 2015 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/&title=《Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/&title=《Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译》 — Go Further&source=【阅读时间】XXX min XXX words【阅读内容】……" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译》 — Go Further&url=http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAT4AAAE+CAAAAAAxUyPsAAAFUElEQVR42u3cQW4bMRAEQP//0w6QUwxbVPcMNwiQ0smQpSW3KEDDHtofH/Hj8/fj/Pz3n7+/6/PbIx89ef15Dudn8vnUD3z48OHDh+94S6+G2TAlVzs/k8zzvNj58uRj4cOHDx8+fDO+vCjJC4szX4LYLlVyhbzEiQo4fPjw4cOH7zG+TYlTT2v9/CzmwIcPHz58+P4dvjZGT36bb/KTUdqZJB8CfPjw4cOH7wm+pBxpN+GzBvlsDuciaXPNa70OfPjw4cOHr2wk/z8/X37gw4cPHz58cQQ/21Tnx8jyYmLfsN/f+5fr48OHDx8+fBOH4k+e9oF+uxh3x22j/2hu+PDhw4cP32hueQg+CxQS0HPIPmvAb5Zk2CzHhw8fPnz4Xrwy/3reNM6TK8+a6wlrHkbU1Qg+fPjw4cM32sjOYvSkXMgLjvMoM6Z8GfI5vKz48OHDhw8fvpIvuXR+Y0/cZAvavjcvdF62yfHhw4cPH76Ybxap5we/2qCh/Tkvm2aR/cv34sOHDx8+fDHfZmu9n+7mCrNl29zFKqbHhw8fPnz4jkPmh8zycqdtUSet63ORMYszinniw4cPHz58N+dZhAu3So08jNhwt+3w/COFDx8+fPjwJXybFnK+/W5X79YBtVkwUcQl+PDhw4cPX8yXEGyCg3YlZ4fkNqx56B997vDhw4cPH76ALx9gE8TnTPu2+mzB6tYCPnz48OHDt+BrG+TtJM6IbUC/ee+mzRAdUMOHDx8+fPiCDu+sOGi/+POlSm7+bkN9Fdbjw4cPHz58ozZ5/uakaJi1ATb0+7i/brTjw4cPHz58MV9eZLQQ+a3mMcS+1GgLtTfzwYcPHz58+Eq+W6H23WXYBATtss1Gx4cPHz58+DZ8bemQRwxtkZGXLHnQMIsbanp8+PDhw4cv4EvKi1txQNuG388wL63aK+DDhw8fPnwbvjqejgeYbdrzgqmNFVahwLlNjg8fPnz48KXnry78cXJbCiRlRA50Kw6YLQw+fPjw4cOX8+0JZtv+tn0+O16Wz62dw5e6Dx8+fPjw4Vvw5WXK+V17vlnEkBdJ+VIVbXJ8+PDhw4cvVmpj+k3j+emm++xjMQvu8eHDhw8fvpavLT6eKA7aEqRtM9wtmF5Wf/jw4cOHD99Rpp1uu6mebc5nBVDOt5nDD2E9Pnz48OHDN+KbRd71kGX5cjemT0YpRsSHDx8+fPgmPeV685/c3ix237ft83ZCvpxvfosPHz58+PDFfJvt/axRnW/ybx2Aaw/VFQEKPnz48OHDN8neV+3whPJu6/1u+3wTH+DDhw8fPny3+J4rF9rCaBYutJv/2UE3fPjw4cOHr+VLwvdNo7pte88qhLaImUUGUb2DDx8+fPjwBXz5rSbEOegmRNgQz+73h1fiw4cPHz58I77zl30SmucF0Gwz37bk89Hbxj8+fPjw4cO34cujgWS67Rd/frX2Om040s4ka4Lgw4cPHz58H2//+Vd7q3ePtc0KkX1yPmyc48OHDx8+fCO+trU8C/Hvtsw3R+vyQu1N6YYPHz58+PCt+doNeb5Rz3/bjpuHHfkBtWjO+PDhw4cP36QTvSoR8iImp58VSW3TPR+3jhLw4cOHDx++I98mmm/b1bMYPQ/Z29D/wqzw4cOHDx++Ed/sizxvsW+4Zw31TZxRvBIfPnz48OH7K3wJSlIA5Q3s2SGzNiyon8eHDx8+fPge48tj9Hwbn4fyyfXb4D4/VIcPHz58+PDd5cu3+reKjAQlX868hGrp37wSHz58+PDhK/lmQfkMZVMozEqTfJ7JUrUNBnz48OHDh++Pxy/VX9Ob7dHOtgAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            clearTimeout(titleTime);
        } else {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
