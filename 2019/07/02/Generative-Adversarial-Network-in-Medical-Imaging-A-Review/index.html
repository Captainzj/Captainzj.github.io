<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>Generative Adversarial Network in Medical Imaging A Review | Go Further | Stay Hungry, Stay Foolish</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#142421">
    
    
    <meta name="keywords" content="">
    <meta name="description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta property="og:type" content="article">
<meta property="og:title" content="Generative Adversarial Network in Medical Imaging A Review">
<meta property="og:url" content="http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/index.html">
<meta property="og:site_name" content="Go Further">
<meta property="og:description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure">
<meta property="og:image" content="http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure">
<meta property="og:image" content="http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure">
<meta property="og:image" content="http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure">
<meta property="og:image" content="http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure">
<meta property="og:updated_time" content="2019-07-02T12:08:24.231Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Generative Adversarial Network in Medical Imaging A Review">
<meta name="twitter:description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta name="twitter:image" content="http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure">
    
        <link rel="alternate" type="application/atom+xml" title="Go Further" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">CaptainSE</h5>
          <a href="mailto:841145636@qq.com" title="841145636@qq.com" class="mail">841145636@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/Captainzj" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Generative Adversarial Network in Medical Imaging A Review</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="検索">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Generative Adversarial Network in Medical Imaging A Review</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-07-02T06:18:42.000Z" itemprop="datePublished" class="page-time">
  2019-07-02
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Paper/">Paper</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#翻译"><span class="post-toc-number">1.</span> <span class="post-toc-text">翻译</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#摘要"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">摘要</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#1-介绍"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">1. 介绍</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#2-背景"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">2. 背景</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-1-Vanilla-GAN"><span class="post-toc-number">1.3.1.</span> <span class="post-toc-text">2.1. Vanilla GAN</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-2-Variants-of-GANs"><span class="post-toc-number">1.3.2.</span> <span class="post-toc-text">2.2. Variants of GANs</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#2-2-1-Varying-objective-of-D"><span class="post-toc-number">1.3.2.1.</span> <span class="post-toc-text">2.2.1. Varying objective of D*</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#2-2-2-Varying-objective-of-G"><span class="post-toc-number">1.3.2.2.</span> <span class="post-toc-text">2.2.2. Varying objective of G</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#2-2-3-Varying-architecture"><span class="post-toc-number">1.3.2.3.</span> <span class="post-toc-text">2.2.3. Varying architecture</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#3-在医学成像中的应用"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">3. 在医学成像中的应用</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-1-Reconstruction"><span class="post-toc-number">1.4.1.</span> <span class="post-toc-text">3.1. Reconstruction</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-2-Medical-Image-Synthesis"><span class="post-toc-number">1.4.2.</span> <span class="post-toc-text">3.2. Medical Image Synthesis</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#3-2-1-Unconditional-Synthesis"><span class="post-toc-number">1.4.2.1.</span> <span class="post-toc-text">3.2.1. Unconditional Synthesis</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#3-2-2-Cross-modality-synthesis"><span class="post-toc-number">1.4.2.2.</span> <span class="post-toc-text">3.2.2. Cross modality synthesis</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#3-2-3-Other-conditional-synthesis"><span class="post-toc-number">1.4.2.3.</span> <span class="post-toc-text">3.2.3. Other conditional synthesis</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-3-Segmentation"><span class="post-toc-number">1.4.3.</span> <span class="post-toc-text">3.3. Segmentation</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-4-Classification"><span class="post-toc-number">1.4.4.</span> <span class="post-toc-text">3.4. Classification</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-5-Detection"><span class="post-toc-number">1.4.5.</span> <span class="post-toc-text">3.5. Detection</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-6-Registration"><span class="post-toc-number">1.4.6.</span> <span class="post-toc-text">3.6. Registration</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-7-Other-works"><span class="post-toc-number">1.4.7.</span> <span class="post-toc-text">3.7. Other works</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#4-讨论"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">4. 讨论</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#4-1-Future-challenges"><span class="post-toc-number">1.5.1.</span> <span class="post-toc-text">4.1. Future challenges</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#4-2-Interesting-future-applications"><span class="post-toc-number">1.5.2.</span> <span class="post-toc-text">4.2. Interesting future applications</span></a></li></ol></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-Generative-Adversarial-Network-in-Medical-Imaging-A-Review"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Generative Adversarial Network in Medical Imaging A Review</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-07-02 14:18:42" datetime="2019-07-02T06:18:42.000Z"  itemprop="datePublished">2019-07-02</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Paper/">Paper</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p>
<a id="more"></a>
<p>论文地址：<a href="https://arxiv.org/abs/1809.07294" target="_blank" rel="noopener">Generative Adversarial Network in Medical Imaging: A Review</a></p>
<p>github Reference link：<a href="https://github.com/xinario/awesome-gan-for-medical-imaging" target="_blank" rel="noopener">Awesome GAN for Medical Imaging</a></p>
<h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><em>摘要</em></h3><p>生成对抗网络由于其数据生成能力而在没有明确建模概率密度函数的情况下在计算机视觉社区中获得了很多关注。 鉴别器带来的对抗性损失提供了一种巧妙的方法，可以将未标记的样本纳入训练并实现更高的顺序一致性。 事实证明，这在许多情况下是有用的，例如域适应，数据增强和图像到图像转换。 这些属性吸引了医学成像领域的研究人员，我们已经看到许多传统和新颖应用的快速采用，如图像重建，分割，检测，分类和跨模态合成。 根据我们的观察，这一趋势将继续下去，因此我们利用对抗性训练计划对医学成像的最新进展进行了回顾，希望能够使对该技术感兴趣的研究人员受益。</p>
<p>关键词：Deeplearning，Generative adversarial network，Generative model，Medical imaging，Review</p>
<h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a><em>1. 介绍</em></h3><p>随着2012年开始的计算机视觉深度学习的复兴（Krizhevsky等，2012），医学成像中深度学习方法的采用大幅增加。据估计，2016年和2017年在主要医学影像相关会议场所和期刊上发表了400多篇论文（Litjens等，2017）。在医学成像领域广泛采用深度学习是因为它具有补充图像解释和增强图像表示和分类的潜力。在本文中，我们将重点放在深度学习领域最有趣的近期突破之一 - 生成对抗网络（GAN） - 以及它们在医学成像领域的潜在应用。<br>GAN是一种特殊类型的神经网络模型，其中两个网络同时被训练，一个侧重于图像生成，另一个侧重于区分。对抗性训练方案因其在抵制领域转移方面的有用性以及产生新图像样本的有效性而在学术界和工业界引起了关注。该模型在许多图像生成任务中实现了最先进的性能，包括文本到图像合成（Xu et al.，2017），超分辨率（Ledig等，2017）和图像 - 图像转换（Zhu et al.，2017a）。<br>与源于20世纪80年代的深度学习不同（Fukushima和Miyake，1982），对抗性的概念相对来说是非常重要的进步（Good-fellow et al.，2014）。本文概述了GAN，描述了它们在医学成像中的有前途的应用，并确定了一些需要解决的挑战，以使它们能够成功应用于其他医学成像相关任务。<br>为了全面概述医学影像中GAN的所有相关工作，我们搜索了包括PubMed，arXiv在内的数据库，国际医学图像计算和计算机辅助干预会议（MICCAI），SPIE医学影像，IEEE国际研讨会生物医学成像（ISBI）和国际深度学习医学影像学会议（MIDL）。我们还合并了上述搜索过程中未识别的交叉引用作品。由于每月都有研究出版物出现，而且没有失去一般性，我们将搜索的截止时间设定为2018年7月30日。仅报告初步结果的arXiv的工作被排除在本次审查之外。基于任务，成像模态和年份的这些论文的描述性统计数据可以在图1中找到。<br>在本文的其余结构如下。我们首先简要介绍第2节中GAN的原理及其一些结构变体。然后在第3节中使用GAN对医学图像分析任务进行全面审查，包括但不限于放射学领域，组织病理学和皮肤病学。我们根据规范任务对所有作品进行分类：重建，图像合成，分割，分类，检测，注册等。第4节总结了该评论，并讨论了前瞻性应用和识别性挑战。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 1" title="1.png">
                </div>
                <div class="image-caption">1.png</div>
            </figure>
<p>图1：（a）根据规范任务对GAN相关论文进行分类。 （b）根据成像模式对GAN相关论文进行分类。 （c）2014年发布的GAN相关论文数量。请注意，一些工作执行了各种任务，并对具有不同模态的数据集进行了评估。 我们在绘制这些图时多次计算这些作品。 基于源域计算与跨域图像传输相关的工作。 图（a）和（b）中的统计数据基于2018年7月30日或之前公布的论文。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 2" title="2.png">
                </div>
                <div class="image-caption">2.png</div>
            </figure>
<p>图2：用于在CT图像上合成肺结节的vanilla GAN的示意图。 上图显示了网络配置。 下面的部分显示了生成器G和鉴别器D的输入，输出和内部特征表示.G将样本$z$从$p(z)$变换为生成的结节$x_g$。 D是二元分类器，其分别区分由$x_g$和$x_r$形成的肺结节的生成和真实图像。</p>
<h3 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a><em>2. 背景</em></h3><h4 id="2-1-Vanilla-GAN"><a href="#2-1-Vanilla-GAN" class="headerlink" title="2.1. Vanilla GAN"></a><em>2.1. Vanilla GAN</em></h4><p>香草GAN（Goodfellow等，2014）是一种生成模型，设计用于直接从所需的数据分布中抽取样本，而无需明确地模拟潜在的概率密度函数。它由两个神经网络组成：发生器G和鉴别器D.G，z的输入是从先前分布p（z）中采样的纯随机噪声，通常选择为高斯分布或均匀分布。简单。预计G，xg的输出与从真实数据分布pr（x）中提取的实际样本xr具有视觉相似性。我们将由θg参数化的G学习的非线性映射函数表示为xg = G（z;θg）。 D的输入是实际或生成的样本。 D，y1的输出是单个值，表示输入是真实或假冒样本的概率。由θd参数化的D学习的映射表示为y1 = D（x;θd）。生成的样本形成分布pg（x），其在成功训练后需要是pr（x）的近似值。图2的顶部显示了香草GAN配置的图示。在该示例中，G生成描绘肺结节的2D CT切片。<br>D的目标是区分这两组图像，而生成器G被训练以尽可能地混淆辨别器D.直观地说，G可以被视为试图生产一些优质假冒伪劣材料的伪造者，D可以被视为试图检测伪造物品的警察。在另一种观点中，我们可以将G视为从D接收奖励信号，这取决于生成的数据是否准确。梯度信息从D传播回G，因此G调整其参数以产生可以欺骗D的输出图像.D和G的训练目标可以用数学表达为：<br>$$ 
L_{D}^{GAN} = max_{D}E_{{x_r}\sim {P_r(x)}}[logD(x_r)+E_{x_g\sim p_g(x)}[log(1-D(x_g))]],\\
L_{D}^{GAN} = min_GE_{x_g\sim p_g(x)}[log(1-D(x_g))].
$$ <br>可以看出，D只是具有最大对数似然目标的二元分类器。 如果鉴别器D在下一个发生器G更新之前被训练为最优，则最小化LGAN被证明等同于最小化pr（x）和pg（x）之间的Jensen-Shannon（JS）偏差（Goodfellow等人，2014））。 训练后的预期结果是xg形成的样本应该接近实际数据分布pr（x）。</p>
<h4 id="2-2-Variants-of-GANs"><a href="#2-2-Variants-of-GANs" class="headerlink" title="2.2. Variants of GANs"></a><em>2.2. Variants of GANs</em></h4><p>上述GAN训练目标被认为是鞍点优化问题（Yadav等，2018），训练通常通过基于梯度的方法完成。 G和D从头开始交替训练，以便它们可以一起进化。但是，G和D训练与JS分歧之间无法保证平衡。因此，一个网络可能不可避免地比另一个网络更强大，在大多数情况下是D.当D变得太强而不是G时，生成的样本变得太容易与实际的分离，从而达到D的梯度逼近零的阶段，没有为G的进一步训练提供指导。由于难以产生有意义的高频细节，因此在生成高分辨率图像时更频繁地发生这种情况。<br>在训练GAN中通常面临的另一个问题是模式崩溃，正如名称所示，这是由G学习的分布pg（x）关注数据分布pr（x）的一些有限模式的情况。因此，它不是产生不同的图像，而是产生一组有限的样本。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 3" title="3.png">
                </div>
                <div class="image-caption">3.png</div>
            </figure>
<p>图3：GAN变体的示意图。 c表示条件向量。 在CGAN和ACGAN中，c是对类标签进行编码的离散分类代码（例如，一个热向量），在InfoGAN中，它也可以是对属性进行编码的连续代码。 xg通常是指生成的图像，但也可以是SGAN中的内部表示。</p>
<h5 id="2-2-1-Varying-objective-of-D"><a href="#2-2-1-Varying-objective-of-D" class="headerlink" title="2.2.1. Varying objective of D*"></a>2.2.1. Varying objective of D*</h5><p>为了稳定训练并避免模式崩溃，已经提出了D的不同损失，例如f-发散（f-GAN）（Nowozin等，2016），最小二乘（LS-GAN）（Mao et al。，2016），铰链损失（Miyato等，2018）和Wasserstein距离（WGAN，WGAN-GP）（Arjovsky等，2017; Gulrajani等，2017）。其中，Wasserstein距离可以说是最受欢迎的指标。作为真/假歧视方案的替代方案，Springenberg（2015）提出了一个基于熵的目标，其中鼓励实际数据进行自信的类预测（CatGAN，图3b）。在EBGAN（Zhao等人，2016）和BEGAN（Berthelot等人，2017）（图3c）中，用于鉴别器的常用编码器架构被替换为自动编码器架构。然后，D的目标变为匹配自动编码器丢失分布而不是数据分布。<br>GAN本身缺乏推断机制，根据定义，推断机制可以预测可能编码输入的潜在向量。因此，在ALI（Dumoulin等人，2016）和BiGAN（Donahue等人，2016）（图3d）中，结合了单独的编码器网络。然后D的目标是分离联合样本（xg，zg）和（xr，zr）。在InfoGAN中（图3e），鉴别器输出潜在向量，该潜向向量编码所生成图像的部分语义特征。鉴别器使所生成的图像与所生成的图像所依赖的潜在属性向量之间的互信息最大化。成功培训后，InfoGAN可以探索固有的数据属性，并根据这些属性执行条件数据生成。已经证明类标签的使用可以进一步提高生成图像的质量，并且通过强制D提供类概率并使用交叉熵损失进行优化（例如在ACGAN中使用）（Odena等， 2016）（图3f）。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 4" title="4.png">
                </div>
                <div class="image-caption">4.png</div>
            </figure>
<p>图4：用于图像到图像转换的cGAN框架。 pix2pix需要对齐的训练数据，而这种约束在CycleGAN中放宽，但通常会受到性能损失的影响。 请注意，在（a）中，我们选择重建损失作为目标一致性的示例。 这种监督与任务有关，可以采取许多其他形式。 （c）它由两个VAEGAN组成，在VAE部分具有共享的潜在向量。</p>
<h5 id="2-2-2-Varying-objective-of-G"><a href="#2-2-2-Varying-objective-of-G" class="headerlink" title="2.2.2. Varying objective of G"></a><em>2.2.2. Varying objective of G</em></h5><p>在香草GAN中，G将噪声z转换为样本xg = G（z）。这通常通过使用解码器网络逐步增加输出的空间大小来实现，直到达到所需的分辨率，如图2所示.Larsen等人。 （2015）提出了变分自动编码器网络（VAE）作为G的基础架构（VAEGAN，图3g），其中它可以使用逐像素重建损失来强制VAE的解码器部分生成与真实图像匹配的结构。<br>GAN的原始设置对其可以生成的数据模式没有任何限制。然而，如果在生成期间提供辅助信息，则可以驱动GAN以输出具有期望属性的图像。在这种情况下，GAN通常被称为条件GAN（cGAN），并且生成过程可以表示为xg = G（z，c）。<br>最常见的条件输入之一c是图像。 pix2pix是第一个基于通用GAN的图像到图像转换框架，由Isola等人提出。 （2016）（图4 a）。此外，任务相关的监督被引入发电机。例如，用于图像恢复的重建损失和用于分割的骰子损失（Milletari等，2016）。这种形式的监督需要一致的训练对。朱等人。 （2017A）; Kim等人。 （2017）通过从头到脚拼接两个发生器来放松这种约束，这样图像可以在两组不成对的样本之间进行转换（图4b）。为简单起见，我们在本文的其余部分选择了CycleGAN来表示这一想法。另一个名为UNIT的模型（图4c）也可以通过将两个VAEGAN组合在一起来执行不成对的图像到图像变换，每个模型对一种模态负责但共享相同的潜在空间（Liu et al。，2017a）。这些图像到图像翻译框架由于其普遍适用性而在医学成像领域中非常流行。<br>除了图像，条件输入可以是类标签（CGAN，图3h）（Mirza和Osindero，2014），文本描述（Zhang et al。，2017a），对象位置（Reed等，2016a） ，b），周围的图像背景（Pathak等，2016），或草图（Sangkloy等，2016）。请注意，上一节中提到的ACGAN也有一个类条件生成器。</p>
<h5 id="2-2-3-Varying-architecture"><a href="#2-2-3-Varying-architecture" class="headerlink" title="2.2.3. Varying architecture"></a><em>2.2.3. Varying architecture</em></h5><p>完全连接的层用作香草GAN中的构建块，但后来被DCGAN中的完全卷积下采样/上采样层取代（Radford等，2015）。 DCGAN表现出更好的训练稳定性，因此迅速填补了文献。如图2所示，DCGAN架构中的发生器通过连续的上采样操作对随机输入噪声矢量进行处理，最终生成一个图像。其重要的成分中的两个是BatchNorm（约费和Szegedy，2015）用于调节EX-牙牙特征尺度，和LeakyRelu（马斯等人，2013），用于预排放死梯度。最近，Miyato等人。 （2018）提出了光谱归一化层，其在鉴别器中对权重进行归一化以调节特征响应值的规模。与训练稳定性提高，一些作品也掺入剩余的连接到这两个属，Tor和鉴别器和与深得多的NET-作品试验（Gulrajani等人，2017年;宫户等人，2018）。 Miyato和Koyama（2018）的工作提出了一种基于投影的方法来结合条件信息而不是直接连接，并发现它有利于提高生成图像的质量。<br>从噪声矢量中直接生成高分辨率图像很难，因此一些工作已经提出以渐进方式处理它。在LAPGAN（图3i）中，Denton等人。 （2015）提出了一堆GAN，每个GAN将更高频率的细节添加到生成的图像中。在SGAN，甘斯的CAS-杜松也用于但每个GAN产生越来越低的级表示（Huang等人，2017），其与从区别地训练模型中提取的分层表示进行比较。卡拉斯等人。 （2017）采用了另一种方式，通过向它们添加新层来逐步增长发生器和鉴别器，而不是在前一个GAN之上堆叠另一个GAN（PGGAN）。在条件设定中也探索了这种进步的想法（Wang等，2017b）。<br>最具代表性的GAN的示意图如图3所示。它们是GAN，CatGAN，EBGAN / BEGAN，ALI / BiGAN，InfoGAN，ACGAN，VAEGAN，CGAN，LAPGAN，SGAN。三个流行的图像到图像转换cGAN（pix2pix，CycleGAN和UNIT）如图4所示。为了对这些不同的GAN变体进行更深入的回顾和实证评估，我们引用了读者（Huang et al。 ，2018; Creswell等，2018; Kurach等，2018）。</p>
<h3 id="3-在医学成像中的应用"><a href="#3-在医学成像中的应用" class="headerlink" title="3. 在医学成像中的应用"></a><em>3. 在医学成像中的应用</em></h3><p>GAN通常有两种用于医学成像的方法。 第一个侧重于生成方面，它可以帮助探索和发现训练数据的基础结构和学习生成新图像。 这个属性使GAN在应对数据稀缺性和患者隐私方面非常有前途。 第二个侧重于辨别方面，其中鉴别器D可以被视为正常图像的学习先验，使得当呈现异常图像时它可以用作正则化器或检测器。 图5提供了GAN相关应用的示例，示例（a），（b），（c），（d），（e），（f）侧重于生成方面和示例（g）利用 歧视方面。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 5" title="5.png">
                </div>
                <div class="image-caption">5.png</div>
            </figure>
<p>图5：使用GAN的示例应用程序。数字直接从相应的纸张中裁剪。 （a）左侧显示噪声污染的低剂量CT，右侧显示去噪的CT，其很好地保留了肝脏中的低对比度区域（Yi和Babyn，2018）。 （b）左侧显示MR图像，右侧显示合成的相应CT。在所生成的CT图像中很好地描绘了骨结构（Wolterink等，2017a）。 （c）生成的视网膜眼底图像具有左侧血管图中描绘的精确血管结构（Costa等，2017b）。 （d）随机噪声（恶性和良性混合物）随机产生的皮肤病变（Yi et al。，2018）。 （e）成人胸部X射线的器官（肺和心脏）分割实例。肺和心脏的形状受到对抗性损失的调节（Dai等，2017b）。 （f）第三列显示了SWI序列中的域适应脑损伤分割结果，未经相应的手动注释训练（Kamnitsas等，2017）。 （g）视网膜光学相干断层扫描图像的异常检测（Schlegl等，2017）。</p>
<h4 id="3-1-Reconstruction"><a href="#3-1-Reconstruction" class="headerlink" title="3.1. Reconstruction"></a><em>3.1. Reconstruction</em></h4><p>由于临床设置的限制，例如辐射剂量和患者舒适度，所获取的医学图像的诊断质量可能受到噪声和伪影的限制。在过去的十年中，我们已经看到了重建方法的范式转变，从分析到迭代，现在转向基于机器学习的方法。这些基于数据驱动学习的方法要么学会将原始感官输入直接传输到输出图像，要么作为后处理步骤来减少图像噪声和消除伪像。本节中回顾的大多数方法都是直接从计算机视觉文献中借鉴的，这些文献将后处理作为图像到图像的翻译问题，其中cGAN的条件输入以某些形式受到损害，例如低空间分辨率，噪声污染，欠采样或混叠。一个例外是MR图像，其中傅立叶变换用于将原始K空间数据合并到重建中。<br>基本pix2pix框架已用于低剂量CT去噪（Wolterink等，2017b），MR重建（Chen等，2018b; Kim等，2018; Dar等，2018b; Shitrit和Raviv，2017 ）和PET去噪（Wang等，2018b）。预先训练的VGG网（Simonyan和Zisserman，2014）进一步纳入优化框架，以确保感知相似性（Yang等，2017b; Yu等，2017; Yang等，2018a; Armanious et al。，2018; Mahapatra，2017）。 Yi和Babyn（2018）介绍了一种预训练锐度检测网络，明确约束去噪CT的清晰度，特别是低对比度区域。 Mahapatra（2017）计算了一个局部显着图，以突出视网膜眼底成像超分辨率过程中的血管。 Liao等人研究了类似的想法。 （2018）稀疏视图CT重建。他们计算焦点图以调整重建输出，以确保网络集中在重要区域。除了确保图像域数据保真度之外，当在MR重建中可获得原始K空间数据时，也施加频域数据保真度（Quan等，2018; Mardani等，2017; Yang等，2018a）。<br>其他类型的损失已被用于突出重建中的局部图像结构，例如基于其感知相关性重新衡量每个像素的重要性的显着性损失（Mahapatra，2017）以及PET去噪中的样式内容损失（ Armanious等，2018）。在运动器官的图像重建中，很难获得成对的训练样本。因此，Rav`ı等人。 （2018）提出了一种基于物理采集的损失来调节生成的用于内镜超分辨率的图像结构和Kang等人。 （2018）提出在心脏CT的去噪中使用CycleGAN以及身份损失。 Wolterink等人。 （2017b）发现，在低剂量CT去噪中，当从pix2pix帧中去除图像域保真度损失时，仍然可以获得有意义的结果，但是可以改变局部图像结构。表1总结了与医学图像重建相关的论文。</p>
<p>可以注意到，对于所有重建任务，基础方法几乎相同。 MR是特殊情况，因为它具有明确定义的前向和后向操作，即傅立叶变换，因此可以结合原始K空间数据。可以应用相同的方法将正弦图数据合并到CT重建过程中，但是我们还没有看到任何使用这个想法的研究，可能是因为正弦图数据很难获得。使用的数据越多，原始K空间或来自其他序列的图像，重建结果越好。一般而言，使用对抗性损失产生的视觉吸引力比单独使用像素化重建损失更具吸引力。但是使用对抗性损失来匹配生成的和实际的数据分布可能会使模型隐藏不可见的结构。如果配对样本可用，像素重建丢失有助于解决这个问题，并且如果模型是在所有健康图像上训练但是用于重建具有病理的图像，则由于域不匹配，幻觉问题仍然存在。科恩等人。 （2018）进行了广泛的实验来研究这个问题，并建议重建图像不应该用于放射科医师的直接诊断，除非模型已经过适当的验证。</p>
<p>然而，即使数据集经过精心策划以匹配培训和测试分布，还有其他问题可以进一步提升性能。 我们已经看到pix2pix框架引入了各种不同的损耗，如表2所示，以提高本地结构的重建保真度。 然而，除了依赖人类观察者或下游图像分析任务之外，没有可靠的方法来比较它们的有效性。 人类观察者目前缺乏基于GAN的重建方法的大规模统计分析。 此外，用于图像重建的公共数据集不适用于进一步的医学图像分析，这在上游重建和下游分析任务之间留下了空白。 应创建新的参考标准数据集，以便更好地比较这些基于GAN的方法。</p>
<h4 id="3-2-Medical-Image-Synthesis"><a href="#3-2-Medical-Image-Synthesis" class="headerlink" title="3.2. Medical Image Synthesis"></a><em>3.2. Medical Image Synthesis</em></h4><p>根据机构协议，如果诊断图像旨在用于出版物或发布到公共领域，则可能需要患者同意（Clinical Pracice Committee，2000）。医学图像合成是GAN最重要的用途之一，因为与诊断医学图像数据相关的隐私问题以及每种病理学的阳性病例数量通常不足。缺乏医学图像的专家对于采用监督培训方法提出了另一个挑战。尽管多个医疗保健机构正在进行协作，目的是建立一个大型的开放式访问数据集，例如：生物银行，国家生物医学影像档案馆（NBIA），癌症影像档案馆（TCIA）和北美放射学家协会（RSNA），这个问题仍然存在并限制了研究人员可能获得的图像数量。<br>增加训练样本的传统方法包括缩放，旋转，翻转，平移和弹性变形（Simard等，2003）。然而，这些变换不能解释由不同成像方案或序列引起的变化，更不用说特定病理学的大小，形状，位置和外观的变化。 GAN提供了更通用的解决方案，并且已经在许多工作中用于增强具有有希望的结果的训练图像。</p>
<h5 id="3-2-1-Unconditional-Synthesis"><a href="#3-2-1-Unconditional-Synthesis" class="headerlink" title="3.2.1. Unconditional Synthesis"></a><em>3.2.1. Unconditional Synthesis</em></h5><p>无条件合成是指从随机噪声生成图像而没有任何其他条件信息。医学成像领域通常采用的技术包括DCGAN，WGAN和PGGAN，因为它们具有良好的训练稳定性。前两种方法可以处理高达256×256的图像分辨率，但如果需要更高分辨率的图像，PGGAN中提出的渐进技术是一种选择。只要图像之间的图像变化不太大，例如肺结节和肝脏病变，就可以通过直接使用作者发布的代码库生成逼真的图像。为了使生成的图像对下游任务有用，大多数研究为每个单独的班级训练了一个单独的发生器;例如，Frid-Adar等人。 （2018）使用三种DCGAN产生三类肝脏病变（囊肿，转移瘤和血管瘤）的合成样本;发现生成的样本对于病变分类任务是有益的，当与实际训练数据相结合时，其灵敏度和特异性均得到提高。 Bermudez等人。 （2018）声称神经放射学家发现生成的MR图像质量与真实图像质量相当，但解剖学准确性存在差异。表4总结了与无条件医学图像合成相关的论文。</p>
<h5 id="3-2-2-Cross-modality-synthesis"><a href="#3-2-2-Cross-modality-synthesis" class="headerlink" title="3.2.2. Cross modality synthesis"></a><em>3.2.2. Cross modality synthesis</em></h5><p>由于多种原因，交叉模态合成（例如基于MR图像生成类似CT的图像）被认为是有用的，其中之一是减少额外的采集时间和成本。另一个原因是生成新的训练样本，其外观受到可用模态中描绘的解剖结构的约束。本节中回顾的大多数方法与3.1节中的方法有许多相似之处。基于pix2pix的框架用于可以共同注册不同图像模态数据以确保数据保真度的情况。基于CycleGAN的框架用于处理注册具有挑战性的更一般情况，例如在汽车应用中。在Wolterink等人的一项研究中。 （2017a）从MR图像合成脑CT图像，作者发现使用不成对图像的训练甚至比使用对齐图像更好。这很可能是因为刚性配准不能很好地处理咽喉，口腔，椎骨和鼻腔的局部对齐。 Hiasa等。 （2018）在训练中进一步引入梯度一致性损失以提高边界处的准确性。张等人。 （2018c）发现在交叉模态合成中仅使用循环损失不足以减轻变换中的几何失真。因此，他们采用了从两个分段器（分段网络）获得的形状一致性损失。每个分段或将相应的图像模态分割成语义标签，并在翻译期间提供对解剖结构的隐式形状约束。为了使整个系统端到端可训练，需要从两种模态中获得训练图像的语义标签。张等人。 （2018b）和陈等人。 （2018a）提出在仅使用一种模态的标签的循环转移中也使用分段器。因此，在图像传输网络的训练期间离线训练分段器并固定。如第2节所述，UNIT和CycleGAN是两个同等有效的非配对交叉模态综合框架。结果发现，这两个框架几乎同样适用于T1和T2加权MR图像之间的转换（Welander等，2018）。与交叉模态医学图像合成相关的论文总结在表5中。</p>
<h5 id="3-2-3-Other-conditional-synthesis"><a href="#3-2-3-Other-conditional-synthesis" class="headerlink" title="3.2.3. Other conditional synthesis"></a><em>3.2.3. Other conditional synthesis</em></h5><p>医学图像可以通过对分割图，文本，位置或合成图像等的约束来生成。这对于在非常见条件下合成图像非常有用，例如肺结节接触肺部边界（Jin等，2018b）。 此外，条件分割图也可以从GAN（Guibas等，2017）或从预训练的分割网络（Costa等，2017a）生成，通过使该生成为两阶段过程。 Mok和Chung（2018）使用cGAN来增强用于脑肿瘤分割的训练图像。 生成器以分割图为条件，并以粗略到精细的方式生成脑MR图像。 为了确保在生成的图像中用清晰的边界很好地描绘肿瘤，它们进一步迫使发生器在生成过程中输出肿瘤边界。 表6总结了综合工作的完整清单。</p>
<h4 id="3-3-Segmentation"><a href="#3-3-Segmentation" class="headerlink" title="3.3. Segmentation"></a><em>3.3. Segmentation</em></h4><p>通常，研究人员使用像素方式或体素方式的损失（例如交叉熵）进行分割。尽管U-net（Ronneberger等，2015）用于结合低级和高级特征，但无法保证最终分割图中的空间一致性。传统上，通过结合空间相关性，通常采用条件随机场（CRF）和图切割方法进行分割细化。它们的局限性在于它们只考虑成对电位，这可能会导致低对比度区域出现严重的边界泄漏。另一方面，鉴别器引入的对抗性损失可以考虑高阶电位（Yang et al。，2017a）。在这种情况下，鉴别器可以被视为形状调节器。这种调节效应也可以应用于分离器的内部特征，以实现域（不同扫描仪，成像协议，模态）不变性（Kamnitsas等，2017; Dou等，2018）。<br>薛等人。 （2018）在判别器中使用了多尺度L1损耗，其中来自不同深度的特征被比较。这证明在分段图上实施多尺度空间约束是有效的，并且该系统在BRATS 13和15挑战中实现了最先进的性能。张等人。 （2017c）建议在分割流水线中使用带注释和未注释的图像。注释图像的使用方式与（Xue et al。，2018; Son et al。，2017）相同，其中应用了元素损失和经济损失。另一方面，未注释的图像仅用于计算分割图以混淆鉴别器。 Li和Shen（2018）将pix2pix与ACGAN结合用于分割不同细胞类型的荧光显微镜图像。他们发现辅助分类器分支的引入为判别器和分段器提供了调节。<br>与上述分段工作不同，其中使用经验训练来确保最终分割图上的更高阶结构一致性，（Zhu等，2017b）中的对抗训练方案强制网络不变性对训练样本的小扰动。为了减少小数据集的过度拟合。表8总结了与医学图像分割相关的论文。</p>
<h4 id="3-4-Classification"><a href="#3-4-Classification" class="headerlink" title="3.4. Classification"></a><em>3.4. Classification</em></h4><p>胡等人。 （2017a）在组织病理学图像中使用组合的WGAN和InfoGAN进行无监督的细胞水平特征表示学习，而Yi等人。 （2018）将WGAN和CatGAN组合用于皮肤镜检查图像的无监督和半监督特征表示学习。两个作品都从鉴别器中提取特征，并在顶部构建分类器。 Madani等人。 （2018b）和Lahiri等。 （2017）分别采用DCGAN的半监督训练方案进行胸部异常分类和视网膜血管分类。他们发现，半监督的DCGAN可以实现与传统监督的CNN相当的性能，其中标记数据的数量级更少。此外，Madani等人。 （2018b）还表明，通过简单地向鉴别器提供未标记的测试域图像，平面损失可以减少域过度拟合。<br>大多数使用GAN生成新训练样本的其他作品已在第3.2.1节中提及。这些研究应用了两个阶段的过程，第一阶段学习增强图像，第二阶段学习通过采用传统的分类网络进行分类。这两个阶段是脱节训练的，两者之间没有任何沟通。优点是，如果提出更先进的无条件综合架构，这两个组件可以轻松更换，而下端则必须分别对每个类进行生成（N类N个模型），这不是内存并且计算效率高。能够执行多个类别的条件合成的单个模型是积极的研究方向（Brock等，2018）。令人惊讶的是，Frid-Adar等人。 （2018）发现，对于每个病变类别使用单独的GAN（DCGAN）导致病变分类的性能比对所有类别使用统一的GAN（ACGAN）更好。潜在的原因还有待探索。此外，（Fin- layson等，2018）认为，从GAN产生的图像可以作为中等数据体系中的有效增强，但在高或低数据体系中可能没有帮助。</p>
<h4 id="3-5-Detection"><a href="#3-5-Detection" class="headerlink" title="3.5. Detection"></a><em>3.5. Detection</em></h4><p>Schlegl等人。 （2017）使用GAN来学习一系列正常的解剖变异性，并提出了一种新的异常评分方案，该方案基于测试图像潜在代码对学习流形的适应性。学习过程以无人监督的方式进行，并通过最佳的异常检测在光学相干断层扫描（OCT）图像上的表现来证明其有效性。 Alex等人。 （2017）使用GAN对MR图像进行脑损伤检测。发生器用于模拟正常贴片的分布，并且训练的鉴别器用于计算以测试图像中的每个像素为中心的贴片的后验概率。 Chen和Konukoglu（2018）使用对抗性自动编码器来学习健康脑MR图像的数据分布。然后通过探索学习的潜在空间将病变图像映射到没有病变的图像，并且可以通过计算这两个图像的残差来突出病变。我们可以看到所有检测研究都针对难以枚举的异常。<br>在图像重建部分中，已经观察到如果目标分布是由没有病理学的医学图像形成的，则由于分布匹配效应，可以在基于CycleGAN的非配对图像转移中去除图像内的病变。然而，在这里可以看出，如果目标和源域具有相同的成像模态，仅在正常和异常组织方面不同，则这种不良效应实际上可以用于异常检测。</p>
<h4 id="3-6-Registration"><a href="#3-6-Registration" class="headerlink" title="3.6. Registration"></a><em>3.6. Registration</em></h4><p>cGAN还可用于多模态或单模式图像配准。在这种情况下，生成器将生成变换参数，例如， 6用于3D刚性变换，12用于3D仿射变换，或变换后的图像。然后，鉴别器从未对准的图像对中区分对齐的图像对。空间转换网络（Jaderberg等，2015）通常插在这两个网络之间，以实现端到端的培训。严等人。 （2018b）使用该框架对前列腺MR进行经直肠超声（TRUS）图像配准。配对的培训数据是通过专家手动注册获得的。 （Mahapatra等，2018）使用CycleGAN进行多模态（视网膜）和单模（MR）可变形配准，其中发生器产生变换图像和变形场。 Tanner等。 （2018）通过首先将源域图像变换到目标域然后采用单模态图像相似性度量来进行配准，采用CycleGAN用于MR和CT之间的可变形图像配准。他们发现这种方法最多只能达到与传统的多模态可变形配准方法相似的性能。</p>
<h4 id="3-7-Other-works"><a href="#3-7-Other-works" class="headerlink" title="3.7. Other works"></a><em>3.7. Other works</em></h4><p>cGAN已被用于基于单个术前图像对患者特定运动分布进行建模（Hu等，2017c）; 突出显示对疾病最负责的区域（Baumgartner等，2017）和内窥镜视频数据的重新着色（Ross等，2018）。 在（Mahmood等，2018）中，pix2pix用于放射治疗中的治疗计划，通过预测CT图像的剂量分布图。</p>
<h3 id="4-讨论"><a href="#4-讨论" class="headerlink" title="4. 讨论"></a>4. 讨论</h3><p>可在我们的GitHub存储库中找到已审阅论文的完整列表。在2017年和2018年，GAN相关论文的数量显着增加。这些论文中约有50％研究图像合成，交叉模态图像合成是GAN最重要的应用。 MR被列为GAN相关文献中探索的最常见的成像模式。我们认为应用GAN进行MR图像分析的部分原因是由于常规获取多个序列以提供补充信息。由于获取每个序列需要大量的采集时间，如果可以减少采集序列的数量，GAN有可能减少MR采集时间。由于图像到图像转换框架的普及，这些研究中的另外35％属于分割和重建组。在这些情况下的平行训练对发电机的输出施加了强大的形状和纹理调节，这使得它在这两项任务中非常有前途。这些研究中只有6％用于分类，最有效的用例是对抗域转移。检测和注册的研究数量非常有限，很难得出任何结论。<br>对于那些使用GAN进行分类数据增强的研究，大多数都专注于生成易于对齐的微小物体，如结节，病变和细胞。我们认为部分原因是这些图像的内容变化相对于完整的上下文图像相对较小，这使得当前技术的训练更加稳定。另一个原因可能与研究的计算预算有关，因为高分辨率图像的训练需要大量的GPU时间。尽管有研究将GAN应用于合成整个胸部X射线（Madani等，2018a，b），但有效性仅在相当容易的任务中显示，例如心脏异常分类和中等大小的数据方案，例如几千张图片。随着大量标记数据集的出现，例如CheXpert（Irvin等，2019），GAN的潜力将在于合成非常见的病理病例，最有可能通过条件生成来条件信息由医学专家。<br>不同的成像模式通过利用组织对某些物理介质（例如X射线或磁场）的响应而起作用，因此可以彼此提供互补的诊断信息。作为监督深度学习的常见实践，标记一种模态类型的图像以训练网络以完成期望的任务。即使基础解剖结构相同，当切换模态时也重复该过程，导致人力的浪费。对数训练，或更具体地说是不成对的交叉模态翻译，可以在所有模态中重复使用标签，并为无监督转移学习开辟了新途径（Dou et al。，2018）。</p>
<h4 id="4-1-Future-challenges"><a href="#4-1-Future-challenges" class="headerlink" title="4.1. Future challenges"></a><em>4.1. Future challenges</em></h4><p>在图像重建和交叉模态图像合成中，大多数工作仍然采用传统的浅参考指标，如MAE，PSNR或SSIM进行定量评估。但是，这些测量不符合图像的视觉质量。即使像素方式丢失的直接优化产生次优（模糊）结果，但它为这些测量提供的数字高于使用对抗性损失。在基于GAN的工程的水平比较中解释这些数字变得越来越困难，特别是当结合表2所示的外部损失时。缓解此问题的一种方法是使用下游任务（例如分段或分类）来验证生成样本的质量。另一种方法是招募领域专家，但这种方法昂贵，耗时且难以扩展。最近，张等人。 （2018a）提出了学习的感知图像路径相似性（LPIPS），其优于先前的度量。 MedGAN（Armanious等，2018）已经采用它来评估生成的图像质量，但是与经验丰富的人类观察者的主观测量相比，看到它对不同类型的医学图像的有效性是有意义的。广泛的研究。对于自然图像，无条件生成的样本质量和多样性通常通过初始评分（Salimans等，2016），随机选择的合成样本对中的平均MS-SSIM度量来衡量（Odena等，2016），或Fre chet Inception distance（FID）（Heusel et al。，2017）。这些医学图像指标的有效性仍有待探索。<br>除了GAN的许多积极效用之外，现有的文献也突出了它们在医学成像方面的缺点。虽然跨域图像到图像转换在医学成像中提供了许多GAN的预期应用，但Cohen等人。 （2018）警告不要使用生成的图像进行解释。他们观察到，由于匹配目标域的数据分布（从训练数据中获取），并且可能与测试数据分布完全不同，因此CycleGAN网络（对于非配对数据）可能会受到偏差。当目标域中提供的数据具有某些类的过高或过低表示时，作者还观察到条件GAN（对于配对数据）的偏差。最近的另一项工作（Mirsky等，2019）证明了使用3D条件GAN对3D医学成像进行严重篡改的可能性。</p>
<h4 id="4-2-Interesting-future-applications"><a href="#4-2-Interesting-future-applications" class="headerlink" title="4.2. Interesting future applications"></a><em>4.2. Interesting future applications</em></h4><p>与其他深度学习神经网络模型类似，本文中演示的各种GAN应用直接关系到改善放射学工作流程和患者护理。然而，GAN的优势在于他们以无人监督和/或弱监督的方式学习的能力。特别是，我们认为由cGAN实现的图像到图像的转换可以在医学成像中具有各种其他有用的应用。例如，恢复使用某些伪影（例如运动）获取的MR图像，尤其是在儿科设置中，可能有助于减少重复检查的次数;检测植入装置，例如， X射线上的钉，线，管，起搏器和人工瓣膜。<br>探索用于图像字幕任务的GAN（Dai等人，2017a; Shetty等人，2017; Melnyk等人，2018; Fedus等人，2018）可能导致半自动生成医学成像报告（Jing等人。，2017）可能会缩短图像报告时间。对抗性文本分类的成功（Liu et al。，2017b）也提示了GAN在从自由文本临床适应症中提高自动MR协议生成等系统的性能方面的潜在用途（Sohn等，2017）。自动化系统可以改善MRI等待时间<br>正在崛起（CIHI，2017）以及加强患者护理。 cGAN，特别是CycleGAN应用，例如卸妆（Chang et al。，2018），可以扩展到医疗成像应用<br>通过去除诸如石膏之类的伪像来改善骨骼X射线图像，以便于增强观察效果。这可能有助于放射科医师评估细骨特征，可能有助于更好地检测最初隐匿性骨折，并有助于更有效地评估骨愈合的进展。 GAN在无监督异常检测中的成功（Schlegl等，2017）可以帮助实现以无人监督的方式检测各种模态的医学图像中的异常的任务。这种算法可用于确定放射科医师工作清单的优先顺序，从而缩短报告临界发现的周转时间（Gal Yaniv，2018）。我们还期望通过文本描述（Bodnar，2018）见证GAN在医学图像合成中的实用性，特别是对于罕见病例，以填补用于医学图像分类任务的训练监督神经网络所需的训练样本的差距。 。<br>最后，我们想指出的是，尽管文献中报道了许多有希望的结果，但在医学成像中采用GAN仍处于起步阶段，目前尚无临床应用于基于GAN的方法的突破性应用。</p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    最終更新：<time datetime="2019-07-02T12:08:24.231Z" itemprop="dateUpdated">2019-07-02 20:08:24</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="CaptainSE">
            CaptainSE
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            

            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/&title=《Generative Adversarial Network in Medical Imaging A Review》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/&title=《Generative Adversarial Network in Medical Imaging A Review》 — Go Further&source=【阅读时间】XXX min XXX words【阅读内容】……" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Generative Adversarial Network in Medical Imaging A Review》 — Go Further&url=http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/04/15/Win10-Ubuntu18-04/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Win10+Ubuntu18.04.md</h4>
      </a>
    </div>
  
</nav>



    

















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢老板~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>このブログの内容物は<a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ja">クリエイティブ・コモンズ 表示 - 非営利 - 継承 4.0 国際ライセンスの下に提供されています</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>CaptainSE &copy; 2015 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/&title=《Generative Adversarial Network in Medical Imaging A Review》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/&title=《Generative Adversarial Network in Medical Imaging A Review》 — Go Further&source=【阅读时间】XXX min XXX words【阅读内容】……" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Generative Adversarial Network in Medical Imaging A Review》 — Go Further&url=http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADK0lEQVR42u3aQXIbMQwEQP//08o1VclKA2DlEqHekytWlmj6MCLAn5/4eVw8zz+TvOfvf3/++epaNzzY2NjYh7AfT59/y0qKfv5/r2pISHltL7YGGxsbex07KSUvsRd48/BLLNjY2NjYeaRVgzBn5FGKjY2NjZ2ETRWcbFbC/qUAw8bGxj6EnWOqRfSCrbc1b+mlYWNjY388O5+Kfv7Pb5lvY2NjY38w+1F88gNMLw57V4LKCmxsbOxF7PmVnfddzcnTJ7ms+Z+qsLGxsVewe2PX6gAgOd70gq0XeNjY2Nib2PlVmKTQfJRbjaX5ECJ6MzY2Nvax7OTre7Wg3t5XB8O9ww82Njb2t7GrDaPJMODe6zijAMPGxsY+ij2JmV6j5x1jg16/CBsbG/t0dhIz1RKrI4feAGA0eMDGxsZexE6OGfnWTFpFSQ3JU/gTYmNjYy9iV3l58PQ2dzIwKF8ewsbGxj6cXW0ePV+g2gaqll59AzY2NjZ2r1kzv14zubiT1IyNjY29lT3fjryIaiMp377okIONjY29mp0fP5Ih7l0XgCZjg8v5NjY2NvYKdq+FlH/1T1o8+RWfatBiY2NjfwM7D7Beu783DMhbTtWWFjY2NvY+djJSzY8K+cK9AUAvPl8kNjY2NvY6drWhk0dONeryFlU+QrgMMGxsbOxj2fMDwF0j2+owYHRAwsbGxl7E7pVVHc3mIXRXsEVXfLCxsbFXsJMl89jIt+muA0x1mIGNjY29lT16RfECzWTQ22szXV7ZwcbGxl7Bvqvc3mGmF055Kyoa9GJjY2OvYOeXY6rB1is9qa36/p/81djY2NgHsnsRNW/Zz2MsqedyRWxsbOyl7Hnk5OA8XQs5nL8BGxsbewX7UXzy0e98yyZB1ZxvY2NjYx/IrgbA83LzI0r52NBaqzpCxsbGxj6XXQ2t+W8njPzw8+K32NjY2OvY+ch28uW+N+LtrRhd1sHGxsb+enb1Dkz++bxRlb/ntgDDxsbGPpydY0ZXaoIYGx1UsLGxsdexe42hyUi4NzboHY1GN5WwsbGxP55dbQYl7N6W9SpJNre6CjY2NvYh7D87iTZhfUtEuQAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            clearTimeout(titleTime);
        } else {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
