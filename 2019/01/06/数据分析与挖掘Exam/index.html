<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>数据分析与挖掘Exam | Go Further | Stay Hungry, Stay Foolish</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#142421">
    
    
    <meta name="keywords" content="">
    <meta name="description" content="【阅读时间】XXX min XXX words【内容描述】《数据分析与挖掘》之后篇">
<meta property="og:type" content="article">
<meta property="og:title" content="数据分析与挖掘Exam">
<meta property="og:url" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/index.html">
<meta property="og:site_name" content="Go Further">
<meta property="og:description" content="【阅读时间】XXX min XXX words【内容描述】《数据分析与挖掘》之后篇">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/DataStatistical.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/SimilarityDissimilarityAndProximity.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/dataPreprocess_mainTask.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/DataQualityIssues.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Summary_datapreprocessing.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/ConfusionMatrix.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/classification_summary1.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/classification_summary2.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/classification_summary3.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/AprioriExample.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/fp_Summary.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/k-Means.gif">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CFTree.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/DBSCAN_BasicConcept.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/clustering_Summary.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/clustering_Summary2.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CPM1.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CPM2.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/graphMining_Summary.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CommunityDetection_Summary.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/ProximityMeasureforBinaryAttributes.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Dissimilarity%20betweenAsymmetricBinaryVariables.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CalculatingCosineSimilarity.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CorrelationAnalysis.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CorrelationAnalysisExample.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CorrelationBetweenTwoNumericalVariables.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/VarianceForSingleVariable.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CovarianceForTwoVariables.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Example_CalculationOfCovariance.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Normalization.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Binning.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/BinningExample.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/BinningvsClustering.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Example-AttributeSelection_withInformationGain.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/ComputationOfGiniIndex.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/ClassifierEvaluationMetrics.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/ClassifierEvaluationMetrics1.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/ClassifierEvaluationMetricsExample.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Naïve%20Bayesian%20Classifier0.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Naïve%20Bayesian%20Classifier1.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/AvoidingtheZero-ProbabilityProblem.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Bayesian%20Network1.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Bayesian%20Network2.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Bayesian%20Network3.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/Bayesian%20Network4.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/SVM_Example.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/AssociationRules.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/FP-TreeConstruct.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/FP-TreeFindPatterns.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/FP-Tree_mConditional.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/MiningEachConditionalFP-tree.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/KernelK-Means.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CPMExample1.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CPMExample2.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/CPMExample3.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/SpectralClusteringExample1.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/SpectralClusteringExample2.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/MarkovChains.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/MarkovChains2.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/WeightedGraphs.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/MCL_SelfLoop.png">
<meta property="og:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/MCL_Inflation.png">
<meta property="og:updated_time" content="2019-01-08T13:55:42.778Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="数据分析与挖掘Exam">
<meta name="twitter:description" content="【阅读时间】XXX min XXX words【内容描述】《数据分析与挖掘》之后篇">
<meta name="twitter:image" content="http://yoursite.com/2019/01/06/数据分析与挖掘Exam/DataStatistical.png">
    
        <link rel="alternate" type="application/atom+xml" title="Go Further" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">CaptainSE</h5>
          <a href="mailto:841145636@qq.com" title="841145636@qq.com" class="mail">841145636@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/Captainzj" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">数据分析与挖掘Exam</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">数据分析与挖掘Exam</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-01-06T10:56:52.000Z" itemprop="datePublished" class="page-time">
  2019-01-06
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/XD/">XD</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Exam"><span class="post-toc-number">1.</span> <span class="post-toc-text">Exam</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Introduction"><span class="post-toc-number">2.</span> <span class="post-toc-text">Introduction</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Data"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">Data</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-types"><span class="post-toc-number">2.1.1.</span> <span class="post-toc-text">Data types</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-statistics"><span class="post-toc-number">2.1.2.</span> <span class="post-toc-text">Data statistics</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Data-preprocessing"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">Data preprocessing</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-cleaning"><span class="post-toc-number">2.2.1.</span> <span class="post-toc-text">Data cleaning</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#How-to-Handle-Missing-Data"><span class="post-toc-number">2.2.1.1.</span> <span class="post-toc-text">How to Handle Missing Data?</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#How-to-Handle-Noisy-Data"><span class="post-toc-number">2.2.1.2.</span> <span class="post-toc-text">How to Handle Noisy Data?</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-integrating"><span class="post-toc-number">2.2.2.</span> <span class="post-toc-text">Data integrating</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Handling-Redundancy-in-Data-Integration"><span class="post-toc-number">2.2.2.1.</span> <span class="post-toc-text">Handling Redundancy in Data Integration</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-transforming"><span class="post-toc-number">2.2.3.</span> <span class="post-toc-text">Data transforming</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-Reduction"><span class="post-toc-number">2.2.4.</span> <span class="post-toc-text">Data Reduction</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#classification"><span class="post-toc-number">3.</span> <span class="post-toc-text">classification</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Algorithms"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">Algorithms</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Decision-tree-ID3-C4-5-CART"><span class="post-toc-number">3.1.1.</span> <span class="post-toc-text">Decision tree-ID3,C4.5,CART</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#SVM"><span class="post-toc-number">3.1.2.</span> <span class="post-toc-text">SVM</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Bayes"><span class="post-toc-number">3.1.3.</span> <span class="post-toc-text">Bayes</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ANN"><span class="post-toc-number">3.1.4.</span> <span class="post-toc-text">ANN</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#DeepLearning"><span class="post-toc-number">3.1.4.1.</span> <span class="post-toc-text">DeepLearning</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Model-evaluation-and-selection"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">Model evaluation and selection</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Confusion-matrix-and-criteria"><span class="post-toc-number">3.2.1.</span> <span class="post-toc-text">Confusion matrix and criteria</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Cross-evaluation"><span class="post-toc-number">3.2.2.</span> <span class="post-toc-text">Cross-evaluation</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Ensemble-methods"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">Ensemble methods</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Bagging"><span class="post-toc-number">3.3.1.</span> <span class="post-toc-text">Bagging</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Boosting"><span class="post-toc-number">3.3.2.</span> <span class="post-toc-text">Boosting</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Random-Forest"><span class="post-toc-number">3.3.3.</span> <span class="post-toc-text">Random Forest</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary-1"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Frequent-patterns"><span class="post-toc-number">4.</span> <span class="post-toc-text">Frequent patterns</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Algorithm"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">Algorithm</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Apriori"><span class="post-toc-number">4.1.1.</span> <span class="post-toc-text">Apriori</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#FP-growth"><span class="post-toc-number">4.1.2.</span> <span class="post-toc-text">FP-growth</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary-2"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Clustering"><span class="post-toc-number">5.</span> <span class="post-toc-text">Clustering</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Algorithms-1"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">Algorithms</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Partition-based—k-means"><span class="post-toc-number">5.1.1.</span> <span class="post-toc-text">Partition-based—k-means</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Hierarchical-based—two-ways"><span class="post-toc-number">5.1.2.</span> <span class="post-toc-text">Hierarchical-based—two ways</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Brich"><span class="post-toc-number">5.1.2.1.</span> <span class="post-toc-text">Brich</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Density-based—DBSCAN"><span class="post-toc-number">5.1.3.</span> <span class="post-toc-text">Density-based—DBSCAN</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#AP-2007-Science"><span class="post-toc-number">5.1.4.</span> <span class="post-toc-text">AP (2007, Science)</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Local-density-based-2014-Science"><span class="post-toc-number">5.1.5.</span> <span class="post-toc-text">Local density-based (2014, Science)</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary-3"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Graph-clustering"><span class="post-toc-number">6.</span> <span class="post-toc-text">Graph clustering</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Algorithms-2"><span class="post-toc-number">6.1.</span> <span class="post-toc-text">Algorithms</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CPM"><span class="post-toc-number">6.1.1.</span> <span class="post-toc-text">CPM</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Spectral-clustering"><span class="post-toc-number">6.1.2.</span> <span class="post-toc-text">Spectral clustering</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#GNandQ"><span class="post-toc-number">6.1.3.</span> <span class="post-toc-text">GNandQ</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#MCL"><span class="post-toc-number">6.1.4.</span> <span class="post-toc-text">MCL</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary-4"><span class="post-toc-number">6.2.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Calculate"><span class="post-toc-number">7.</span> <span class="post-toc-text">Calculate</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#dataProcessing"><span class="post-toc-number">7.1.</span> <span class="post-toc-text">dataProcessing</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Proximity-Measure-for-Binary-Attributes"><span class="post-toc-number">7.1.1.</span> <span class="post-toc-text">Proximity Measure for Binary Attributes</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Correlation-Analysis"><span class="post-toc-number">7.1.2.</span> <span class="post-toc-text">Correlation Analysis</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Covariance"><span class="post-toc-number">7.1.3.</span> <span class="post-toc-text">Covariance</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Normalization"><span class="post-toc-number">7.1.4.</span> <span class="post-toc-text">Normalization</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Simple-Discretization-Binning"><span class="post-toc-number">7.1.5.</span> <span class="post-toc-text">Simple Discretization: Binning</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Classification"><span class="post-toc-number">7.2.</span> <span class="post-toc-text">Classification</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ID3-Decision-Tree：Information-Gain"><span class="post-toc-number">7.2.1.</span> <span class="post-toc-text">ID3 Decision Tree：Information Gain</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CART-Decision-Tree：-Gini-Index"><span class="post-toc-number">7.2.2.</span> <span class="post-toc-text">CART Decision Tree： Gini Index</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Classifier-Evaluation-Metrics-Example"><span class="post-toc-number">7.2.3.</span> <span class="post-toc-text">Classifier Evaluation Metrics: Example</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Naïve-Bayesian-Classifier"><span class="post-toc-number">7.2.4.</span> <span class="post-toc-text">Naïve Bayesian Classifier</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Bayesian-Belief-Network"><span class="post-toc-number">7.2.5.</span> <span class="post-toc-text">Bayesian Belief Network</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#SVM-1"><span class="post-toc-number">7.2.6.</span> <span class="post-toc-text">SVM</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Frequent-patterns-1"><span class="post-toc-number">7.3.</span> <span class="post-toc-text">Frequent patterns</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Association-Rules-supprt-amp-amp-confidence"><span class="post-toc-number">7.3.1.</span> <span class="post-toc-text">Association Rules: supprt &amp;&amp; confidence</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#FP-growth-1"><span class="post-toc-number">7.3.2.</span> <span class="post-toc-text">FP-growth</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Clustering-1"><span class="post-toc-number">7.4.</span> <span class="post-toc-text">Clustering</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Kernel-K-Means"><span class="post-toc-number">7.4.1.</span> <span class="post-toc-text">Kernel K-Means</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Community-detection"><span class="post-toc-number">7.5.</span> <span class="post-toc-text">Community detection</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Spectral-Clustering"><span class="post-toc-number">7.5.1.</span> <span class="post-toc-text">Spectral Clustering</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#MCL-1"><span class="post-toc-number">7.5.2.</span> <span class="post-toc-text">MCL</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Conclusion"><span class="post-toc-number">8.</span> <span class="post-toc-text">Conclusion</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Classification-1"><span class="post-toc-number">8.1.</span> <span class="post-toc-text">Classification</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#经典"><span class="post-toc-number">8.1.1.</span> <span class="post-toc-text">经典</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#集成方法"><span class="post-toc-number">8.1.2.</span> <span class="post-toc-text">集成方法</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#频繁模式"><span class="post-toc-number">8.2.</span> <span class="post-toc-text">频繁模式</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#聚类"><span class="post-toc-number">8.3.</span> <span class="post-toc-text">聚类</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#图聚类"><span class="post-toc-number">8.4.</span> <span class="post-toc-text">图聚类</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Todo"><span class="post-toc-number">9.</span> <span class="post-toc-text">Todo</span></a></li></ol>
        </nav>
    </aside>


<article id="post-数据分析与挖掘Exam"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">数据分析与挖掘Exam</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-01-06 18:56:52" datetime="2019-01-06T10:56:52.000Z"  itemprop="datePublished">2019-01-06</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/XD/">XD</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>【阅读时间】XXX min XXX words<br>【内容描述】<a href="https://captainzj.github.io/2019/01/06/数据分析与挖掘/" target="_blank" rel="noopener">《数据分析与挖掘》</a>之后篇</p>
<a id="more"></a>
<h2 id="Exam"><a href="#Exam" class="headerlink" title="Exam"></a>Exam</h2><table>
<thead>
<tr>
<th style="text-align:center">题型</th>
<th style="text-align:center">题量（道）</th>
<th style="text-align:center">分值（分）</th>
<th style="text-align:center">总计（分）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">选择题</td>
<td style="text-align:center">10</td>
<td style="text-align:center">2</td>
<td style="text-align:center">20</td>
</tr>
<tr>
<td style="text-align:center">计算题</td>
<td style="text-align:center">3</td>
<td style="text-align:center">10</td>
<td style="text-align:center">30</td>
</tr>
<tr>
<td style="text-align:center">简答题</td>
<td style="text-align:center">4</td>
<td style="text-align:center">5</td>
<td style="text-align:center">20</td>
</tr>
<tr>
<td style="text-align:center">论述题</td>
<td style="text-align:center">2</td>
<td style="text-align:center">15</td>
<td style="text-align:center">30</td>
</tr>
</tbody>
</table>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>What is datamining </p>
<p>从数据中发现知识</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> 统计分析  +     分析报告      -&gt;&gt;     数据挖掘</span><br><span class="line">(数据统计) + (协同过滤关联分析) -&gt;&gt; (找寻事物的隐含规律)</span><br><span class="line"> (描述性)  +   (预测性概率)    -&gt;&gt;     (规范性)</span><br><span class="line"> </span><br><span class="line">- 分析报告一般是整个事件发生结束以后的总结（描述性）。</span><br><span class="line">- 统计分析能利用大量的历史样本来预测整个事件总体未来的走向（预测性概率）。</span><br><span class="line">- 数据挖掘则透过事件的表象发现隐藏在背后的蛛丝马迹，从而找到潜伏的规律以及看似无关事物之间背后的联系，用此来洞察未来（规范性）。</span><br></pre></td></tr></table></figure>
</li>
<li><p>What is machine learning</p>
<p> 计算机程序基于数据自动地学习识别复杂的模式，并作出智能的决断。 允许程序可以根据提供的数据进行自动的学习，它可以使你的程序变得更”聪明”。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- 监督学习 (分类)</span><br><span class="line">- 无监督学习 (聚类)</span><br><span class="line">- 半监督学习: 在学习模型时，它使用标记(学习模型)和未标记(改进类边界)的实例。</span><br><span class="line">- 主动学习: 通过主动地从用户获取知识来提高模型质量</span><br></pre></td></tr></table></figure>
</li>
<li><p>What is artificial intelligence </p>
<p>在计算机科学的基础上，综合信息论、心理学、生理学、语言学、逻辑学和数学等知识，制造能<code>模拟人类智能行为</code>的计算机系统的边缘学科。</p>
</li>
</ul>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">计算公式</span><br></pre></td></tr></table></figure>
<h4 id="Data-types"><a href="#Data-types" class="headerlink" title="Data types"></a>Data types</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">* <span class="string">'Relational records'</span>  关系记录</span><br><span class="line">  - Relational tables, highly structured 关系表，高度结构化</span><br><span class="line">* <span class="string">'Data matrix'</span>, e.g., numerical matrix, crosstabs 数据矩阵，例如数值矩阵，交叉表</span><br><span class="line">* <span class="string">'Transaction data'</span> 交易数据</span><br><span class="line">* <span class="string">'Document data'</span>: Term-frequency vector (matrix) of text documents</span><br><span class="line">  文档数据：文本文档的术语 - 频率向量（矩阵）</span><br><span class="line">* <span class="string">'Transportation network'</span> 交通网络</span><br><span class="line">* <span class="string">'World Wide Web'</span> 万维网</span><br><span class="line">* <span class="string">'Molecular Structures'</span> 分子结构</span><br><span class="line">* <span class="string">'Social or information networks'</span> 社交或信息网络</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>Attribute Types</strong> </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">* <span class="string">'Nominal'</span>: categories, states, <span class="keyword">or</span> “names of things” <span class="comment"># 标称属性</span></span><br><span class="line">* <span class="string">'Binary'</span> : Nominal attribute <span class="keyword">with</span> only <span class="number">2</span> states (<span class="number">0</span> <span class="keyword">and</span> <span class="number">1</span>)<span class="comment"># 二元属性:仅有2个状态（0和1）的标称属性</span></span><br><span class="line">* <span class="string">'Ordinal'</span>  <span class="comment"># 序数属性</span></span><br><span class="line">	- Values have a meaningful order (ranking) but magnitude between successive values <span class="keyword">is</span> <span class="keyword">not</span> known</span><br><span class="line">	  值具有有意义的顺序（排名），但连续值之间的大小未知</span><br><span class="line">* <span class="string">'Numeric'</span> <span class="comment"># 数值属性</span></span><br><span class="line"> 	- `Interval`: Measured on a scale of equal-sized units 间隔: 按相同大小的单位测量</span><br><span class="line">	- `Ratio` 比率(倍数)</span><br><span class="line">	   We can speak of values <span class="keyword">as</span> being an order of magnitude larger than the unit of measurement (<span class="number">10</span> K˚ <span class="keyword">is</span> twice <span class="keyword">as</span> high <span class="keyword">as</span> <span class="number">5</span> K˚).</span><br><span class="line">	   我们可以说价值比测量单位大一个数量级（<span class="number">10</span>K˚是<span class="number">5</span>K˚的两倍）</span><br><span class="line">* <span class="string">'Discrete Attribute'</span>  <span class="comment"># 离散属性</span></span><br><span class="line">	- Sometimes, represented <span class="keyword">as</span> integer variables 有时，表示为`整数变量`</span><br><span class="line">	- Note: Binary attributes are a special case of discrete attributes </span><br><span class="line">	  注意：二进制属性是离散属性的特例</span><br><span class="line">* <span class="string">'Continuous Attribute'</span>  <span class="comment"># 连续属性</span></span><br><span class="line">	- Practically, real values can only be measured <span class="keyword">and</span> represented using a finite number of digits</span><br><span class="line">	  实际上，只能使用有限数字来测量和表示实际值</span><br><span class="line">	- Continuous attributes are typically represented <span class="keyword">as</span> floating-point variables</span><br><span class="line">	  连续属性通常表示为`浮点变量`</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Data-statistics"><a href="#Data-statistics" class="headerlink" title="Data statistics"></a>Data statistics</h4><ul>
<li><strong>Motivation</strong>: To better understand the data: central tendency, variation and spread</li>
</ul>
<center><br>    <img src="/2019/01/06/数据分析与挖掘Exam/DataStatistical.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/SimilarityDissimilarityAndProximity.png" width="600"><br></center>

<h3 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">常用方法  缺失值 处理方法</span><br></pre></td></tr></table></figure>
<center><br><img src="/2019/01/06/数据分析与挖掘Exam/dataPreprocess_mainTask.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/DataQualityIssues.png" width="600"><br></center>

<h4 id="Data-cleaning"><a href="#Data-cleaning" class="headerlink" title="Data cleaning"></a>Data cleaning</h4><ul>
<li>Handle <code>missing data</code>(Incomplete), smooth <code>noisy data</code>,identify or remove <code>outliers</code>, and resolve <code>inconsistencies</code> 处理丢失的数据，平滑噪声数据，识别或删除异常值，并解决不一致问题</li>
</ul>
<h5 id="How-to-Handle-Missing-Data"><a href="#How-to-Handle-Missing-Data" class="headerlink" title="How to Handle Missing Data?"></a>How to Handle Missing Data?</h5><ul>
<li>Data is not always available:  many tuples have <code>no recorded value</code> for several attributes</li>
</ul>
<blockquote>
<p> 1.忽略元组  2.手动填充  3. 自动（以”unknown”/“均值”/“最可能的值’’）填充</p>
</blockquote>
<h5 id="How-to-Handle-Noisy-Data"><a href="#How-to-Handle-Noisy-Data" class="headerlink" title="How to Handle Noisy Data?"></a>How to Handle Noisy Data?</h5><ul>
<li>Noise: random <code>error or variance</code> in a measured variable</li>
</ul>
<blockquote>
<p>1.binning平滑  2.回归(拟合平滑) 3.聚类（无监督,检查并删除异常点） 4.半监督（检测可疑值并由人查验）</p>
</blockquote>
<h4 id="Data-integrating"><a href="#Data-integrating" class="headerlink" title="Data integrating"></a>Data integrating</h4><ul>
<li>Integration of multiple databases, data cubes, or files 集成多个数据库，数据立方体或文件</li>
</ul>
<blockquote>
<p>1.数据集成 2.模式集成 3.实体识别 4.检测和解决数据值的冲突</p>
</blockquote>
<h5 id="Handling-Redundancy-in-Data-Integration"><a href="#Handling-Redundancy-in-Data-Integration" class="headerlink" title="Handling Redundancy in Data Integration"></a>Handling Redundancy in Data Integration</h5><blockquote>
<p>1.冗余原因（对象标识不同、派生数据）2.检测手段（相关性和协方差分析）3.仔细整合</p>
</blockquote>
<h4 id="Data-transforming"><a href="#Data-transforming" class="headerlink" title="Data transforming"></a>Data transforming</h4><ul>
<li>A function that maps the entire set of values of a given attribute to a new set of replacement values s.t. each old value can be identified with one of the new values  一种函数，它将给定属性的整个值集映射到一组新的替换值 使得 可以使用其中一个新值标识每个旧值</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Methods</span><br><span class="line">  - <span class="string">'Smoothing'</span>: Remove noise <span class="keyword">from</span> data <span class="comment"># 平滑</span></span><br><span class="line">  - <span class="string">'Attribute/feature construction'</span>  <span class="comment"># 属性/特征构建</span></span><br><span class="line">    - New attributes constructed <span class="keyword">from</span> the given ones</span><br><span class="line">  - <span class="string">'Aggregation'</span>: Summarization, data cube construction  <span class="comment"># 聚合</span></span><br><span class="line">  - <span class="string">'Normalization'</span>: Scaled to fall within a smaller, specified range  <span class="comment"># 归一化</span></span><br><span class="line">    - min-max normalization</span><br><span class="line">    - z-score normalization</span><br><span class="line">    - normalization by decimal scaling</span><br><span class="line">  - <span class="string">'Discretization'</span>: Concept hierarchy climbing <span class="comment"># 离散化</span></span><br></pre></td></tr></table></figure>
<h4 id="Data-Reduction"><a href="#Data-Reduction" class="headerlink" title="Data Reduction"></a>Data Reduction</h4><ul>
<li><p>Obtain a reduced representation of the data set  获得数据集的缩减表示</p>
<ul>
<li>much smaller in volume but yet produces almost the same analytical results 体积小得多，但产生几乎相同的分析结果</li>
</ul>
</li>
<li><p><strong>Methods for data reduction</strong></p>
</li>
<li><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># data reduction </span></span><br><span class="line">- <span class="string">'Regression and Log-Linear Models'</span>  （Parametric methods）</span><br><span class="line">- <span class="string">'Histograms, clustering, sampling'</span>  （Non-parametric methods）</span><br><span class="line">	&gt; sampling: 选择具有代表性的子集；简单随机、放回、不放回、分层抽样</span><br><span class="line">- <span class="string">'Data cube aggregation'</span> 数据立方体聚合</span><br><span class="line">- <span class="string">'Data Compression'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Some typical dimensionality methods</span></span><br><span class="line">* Principal Component Analysis (<span class="string">'PCA'</span>) </span><br><span class="line">* Supervised <span class="keyword">and</span> nonlinear techniques</span><br><span class="line">	- <span class="string">'Feature subset selection'</span> 找寻合适子集(仅收集与分析任务相关的属性)</span><br><span class="line">	- <span class="string">'Feature creation'</span></span><br><span class="line">		- `Attribute extraction`  高维映射至低维(降维)</span><br><span class="line">		- `Attribute construction`</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><center><br><img src="/2019/01/06/数据分析与挖掘Exam/Summary_datapreprocessing.png" width="600"><br></center>

<h2 id="classification"><a href="#classification" class="headerlink" title="classification"></a>classification</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">构建模型 应用模型(预测) 描述过程</span><br></pre></td></tr></table></figure>
<ul>
<li><p>What is classification?</p>
<p>根据训练集和类标签（分类属性中的值）构建模型，并将其用于分析新数据，预测其标签。</p>
<ul>
<li><p>Supervised learning</p>
<p><strong>监督学习</strong>是机器学习任务的一种。它<code>从有标记的训练数据中推导出预测标签</code>。有标记的训练数据是指每个训练实例都包括输入和期望的输出。一句话：<strong>给定数据，预测标签</strong>。(分类、回归)</p>
<ul>
<li><strong>无监督学习</strong>是机器学习任务的一种。它<code>从无标记的训练数据中推断结论</code>。最典型的无监督学习就是聚类分析，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。一句话：<strong>给定数据，寻找隐藏的结构</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p>Steps</p>
<ol>
<li><p>模型构建：根据数据集特征构建合适的分类模型，并使用训练集样本进行模型训练</p>
</li>
<li><p>模型验证与测试：将已知的测试样品<code>标签</code>与模型使用测试集所得的<code>分类结果</code>进行比较，而后使用验证集改进模型准确率</p>
</li>
<li><p>模型部署：如果准确度可接受，便可使用此模型分类新数据</p>
</li>
</ol>
</li>
</ul>
<h3 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h3><blockquote>
<p>1.决策树：基于规则   2.贝叶斯：基于概率  3.ANN: 机器学习最优化 </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">算法 思路 优缺点</span><br><span class="line">考 朴素贝叶斯</span><br><span class="line">解释 svm ann 大概了解DeepLearning 理论描述</span><br></pre></td></tr></table></figure>
<h4 id="Decision-tree-ID3-C4-5-CART"><a href="#Decision-tree-ID3-C4-5-CART" class="headerlink" title="Decision tree-ID3,C4.5,CART"></a>Decision tree-ID3,C4.5,CART</h4><ul>
<li><p>Algorithm Step</p>
<p>树以自上而下，递归，分而治之的方式构建</p>
<ul>
<li>一开始，所有的训练样例都是根源</li>
<li><strong>样例基于被选定的属性递归地划分</strong>(在每个节点上，基于<code>该节点上的训练示例</code>以及<code>启发式或统计度量（例如，信息增益）</code>来<code>选择属性</code>。)</li>
</ul>
<p>停止条件</p>
<ul>
<li>给定节点的所有样本都属于<code>同一个类</code></li>
<li>没有<code>剩余属性</code>可用于进一步分区</li>
<li>没有<code>剩余样例</code></li>
</ul>
</li>
<li><p>Basic Concepts</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">- Entropy 信息熵:  表征混乱程度 </span><br><span class="line">- Conditional Entropy条件熵: 在已知随机变量X的条件下随机变量Y的不确定性(概率)</span><br><span class="line">- Mutual Information互信息/Information gain信息增益: 得知特征X的信息而使得类Y的信息的不确定性减少的程度(越大越好) -&gt;&gt; 'ID3决策树'</span><br><span class="line">- Gain Ratio信息增益比: 解决使用信息增益存在偏向于选择取值较多的特征的问题(越大越好) -&gt;&gt; 'C4.5决策树' </span><br><span class="line">- GINI index基尼指数： 表征不纯度(越小越好)  -&gt;&gt;  'CART分类树'</span><br></pre></td></tr></table></figure>
</li>
<li><p>决策树算法比较</p>
<p>| 算法 |  支持模型  | 树结构 |     特征选择     | 连续值处理 | 缺失值处理 |  剪枝  |<br>| :–: | :——–: | :—-: | :————–: | :——–: | :——–: | :—-: |<br>| ID3  |    分类    | 多叉树 |     信息增益     |   不支持   |   不支持   | 不支持 |<br>| C4.5 |    分类    | 多叉树 |    信息增益比    |    支持    |    支持    |  支持  |<br>| CART | 分类，回归 | 二叉树 | 基尼系数，均方差 |    支持    |    支持    |  支持  |</p>
</li>
</ul>
<h4 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h4><ul>
<li>Define: Find a linear/non-linear hyperplane (decision boundary) that will separate the data</li>
<li>Optimize：希望所有的点都离超平面远 -&gt;  可以让离超平面比较近的点尽可能的远离超平面</li>
<li>kernel核函数：将数据从低维空间映射到高维空间</li>
</ul>
<h4 id="Bayes"><a href="#Bayes" class="headerlink" title="Bayes"></a>Bayes</h4><ul>
<li><p><strong>贝叶斯定理：</strong> $P(H|X)=\frac{P(X|H)P(H)}{P(X)}$</p>
</li>
<li><p>Naïve Bayesian Classifier</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Advantages</span></span><br><span class="line"> Easy to implement  易于实现</span><br><span class="line"> Good results obtained <span class="keyword">in</span> most of the cases 大多数情况下获得了良好的结果</span><br><span class="line"><span class="comment"># Disadvantages</span></span><br><span class="line"> Assumption: class conditional independence, therefore loss of accuracy类条件独立-&gt;准确率缺失</span><br><span class="line"> Practically, dependencies exist among variables</span><br><span class="line">  实际上，变量之间存在<span class="string">'依赖'</span>关系</span><br><span class="line">	 E.g., hospitals: patients: Profile: age, family history, etc. Symptoms: fever, cough etc., Disease: lung cancer, diabetes,etc.</span><br><span class="line">	 Dependencies among these cannot be modeled by Naïve Bayesian Classifier</span><br><span class="line">      这些依赖关系不能用朴素贝叶斯分类器建模</span><br><span class="line"><span class="comment"># How to deal with these dependencies? Bayesian Belief Networks</span></span><br><span class="line">  如何处理这些依赖关系？贝叶斯置信网络</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="ANN"><a href="#ANN" class="headerlink" title="ANN"></a>ANN</h4><ul>
<li><p>受生物神经元的启发，将<strong>多输入单输出</strong>的信息处理单元作为人工神经网络中的一个神经元。人工神经网络的基本结构如下：输入层(输入层的神经元数目对应于训练集数据的属性数目)、隐藏层、输出层(输出层的神经元数目对应于网络预测的分类数目)</p>
</li>
<li><p>Backpropagation</p>
<blockquote>
<p>Backpropagate the error (by updating weights and biases)</p>
</blockquote>
</li>
<li><h5 id="DeepLearning"><a href="#DeepLearning" class="headerlink" title="DeepLearning"></a>DeepLearning</h5><blockquote>
<p>通过更深的layers，自动提取特征（构建特征空间），以达到更”深层次”的学习效果</p>
</blockquote>
</li>
</ul>
<h3 id="Model-evaluation-and-selection"><a href="#Model-evaluation-and-selection" class="headerlink" title="Model evaluation and selection"></a>Model evaluation and selection</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">混淆矩阵 准确率 错误率 等等指标 交叉验证</span><br></pre></td></tr></table></figure>
<h4 id="Confusion-matrix-and-criteria"><a href="#Confusion-matrix-and-criteria" class="headerlink" title="Confusion matrix and criteria"></a>Confusion matrix and criteria</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/ConfusionMatrix.png" width="600"><br></center>

<ul>
<li><p><strong>Issues Affecting Model Selection</strong></p>
<blockquote>
<p>1.准确性 2.速度 3.鲁棒性 4.可伸缩性 5.可解释性 6.其他措施，例如规则的好处，例如决策树大小或分类规则的紧凑性</p>
</blockquote>
</li>
</ul>
<h4 id="Cross-evaluation"><a href="#Cross-evaluation" class="headerlink" title="Cross-evaluation"></a>Cross-evaluation</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Cross-validation (k-fold, where k = 10 is most popular)</span></span><br><span class="line">- 随机将数据划分为k个互斥的子集，每个子集的大小大致相等</span><br><span class="line">- 在第i次迭代中，使用Di作为测试集，使用其他作为训练集</span><br><span class="line">- 留一个：k折叠，其中k = <span class="string">'#'</span> 元组的数量，对于小尺寸数据</span><br><span class="line">- *分层交叉验证*：折叠是分层的，因此每个折叠中的类分布与初始数据中的类别分布大致相同</span><br></pre></td></tr></table></figure>
<h3 id="Ensemble-methods"><a href="#Ensemble-methods" class="headerlink" title="Ensemble methods"></a>Ensemble methods</h3><ul>
<li>使用模型组合来提高准确性</li>
</ul>
<h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><ul>
<li>Bagging: 使用训练集的子集训练每个模型，并且并行学习模型</li>
</ul>
<h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><blockquote>
<p>（Bagging）投票得分    -&gt;  （Boosting）加权得分</p>
</blockquote>
<ul>
<li>Boosting：训练每个新模型实例以强调先前模型错误分类的训练实例，以及按顺序学习的模型</li>
</ul>
<h4 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h4><blockquote>
<p>Avariation of bagging for decision trees 对<strong>决策树的bagging</strong>的’变异’</p>
</blockquote>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><center><br><img src="/2019/01/06/数据分析与挖掘Exam/classification_summary1.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/classification_summary2.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/classification_summary3.png" width="600"><br></center>

<h2 id="Frequent-patterns"><a href="#Frequent-patterns" class="headerlink" title="Frequent patterns"></a>Frequent patterns</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">频繁模式</span><br><span class="line">频繁子图 不考 hhhhh</span><br><span class="line">关联规则重点掌握</span><br><span class="line">必须明白 apriori fp-growth </span><br><span class="line">k -&gt; k+1</span><br></pre></td></tr></table></figure>
<ul>
<li><p>What is a frequent pattern </p>
<p>频繁模式是数据集中频繁出现(满足最小支持度)的项集、序列或子结构。</p>
<ul>
<li><p>Association rule </p>
<p>关联规则就是有关联的规则，形式是这样定义的：<em>两个不相交的非空集合X、Y，如果有X–&gt;Y，就说X–&gt;Y是一条关联规则</em>。</p>
</li>
</ul>
</li>
</ul>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><h4 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h4><ul>
<li><p>Apriori定律</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> 如果一个集合是频繁项集，则它的所有<span class="string">'子集'</span>都是频繁项集。</span><br><span class="line"><span class="number">2.</span> 如果一个集合不是频繁项集，则它的所有<span class="string">'超集'</span>都不是频繁项集。</span><br></pre></td></tr></table></figure>
</li>
<li><p>Step</p>
<center><br><img src="/2019/01/06/数据分析与挖掘Exam/AprioriExample.png" width="600"><br></center>

</li>
</ul>
<h4 id="FP-growth"><a href="#FP-growth" class="headerlink" title="FP-growth"></a>FP-growth</h4><p>FpGrowth算法通过构造一个树结构来压缩数据记录，使得挖掘频繁项集只需要<strong>扫描两次数据记录</strong>，而且该算法不需要生成候选集合，所以效率会比较高。</p>
<ul>
<li><p><strong>Method</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> For each frequent item, construct its conditional pattern-base, <span class="keyword">and</span> then its conditional FP-tree</span><br><span class="line">  对于每个频繁项，<span class="string">'构造其条件模式库'</span>，然后<span class="string">'构造其条件FP树'</span></span><br><span class="line"> Repeat the process on <span class="string">'each'</span> newly created conditional FP-tree</span><br><span class="line">  对每个新创建的条件FP树重复此过程 (<span class="string">"Recursion: Mining Each Conditional FP-tree"</span>)</span><br><span class="line"> Until the resulting FP-tree <span class="keyword">is</span> empty, <span class="keyword">or</span> it contains only one path—single path will generate all the combinations of its sub-paths, each of which <span class="keyword">is</span> a frequent pattern</span><br><span class="line">  在生成的FP树为空之前，或者它只包含一个路径 - 单个路径将生成其子路径的所有组合，<span class="string">'每个路径都是一个频繁的模式'</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><center><br><img src="/2019/01/06/数据分析与挖掘Exam/fp_Summary.png" width="600"><br></center>

<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kmeans 相关算法</span><br><span class="line">分析比较</span><br><span class="line">ap基本概念</span><br></pre></td></tr></table></figure>
<ul>
<li><p>What is clustering</p>
<p>聚类就是按照某个特定标准(如距离准则)<strong>把一个数据集分割成不同的类或簇</strong>，使得同一个簇内的数据对象的<code>相似性</code>尽可能大，同时不在同一个簇中的数据对象的<code>差异性</code>也尽可能地大。即聚类后<code>同一类的数据尽可能聚集到一起，不同数据尽量分离</code>。</p>
<ul>
<li><p>Unsupervised learning </p>
<p>聚类是一种<code>输入数据无标签</code>的“分类”方式（即非监督学习），通常并不需要使用训练数据进行学习，仅把相似的东西聚到一起，并不关心所得的簇具体代表什么</p>
</li>
</ul>
</li>
</ul>
<h3 id="Algorithms-1"><a href="#Algorithms-1" class="headerlink" title="Algorithms"></a>Algorithms</h3><h4 id="Partition-based—k-means"><a href="#Partition-based—k-means" class="headerlink" title="Partition-based—k-means"></a>Partition-based—k-means</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/k-Means.gif" width="400"><br></center>

<p>k-Means : 选取<code>平均值</code>作为新的聚类中心 </p>
<ul>
<li>k-means对初始值的设置很敏感 <ul>
<li><strong>K-means++</strong>：选取<code>与当前所属聚类中心距离最远</code>的点作为新的聚类中心</li>
</ul>
</li>
<li>k-means对噪声和离群值非常敏感<ul>
<li><strong>K-Medoids</strong>：选取<code>中心点</code>（计算该点到当前聚簇中所有点距离之和，最终距离之后最小的点）作为新的聚类中心</li>
<li><strong>K-Medians</strong>：选取<code>中位数</code>作为新的聚类中心</li>
</ul>
</li>
<li><p>k-means只用于numerical，不适用于categorical类型数据</p>
<ul>
<li><strong>K-Modes</strong>：选取<code>众数</code>作为新的聚类中心</li>
</ul>
</li>
<li><p>k-means不能解决非凸non-convex数据</p>
<ul>
<li><strong>Kernel K-Means</strong>：使用核函数(多项式/高斯径向基/Sigmoid核函数)将数据投影到高维特征空间，然后执行K-Means聚类</li>
</ul>
</li>
</ul>
<h4 id="Hierarchical-based—two-ways"><a href="#Hierarchical-based—two-ways" class="headerlink" title="Hierarchical-based—two ways"></a>Hierarchical-based—two ways</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'凝聚'</span>：从单一集群开始，一次连续合并两个集群，构建<span class="string">'自下而上'</span>的集群层次结构</span><br><span class="line">- <span class="string">'分裂'</span>：从一个庞大的宏集群开始，将其连续分成两组，生成一个<span class="string">'自上而下'</span>的集群层次结构</span><br></pre></td></tr></table></figure>
<h5 id="Brich"><a href="#Brich" class="headerlink" title="Brich"></a>Brich</h5><center><br><img src="/2019/01/06/数据分析与挖掘Exam/CFTree.png" width="500"><br></center>

<h4 id="Density-based—DBSCAN"><a href="#Density-based—DBSCAN" class="headerlink" title="Density-based—DBSCAN"></a>Density-based—DBSCAN</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/DBSCAN_BasicConcept.png" width="500"><br></center>

<p>DBSCAN密度聚类：由密度可达关系导出的<code>最大密度相连的样本集合</code>，即为我们最终聚类的一个类别，或者说一个簇。</p>
<h4 id="AP-2007-Science"><a href="#AP-2007-Science" class="headerlink" title="AP (2007, Science)"></a>AP (2007, Science)</h4><p>AP算法通过迭代过程不断更新每一个点的$responsibility$和$availability$,直到产生$m$个高质量的$exemplar$,同时将其余的数据点分配到相应的聚类中。</p>
<h4 id="Local-density-based-2014-Science"><a href="#Local-density-based-2014-Science" class="headerlink" title="Local density-based (2014, Science)"></a>Local density-based (2014, Science)</h4><blockquote>
<p>1.找出聚类中心  2.剩余点的类别指派 3.去除噪音</p>
</blockquote>
<h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><center><br><img src="/2019/01/06/数据分析与挖掘Exam/clustering_Summary.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/clustering_Summary2.png" width="600"><br></center>

<h2 id="Graph-clustering"><a href="#Graph-clustering" class="headerlink" title="Graph clustering"></a>Graph clustering</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">谱聚类 modularity模块化 复杂网络的概念</span><br></pre></td></tr></table></figure>
<ul>
<li><p>What is graph clustering </p>
<ul>
<li><p>Complex network</p>
<p>在我们的现实生活中，许多复杂系统都可以建模成一种复杂网络进行分析，比如常见的电力网络、航空网络、交通网络、计算机网络以及社交网络等等。复杂网络不仅是一种数据的表现形式，它同样也是一种科学研究的手段。</p>
</li>
<li><p>Graph clustering</p>
<p>和特征聚类不同，图聚类比较难以观察，部分算法会以各点之间的距离作为突破口，可以这样形容：张三，是王五的好朋友，刚认识李四，对赵六很是反感。那么，对于该节点，我们无法直接得出他的特征，但能知道他的<code>活动圈</code>。利用图聚类，可以将同一社交范围的人聚合到一起。</p>
</li>
<li><p>Community </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'k-clique community'</span>: Union of all k-cliques that can be `reached <span class="keyword">from</span> each other` through a series of adjacent k-cliques.</span><br><span class="line">k-派系连通：一个k-派系可以通过若干个相邻的k-派系到达另一个k-派系，则称这两个k-派系彼此联通</span><br></pre></td></tr></table></figure>
</li>
<li><p>Module  </p>
</li>
</ul>
</li>
</ul>
<h3 id="Algorithms-2"><a href="#Algorithms-2" class="headerlink" title="Algorithms"></a>Algorithms</h3><h4 id="CPM"><a href="#CPM" class="headerlink" title="CPM"></a>CPM</h4><center class="half"><br><img src="/2019/01/06/数据分析与挖掘Exam/CPM1.png" width="300"><br><img src="/2019/01/06/数据分析与挖掘Exam/CPM2.png" width="300"><br></center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Step1: 找到网络中大小为K的完全子图 Locate maximal cliques </span><br><span class="line">Step2: 将每个完全子图定义为一个节点，建立一个重叠矩阵</span><br><span class="line">Step3: 将重叠矩阵变成社团邻接矩阵(其中重叠矩阵中对角线小于k、非对角线小于k<span class="number">-1</span>的元素全置为<span class="number">0</span>,所有非<span class="number">0</span>项置为<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Spectral-clustering"><a href="#Spectral-clustering" class="headerlink" title="Spectral clustering"></a>Spectral clustering</h4><p>谱聚类（Spectral Clustering），就是先用<code>Laplacian eigenmaps对数据降维</code>（简单地说，就是先将数据转换成邻接矩阵或相似性矩阵，再转换成Laplacian矩阵，再对Laplacian矩阵进行特征分解，把最小的K个特征向量排列在一起），然后再<code>使用k-means</code>完成聚类。谱聚类是个很好的方法，效果通常比k-means好，计算复杂度还低，这都要归功于降维的作用。 </p>
<h4 id="GNandQ"><a href="#GNandQ" class="headerlink" title="GNandQ"></a>GNandQ</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> 计算网络中所有边缘的中介性.</span><br><span class="line"><span class="number">2.</span> 去除间隙最大的边缘.</span><br><span class="line"><span class="number">3.</span> 重新计算受移除影响的所有边缘的间隙.</span><br><span class="line"><span class="number">4.</span> 从步骤<span class="number">2</span>重复，直到没有边缘.</span><br></pre></td></tr></table></figure>
<h4 id="MCL"><a href="#MCL" class="headerlink" title="MCL"></a>MCL</h4><p>在MCL中， <strong>Expansion</strong> 和 <strong>Inflation</strong> 将不断的交替进行，<strong>Expansion</strong> 使得不同的区域之间的联系加强，而 <strong>Inflation</strong> 则不断的<code>分化</code>各点之间的联系(强者恒强，弱者恒弱)。经过多次迭代，将渐渐出现聚集现象，以此便达到了聚类的效果。</p>
<h3 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h3><center><br><img src="/2019/01/06/数据分析与挖掘Exam/graphMining_Summary.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/CommunityDetection_Summary.png" width="600"><br></center>

<h2 id="Calculate"><a href="#Calculate" class="headerlink" title="Calculate"></a>Calculate</h2><h3 id="dataProcessing"><a href="#dataProcessing" class="headerlink" title="dataProcessing"></a>dataProcessing</h3><h4 id="Proximity-Measure-for-Binary-Attributes"><a href="#Proximity-Measure-for-Binary-Attributes" class="headerlink" title="Proximity Measure for Binary Attributes"></a>Proximity Measure for Binary Attributes</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/ProximityMeasureforBinaryAttributes.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/Dissimilarity betweenAsymmetricBinaryVariables.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/CalculatingCosineSimilarity.png" width="600"><br></center>

<h4 id="Correlation-Analysis"><a href="#Correlation-Analysis" class="headerlink" title="Correlation Analysis"></a>Correlation Analysis</h4><center><br>    <img src="/2019/01/06/数据分析与挖掘Exam/CorrelationAnalysis.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/CorrelationAnalysisExample.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/CorrelationBetweenTwoNumericalVariables.png" width="600"><br></center>

<h4 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h4><center><br>    <img src="/2019/01/06/数据分析与挖掘Exam/VarianceForSingleVariable.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/CovarianceForTwoVariables.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/Example_CalculationOfCovariance.png" width="600"><br></center>

<h4 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/Normalization.png" width="600"><br></center>

<h4 id="Simple-Discretization-Binning"><a href="#Simple-Discretization-Binning" class="headerlink" title="Simple Discretization: Binning"></a>Simple Discretization: Binning</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/Binning.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/BinningExample.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/BinningvsClustering.png" width="600"><br></center>

<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><h4 id="ID3-Decision-Tree：Information-Gain"><a href="#ID3-Decision-Tree：Information-Gain" class="headerlink" title="ID3 Decision Tree：Information Gain"></a>ID3 Decision Tree：Information Gain</h4><center><br>    <img src="/2019/01/06/数据分析与挖掘Exam/Example-AttributeSelection_withInformationGain.png" width="600"><br></center>

<h4 id="CART-Decision-Tree：-Gini-Index"><a href="#CART-Decision-Tree：-Gini-Index" class="headerlink" title="CART Decision Tree： Gini Index"></a>CART Decision Tree： Gini Index</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/ComputationOfGiniIndex.png" width="600"><br></center>

<h4 id="Classifier-Evaluation-Metrics-Example"><a href="#Classifier-Evaluation-Metrics-Example" class="headerlink" title="Classifier Evaluation Metrics: Example"></a>Classifier Evaluation Metrics: Example</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/ClassifierEvaluationMetrics.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/ClassifierEvaluationMetrics1.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/ClassifierEvaluationMetricsExample.png" width="600"><br></center>

<h4 id="Naïve-Bayesian-Classifier"><a href="#Naïve-Bayesian-Classifier" class="headerlink" title="Naïve Bayesian Classifier"></a>Naïve Bayesian Classifier</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/Naïve Bayesian Classifier0.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/Naïve Bayesian Classifier1.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/AvoidingtheZero-ProbabilityProblem.png" width="600"><br></center>

<h4 id="Bayesian-Belief-Network"><a href="#Bayesian-Belief-Network" class="headerlink" title="Bayesian Belief Network"></a>Bayesian Belief Network</h4><center><br>    <img src="/2019/01/06/数据分析与挖掘Exam/Bayesian Network1.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/Bayesian Network2.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/Bayesian Network3.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/Bayesian Network4.png" width="600"><br></center>

<h4 id="SVM-1"><a href="#SVM-1" class="headerlink" title="SVM"></a>SVM</h4><p>$\vec{w}=\sum_{i=1}^{N}\lambda_iy_i\vec{x_i} $        </p>
<p>$\lambda_i(y_i(\vec{w} * \vec{x_i}+b ))=0$</p>
<center><br><img src="/2019/01/06/数据分析与挖掘Exam/SVM_Example.png" width="600"><br></center>

<h3 id="Frequent-patterns-1"><a href="#Frequent-patterns-1" class="headerlink" title="Frequent patterns"></a>Frequent patterns</h3><h4 id="Association-Rules-supprt-amp-amp-confidence"><a href="#Association-Rules-supprt-amp-amp-confidence" class="headerlink" title="Association Rules: supprt &amp;&amp; confidence"></a>Association Rules: supprt &amp;&amp; confidence</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/AssociationRules.png" width="600"><br></center>

<h4 id="FP-growth-1"><a href="#FP-growth-1" class="headerlink" title="FP-growth"></a>FP-growth</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/FP-TreeConstruct.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/FP-TreeFindPatterns.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/FP-Tree_mConditional.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/MiningEachConditionalFP-tree.png" width="600"><br></center>

<h3 id="Clustering-1"><a href="#Clustering-1" class="headerlink" title="Clustering"></a>Clustering</h3><h4 id="Kernel-K-Means"><a href="#Kernel-K-Means" class="headerlink" title="Kernel K-Means"></a>Kernel K-Means</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/KernelK-Means.png" width="600"><br></center>

<h3 id="Community-detection"><a href="#Community-detection" class="headerlink" title="Community detection"></a>Community detection</h3><center><br>    <img src="/2019/01/06/数据分析与挖掘Exam/CPMExample1.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/CPMExample2.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/CPMExample3.png" width="600"><br></center>

<h4 id="Spectral-Clustering"><a href="#Spectral-Clustering" class="headerlink" title="Spectral Clustering"></a>Spectral Clustering</h4><center><br><img src="/2019/01/06/数据分析与挖掘Exam/SpectralClusteringExample1.png" width="600"><br><img src="/2019/01/06/数据分析与挖掘Exam/SpectralClusteringExample2.png" width="600"><br></center>

<h4 id="MCL-1"><a href="#MCL-1" class="headerlink" title="MCL"></a>MCL</h4><center><br>    <img src="/2019/01/06/数据分析与挖掘Exam/MarkovChains.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/MarkovChains2.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/WeightedGraphs.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/MCL_SelfLoop.png" width="600"><br>    <img src="/2019/01/06/数据分析与挖掘Exam/MCL_Inflation.png" width="600"><br></center>


<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><h3 id="Classification-1"><a href="#Classification-1" class="headerlink" title="Classification"></a>Classification</h3><h4 id="经典"><a href="#经典" class="headerlink" title="经典"></a>经典</h4><table>
<thead>
<tr>
<th style="text-align:center">算法</th>
<th style="text-align:center">关注点</th>
<th style="text-align:center">注意</th>
<th style="text-align:center">相似度量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Decision Tree</td>
<td style="text-align:center">选择属性的方式</td>
<td style="text-align:center">信息增益、信息增益比、基尼指数</td>
<td style="text-align:center">同左</td>
</tr>
<tr>
<td style="text-align:center">SVM</td>
<td style="text-align:center">核函数</td>
<td style="text-align:center">让离超平面比较近的点尽可能的远离超平面</td>
<td style="text-align:center">高维映射</td>
</tr>
<tr>
<td style="text-align:center">Bayes</td>
<td style="text-align:center">后验概率最大化</td>
<td style="text-align:center">若属性间存在依赖关联，使用贝叶斯置信网络</td>
<td style="text-align:center">概率</td>
</tr>
<tr>
<td style="text-align:center">ANN</td>
<td style="text-align:center">“黑盒”</td>
<td style="text-align:center">后向传播更新权重，优化模型</td>
</tr>
</tbody>
</table>
<h4 id="集成方法"><a href="#集成方法" class="headerlink" title="集成方法"></a>集成方法</h4><table>
<thead>
<tr>
<th style="text-align:center">算法</th>
<th style="text-align:center">关注点</th>
<th style="text-align:center">注意</th>
<th style="text-align:center">相似度量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Bagging</td>
<td style="text-align:center">平等”投票”</td>
<td style="text-align:center">票数高的预测即为最终预测结果</td>
<td style="text-align:center">多模型</td>
</tr>
<tr>
<td style="text-align:center">Boosting</td>
<td style="text-align:center">加权”投票”</td>
<td style="text-align:center">权重为模型准确率</td>
<td style="text-align:center">多模型</td>
</tr>
<tr>
<td style="text-align:center">Random Forest</td>
<td style="text-align:center">“决策树”的装袋</td>
<td style="text-align:center">随机选择属性(平等)、随机线性组合(权重)</td>
<td style="text-align:center">多模型</td>
</tr>
</tbody>
</table>
<h3 id="频繁模式"><a href="#频繁模式" class="headerlink" title="频繁模式"></a>频繁模式</h3><table>
<thead>
<tr>
<th style="text-align:center">算法</th>
<th style="text-align:center">关注点</th>
<th style="text-align:center">注意</th>
<th style="text-align:center">相似度量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Apriori</td>
<td style="text-align:center">候选消除算法</td>
<td style="text-align:center">两条定律</td>
<td style="text-align:center">最小支持度</td>
</tr>
<tr>
<td style="text-align:center">FP-growth</td>
<td style="text-align:center">扫描两遍数据记录即可</td>
<td style="text-align:center">构造FP-Tree,基于条件模式(前缀)递归构造FP树</td>
<td style="text-align:center">最小支持度</td>
</tr>
</tbody>
</table>
<h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><table>
<thead>
<tr>
<th style="text-align:center">算法</th>
<th style="text-align:center">关注点</th>
<th style="text-align:center">注意</th>
<th style="text-align:center">相似度量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">K-Means</td>
<td style="text-align:center">以”均值”更新聚类中心</td>
<td style="text-align:center">++:最远点、Medoid:中心点、Medians:中位数、Mode、Kernel</td>
<td style="text-align:center">距离</td>
</tr>
<tr>
<td style="text-align:center">BIRCH</td>
<td style="text-align:center">CF-Tree结点即聚类的簇</td>
<td style="text-align:center">利用层次方法的平衡迭代规约和聚类：使用CF树并逐步调整子集群的质量</td>
<td style="text-align:center">最大样本半径阈值T</td>
</tr>
<tr>
<td style="text-align:center">DBSCAN</td>
<td style="text-align:center">由密度可达关系导出的最大密度相连的样本集合，即簇</td>
<td style="text-align:center"></td>
<td style="text-align:center">(ϵ, MinPts)</td>
</tr>
<tr>
<td style="text-align:center">E-M algorithm</td>
<td style="text-align:center">”最大似然估计“</td>
<td style="text-align:center">K-Means （距离） -&gt;&gt; E-M algorithm (概率分布)</td>
<td style="text-align:center">概率</td>
</tr>
<tr>
<td style="text-align:center">AP算法</td>
<td style="text-align:center">不断更新每一个点的responsibility和availability</td>
<td style="text-align:center">产生m个高质量的exemplar后，指派剩余点</td>
<td style="text-align:center">归属度、吸引度</td>
</tr>
<tr>
<td style="text-align:center">Local density-based</td>
<td style="text-align:center">假设聚类中心周围都是密度比其低的点，同时这些点到该聚类中心的距离比其到其他聚类中心更近</td>
<td style="text-align:center">δ<em>{min}和ρ</em>{min}找到聚类中心后，指派剩余点</td>
<td style="text-align:center">密度、距离</td>
</tr>
</tbody>
</table>
<h3 id="图聚类"><a href="#图聚类" class="headerlink" title="图聚类"></a>图聚类</h3><table>
<thead>
<tr>
<th style="text-align:center">算法</th>
<th style="text-align:center">关注点</th>
<th style="text-align:center">注意</th>
<th style="text-align:center">相似度量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">CPM</td>
<td style="text-align:center">完全子图-&gt;重叠矩阵-&gt;社区邻接矩阵</td>
<td style="text-align:center">构造矩阵时完全子图皆为结点</td>
<td style="text-align:center">重叠结点</td>
</tr>
<tr>
<td style="text-align:center">谱聚类</td>
<td style="text-align:center">拉普拉斯特征映射（降维） + K-Means</td>
<td style="text-align:center">构造邻接矩阵的方式选择、Ratio-Cut/N-Cut</td>
<td style="text-align:center">距离</td>
</tr>
<tr>
<td style="text-align:center">G Nand Q</td>
<td style="text-align:center">删除边介数最大的边</td>
<td style="text-align:center">边介数:网络中任意两个节点通过此边的最短路径的数目</td>
<td style="text-align:center">边介数</td>
</tr>
<tr>
<td style="text-align:center">MCL</td>
<td style="text-align:center"><strong>Expansion</strong> 和 <strong>Inflation</strong> 将不断的交替进行</td>
<td style="text-align:center"><strong>Expansion</strong>：加强区域间的联系（随机游走）<br>Inflation：分化联系（强者恒强）</td>
<td style="text-align:center">概率</td>
</tr>
</tbody>
</table>
<h2 id="Todo"><a href="#Todo" class="headerlink" title="Todo"></a>Todo</h2><ul>
<li><p>看相关参考书目《数据挖掘：概念与技术》《数据挖掘导论》课后例题  着重看”简单计算”</p>
</li>
<li><p>Collaborative Filtering</p>
<p><a href="https://www.cnblogs.com/pinard/p/6349233.html" target="_blank" rel="noopener">协同过滤推荐算法总结</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6351319.html" target="_blank" rel="noopener">矩阵分解在协同过滤推荐算法中的应用</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6362647.html" target="_blank" rel="noopener">SimRank协同过滤推荐算法</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/pinard/p/6912636.html" target="_blank" rel="noopener">EM算法原理总结</a></p>
</li>
<li><p>特征工程</p>
<p><a href="https://www.cnblogs.com/pinard/p/9032759.html" target="_blank" rel="noopener">特征工程之特征选择</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/9061549.html" target="_blank" rel="noopener">特征工程之特征表达</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/9093890.html" target="_blank" rel="noopener">特征工程之特征预处理</a></p>
</li>
</ul>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2019-01-08T13:55:42.778Z" itemprop="dateUpdated">2019-01-08 21:55:42</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="CaptainSE">
            CaptainSE
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            

            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/01/06/数据分析与挖掘Exam/&title=《数据分析与挖掘Exam》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/01/06/数据分析与挖掘Exam/&title=《数据分析与挖掘Exam》 — Go Further&source=【阅读时间】XXX min XXX words【内容描述】《数据分析与挖掘》之后篇" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/01/06/数据分析与挖掘Exam/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《数据分析与挖掘Exam》 — Go Further&url=http://yoursite.com/2019/01/06/数据分析与挖掘Exam/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/01/06/数据分析与挖掘Exam/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/01/09/Pokemon-Model/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">Pokémon_Model</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2019/01/01/数据分析与挖掘/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">数据分析与挖掘</h4>
      </a>
    </div>
  
</nav>



    

















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢老板~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>CaptainSE &copy; 2015 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/01/06/数据分析与挖掘Exam/&title=《数据分析与挖掘Exam》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/01/06/数据分析与挖掘Exam/&title=《数据分析与挖掘Exam》 — Go Further&source=【阅读时间】XXX min XXX words【内容描述】《数据分析与挖掘》之后篇" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/01/06/数据分析与挖掘Exam/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《数据分析与挖掘Exam》 — Go Further&url=http://yoursite.com/2019/01/06/数据分析与挖掘Exam/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/01/06/数据分析与挖掘Exam/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACOklEQVR42u3awa7CMAxEUf7/p4v0Vk8qTWfGKSLOzQpVpeR0YezYr5e8jr91/vz/yvme87q6f3x92oIBA8ayjGO4xlsfU+fec7U3GDBg7MNQgqwbOt2tK6/v8joMGDBgmOmg+yKUJ8CAAQOGHnD1BM6tNGHAgAFjzNAPyLKfzBLNR2pxGDBgLMioNAae/vyl/gYMGDB+mHGYSy9N3bBb2hUMGDBaM/T2ZGWoItu0ewAHAwaMrows1I6DbCVMK+2HUmoIAwaMBRlKMHULTpen7MR4DgwYMLZhuGVqJUBnh3owYMDYjaEPNGTlq/600p0wYMBoynCDqVuOuoMUIQkGDBitGW4B+cTkg7QtN9GEAQNGa4a7IbfBqZSv9aIaBgwYnRjKtlySztaD7M0fAwwYMFoz3JahO0ZmdycKgRgGDBg7MGaVslkDQE80L18cDBgwmjKUcJbNblQKXeW7HzJcGDBgtGbogVVP5pREUw/EN68DBgwYrRlZ2WnnnnKzQX+a0dmAAQPG4gx96EFP1Oo8u5CGAQPGBoxs3MEdjKj8J9w8GQYMGK0Z2dfcUCs1IM0SOuxvwIABY0HGYS59i+7hWtZyuJxugwEDRiNGJdhl6aDe5qyPlMGAAaMTo3JwljUs9fakfmAHAwaMHRh6kH1izCsb9bg5boMBA8aWjCxY64f77tPCgAsDBowNGHbSVq6qpWANAwaMDRj6IVr9JE8anjAHMmDAgNGbMWFeQygy5x7YTWthwoAB49cZb3z3ddP+PTzSAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            clearTimeout(titleTime);
        } else {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
