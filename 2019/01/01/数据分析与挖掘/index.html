<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>数据分析与挖掘 | Go Further | Stay Hungry, Stay Foolish</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#142421">
    
    
    <meta name="keywords" content="">
    <meta name="description" content="【阅读时间】XXX min XXX words【内容描述】 基于《数据挖掘：概念与技术》的简述，欲了解细节，强烈建议读原书！！！">
<meta property="og:type" content="article">
<meta property="og:title" content="数据分析与挖掘">
<meta property="og:url" content="http://yoursite.com/2019/01/01/数据分析与挖掘/index.html">
<meta property="og:site_name" content="Go Further">
<meta property="og:description" content="【阅读时间】XXX min XXX words【内容描述】 基于《数据挖掘：概念与技术》的简述，欲了解细节，强烈建议读原书！！！">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/NormalDistributionCurve.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/Boxplot.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/ProximityMeasureforBinaryAttributes.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/Dissimilarity%20betweenAsymmetricBinaryVariables.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/CalculatingCosineSimilarity.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/dataPreprocess_mainTask.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/DataCubeAggregation.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/DataCompression.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/Normalization.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/ConceptHierarchyGeneration.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/AutomaticConceptHierarchyGeneration.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/Summary_datapreprocessing.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/SupervisedLearning.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/ANN_Architecture.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/BootstrapAggregation.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/ConfusionMatrix.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/classification_summary1.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/classification_summary2.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/classification_summary3.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/AssociationRules.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/Apriori.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/FP-TreeConstruct.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/FP-TreeFindPatterns.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/FP-Tree_mConditional.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/MiningEachConditionalFP-tree.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/fp_Summary.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/k-Means.gif">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/CFTree.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/DBSCAN_BasicConcept.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/LocalDensity-based.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/clustering_Summary.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/clustering_Summary2.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/CPM1.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/CPM2.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/SpectralClustering.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/graphMining_Summary.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/CommunityDetection_Summary.png">
<meta property="og:updated_time" content="2019-01-08T12:24:00.099Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="数据分析与挖掘">
<meta name="twitter:description" content="【阅读时间】XXX min XXX words【内容描述】 基于《数据挖掘：概念与技术》的简述，欲了解细节，强烈建议读原书！！！">
<meta name="twitter:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘/NormalDistributionCurve.png">
    
        <link rel="alternate" type="application/atom+xml" title="Go Further" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">CaptainSE</h5>
          <a href="mailto:841145636@qq.com" title="841145636@qq.com" class="mail">841145636@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/Captainzj" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">数据分析与挖掘</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">数据分析与挖掘</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-01-01T06:42:25.000Z" itemprop="datePublished" class="page-time">
  2019-01-01
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/XD/">XD</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Introduction"><span class="post-toc-number">1.</span> <span class="post-toc-text">Introduction</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Data"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">Data</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-types"><span class="post-toc-number">1.1.1.</span> <span class="post-toc-text">Data types</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-statistics"><span class="post-toc-number">1.1.2.</span> <span class="post-toc-text">Data statistics</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Data-dispersion-分散-characteristics"><span class="post-toc-number">1.1.2.1.</span> <span class="post-toc-text">Data dispersion(分散) characteristics</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Graphic-Displays-of-Basic-Statistical-Descriptions"><span class="post-toc-number">1.1.2.2.</span> <span class="post-toc-text">Graphic Displays of Basic Statistical Descriptions</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Distance-on-Numeric-Data"><span class="post-toc-number">1.1.2.3.</span> <span class="post-toc-text">Distance on Numeric Data</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Proximity-邻近-Measure-for-Binary-Attributes"><span class="post-toc-number">1.1.2.4.</span> <span class="post-toc-text">Proximity 邻近 Measure for Binary Attributes</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Data-preprocessing"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">Data preprocessing</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-cleaning"><span class="post-toc-number">1.2.1.</span> <span class="post-toc-text">Data cleaning</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#How-to-Handle-Missing-Data"><span class="post-toc-number">1.2.1.1.</span> <span class="post-toc-text">How to Handle Missing Data?</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#How-to-Handle-Noisy-Data"><span class="post-toc-number">1.2.1.2.</span> <span class="post-toc-text">How to Handle Noisy Data?</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-integrating"><span class="post-toc-number">1.2.2.</span> <span class="post-toc-text">Data integrating</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Handling-Redundancy-in-Data-Integration"><span class="post-toc-number">1.2.2.1.</span> <span class="post-toc-text">Handling Redundancy in Data Integration</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-Reduction"><span class="post-toc-number">1.2.3.</span> <span class="post-toc-text">Data Reduction</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Methods-for-data-reduction"><span class="post-toc-number">1.2.3.1.</span> <span class="post-toc-text">Methods for data reduction</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Dimensionality-Reduction-Techniques"><span class="post-toc-number">1.2.3.2.</span> <span class="post-toc-text">Dimensionality Reduction Techniques</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-transforming"><span class="post-toc-number">1.2.4.</span> <span class="post-toc-text">Data transforming</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Normalization-归一化"><span class="post-toc-number">1.2.4.1.</span> <span class="post-toc-text">Normalization 归一化</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Concept-hierarchy-generation-概念层次生成"><span class="post-toc-number">1.2.4.2.</span> <span class="post-toc-text">Concept hierarchy generation 概念层次生成</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Classification"><span class="post-toc-number">2.</span> <span class="post-toc-text">Classification</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Algorithms"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">Algorithms</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Decision-tree-ID3-C4-5-CART"><span class="post-toc-number">2.1.1.</span> <span class="post-toc-text">Decision tree-ID3,C4.5,CART</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#SVM"><span class="post-toc-number">2.1.2.</span> <span class="post-toc-text">SVM</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Bayes"><span class="post-toc-number">2.1.3.</span> <span class="post-toc-text">Bayes</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ANN"><span class="post-toc-number">2.1.4.</span> <span class="post-toc-text">ANN</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#DeepLearning"><span class="post-toc-number">2.1.4.1.</span> <span class="post-toc-text">DeepLearning</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Ensemble-methods"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">Ensemble methods</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Bagging-Bootstrap-Aggregation"><span class="post-toc-number">2.2.1.</span> <span class="post-toc-text">Bagging: Bootstrap Aggregation</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Boosting"><span class="post-toc-number">2.2.2.</span> <span class="post-toc-text">Boosting</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Random-Forest"><span class="post-toc-number">2.2.3.</span> <span class="post-toc-text">Random Forest</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Model-evaluation-and-selection"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">Model evaluation and selection</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Classifier-Evaluation-Metrics"><span class="post-toc-number">2.3.1.</span> <span class="post-toc-text">Classifier Evaluation Metrics</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Confusion-matrix-and-criteria"><span class="post-toc-number">2.3.1.1.</span> <span class="post-toc-text">Confusion matrix and criteria</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Estimating-a-classifier’s-accuracy"><span class="post-toc-number">2.3.2.</span> <span class="post-toc-text">Estimating a classifier’s accuracy</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Cross-evaluation"><span class="post-toc-number">2.3.2.1.</span> <span class="post-toc-text">Cross-evaluation</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Issues-Affecting-Model-Selection"><span class="post-toc-number">2.3.3.</span> <span class="post-toc-text">Issues Affecting Model Selection</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary-1"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Frequent-patterns"><span class="post-toc-number">3.</span> <span class="post-toc-text">Frequent patterns</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Association-rule"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">Association rule</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Algorithm"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">Algorithm</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Apriori"><span class="post-toc-number">3.2.1.</span> <span class="post-toc-text">Apriori</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#FP-growth"><span class="post-toc-number">3.2.2.</span> <span class="post-toc-text">FP-growth</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary-2"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Clustering"><span class="post-toc-number">4.</span> <span class="post-toc-text">Clustering</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Partition-based—k-means"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">Partition-based—k-means</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Hierarchical-based—two-ways"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">Hierarchical-based—two ways</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#BIRCH"><span class="post-toc-number">4.2.1.</span> <span class="post-toc-text">BIRCH</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CURE"><span class="post-toc-number">4.2.2.</span> <span class="post-toc-text">CURE</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CHAMELEON"><span class="post-toc-number">4.2.3.</span> <span class="post-toc-text">CHAMELEON</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Probabilistic-Hierarchical-Clustering"><span class="post-toc-number">4.2.4.</span> <span class="post-toc-text">Probabilistic Hierarchical Clustering</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Density-based—DBSCAN"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">Density-based—DBSCAN</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#DBSCAN"><span class="post-toc-number">4.3.1.</span> <span class="post-toc-text">DBSCAN</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#E-M-algorithm"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">E-M algorithm</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#AP-2007-Science"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">AP (2007, Science)</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Local-density-based-2014-Science"><span class="post-toc-number">4.6.</span> <span class="post-toc-text">Local density-based (2014, Science)</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Evaluation-of-Clustering"><span class="post-toc-number">4.7.</span> <span class="post-toc-text">Evaluation of Clustering</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Clustering-evaluation"><span class="post-toc-number">4.7.1.</span> <span class="post-toc-text">Clustering evaluation</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Three-categorization-of-measures"><span class="post-toc-number">4.7.1.1.</span> <span class="post-toc-text">Three categorization of measures</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Clustering-stability"><span class="post-toc-number">4.7.2.</span> <span class="post-toc-text">Clustering stability</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Methods-for-Finding-K-the-Number-of-Clusters"><span class="post-toc-number">4.7.2.1.</span> <span class="post-toc-text">Methods for Finding K, the Number of Clusters</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Clustering-tendency"><span class="post-toc-number">4.7.3.</span> <span class="post-toc-text">Clustering tendency</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary-3"><span class="post-toc-number">4.8.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Graph-clustering"><span class="post-toc-number">5.</span> <span class="post-toc-text">Graph clustering</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Community-detection-algorithms"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">Community detection_algorithms</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#CPM-（Clique-Percolation-Method）"><span class="post-toc-number">5.1.1.</span> <span class="post-toc-text">CPM （Clique Percolation Method）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Spectral-clustering"><span class="post-toc-number">5.1.2.</span> <span class="post-toc-text">Spectral clustering</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Modularity-based-methods-–-G-Nand-Q"><span class="post-toc-number">5.1.3.</span> <span class="post-toc-text">Modularity based methods – G Nand Q</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#MCL"><span class="post-toc-number">5.1.4.</span> <span class="post-toc-text">MCL</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Random-walk"><span class="post-toc-number">5.1.4.1.</span> <span class="post-toc-text">Random walk</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Markov-chains"><span class="post-toc-number">5.1.4.2.</span> <span class="post-toc-text">Markov chains</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#MCL-Algorithm"><span class="post-toc-number">5.1.4.3.</span> <span class="post-toc-text">MCL Algorithm</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary-4"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Todo"><span class="post-toc-number">6.</span> <span class="post-toc-text">Todo</span></a></li></ol>
        </nav>
    </aside>


<article id="post-数据分析与挖掘"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">数据分析与挖掘</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-01-01 14:42:25" datetime="2019-01-01T06:42:25.000Z"  itemprop="datePublished">2019-01-01</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/XD/">XD</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>【阅读时间】XXX min XXX words<br>【内容描述】 基于《数据挖掘：概念与技术》的简述，欲了解细节，强烈建议读原书！！！</p>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><h4 id="Data-types"><a href="#Data-types" class="headerlink" title="Data types"></a>Data types</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">* <span class="string">'Relational records'</span>  关系记录</span><br><span class="line">  - Relational tables, highly structured 关系表，高度结构化</span><br><span class="line">* <span class="string">'Data matrix'</span>, e.g., numerical matrix, crosstabs 数据矩阵，例如数值矩阵，交叉表</span><br><span class="line">* <span class="string">'Transaction data'</span> 交易数据</span><br><span class="line">* <span class="string">'Document data'</span>: Term-frequency vector (matrix) of text documents</span><br><span class="line">  文档数据：文本文档的术语 - 频率向量（矩阵）</span><br><span class="line">* <span class="string">'Transportation network'</span> 交通网络</span><br><span class="line">* <span class="string">'World Wide Web'</span> 万维网</span><br><span class="line">* <span class="string">'Molecular Structures'</span> 分子结构</span><br><span class="line">* <span class="string">'Social or information networks'</span> 社交或信息网络</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>Attribute Types</strong> </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">* <span class="string">'Nominal'</span>: categories, states, <span class="keyword">or</span> “names of things” <span class="comment"># 标称属性: 类别，状态或“事物名称”</span></span><br><span class="line">	- Hair_color = &#123;auburn, black, blond, brown, grey, red, white&#125;</span><br><span class="line">	- marital status, occupation, ID numbers, zip codes 婚姻状况，职业，身份证号码，邮政编码</span><br><span class="line">* <span class="string">'Binary'</span> <span class="comment"># 二元属性</span></span><br><span class="line">	- Nominal attribute <span class="keyword">with</span> only <span class="number">2</span> states (<span class="number">0</span> <span class="keyword">and</span> <span class="number">1</span>) 仅有<span class="number">2</span>个状态（<span class="number">0</span>和<span class="number">1</span>）的标称属性</span><br><span class="line">	- Symmetric binary: both outcomes equally important.  e.g., gender</span><br><span class="line">	  对称二元：两种结果同样重要. 例如，性别</span><br><span class="line">	- Asymmetric binary: outcomes <span class="keyword">not</span> equally important.  e.g., medical test (positive vs. negative)</span><br><span class="line">	  不对称二元：结果不是同等重要的.  例如，医学检验（正面与负面）</span><br><span class="line">		- Convention: assign <span class="number">1</span> to most important outcome (e.g., HIV positive)</span><br><span class="line">		  公约：为最重要的结果指定<span class="number">1</span>  （例如艾滋病毒阳性）</span><br><span class="line">* <span class="string">'Ordinal'</span>  <span class="comment"># 序数属性</span></span><br><span class="line">	- Values have a meaningful order (ranking) but magnitude between successive values <span class="keyword">is</span> <span class="keyword">not</span> known</span><br><span class="line">	  值具有有意义的顺序（排名），但连续值之间的大小未知</span><br><span class="line">	- Size = &#123;small, medium, large&#125;, grades, army rankings</span><br><span class="line">	  大小= &#123;小，中，大&#125;，成绩，军队排名</span><br><span class="line">* <span class="string">'Numeric'</span> <span class="comment"># 数值属性</span></span><br><span class="line"> 	- `Interval` 间隔</span><br><span class="line">		- Measured on a scale of equal-sized units  按相同大小的单位测量</span><br><span class="line">		- Values have order 数值有序 E.g., temperature <span class="keyword">in</span> C˚<span class="keyword">or</span> F˚, calendar dates</span><br><span class="line">		- No true zero-point 无真正<span class="string">"零点"</span></span><br><span class="line">	- `Ratio` 比率(倍数)</span><br><span class="line">		- Inherent zero-point 固有零点</span><br><span class="line">		- We can speak of values <span class="keyword">as</span> being an order of magnitude larger than the unit of measurement (<span class="number">10</span> K˚ <span class="keyword">is</span> twice <span class="keyword">as</span> high <span class="keyword">as</span> <span class="number">5</span> K˚).</span><br><span class="line">		  我们可以说价值比测量单位大一个数量级（<span class="number">10</span>K˚是<span class="number">5</span>K˚的两倍）</span><br><span class="line">			- e.g., temperature <span class="keyword">in</span> Kelvin, length, counts, monetary quantities</span><br><span class="line">			  例如，以开尔文为单位的温度，长度，计数，货币数量</span><br><span class="line">* <span class="string">'Discrete Attribute'</span>  <span class="comment"># 离散属性</span></span><br><span class="line">	- Has only a finite <span class="keyword">or</span> countably infinite set of values 只有一组有限或可数无限的值</span><br><span class="line">		- E.g., zip codes, profession, <span class="keyword">or</span> the set of words <span class="keyword">in</span> a collection of documents </span><br><span class="line">		  例如，邮政编码，专业或文档集合中的单词集</span><br><span class="line">	- Sometimes, represented <span class="keyword">as</span> integer variables 有时，表示为`整数变量`</span><br><span class="line">	- Note: Binary attributes are a special case of discrete attributes </span><br><span class="line">	  注意：二进制属性是离散属性的特例</span><br><span class="line">* <span class="string">'Continuous Attribute'</span>  <span class="comment"># 连续属性</span></span><br><span class="line">	- Has real numbers <span class="keyword">as</span> attribute values  将实数作为属性值</span><br><span class="line">		- E.g., temperature, height, <span class="keyword">or</span> weight  例如，温度，高度或重量</span><br><span class="line">	- Practically, real values can only be measured <span class="keyword">and</span> represented using a finite number of digits</span><br><span class="line">	  实际上，只能使用有限数字来测量和表示实际值</span><br><span class="line">	- Continuous attributes are typically represented <span class="keyword">as</span> floating-point variables</span><br><span class="line">	  连续属性通常表示为`浮点变量`</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Data-statistics"><a href="#Data-statistics" class="headerlink" title="Data statistics"></a>Data statistics</h4><h5 id="Data-dispersion-分散-characteristics"><a href="#Data-dispersion-分散-characteristics" class="headerlink" title="Data dispersion(分散) characteristics"></a>Data dispersion(分散) characteristics</h5><ul>
<li><p><strong>Mean</strong></p>
<ul>
<li><p>Mean (<code>algebraic measure</code>) (sample vs. population):</p>
<p>Note: $n$ is sample size and $N$ is population size. </p>
<p>$\underbrace{\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i}_\text{sample}$    </p>
<p>$\underbrace{\mu  = \frac{1}{N}\sum_{i=1}^{n}x_i}_\text{population}$</p>
</li>
<li><p>Weighted arithmetic mean: 算术平均数/<code>加权平均数</code></p>
<p>$\bar{x}=\frac{\sum_{i=1}^{n}w_ix}{\sum_{i=1}^{n}w_i}$</p>
</li>
</ul>
</li>
<li><p><strong>Median</strong></p>
<p>Middle value if odd number of values, or average of the middle two values otherwise</p>
</li>
<li><p><strong>Mode</strong><br>Value that occurs <code>most frequently</code> in the data</p>
</li>
<li><p><strong>Properties of Normal Distribution Curve</strong></p>
<center><br>    <img src="/2019/01/01/数据分析与挖掘/NormalDistributionCurve.png" style="zoom:40%"><br></center>
</li>
<li><p><strong>Variance and Standard Deviation (sample: s, population: σ)</strong></p>
<ul>
<li><p><strong>Variance</strong>: (algebraic, scalable computation)</p>
<p>$s^2=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-x)^2=\frac{1}{n-1}[\sum_{i=1}^{n}{x_i}^2-\frac{1}{n}(\sum_{i=1}^{n}x_i)^2]$</p>
<p>${\sigma}^2 =\frac{1}{N}\sum_{i=1}^{n}(x_i-\mu)^2 = \frac{1}{N}\sum_{i=1}^{n}{x_i}^2-{\mu}^2 $</p>
</li>
<li><p><strong>Standard deviation</strong> $s$ (or $σ$) is the square root of variance $s^2$ (or $σ^2$)</p>
<p>$s = \sqrt{s^2}$</p>
<p>$\sigma = \sqrt{\sigma^2} $</p>
</li>
</ul>
</li>
<li><p><strong>standardized measure (z-score)</strong></p>
<p>$z=\frac{x-\mu}{\sigma}$</p>
</li>
</ul>
<h5 id="Graphic-Displays-of-Basic-Statistical-Descriptions"><a href="#Graphic-Displays-of-Basic-Statistical-Descriptions" class="headerlink" title="Graphic Displays of Basic Statistical Descriptions"></a>Graphic Displays of Basic Statistical Descriptions</h5><ul>
<li><p><strong>Boxplot:</strong> graphic display of five-number summary  箱线图</p>
<div style="width:1000px;margin:0;padding:0"><br>    <div style="float:left;width:200px;"><img src="/2019/01/01/数据分析与挖掘/Boxplot.png"></div><br>    <div style="float:right;width:800px;padding:14px"><br>        &bull; <b>Quartiles</b>: Q1 (25th percentile), Q3 (75th percentile)<br><br>        &bull; <b>Inter-quartile range</b>: IQR = Q3 – Q1 <br><br>        &bull; <b>Five number summary</b>: min, Q1, median, Q3, max<br><br>        &bull; <b>Boxplot</b>: Data is represented with a box<br><br>        &emsp; &bull; <b>Q1, Q3, IQR</b>:  The ends of the box are at the first and third quartiles, i.e., the height of the box is IQR<br><br>        &emsp; &bull; <b>Median (Q2)</b> is marked by a line within the box <br><br>        &emsp; &bull; <b>Whiskers</b>: two lines outside the box extended to Minimum and Maximum<br><br>    </div><br>    <div style="clear:both"></div><br></div>
</li>
<li><p><strong>Histogram:</strong> x-axis are values, y-axis repres. frequencies 柱状图/直方图</p>
</li>
<li><p><strong>Quantile plot:</strong>  each value $x_i$  is paired with $f_i$  indicating that approximately $100 f_i \%$ of data  are ​$\leq  x_i$ 分位图</p>
</li>
<li><p><strong>Quantile-quantile (q-q) plot:</strong> graphs the quantiles of one univariant distribution against the corresponding quantiles of another 绘制一个单变量分布的分位数与另一个分配的相应分位数的关系图.QQPlot图是用于直观验证一组数据是否来自某个分布，或者验证某两组数据是否来自同一（族）分布。在教学和软件中<code>常用的是检验数据是否来自于正态分布</code>。</p>
</li>
<li><p><strong>Scatter plot:</strong> each pair of values is a pair of coordinates and plotted as points in the plane 每对值是一对坐标并绘制为平面中的点 </p>
</li>
</ul>
<h5 id="Distance-on-Numeric-Data"><a href="#Distance-on-Numeric-Data" class="headerlink" title="Distance on Numeric Data"></a>Distance on Numeric Data</h5><p><strong>Dissimilarity (distance) matrix</strong>:  Usually symmetric, thus a <code>triangular matrix</code><br>$$<br>\begin{pmatrix}<br>0 &amp;  &amp;  &amp; \<br>d(2,1) &amp; 0  &amp;  &amp; \<br>… &amp; … &amp; … &amp; \<br>d(n,1) &amp; d(n,2)  &amp; … &amp; 0<br>\end{pmatrix}<br>$$</p>
<ul>
<li><p><strong>Minkowski distance</strong><br>$$<br>d(i,j)=\sqrt[p]{\left | x_{i1}-x_{j1} \right |^p+\left | x_{i2}-x_{j2} \right |^p+……+\left | x_{il}-x_{jl} \right |^p}<br>$$</p>
</li>
<li><p>$p = 1: (L_1 norm)$ <strong>Manhattan (or city block) distance</strong></p>
<p>E.g.,the Hamming distance: the number of bits that are different between two binary<br>vectors<br>$$<br>d(i,j)=\left | x_{i1}-x_{j1} \right |+\left | x_{i2}-x_{j2} \right |+……+\left | x_{il}-x_{jl} \right |<br>$$</p>
</li>
<li><p>$p = 2:  (L_2 norm)$ <strong>Euclidean distance</strong><br>$$<br>d(i,j)=\sqrt{\left | x_{i1}-x_{j1} \right |^2+\left | x_{i2}-x_{j2} \right |^2+……+\left | x_{il}-x_{jl} \right |^2}<br>$$</p>
</li>
<li><p>$p→ ∞: (L_{max} norm,L_∞ norm) $<strong>“supremum” distance</strong></p>
<p>The maximum difference between any component (attribute) of the vectors<br>$$<br>d(i,j)=\lim_{p→ ∞}\sqrt[p]{\left | x_{i1}-x_{j1} \right |^p+\left | x_{i2}-x_{j2} \right |^p+……+\left | x_{il}-x_{jl} \right |^p}=\max_{f=1}^{l}\left|x_{if}-x_{jf}\right|<br>$$</p>
</li>
</ul>
<h5 id="Proximity-邻近-Measure-for-Binary-Attributes"><a href="#Proximity-邻近-Measure-for-Binary-Attributes" class="headerlink" title="Proximity 邻近 Measure for Binary Attributes"></a>Proximity 邻近 Measure for Binary Attributes</h5><center><br>    <img src="/2019/01/01/数据分析与挖掘/ProximityMeasureforBinaryAttributes.png" width="600"><br>    <img src="/2019/01/01/数据分析与挖掘/Dissimilarity betweenAsymmetricBinaryVariables.png" width="600"><br>    <img src="/2019/01/01/数据分析与挖掘/CalculatingCosineSimilarity.png" width="600"><br></center>

<h3 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h3><center><br><img src="/2019/01/01/数据分析与挖掘/dataPreprocess_mainTask.png" width="600"><br></center>

<h4 id="Data-cleaning"><a href="#Data-cleaning" class="headerlink" title="Data cleaning"></a>Data cleaning</h4><ul>
<li><p>Handle <code>missing data</code>(Incomplete), smooth <code>noisy data</code>,identify or remove <code>outliers</code>, and resolve <code>inconsistencies</code> 处理丢失的数据，平滑噪声数据，识别或删除异常值，并解决不一致问题</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">* <span class="string">'Data discrepancy detection'</span>  数据差异检测</span><br><span class="line">    - Use metadata (e.g., domain, range, dependency, distribution) </span><br><span class="line">      使用元数据（例如，域，范围，依赖关系，分发）</span><br><span class="line">    - Check field overloading  检查字段重载</span><br><span class="line">    - Check uniqueness rule, consecutive rule <span class="keyword">and</span> null rule</span><br><span class="line">      检查唯一性规则，连续规则和空规则</span><br><span class="line">    - Use commercial tools 使用商业工具</span><br><span class="line">      - Data scrubbing: use simple domain knowledge (e.g., postal code, spell-check) to detect errors <span class="keyword">and</span> make corrections </span><br><span class="line">        数据清理：使用简单的域知识（例如，邮政编码，拼写检查）来检测错误并进行更正</span><br><span class="line">      - Data auditing: by analyzing data to discover rules <span class="keyword">and</span> relationship to detect violators (e.g., correlation <span class="keyword">and</span> clustering to find outliers)  </span><br><span class="line">        数据审计：通过分析数据来发现规则和检测违规者的关系（例如，关联和聚类以查找异常值）</span><br><span class="line"></span><br><span class="line">* <span class="string">'Data migration and integration'</span> 数据迁移和集成</span><br><span class="line">  - Data migration tools: allow transformations to be specified 数据迁移工具：允许指定转换</span><br><span class="line">  - ETL (Extraction/Transformation/Loading) tools: allow users to specify transformations through a graphical user interface  </span><br><span class="line">    ETL（提取/转换/加载）工具：允许用户通过图形用户界面指定转换</span><br><span class="line"></span><br><span class="line">* <span class="string">'Integration of the two processes'</span> </span><br><span class="line">  - Iterative <span class="keyword">and</span> interactive (e.g., Potter’s Wheels)  迭代和互动（例如，波特的轮子）</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="How-to-Handle-Missing-Data"><a href="#How-to-Handle-Missing-Data" class="headerlink" title="How to Handle Missing Data?"></a>How to Handle Missing Data?</h5><blockquote>
<p>1.忽略元组  2.手动填充  3. 自动（以unknown/均值/最可能的值）填充</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">* <span class="string">'Ignore the tuple'</span>: usually done when <span class="class"><span class="keyword">class</span> <span class="title">label</span> <span class="title">is</span> <span class="title">missing</span> <span class="params">(when doing classification)</span>—<span class="title">not</span> <span class="title">effective</span> <span class="title">when</span> <span class="title">the</span> % <span class="title">of</span> <span class="title">missing</span> <span class="title">values</span> <span class="title">per</span> <span class="title">attribute</span> <span class="title">varies</span> <span class="title">considerably</span> </span></span><br><span class="line"><span class="class">  忽略元组：通常在缺少类标签时（进行分类时）完成 - 当每个属性的缺失值百分比变化很大时<span class="params">(该属性)</span>无效</span></span><br><span class="line"><span class="class"></span></span><br><span class="line">* Fill in the missing value 'manually': tedious + infeasible? </span><br><span class="line">  手动填写缺失值：单调乏味+不可行？</span><br><span class="line"></span><br><span class="line">* Fill <span class="keyword">in</span> it <span class="string">'automatically'</span> <span class="keyword">with</span></span><br><span class="line">  - a <span class="keyword">global</span> constant : e.g., “unknown”, a new <span class="class"><span class="keyword">class</span>?! </span></span><br><span class="line"><span class="class">  - <span class="title">the</span> <span class="title">attribute</span> <span class="title">mean</span>  属性平均值<span class="params">(与下一条的不同？)</span></span></span><br><span class="line"><span class="class">  - <span class="title">the</span> <span class="title">attribute</span> <span class="title">mean</span> <span class="title">for</span> <span class="title">all</span> <span class="title">samples</span> <span class="title">belonging</span> <span class="title">to</span> <span class="title">the</span> <span class="title">same</span> <span class="title">class</span>:</span> <span class="string">'smarter'</span></span><br><span class="line">  - the most probable value: inference-based such <span class="keyword">as</span> Bayesian formula <span class="keyword">or</span> decision tree</span><br></pre></td></tr></table></figure>
<h5 id="How-to-Handle-Noisy-Data"><a href="#How-to-Handle-Noisy-Data" class="headerlink" title="How to Handle Noisy Data?"></a>How to Handle Noisy Data?</h5><blockquote>
<p>1.分档  2.回归 3.聚类（无监督） 4.半监督</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">* <span class="string">'Binning'</span> 分档</span><br><span class="line">  - First sort data <span class="keyword">and</span> partition into (equal-frequency) bins 首先将数据排序并分区为（等频）箱</span><br><span class="line">  - Then one can smooth by bin means, smooth by bin median, smooth by bin boundaries, etc.然后，可以通过bin均值平滑，通过bin中值平滑，通过bin边界平滑等。</span><br><span class="line">* <span class="string">'Regression'</span> 回归</span><br><span class="line">  - Smooth by fitting the data into regression functions 通过将数据拟合到回归函数中来平滑</span><br><span class="line">* <span class="string">'Clustering'</span> 聚类</span><br><span class="line">  - Detect <span class="keyword">and</span> remove outliers 检测并删除异常值</span><br><span class="line">* <span class="string">'Semi-supervised'</span>: Combined computer <span class="keyword">and</span> human inspection 半监督：计算机和人工检查相结合</span><br><span class="line">  - Detect suspicious values <span class="keyword">and</span> check by human (e.g., deal <span class="keyword">with</span> possible outliers) 检测可疑值并由人查验（例如，处理可能的异常值）</span><br></pre></td></tr></table></figure>
<h4 id="Data-integrating"><a href="#Data-integrating" class="headerlink" title="Data integrating"></a>Data integrating</h4><p>Integration of multiple databases, data cubes, or files 集成多个数据库，数据立方体或文件</p>
<blockquote>
<p>1.数据集成 2.模式集成 3.实体识别 4.检测和解决数据值的冲突</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'Data integration'</span></span><br><span class="line">  - Combining data <span class="keyword">from</span> multiple sources into a coherent store </span><br><span class="line">    将来自多个来源的数据组合到一个连贯的存储中</span><br><span class="line">- <span class="string">'Schema integration'</span>: e.g., A.cust-id \equiv  B.cust-<span class="comment">#  模式集成</span></span><br><span class="line">  - Integrate metadata <span class="keyword">from</span> different sources </span><br><span class="line">	集成来自不同来源的元数据</span><br><span class="line">- <span class="string">'Entity identification'</span> 实体识别</span><br><span class="line">  - Identify real world entities <span class="keyword">from</span> multiple data sources, e.g., Bill Clinton = William Clinton  </span><br><span class="line">    从多个数据源中识别真实世界的实体，例如Bill Clinton = William Clinton</span><br><span class="line">- <span class="string">'Detecting and resolving data value conflicts'</span>  检测和解决数据值冲突</span><br><span class="line">  - For the same real world entity, attribute values <span class="keyword">from</span> different sources are different </span><br><span class="line">    对于相同的现实世界实体，来自不同来源的属性值是不同的</span><br><span class="line">  - Possible reasons: different representations, different scales, e.g., metric vs. British units </span><br><span class="line">    可能的原因：不同的表示，不同的比例，例如，公制与英制单位</span><br></pre></td></tr></table></figure>
<h5 id="Handling-Redundancy-in-Data-Integration"><a href="#Handling-Redundancy-in-Data-Integration" class="headerlink" title="Handling Redundancy in Data Integration"></a>Handling Redundancy in Data Integration</h5><blockquote>
<p>1.冗余原因（对象标识、派生数据）2.检测手段（相关性和协方差分析）3.仔细整合</p>
</blockquote>
<ul>
<li><p>Redundant data occur often when integration of multiple databases 当多个数据库集成时，通常会出现冗余数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'Object identification'</span>:  The same attribute <span class="keyword">or</span> object may have different names <span class="keyword">in</span> different databases </span><br><span class="line">  对象标识：相同的属性或对象在不同的数据库中可能具有不同的名称</span><br><span class="line">- <span class="string">'Derivable data:'</span> One attribute may be a “derived” attribute <span class="keyword">in</span> another table, e.g., annual revenue </span><br><span class="line">  派生数据：一个属性可以是另一个表中的“派生”属性，例如年收入</span><br></pre></td></tr></table></figure>
</li>
<li><p>Redundant attributes may be able to be detected by <a href="#Correlation Analysis">correlation analysis</a> and <a href="#Covariance">covariance analysis</a> 可以通过相关性分析和协方差分析来检测冗余属性</p>
</li>
<li><p>Careful integration of the data from multiple sources may help reduce/avoid redundancies and inconsistencies and improve mining speed and quality  仔细整合来自多个来源的数据可能有助于减少/避免冗余和不一致，并提高”采矿“速度和质量</p>
</li>
</ul>
<h4 id="Data-Reduction"><a href="#Data-Reduction" class="headerlink" title="Data Reduction"></a>Data Reduction</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'Data reduction'</span> 数据压缩</span><br><span class="line">  - Obtain a reduced representation of the data set  获得数据集的缩减表示</span><br><span class="line">    - much smaller <span class="keyword">in</span> volume but yet produces almost the same analytical results </span><br><span class="line">      体积小得多，但产生几乎相同的分析结果</span><br><span class="line">- <span class="string">'Why data reduction ?'</span>—A database/data warehouse(仓库) may store terabytes of data</span><br><span class="line">  - Complex analysis may take a very long time to run on the complete data set </span><br><span class="line">    复杂分析可能需要很长时间才能在完整数据集上运行</span><br></pre></td></tr></table></figure>
<h5 id="Methods-for-data-reduction"><a href="#Methods-for-data-reduction" class="headerlink" title="Methods for data reduction"></a><strong>Methods for data reduction</strong></h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'Regression and Log-Linear Models'</span>  （Parametric methods）</span><br><span class="line">- <span class="string">'Histograms, clustering, sampling'</span>  （Non-parametric methods）</span><br><span class="line">	&gt; sampling: 选择具有代表性的子集；简单随机、放回、不放回、分层抽样</span><br><span class="line">- <span class="string">'Data cube aggregation'</span> 数据立方体聚合</span><br><span class="line">- <span class="string">'Data Compression'</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>Data cube aggregation 数据立方体聚合</p>
<p>The aggregated data for <strong>an individual entity of interest</strong>  感兴趣的实体聚合数据</p>
<center><br><img src="/2019/01/01/数据分析与挖掘/DataCubeAggregation.png" style="zoom:40%"><br></center>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- Demographic Data &apos;人口统计学数据&apos;（Service、Age、Gender、Job level、Workforce Segment）</span><br><span class="line">- Organisational process data &apos;组织行为处理数据&apos;（Performance rating、Potential rating、Salary increases、Turnover、in training &amp; development）</span><br><span class="line">- Predictive attitudinal data &apos;预测态度数据&apos;（Competencies能力、Intention to stay、AffectIve commitment、Job satisfaction、Discretionary自动支配 effort）</span><br></pre></td></tr></table></figure>
</li>
<li><p>Data compression</p>
<center><br><img src="/2019/01/01/数据分析与挖掘/DataCompression.png" style="zoom:30%"><br></center>

</li>
</ul>
<h5 id="Dimensionality-Reduction-Techniques"><a href="#Dimensionality-Reduction-Techniques" class="headerlink" title="Dimensionality Reduction Techniques"></a>Dimensionality Reduction Techniques</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Dimensionality reduction methodologies</span></span><br><span class="line">- `Feature selection`: Find a subset of the original variables (<span class="keyword">or</span> features, attributes)    找寻合适子集(仅收集与分析任务相关的属性)</span><br><span class="line">- `Feature extraction`: Transform the data <span class="keyword">in</span> the high-dimensional space to a space of fewer dimensions  </span><br><span class="line">    高维映射至低维(降维)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Some typical dimensionality methods</span></span><br><span class="line">* Principal Component Analysis (<span class="string">'PCA'</span>) </span><br><span class="line">    - A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components </span><br><span class="line">    一种统计过程，它使用<span class="string">'正交变换'</span>将可能相关变量的一组观察值转换为一组称为主成分的线性不相关变量值</span><br><span class="line">    - The original data are projected onto a much smaller space, resulting <span class="keyword">in</span> dimensionality reduction </span><br><span class="line">    将原始数据投影到更小的空间，从而减少维数 (Feature extraction 降维)</span><br><span class="line">    - Method:  Find the eigenvectors of the covariance matrix, <span class="keyword">and</span> these eigenvectors define the new space </span><br><span class="line">    找到协方差矩阵的特征向量，这些特征向量定义新的空间</span><br><span class="line"></span><br><span class="line">* <span class="string">'Supervised and nonlinear techniques'</span></span><br><span class="line">- `Feature subset selection`</span><br><span class="line">    - Best combined attribute selection(Best step-wise feature selection) <span class="keyword">and</span> elimination(Repeatedly eliminate the worst attribute)</span><br><span class="line">- `Feature creation`</span><br><span class="line">    - Create new attributes (features) that can capture the important information <span class="keyword">in</span> a data set more effectively than the original ones </span><br><span class="line">    创建新属性（功能），可以比原始信息更有效地捕获数据集中的重要信息   (比如，从成绩单中得出平均分/绩点)</span><br><span class="line">    - <span class="string">'Three general methodologies'</span></span><br><span class="line">    - `Attribute extraction` 降维</span><br><span class="line">        - Domain-specific</span><br><span class="line">        - Mapping data to new space (see: data reduction)</span><br><span class="line">        E.g., Fourier transformation, wavelet transformation, manifold approaches (<span class="keyword">not</span> covered)</span><br><span class="line">    - `Attribute construction` </span><br><span class="line">        - Combining features (see: discriminative frequent patterns <span class="keyword">in</span> Chapter on “Advanced Classification”)</span><br><span class="line">        - Data discretization</span><br></pre></td></tr></table></figure>
<h4 id="Data-transforming"><a href="#Data-transforming" class="headerlink" title="Data transforming"></a>Data transforming</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- A function that maps the entire set of values of a given attribute to a new set of replacement values s.t. each old value can be identified <span class="keyword">with</span> one of the new values  </span><br><span class="line">一种函数，它将给定属性的整个值集映射到一组新的替换值 使得 可以使用其中一个新值标识每个旧值</span><br><span class="line"></span><br><span class="line">- Methods</span><br><span class="line">  - <span class="string">'Smoothing'</span>: Remove noise <span class="keyword">from</span> data</span><br><span class="line">  - <span class="string">'Attribute/feature construction'</span>  属性/特征构建</span><br><span class="line">    - New attributes constructed <span class="keyword">from</span> the given ones</span><br><span class="line">  - <span class="string">'Aggregation'</span>: Summarization, data cube construction</span><br><span class="line">  - <span class="string">'Normalization'</span>: Scaled to fall within a smaller, specified range</span><br><span class="line">    - min-max normalization</span><br><span class="line">    - z-score normalization</span><br><span class="line">    - normalization by decimal scaling</span><br><span class="line">  - <span class="string">'Discretization'</span> 离散化: Concept hierarchy climbing</span><br></pre></td></tr></table></figure>
<h5 id="Normalization-归一化"><a href="#Normalization-归一化" class="headerlink" title="Normalization 归一化"></a>Normalization 归一化</h5><center><br><img src="/2019/01/01/数据分析与挖掘/Normalization.png" width="600"><br></center>

<h5 id="Concept-hierarchy-generation-概念层次生成"><a href="#Concept-hierarchy-generation-概念层次生成" class="headerlink" title="Concept hierarchy generation 概念层次生成"></a>Concept hierarchy generation 概念层次生成</h5><center><br><img src="/2019/01/01/数据分析与挖掘/ConceptHierarchyGeneration.png" width="600"><br></center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Data Discretization Methods</span></span><br><span class="line">- <span class="string">'Binning'</span> </span><br><span class="line">	- Top-down split, unsupervised</span><br><span class="line">- <span class="string">'Histogram'</span> analysis</span><br><span class="line">	- Top-down split, unsupervised</span><br><span class="line">- <span class="string">'Clustering'</span> analysis </span><br><span class="line">	- Unsupervised, top-down split <span class="keyword">or</span> bottom-up merge</span><br><span class="line">- <span class="string">'Decision-tree'</span> analysis</span><br><span class="line">	- Supervised, top-down split</span><br><span class="line">- <span class="string">'Correlation'</span> (e.g., χ<span class="number">2</span>) analysis </span><br><span class="line">	- Unsupervised, bottom-up merge</span><br><span class="line">- Note: All the methods can be applied `recursively`</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Concept hierarchy organizes concepts (i.e., attribute values) hierarchically <span class="keyword">and</span> <span class="keyword">is</span> usually associated <span class="keyword">with</span> each dimension <span class="keyword">in</span> a data warehouse </span><br><span class="line">  概念层次结构按层次组织概念（例如属性值），并且通常与数据仓库中的每个维度相关联</span><br><span class="line">- Concept hierarchies facilitate drilling <span class="keyword">and</span> rolling <span class="keyword">in</span> data warehouses to view data <span class="keyword">in</span> multiple granularity </span><br><span class="line">  概念层次结构有助于在数据仓库中钻取和滚动，以多种粒度查看数据</span><br><span class="line">- Concept hierarchy formation: Recursively reduce the data by collecting <span class="keyword">and</span> replacing low level concepts (such <span class="keyword">as</span> numeric values <span class="keyword">for</span> age) by higher level concepts (such <span class="keyword">as</span> youth, adult, <span class="keyword">or</span> senior) </span><br><span class="line">   概念层次结构：通过收集和替换更高级别概念（例如青年，成人或高级）的低级概念（例如年龄的数字值）来递归地减少数据</span><br><span class="line">- Concept hierarchies can be explicitly specified by domain experts <span class="keyword">and</span>/<span class="keyword">or</span> data warehouse designers </span><br><span class="line">  概念层次结构可以由域专家和/或数据仓库设计者明确指定</span><br><span class="line">- Concept hierarchy can be automatically formed <span class="keyword">for</span> both numeric <span class="keyword">and</span> nominal data—For numeric data, use discretization methods shown </span><br><span class="line">  可以为数字和标称数据自动形成概念层次结构 - 对于数字数据，使用显示的离散化方法</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Concept Hierarchy Generation for Nominal Data</span></span><br><span class="line">- Specification of a partial/total ordering of attributes explicitly at the schema level by users <span class="keyword">or</span> experts</span><br><span class="line">	- <span class="string">'street &lt; city &lt; state &lt; country'</span></span><br><span class="line">- Specification of a hierarchy <span class="keyword">for</span> a set of values by explicit data grouping</span><br><span class="line">	- <span class="string">'&#123;Urbana, Champaign, Chicago&#125; &lt; Illinois'</span></span><br><span class="line">- Specification of only a partial set of attributes</span><br><span class="line">	- E.g., only <span class="string">'street &lt; city'</span>, <span class="keyword">not</span> others</span><br><span class="line">- Automatic generation of hierarchies (<span class="keyword">or</span> attribute levels) by the analysis of the number of distinct values</span><br><span class="line">	- E.g., <span class="keyword">for</span> a set of attributes:<span class="string">'&#123;street, city, state, country&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Automatic Concept Hierarchy Generation</span></span><br><span class="line">- Some hierarchies can be automatically generated based on <span class="string">'the analysis of the number'</span> of distinct values per attribute <span class="keyword">in</span> the data set </span><br><span class="line">	- The attribute <span class="keyword">with</span> the most distinct values <span class="keyword">is</span> placed at the lowest level of the hierarchy</span><br><span class="line">	- Exceptions, e.g., weekday, month, quarter, year</span><br></pre></td></tr></table></figure>
<center><br><img src="/2019/01/01/数据分析与挖掘/AutomaticConceptHierarchyGeneration.png" width="600"><br></center>

<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><center><br>    <img src="/2019/01/01/数据分析与挖掘/Summary_datapreprocessing.png" width="600"><br></center>

<h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><ul>
<li><p><strong>What is classification?</strong></p>
<p>根据训练集和类标签（分类属性中的值）构建模型，并将其用于分类新数据，预测其标签。</p>
<ul>
<li><p>Supervised learning </p>
<center><br><img src="/2019/01/01/数据分析与挖掘/SupervisedLearning.png" width="600"><br></center>



</li>
</ul>
</li>
</ul>
<pre><code>**监督学习**是机器学习任务的一种。它`从有标记的训练数据中推导出预测标签`。有标记的训练数据是指每个训练实例都包括输入和期望的输出。一句话：**给定数据，预测标签**。(分类、回归)

**无监督学习**是机器学习任务的一种。它`从无标记的训练数据中推断结论`。最典型的无监督学习就是聚类分析，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。一句话：**给定数据，寻找隐藏的结构**。
</code></pre><ul>
<li><p><strong>Classification Steps</strong> </p>
<ol>
<li><strong>Model construction</strong> 模型构建</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Each sample <span class="keyword">is</span> assumed to belong to a predefined <span class="class"><span class="keyword">class</span> <span class="params">(shown by the class label)</span> </span></span><br><span class="line"><span class="class">  假设每个样本属于预定义的类（由类标签显示）</span></span><br><span class="line"><span class="class">- <span class="title">The</span> <span class="title">set</span> <span class="title">of</span> <span class="title">samples</span> <span class="title">used</span> <span class="title">for</span> <span class="title">model</span> <span class="title">construction</span> <span class="title">is</span> <span class="title">training</span> <span class="title">set</span>  </span></span><br><span class="line"><span class="class">  用于模型构建的样本集是训练集</span></span><br><span class="line"><span class="class">- '<span class="title">Model</span>':</span> Represented <span class="keyword">as</span> decision trees, rules, mathematical formulas, <span class="keyword">or</span> other forms </span><br><span class="line">  模型：表示为决策树，规则，数学公式或其他形式</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><strong>Model Validation and Testing</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'Test'</span>: Estimate accuracy of the model </span><br><span class="line">  - The known label of test sample <span class="keyword">is</span> compared <span class="keyword">with</span> the classified result <span class="keyword">from</span> the model  </span><br><span class="line">    将已知的测试样品标签与模型的分类结果进行比较</span><br><span class="line">  - Accuracy: % of test set samples that are correctly classified by the model </span><br><span class="line">    准确度：按模型正确分类的测试集样本的百分比 </span><br><span class="line">  - Test set <span class="keyword">is</span> independent of training set  测试集独立于训练集</span><br><span class="line">- <span class="string">'Validation'</span>: If the test set <span class="keyword">is</span> used to select <span class="keyword">or</span> refine models, it <span class="keyword">is</span> called validation (development/test) set </span><br><span class="line">  验证：如果测试集用于选择或改进模型，则称为验证（开发/测试）集 To be better【与测试集相较，强调refine models】</span><br></pre></td></tr></table></figure>
<ol start="3">
<li><strong>Model Deployment</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">If the accuracy <span class="keyword">is</span> acceptable, use the model to classify new data 【模型部署】</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a><strong>Algorithms</strong></h3><h4 id="Decision-tree-ID3-C4-5-CART"><a href="#Decision-tree-ID3-C4-5-CART" class="headerlink" title="Decision tree-ID3,C4.5,CART"></a><a href="https://captainzj.github.io/2018/11/24/Classification-Algorithm/" target="_blank" rel="noopener">Decision tree-ID3,C4.5,CART</a></h4><p><a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a>、<a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)</a>、<a href="https://www.cnblogs.com/pinard/p/6056319.html" target="_blank" rel="noopener">scikit-learn决策树算法类库使用小结</a> </p>
<ul>
<li><p>Basic algorithm </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Tree <span class="keyword">is</span> constructed <span class="keyword">in</span> a top-down, recursive, divide-<span class="keyword">and</span>-conquer manner</span><br><span class="line">  树以自上而下，递归，分而治之的方式构建</span><br><span class="line">    - At start, all the training examples are at the root</span><br><span class="line">      一开始，所有的训练样例都是根源</span><br><span class="line">    - Examples are partitioned recursively based on selected attributes</span><br><span class="line">      样例基于被选定的属性递归地划分</span><br><span class="line">    - On each node, attributes are selected based on the training examples on that node, <span class="keyword">and</span> a heuristic <span class="keyword">or</span> statistical measure (e.g., information gain)</span><br><span class="line">      在每个节点上，基于<span class="string">'该节点上的训练示例'</span>以及<span class="string">'启发式或统计度量（例如，信息增益）'</span>来<span class="string">'选择属性'</span>。</span><br></pre></td></tr></table></figure>
<ul>
<li>Conditions for stopping partitioning </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- All samples <span class="keyword">for</span> a given node belong to the same <span class="class"><span class="keyword">class</span> 给定节点的所有样本都属于同一个类</span></span><br><span class="line"><span class="class">- <span class="title">There</span> <span class="title">are</span> <span class="title">no</span> <span class="title">remaining</span> <span class="title">attributes</span> <span class="title">for</span> <span class="title">further</span> <span class="title">partitioning</span>  没有剩余属性可用于进一步分区</span></span><br><span class="line"><span class="class">- <span class="title">There</span> <span class="title">are</span> <span class="title">no</span> <span class="title">samples</span> <span class="title">left</span>  没有剩下的样例</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Prediction</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'Majority voting'</span> <span class="keyword">is</span> employed <span class="keyword">for</span> classifying the leaf</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://captainzj.github.io/2018/11/24/Classification-Algorithm/#%E4%BF%A1%E6%81%AF%E7%86%B5-Entropy" target="_blank" rel="noopener">Entropy</a>   <a href="#Example: Attribute Selection with Information Gain">Example: Attribute Selection with Information Gain</a>   <a href="#Computation of Gini Index">Computation of Gini Index</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- Entropy 信息熵:  表征混乱程度 </span><br><span class="line">- Conditional Entropy条件熵: 在已知随机变量X的条件下随机变量Y的不确定性(概率)</span><br><span class="line">- Mutual Information互信息/Information gain信息增益: 得知特征X的信息而使得类Y的信息的不确定性减少的程度(越大越好) -&gt;&gt; ID3决策树</span><br><span class="line">- Gain Ratio信息增益比: 解决使用信息增益存在偏向于选择取值较多的特征的问题(越大越好) -&gt;&gt; C4.5决策树 </span><br><span class="line">- GINI index基尼指数： 表征不纯度(越小越好)  -&gt;&gt;  CART分类树</span><br></pre></td></tr></table></figure>
</li>
<li><p>剪枝</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">* 预剪枝(prepruning),通过提前停止树的构建(例如，通过决定在给定的结点`不再分裂或划分`训练元组的子集)而对树<span class="string">"剪枝"</span>。如果划分一个结点的元组导致低于预定义(信息增益、基尼指数等度量方式)阈值的划分，则给定子集的进一步划分将停止。然而，选取一个适当的阈值是困难的。高阈值可能导致过分简化的树，而低阈值可能使得树的简化不足。</span><br><span class="line">* 后剪枝(postpruning)，它由<span class="string">"完全生长"</span>的树剪去子树。通过删除结点的分支并用树叶替换它而剪掉给定结点上的子树。</span><br><span class="line">	- E.g. CART使用代价复杂度剪枝算法</span><br><span class="line">* 组合方法：预剪枝和后剪枝交叉使用。后剪枝所需要的计算比预剪枝多，但是通常产生更可靠的树。</span><br></pre></td></tr></table></figure>
</li>
<li><p>Advantage</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1）&apos;简单直观&apos;，生成的决策树很直观。</span><br><span class="line">2）基本&apos;不需要预处理&apos;，不需要提前归一化，处理缺失值。</span><br><span class="line">3）使用决策树预测的代价是O(log2m)。 m为样本数。</span><br><span class="line">4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。</span><br><span class="line">5）可以&apos;处理多维度输出&apos;的分类问题。</span><br><span class="line">6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上有很好的&apos;可解释性&apos;</span><br><span class="line">7）可以交叉验证的&apos;剪枝&apos;来选择模型，从而提高泛化能力。</span><br><span class="line">8）对于异常点的&apos;容错能力&apos;好，健壮性高。</span><br></pre></td></tr></table></figure>
</li>
<li><p>Disadvantage</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>）决策树算法非常容<span class="string">'易过拟合'</span>，导致<span class="string">'泛化能力不强'</span>.可以通过设置节点最少样本数量和限制决策树深度来改进.</span><br><span class="line"><span class="number">2</span>）决策树会因为样本发生一点点的改动，就会导致树<span class="string">'结构的剧烈改变'</span>。这个可以通过集成学习之类的方法解决.</span><br><span class="line"><span class="number">3</span>）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，<span class="string">'容易陷入局部最优'</span>。可以通过集成学习之类的方法来改善。</span><br><span class="line"><span class="number">4</span>）有些比较<span class="string">'复杂的关系'</span>，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</span><br><span class="line"><span class="number">5</span>）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</span><br><span class="line">6）处理大数据集，决策树的构造可能变得效率低下('可伸缩问题') -&gt;&gt; RainForest(雨林)，能适应可用的内存量，并应用于任意决策树归纳算法</span><br></pre></td></tr></table></figure>
</li>
<li><p>决策树算法比较</p>
<p>|  算法  |  支持模型  | 树结构 |      特征选择      | 连续值处理 | 缺失值处理 |  剪枝  |<br>| :—-: | :——–: | :—-: | :—————-: | :——–: | :——–: | :—-: |<br>| <code>ID3</code>  |    分类    | 多叉树 |     <code>信息增益</code>     |   不支持   |   不支持   | 不支持 |<br>| <code>C4.5</code> |    分类    | 多叉树 |    <code>信息增益比</code>    |    支持    |    支持    |  支持  |<br>| <code>CART</code> | 分类，回归 | 二叉树 | <code>基尼系数，均方差</code> |    支持    |    支持    |  支持  |</p>
</li>
</ul>
<h4 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h4><ul>
<li>Define: Find a linear/non-linear hyperplane (decision boundary) that will separate the data</li>
<li>Optimize：希望所有的点都离超平面远 -&gt;  可以让离超平面比较近的点尽可能的远离超平面</li>
<li>kernel核函数：将数据从低维空间映射到高维空间</li>
</ul>
<p><a href="https://www.cnblogs.com/pinard/p/6097604.html" target="_blank" rel="noopener">支持向量机原理(一) 线性支持向量机</a>、<a href="https://www.cnblogs.com/pinard/p/6100722.html" target="_blank" rel="noopener">支持向量机原理(二) 线性支持向量机的软间隔最大化模型</a>、<a href="https://www.cnblogs.com/pinard/p/6103615.html" target="_blank" rel="noopener">支持向量机原理(三)线性不可分支持向量机与核函数</a>、<a href="https://www.cnblogs.com/pinard/p/6111471.html" target="_blank" rel="noopener">支持向量机原理(四)SMO算法原理</a>、<a href="https://www.cnblogs.com/pinard/p/6113120.html" target="_blank" rel="noopener">支持向量机原理(五)线性支持回归</a>、<a href="https://www.cnblogs.com/pinard/p/6117515.html" target="_blank" rel="noopener">scikit-learn 支持向量机算法库使用小结</a>、<a href="https://www.cnblogs.com/pinard/p/6126077.html" target="_blank" rel="noopener">支持向量机高斯核调参小结</a></p>
<h4 id="Bayes"><a href="#Bayes" class="headerlink" title="Bayes"></a>Bayes</h4><ul>
<li><p>Basic Concepts</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">* P(H|X): 后验概率. 基于相关性描述(证据)X, <span class="string">'假设H'</span>成立的概率</span><br><span class="line">* P(H): 先验概率.  一般情况下(没有相关约束)，<span class="string">'假设H'</span>成立的概率</span><br><span class="line"></span><br><span class="line">在分类问题中,希望确定 给定“证据”X，假设H成立的概率P(H|X).</span><br></pre></td></tr></table></figure>
<p><strong>贝叶斯定理：</strong> $P(H|X)=\frac{P(X|H)P(H)}{P(X)}$</p>
</li>
<li><p>Naïve Bayesian Classifier</p>
<blockquote>
<p>后验概率最大化来判断分类</p>
</blockquote>
<p>1) 如果没有Y的先验概率，则计算Y的K个先验概率：$P(Y=C_k)$</p>
<p>2) 分别计算第k个类别的第j维特征的第l个个取值条件概率：$P(X_j=x_{jl}|Y=C_k)$</p>
<p>3）对于实例$X^{(test)}$，分别计算：$P(Y=C_k)\prod_{j=1}^{n}P(X_j=x_j^{(test)}|Y=C_k)$</p>
<p>4) 确定实例$X^{(test)}$的分类$C_{result}  = \underbrace{argmax}_{C_k}P(Y=C_k)\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k) $ </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Advantages</span></span><br><span class="line"> Easy to implement  易于实现</span><br><span class="line"> Good results obtained <span class="keyword">in</span> most of the cases 大多数情况下获得了良好的结果</span><br><span class="line"><span class="comment"># Disadvantages</span></span><br><span class="line"> Assumption: class conditional independence, therefore loss of accuracy类条件独立-&gt;准确率缺失</span><br><span class="line"> Practically, dependencies exist among variables</span><br><span class="line">  实际上，变量之间存在依赖关系</span><br><span class="line">	 E.g., hospitals: patients: Profile: age, family history, etc. Symptoms: fever, cough etc., Disease: lung cancer, diabetes,etc.</span><br><span class="line">	 Dependencies among these cannot be modeled by Naïve Bayesian Classifier</span><br><span class="line">      这些依赖关系不能用朴素贝叶斯分类器建模</span><br><span class="line"><span class="comment"># How to deal with these dependencies? Bayesian Belief Networks</span></span><br><span class="line">  如何处理这些依赖关系？贝叶斯置信网络</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><a href="https://www.cnblogs.com/pinard/p/6069267.html" target="_blank" rel="noopener">朴素贝叶斯算法原理小结</a>、<a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">scikit-learn 朴素贝叶斯类库使用小结</a></p>
<h4 id="ANN"><a href="#ANN" class="headerlink" title="ANN"></a>ANN</h4><blockquote>
<p>受生物神经元的启发，将<strong>多输入单输出</strong>的信息处理单元作为人工神经网络中的一个神经元。人工神经网络的基本结构如下：输入层(输入层的神经元数目对应于训练集数据的属性数目)、隐藏层、输出层(输出层的神经元数目对应于网络预测的分类数目)</p>
</blockquote>
<center><br>    <img src="/2019/01/01/数据分析与挖掘/ANN_Architecture.png" width="600"><br></center>

<ul>
<li><p>Backpropagation</p>
<blockquote>
<p>Backpropagate the error (by updating weights and biases)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Terminating condition (when error is very small, etc.)</span></span><br><span class="line"> Small changes <span class="keyword">in</span> weights  已接近<span class="string">"最优"</span></span><br><span class="line"> Small errors  结果可接受</span><br><span class="line"> Number of predefined iterations </span><br><span class="line"></span><br><span class="line"><span class="comment"># more</span></span><br><span class="line"> 反向传播可能会停留在局部最小值，但实际上它通常表现良好</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="DeepLearning"><a href="#DeepLearning" class="headerlink" title="DeepLearning"></a>DeepLearning</h5><blockquote>
<p>通过更深的layers，自动提取特征（构建特征空间），以达到更”深层次”的学习效果</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Train networks <span class="keyword">with</span> many layers (vs. shallow nets <span class="keyword">with</span> just a couple of layers) </span><br><span class="line">  <span class="string">'训练具有多层的网络'</span>（与仅有几层的浅网络）</span><br><span class="line"> More neurons than previous networks 与以前的网络相比，有更多的神经元</span><br><span class="line"> More complex ways to connect layers 更复杂的连接层的方法</span><br><span class="line"> Tremendous computing power to train networks 以巨大计算能力训练网络</span><br><span class="line"> Automatic feature extraction <span class="string">'自动特征提取'</span></span><br><span class="line">- Multiple layers work to build <span class="string">'an improved feature space'</span></span><br><span class="line">  多个层用于构建改进的特征空间</span><br><span class="line"> Analogy: Signals passing through regions of the visual cortex</span><br><span class="line">  类比：信号通过视觉皮层的区域</span><br><span class="line">     Example: For face recognition: edge → nose → face, layer-by-layer</span><br><span class="line">      示例：用于面部识别：边缘→鼻子→面部，逐层</span><br><span class="line">- <span class="string">'Popular Deep Learning Frameworks'</span> <span class="keyword">for</span> Classification </span><br><span class="line"> Deep Feedforward Neural Networks 深度前馈神经网络</span><br><span class="line"> Convolutional Neural Networks 卷积神经网络</span><br><span class="line"> Recurrent Neural Networks 回归神经网络</span><br><span class="line"></span><br><span class="line"><span class="comment"># More</span></span><br><span class="line"> 为解决梯度消失/梯度爆炸、训练退化的问题，提出ResNet</span><br><span class="line"> 为实现特征的复用，提出DenseNet</span><br></pre></td></tr></table></figure>
<h3 id="Ensemble-methods"><a href="#Ensemble-methods" class="headerlink" title="Ensemble methods"></a>Ensemble methods</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Ensemble Methods: Increasing the Accuracy</span></span><br><span class="line">Ensemble methods</span><br><span class="line">	- Use a combination of models to increase accuracy</span><br><span class="line">      使用模型组合来提高准确性</span><br><span class="line">	- Combine a series of k learned models, M1, M2, ..., Mk, <span class="keyword">with</span> the aim of creating an improved model M*</span><br><span class="line">      结合一系列k学习模型，M1，M2，...，Mk，旨在创建一个改进的模型M*</span><br><span class="line"></span><br><span class="line">Popular ensemble methods</span><br><span class="line">	- <span class="string">'Bagging'</span>: Trains each model using a subset of the training set, <span class="keyword">and</span> models learned <span class="keyword">in</span> parallel</span><br><span class="line">      Bagging: 使用训练集的子集训练每个模型，并且并行学习模型</span><br><span class="line">	- <span class="string">'Boosting'</span>: Trains each new model instance to emphasize the training instances that previous models mis-classified, <span class="keyword">and</span> models learned <span class="keyword">in</span> order</span><br><span class="line">      Boosting：训练每个新模型实例以强调先前模型错误分类的训练实例，以及按顺序学习的模型</span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/pinard/p/6131423.html" target="_blank" rel="noopener">集成学习原理小结</a></p>
<h4 id="Bagging-Bootstrap-Aggregation"><a href="#Bagging-Bootstrap-Aggregation" class="headerlink" title="Bagging: Bootstrap Aggregation"></a>Bagging: Bootstrap Aggregation</h4><center><br>    <img src="/2019/01/01/数据分析与挖掘/BootstrapAggregation.png" width="600"><br></center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'Analogy'</span>: Diagnosis based on multiple doctors’ <span class="string">'majority vote'</span></span><br><span class="line">- <span class="string">'Training'</span></span><br><span class="line">	- Given a set D of d tuples, at each iteration i, a training set Di of d tuples <span class="keyword">is</span> sampled <span class="keyword">with</span> replacement <span class="keyword">from</span> D (i.e., bootstrap)</span><br><span class="line">      给定d个元组的D组，在每次迭代i中，对D元组的训练集Di进行<span class="string">'采样'</span>，并用D替换（即<span class="string">'自举'</span>）</span><br><span class="line">	- A classifier model <span class="string">'Mi'</span> <span class="keyword">is</span> learned <span class="keyword">for</span> each training set <span class="string">'Di'</span></span><br><span class="line">      为每个训练集Di学习分类器模型Mi.</span><br><span class="line">- <span class="string">'Classification'</span>: classify an <span class="string">'unknown sample X'</span></span><br><span class="line">	- Each classifier Mi returns its <span class="class"><span class="keyword">class</span> <span class="title">prediction</span></span></span><br><span class="line"><span class="class">	- <span class="title">The</span> <span class="title">bagged</span> <span class="title">classifier</span> <span class="title">M</span>* '<span class="title">counts</span> <span class="title">the</span> <span class="title">votes</span>' <span class="title">and</span> <span class="title">assigns</span> <span class="title">the</span> <span class="title">class</span> <span class="title">with</span> <span class="title">the</span> <span class="title">most</span></span></span><br><span class="line"><span class="class"><span class="title">votes</span> <span class="title">to</span> <span class="title">X</span> </span></span><br><span class="line"><span class="class">- '<span class="title">Prediction</span>':</span> It can be applied to the prediction of <span class="string">'continuous values'</span> by <span class="string">'taking the</span></span><br><span class="line"><span class="string">average value'</span> of each prediction <span class="keyword">for</span> a given test tuple</span><br><span class="line">  预测: 它可以应用于连续值的预测 by 给定测试元组的每个预测的平均值</span><br><span class="line">- <span class="string">'Accuracy'</span>: Improved accuracy <span class="keyword">in</span> prediction</span><br><span class="line">	- Often significantly better than a single classifier derived <span class="keyword">from</span> D</span><br><span class="line">      通常明显优于源自D的单一分类器</span><br><span class="line">	- For noise data: Not considerably worse, more robust</span><br><span class="line">      对于噪声数据：不会更糟，而是更健壮</span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/pinard/p/6156009.html" target="_blank" rel="noopener">Bagging与随机森林算法原理小结</a></p>
<h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><blockquote>
<p>（Bagging）投票得分    -&gt;  （Boosting）加权得分</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Analogy: Consult several doctors, based on <span class="string">'a combination of weighted'</span> diagnoses—weight assigned based on the previous diagnosis accuracy</span><br><span class="line">- How boosting works?</span><br><span class="line">	- Weights are assigned to each training tuple</span><br><span class="line">      <span class="string">'权重'</span>分配给每个训练元组</span><br><span class="line">	- A series of k classifiers <span class="keyword">is</span> iteratively learned</span><br><span class="line">      迭代学习一系列k个分类器</span><br><span class="line">	- After a classifier Mi <span class="keyword">is</span> learned, the weights are updated to allow the subsequent classifier, Mi+<span class="number">1</span>, to <span class="string">'pay more attention to the training tuples that were misclassified'</span> by Mi</span><br><span class="line">      在学习分类器Mi之后，权重被更新以允许随后的分类器Mi+<span class="number">1</span><span class="string">'更多地关注被Mi错误分类的训练元组'</span></span><br><span class="line">	- The final M* <span class="string">'combines the votes'</span> of each individual classifier, where the weight of each classifie<span class="string">r's vote is a function of its accuracy</span></span><br><span class="line"><span class="string">      最终的M*结合了每个分类器的投票，每个分类器的投票'</span>权重<span class="string">'是其'</span>准确性<span class="string">'的函数</span></span><br><span class="line"><span class="string">- Boosting algorithm can be extended for numeric prediction</span></span><br><span class="line"><span class="string">  可以扩展Boosting算法进行'</span>数值预测<span class="string">'</span></span><br><span class="line"><span class="string">- Comparing with bagging: Boosting tends to have greater accuracy, but it also risks overfitting the model to misclassified data</span></span><br><span class="line"><span class="string">  与装袋相比：boosting往往具有更高的'</span>准确性<span class="string">'，但也存在过度拟合模型错误分类数据的'</span>风险<span class="string">'</span></span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/pinard/p/6133937.html" target="_blank" rel="noopener">集成学习之Adaboost算法原理小结</a>、<a href="https://www.cnblogs.com/pinard/p/6136914.html" target="_blank" rel="noopener">scikit-learn Adaboost类库使用小结</a></p>
<h4 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h4><blockquote>
<p>Avariation of bagging for decision trees 对<strong>决策树的bagging</strong>的’变异’</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Random Forest (first proposed by L. Breiman in 2001)</span></span><br><span class="line">- Avariation of bagging <span class="keyword">for</span> decision trees 对决策树的bagging的<span class="string">'变异'</span></span><br><span class="line">- Data bagging 数据装袋</span><br><span class="line">	- Use a subset of training data by sampling <span class="keyword">with</span> replacement <span class="keyword">for</span> each tree</span><br><span class="line">      通过<span class="string">'采样'</span>为每棵树<span class="string">'替换'</span>使用训练数据的子集</span><br><span class="line">- Feature bagging</span><br><span class="line">	- At each node use a random selection of attributes <span class="keyword">as</span> candidates <span class="keyword">and</span> split by the best attribute among them</span><br><span class="line">      在每个节点使用<span class="string">'随机'</span>选择的属性作为<span class="string">'候选'</span>并且以它们中的<span class="string">'最佳'</span>属性进行<span class="string">'划分'</span></span><br><span class="line">- Compared to original bagging,increases the diversity among generated trees</span><br><span class="line">  与原始套袋相比，增加了生成树的<span class="string">'多样性'</span></span><br><span class="line">- During classification, each tree <span class="string">'votes'</span> <span class="keyword">and</span> the <span class="string">'most popular'</span> <span class="class"><span class="keyword">class</span> <span class="title">is</span> <span class="title">returned</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"># <span class="title">Two</span> <span class="title">Methods</span> <span class="title">to</span> <span class="title">construct</span> <span class="title">Random</span> <span class="title">Forest</span></span></span><br><span class="line"><span class="class">	- <span class="title">Forest</span>-<span class="title">RI</span> <span class="params">(random input selection)</span>:</span> Randomly select, at each node, F attributes <span class="keyword">as</span> candidates <span class="keyword">for</span> the split at the node. The CART methodology <span class="keyword">is</span> used to grow the trees to maximum size</span><br><span class="line">      Forest-RI（<span class="string">'随机输入'</span>选择）：在每个节点上随机选择F属性作为节点分割的候选者。 CART方法用于将树增长到最大尺寸</span><br><span class="line">	- Forest-RC (random linear combinations): Creates new attributes (<span class="keyword">or</span> features) that are a linear combination of the existing attributes (reduces the correlation between individual classifiers)</span><br><span class="line">      Forest-RC（<span class="string">'随机线性组合'</span>）：创建属于现有属性的线性组合的新属性（或特征）（减少各个分类器之间的相关性）</span><br><span class="line">- Comparable <span class="keyword">in</span> accuracy to Adaboost, but more robust to errors <span class="keyword">and</span> outliers</span><br><span class="line">  与Adaboost相比具有可比性，但对错误和异常值<span class="string">'更具鲁棒性'</span></span><br><span class="line">- Insensitive to the number of attributes selected <span class="keyword">for</span> consideration at each split, <span class="keyword">and</span> faster than typical bagging <span class="keyword">or</span> boosting</span><br><span class="line">  对每次拆分时选择的<span class="string">'属性数量不敏感'</span>，并且比典型的bagging或boosting<span class="string">'速度更快'</span></span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/pinard/p/6160412.html" target="_blank" rel="noopener">scikit-learn随机森林调参小结</a></p>
<h3 id="Model-evaluation-and-selection"><a href="#Model-evaluation-and-selection" class="headerlink" title="Model evaluation and selection"></a><strong>Model evaluation and selection</strong></h3><h4 id="Classifier-Evaluation-Metrics"><a href="#Classifier-Evaluation-Metrics" class="headerlink" title="Classifier Evaluation Metrics"></a>Classifier Evaluation Metrics</h4><p><a href="#Classifier Evaluation Metrics: Example">Classifier Evaluation Metrics: Example</a></p>
<h5 id="Confusion-matrix-and-criteria"><a href="#Confusion-matrix-and-criteria" class="headerlink" title="Confusion matrix and criteria"></a>Confusion matrix and criteria</h5><center><br>    <img src="/2019/01/01/数据分析与挖掘/ConfusionMatrix.png" width="600"><br></center>

<p><a href="https://www.cnblogs.com/pinard/p/5993450.html" target="_blank" rel="noopener">精确率与召回率，RoC曲线与PR曲线</a></p>
<h4 id="Estimating-a-classifier’s-accuracy"><a href="#Estimating-a-classifier’s-accuracy" class="headerlink" title="Estimating a classifier’s accuracy"></a>Estimating a classifier’s accuracy</h4><h5 id="Cross-evaluation"><a href="#Cross-evaluation" class="headerlink" title="Cross-evaluation"></a>Cross-evaluation</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Cross-validation (k-fold, where k = 10 is most popular)</span></span><br><span class="line">- Randomly partition the data into k mutually exclusive subsets, each approximately equal size </span><br><span class="line">  随机将数据划分为k个互斥的子集，每个子集的大小大致相等</span><br><span class="line">- At i-th iteration, use Di <span class="keyword">as</span> test set <span class="keyword">and</span> others <span class="keyword">as</span> training set </span><br><span class="line">  在第i次迭代中，使用Di作为测试集，使用其他作为训练集</span><br><span class="line">- Leave-one-out: k folds where k = <span class="string">'#'</span> of tuples, <span class="keyword">for</span> small sized data </span><br><span class="line">  留一个：k折叠，其中k = <span class="string">'#'</span> 元组的数量，对于小尺寸数据</span><br><span class="line">- *Stratified cross-validation*: folds are stratified so that class distribution, in each fold is approximately the same as that in the initial data </span><br><span class="line">   *分层交叉验证*：折叠是分层的，因此每个折叠中的类分布与初始数据中的类别分布大致相同</span><br><span class="line"></span><br><span class="line"><span class="comment"># Holdout method</span></span><br><span class="line">- Given data <span class="keyword">is</span> randomly partitioned into two independent sets</span><br><span class="line">  给定数据被随机分成两个独立的集合</span><br><span class="line">	- Training set (e.g., <span class="number">2</span>/<span class="number">3</span>) <span class="keyword">for</span> model construction </span><br><span class="line">      模型构建的训练集（例如，<span class="number">2</span>/<span class="number">3</span>）</span><br><span class="line">	- Test set (e.g., <span class="number">1</span>/<span class="number">3</span>) <span class="keyword">for</span> accuracy estimation</span><br><span class="line">      测试集（例如，<span class="number">1</span>/<span class="number">3</span>）用于准确度估计</span><br><span class="line">- Repeated random sub-sampling validation: a variation of holdout</span><br><span class="line">  重复随机子采样验证：保持的变化</span><br><span class="line">	- Repeat holdout k times, accuracy = avg. of the accuracies obtained</span><br><span class="line">      重复k次，精度= 所得准确性的平均值</span><br><span class="line">      </span><br><span class="line"><span class="comment"># Bootstrap 引导程序</span></span><br><span class="line">- Works well <span class="keyword">with</span> small data sets <span class="string">'适用于小型数据集'</span></span><br><span class="line">- Samples the given training tuples uniformly <span class="keyword">with</span> replacement</span><br><span class="line">  均匀地对给定的训练元组进行<span class="string">'取样'</span></span><br><span class="line">	- Each time a tuple <span class="keyword">is</span> selected, it <span class="keyword">is</span> equally likely to be selected again <span class="keyword">and</span> re-</span><br><span class="line">added to the training set</span><br><span class="line">	  每次选择元组时，同样可能再次选择并重新添加到训练集</span><br><span class="line">- Several bootstrap methods, <span class="keyword">and</span> a common one <span class="keyword">is</span> <span class="number">.632</span> bootstrap</span><br><span class="line">  有几种引导方法，常见的方法是<span class="number">.632</span>引导程序 (大约<span class="number">63.2</span>％的原始数据最终在bootstrap中，其余<span class="number">36.8</span>％形成测试集)</span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/pinard/p/5992719.html" target="_blank" rel="noopener">交叉验证(Cross Validation)原理小结</a></p>
<h4 id="Issues-Affecting-Model-Selection"><a href="#Issues-Affecting-Model-Selection" class="headerlink" title="Issues Affecting Model Selection"></a>Issues Affecting Model Selection</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'Accuracy'</span> 准确性</span><br><span class="line">	- classifier accuracy: predicting <span class="class"><span class="keyword">class</span> <span class="title">label</span></span></span><br><span class="line"><span class="class">- '<span class="title">Speed</span>' 速度</span></span><br><span class="line"><span class="class">	- <span class="title">time</span> <span class="title">to</span> <span class="title">construct</span> <span class="title">the</span> <span class="title">model</span> <span class="params">(training time)</span> 构建模型的时间（训练时间）</span></span><br><span class="line"><span class="class">	- <span class="title">time</span> <span class="title">to</span> <span class="title">use</span> <span class="title">the</span> <span class="title">model</span> <span class="params">(classification/prediction time)</span>  使用模型的时间（分类/预测时间）</span></span><br><span class="line"><span class="class">- '<span class="title">Robustness</span>' 鲁棒性<span class="params">(稳健性)</span>:</span> handling noise <span class="keyword">and</span> missing values</span><br><span class="line">- <span class="string">'Scalability'</span> 可伸缩性: efficiency <span class="keyword">in</span> disk-resident databases 磁盘驻留数据库的效率</span><br><span class="line">- <span class="string">'Interpretability'</span> 可解释性</span><br><span class="line">	- understanding <span class="keyword">and</span> insight provided by the model 模型提供的理解和洞察力</span><br><span class="line">- <span class="string">'Other measures'</span>, e.g., goodness of rules, such <span class="keyword">as</span> decision tree size <span class="keyword">or</span> compactness of classification rules</span><br><span class="line">  其他措施，例如规则的好处，例如决策树大小或分类规则的紧凑性</span><br></pre></td></tr></table></figure>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><center><br><img src="/2019/01/01/数据分析与挖掘/classification_summary1.png" width="600"><br><img src="/2019/01/01/数据分析与挖掘/classification_summary2.png" width="600"><br><img src="/2019/01/01/数据分析与挖掘/classification_summary3.png" width="600"><br></center>

<h2 id="Frequent-patterns"><a href="#Frequent-patterns" class="headerlink" title="Frequent patterns"></a>Frequent patterns</h2><ul>
<li><p><strong>What is a frequent pattern</strong> </p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment"># basic concept</span></span><br><span class="line"> 满足最小支持度的项集F -&gt;&gt; F为'频繁项集'(frequent pattern)</span><br><span class="line"> 项集L的任意超集均为非频繁项集 -&gt;&gt; L为'最大频繁模式'(Max-Pattern)/最大频繁项集(Maximal Frequent Itemset)</span><br><span class="line"> 项集X的直接超集(最小的严格超集)的支持度计数都不等于(小于)ta本身的支持度计数 -&gt;&gt; X为'闭合频繁项集'(closed-pattern)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Association-rule"><a href="#Association-rule" class="headerlink" title="Association rule"></a><strong>Association rule</strong></h3><blockquote>
<p>关联规则就是有关联的规则，形式是这样定义的：<em>两个不相交的非空集合X、Y，如果有X–&gt;Y，就说X–&gt;Y是一条关联规则</em>。举个例子，在上面的表中，我们发现购买啤酒就一定会购买尿布，{啤酒}–&gt;{尿布}就是一条关联规则。关联规则的强度用支持度$Support$和置信度$Confidence$等描述</p>
</blockquote>
<center><br>    <img src="/2019/01/01/数据分析与挖掘/AssociationRules.png" width="600"><br></center>

<p>一般来说，要选择一个数据集合中的频繁数据集，则需要自定义评估标准。最常用的评估标准是用自定义的支持度，或者是自定义支持度和置信度的一个组合。</p>
<ul>
<li><p>关联规则挖掘步骤</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="string">'生成频繁项集'</span></span><br><span class="line">   这一阶段找出所有满足最小支持度的项集，找出的这些项集称为频繁项集</span><br><span class="line"><span class="number">2.</span> <span class="string">'生成规则'</span></span><br><span class="line">   在上一步产生的频繁项集的基础上生成满足最小置信度的规则，产生的规则称为强规则</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><h4 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a><strong>Apriori</strong></h4><p>为了减少频繁项集的生成时间，我们应该尽早的消除一些完全不可能是频繁项集的集合，故而引出Apriori的两条定律：</p>
<ol>
<li><em>如果一个集合是频繁项集，则它的所有子集都是频繁项集</em>。</li>
<li><em>如果一个集合不是频繁项集，则它的所有超集都不是频繁项集。</em></li>
</ol>
<p>利用这两条定律，我们抛掉很多的候选项集，Apriori算法就是利用这两个定理来实现快速挖掘频繁项集的。<a href="https://www.cnblogs.com/pinard/p/6293298.html" target="_blank" rel="noopener">Apriori算法原理总结</a>、<a href="https://www.cnblogs.com/fengfenggirl/p/associate_apriori.html" target="_blank" rel="noopener">关联规则挖掘基本概念与Aprior算法</a></p>
<hr>
<blockquote>
<p>Apriori算法属于候选消除算法，是一个生成候选集、消除不满足条件的候选集、并不断循环直到不再产生候选集的过程。</p>
</blockquote>
<center><br><img src="/2019/01/01/数据分析与挖掘/Apriori.png" width="600"><br></center>

<p>上面的图演示了Apriori算法的过程，注意看由二级频繁项集生成三级候选项集时，没有{牛奶,面包,啤酒}，那是因为{面包,啤酒}不是二级频繁项集，这里利用了Apriori定理。最后生成三级频繁项集后，没有更高一级的候选项集，因此整个算法结束，{牛奶,面包,尿布}是最大频繁子集</p>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># How to generate candidates? </span></span><br><span class="line"> Step <span class="number">1</span>: self-joining Lk</span><br><span class="line"> Step <span class="number">2</span>: pruning</span><br><span class="line"><span class="comment"># Example of Candidate-generation</span></span><br><span class="line"> L3=&#123;abc, abd, acd, ace, bcd&#125;	<span class="comment"># INPUT</span></span><br><span class="line"> Self-joining: L3*L3	<span class="comment"># Step1</span></span><br><span class="line">	 abcd <span class="keyword">from</span> abc <span class="keyword">and</span> abd </span><br><span class="line">     acde <span class="keyword">from</span> acd <span class="keyword">and</span> ace</span><br><span class="line"> Pruning:	<span class="comment"># Step2</span></span><br><span class="line">	 acde <span class="keyword">is</span> removed because ade <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">in</span> L3 </span><br><span class="line"> C4 = &#123;abcd&#125;	<span class="comment"># OUTPUT</span></span><br></pre></td></tr></table></figure>
<p>我们发现Apriori算法是一个候选消除算法，每一次消除都需要扫描一次所有数据记录，造成整个算法在<strong>面临大数据集</strong>时显得效率低下（<code>多次扫描事务数据库</code>、<code>产生大量的候选集</code>、<code>对候选集的支持度计算产生了繁琐的工作量</code>）. 故而，我们需要了解$Fp-Growth$算法（如下）</p>
<h4 id="FP-growth"><a href="#FP-growth" class="headerlink" title="FP-growth"></a><strong>FP-growth</strong></h4><p>FpGrowth算法通过构造一个树结构来压缩数据记录，使得挖掘频繁项集只需要<strong>扫描两次数据记录</strong>，而且该算法不需要生成候选集合，所以效率会比较高。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Step <span class="number">1</span>：扫描数据记录，生成一级频繁项集，并按出现次数由多到少排序</span><br><span class="line">Step <span class="number">2</span>：再次扫描数据记录，对每条记录中出现在Step <span class="number">1</span>产生的表中的项，按表中的顺序排序。初始时，新建一个根结点，标记为null；</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Method</span></span><br><span class="line"> For each frequent item, construct its conditional pattern-base, <span class="keyword">and</span> then its conditional FP-tree</span><br><span class="line">  对于每个频繁项，<span class="string">'构造其条件模式库'</span>，然后<span class="string">'构造其条件FP树'</span></span><br><span class="line"> Repeat the process on <span class="string">'each'</span> newly created conditional FP-tree</span><br><span class="line">  对每个新创建的条件FP树重复此过程 (<span class="string">"Recursion: Mining Each Conditional FP-tree"</span>)</span><br><span class="line"> Until the resulting FP-tree <span class="keyword">is</span> empty, <span class="keyword">or</span> it contains only one path—single path will generate all the combinations of its sub-paths, each of which <span class="keyword">is</span> a frequent pattern</span><br><span class="line">  在生成的FP树为空之前，或者它只包含一个路径 - 单个路径将生成其子路径的所有组合，<span class="string">'每个路径都是一个频繁的模式'</span></span><br></pre></td></tr></table></figure>
<center><br>    <img src="/2019/01/01/数据分析与挖掘/FP-TreeConstruct.png" width="600"><br>    <img src="/2019/01/01/数据分析与挖掘/FP-TreeFindPatterns.png" width="600"><br>    <img src="/2019/01/01/数据分析与挖掘/FP-Tree_mConditional.png" width="600"><br>    <img src="/2019/01/01/数据分析与挖掘/MiningEachConditionalFP-tree.png" width="600"><br></center>

<p><a href="https://www.cnblogs.com/pinard/p/6307064.html" target="_blank" rel="noopener">FP Tree算法原理总结</a>、<a href="https://www.cnblogs.com/fengfenggirl/p/associate_fpgowth.html" target="_blank" rel="noopener">关联规则FpGrowth算法</a></p>
<h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><center><br><img src="/2019/01/01/数据分析与挖掘/fp_Summary.png" width="600"><br></center>

<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><ul>
<li><p>What is clustering</p>
<p>聚类就是按照某个特定标准(如距离准则)<strong>把一个数据集分割成不同的类或簇</strong>，使得同一个簇内的数据对象的<code>相似性</code>尽可能大，同时不在同一个簇中的数据对象的<code>差异性</code>也尽可能地大。即聚类后<code>同一类的数据尽可能聚集到一起，不同数据尽量分离</code>。</p>
<ul>
<li><p>Unsupervised learning </p>
<p>聚类是一种<code>输入数据无标签</code>的“分类”方式（即非监督学习），通常并不需要使用训练数据进行学习，仅把相似的东西聚到一起，并不关心所得的簇具体代表什么</p>
</li>
</ul>
</li>
</ul>
<h3 id="Partition-based—k-means"><a href="#Partition-based—k-means" class="headerlink" title="Partition-based—k-means"></a>Partition-based—k-means</h3><p>K-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，<code>将样本集划分为K个簇</code>。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。</p>
<center><br>    <img src="/2019/01/01/数据分析与挖掘/k-Means.gif" width="400"><br></center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- There are many variants of the K-Means method, varying <span class="keyword">in</span> different aspects</span><br><span class="line">  K-Means方法有许多变体，在不同方面有所不同</span><br><span class="line">- Choosing better initial centroid estimates</span><br><span class="line">  选择更好的初始质心估计</span><br><span class="line"> <span class="string">'k-means对初始值的设置很敏感'</span>: K-means++, Intelligent K-Means, Genetic K-Means</span><br><span class="line">- Choosing different representative prototypes <span class="keyword">for</span> the clusters</span><br><span class="line">  为集群选择不同的代表性原型</span><br><span class="line"> <span class="string">'k-means对噪声和离群值非常敏感'</span>: K-Medoids, K-Medians</span><br><span class="line"> <span class="string">'k-means只用于numerical，不适用于categorical类型数据'</span>: K-Modes</span><br><span class="line">- Applying feature transformation techniques</span><br><span class="line">  应用特征转换技术 </span><br><span class="line"> Weighted K-Means</span><br><span class="line"> <span class="string">'k-means不能解决非凸non-convex数据'</span>: Kernel K-Means</span><br></pre></td></tr></table></figure>
<ul>
<li><p>K-means++：选取新数据点作为新的聚类中心时，遵循的原则是与当前所属聚类中心距离最远的点，被选取作为聚类中心的概率较大</p>
</li>
<li><p>K-Medoids：Instead of taking the <strong>mean</strong> value of the object in a cluster as a reference<br>point, <strong>medoids</strong> can be used, which is the <code>most centrally located</code> object in a cluster 不使用聚类中对象的平均值作为参考点，而是可以使用中心点medoids，它是集群中最集中的对象（计算该点到当前聚簇中所有点距离之和，最终距离之后最小的点，则视为新的中心点）</p>
</li>
<li><p>K-Medians: Instead of taking the <strong>mean</strong> value of the object in a cluster as a reference point, <strong>medians</strong> are used (L1-norm as the distance measure) 不使用聚类中对象的平均值作为参考点，而是使用<code>中位数</code>（L1范数作为距离度量）</p>
</li>
<li><p>K-Modes: An extension to K-Means by replacing <strong>means</strong> of clusters with <strong>modes</strong> 通过用<code>众数</code>替换簇的平均值来扩展K-Means</p>
</li>
<li><p>Kernel K-Means : Project data onto the high-dimensional feature space using the kernel function, and then perform K-Means clustering 使用核函数将数据投影到高维特征空间，然后执行K-Means聚类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Typical kernel functions:</span><br><span class="line"> Polynomial kernel of degree <span class="string">'多项式核函数'</span></span><br><span class="line"> Gaussian radial basis function (RBF) kernel <span class="string">'高斯径向基核函数'</span></span><br><span class="line"> Sigmoid kernel <span class="string">'Sigmoid核函数'</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><a href="https://www.cnblogs.com/pinard/p/6164214.html" target="_blank" rel="noopener">K-Means聚类算法原理</a>、<a href="https://www.cnblogs.com/pinard/p/6169370.html" target="_blank" rel="noopener">用scikit-learn学习K-Means聚类</a></p>
<h3 id="Hierarchical-based—two-ways"><a href="#Hierarchical-based—two-ways" class="headerlink" title="Hierarchical-based—two ways"></a>Hierarchical-based—two ways</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Hierarchical clustering</span><br><span class="line"> Generate a clustering hierarchy(drawn <span class="keyword">as</span> a dendrogram) 生成聚类层次结构（绘制为树形图）</span><br><span class="line"> <span class="string">'Not required to specify K'</span>, the number of clusters 不需要指定聚类簇数</span><br><span class="line"> More deterministic 更具确定性</span><br><span class="line"> No iterative refinement 无迭代校准</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Two categories of algorithms</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'Agglomerative'</span>: Start <span class="keyword">with</span> singleton clusters, continuously merge two clusters at a time to build a bottom-up hierarchy of clusters</span><br><span class="line">  <span class="string">'凝聚'</span>：从单一集群开始，一次连续合并两个集群，构建<span class="string">'自下而上'</span>的集群层次结构</span><br><span class="line">	 Agglomerative clustering varies on <span class="string">'different similarity measures'</span> among clusters</span><br><span class="line">         Single link (<span class="string">'nearest'</span> neighbor) 	 Average link (group <span class="string">'average'</span>)</span><br><span class="line">    	 Complete link (<span class="string">'diameter'</span>直径) 	    Centroid link (<span class="string">'centroid'</span>重心 similarity)</span><br><span class="line">- <span class="string">'Divisive'</span>: Start <span class="keyword">with</span> a huge macro-cluster, split it continuously into two groups, generating a top-down hierarchy of clusters</span><br><span class="line">  <span class="string">'分裂'</span>：从一个庞大的宏集群开始，将其连续分成两组，生成一个<span class="string">'自上而下'</span>的集群层次结构</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="BIRCH"><a href="#BIRCH" class="headerlink" title="BIRCH"></a><strong>BIRCH</strong></h4><blockquote>
<p>BIRCH (Balanced Iterative Reducing and Clustering Using Hierarchies): Use CF-tree and incrementally adjust the quality of sub-clusters 利用层次方法的<code>平衡迭代</code>规约和聚类：使用CF树并逐步调整子集群的质量</p>
</blockquote>
<center><br><img src="/2019/01/01/数据分析与挖掘/CFTree.png" width="500"><br></center>

<p>将所有的训练集样本建立了CF Tree，一个基本的BIRCH算法就完成了，对应的输出就是若干个CF节点，每个节点里的样本点就是一个聚类的簇。<a href="https://www.cnblogs.com/pinard/p/6179132.html" target="_blank" rel="noopener">BIRCH聚类算法原理</a>、<a href="https://www.cnblogs.com/pinard/p/6200579.html" target="_blank" rel="noopener">用scikit-learn学习BIRCH聚类</a></p>
<h4 id="CURE"><a href="#CURE" class="headerlink" title="CURE"></a><strong>CURE</strong></h4><blockquote>
<p>CURE (Clustering Using REpresentatives): Represent a cluster using a set of well-scattered representative points  使用一组分散的代表点来表示聚类</p>
</blockquote>
<h4 id="CHAMELEON"><a href="#CHAMELEON" class="headerlink" title="CHAMELEON"></a>CHAMELEON</h4><blockquote>
<p>Hierarchical Clustering Using Dynamic Modeling,  A graph partitioning approach</p>
</blockquote>
<h4 id="Probabilistic-Hierarchical-Clustering"><a href="#Probabilistic-Hierarchical-Clustering" class="headerlink" title="Probabilistic Hierarchical Clustering"></a>Probabilistic Hierarchical Clustering</h4><blockquote>
<p>Use probabilistic models to measure distances between clusters  使用概率模型来测量簇之间的距离</p>
</blockquote>
<h3 id="Density-based—DBSCAN"><a href="#Density-based—DBSCAN" class="headerlink" title="Density-based—DBSCAN"></a>Density-based—DBSCAN</h3><blockquote>
<p>Clustering based on density (a local cluster criterion), such as density-connected points  基于密度的聚类（局部聚类标准），例如密度连接点</p>
</blockquote>
<h4 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h4><center><br>    <img src="/2019/01/01/数据分析与挖掘/DBSCAN_BasicConcept.png" width="500"><br></center>

<p>图中<code>MinPts=5</code>，红色的点都是<code>核心对象</code>，因为其ϵ-邻域至少有5个样本。黑色的样本是非核心对象。所有核心对象<code>密度直达</code>的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了<code>密度可达</code>的样本序列。在这些密度可达的样本序列的ϵ-邻域内所有的样本相互都是<code>密度相连</code>(对称性)的。</p>
<hr>
<p>DBSCAN密度聚类：由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。<a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">DBSCAN密度聚类</a>、<a href="https://www.cnblogs.com/pinard/p/6217852.html" target="_blank" rel="noopener">用scikit-learn学习DBSCAN聚类</a></p>
<h3 id="E-M-algorithm"><a href="#E-M-algorithm" class="headerlink" title="E-M algorithm"></a>E-M algorithm</h3><blockquote>
<p>K-Means （距离） -&gt;&gt;  E-M algorithm (概率分布)</p>
</blockquote>
<p>假设需要调查某校的男生和女生的身高分布。假设在校园里随机抽样100个男生和100个女生，共200个人。不知道抽取的这200个人里面的每一个人到底是从男生的那个身高分布里面抽取的，还是女生的那个身高分布抽取的。即就是，抽取得到的每个样本都不知道是从哪个分布抽取的。</p>
<p>EM的意思是“Expectation Maximization”，在上述问题中，先随便猜一下男生(身高)的正态分布的参数:如均值和方差是多少。例如男生的均值是1米7，方差是0.1米(当然了，刚开始肯定没那么准)，然后计算出每个人更可能属于第一个还是第二个正态分布中的(例如，这个人的身高是1米8，那很明显，他最大可能属于男生的那个分布)，这个是属于Expectation一步。有了每个人的归属，或者说已经大概地按上面的方法将这200个人分为男生和女生两部分，就可以根据之前说的最大似然那样，通过这些被大概分为男生的n个人来重新估计第一个分布的参数，女生的那个分布同样方法重新估计。这个是Maximization。然后，当更新了这两个分布的时候，每一个属于这两个分布的概率又变了，那么就再需要调整E步……如此往复，直到参数基本不再发生变化为止。</p>
<p>一个最直观了解EM算法思路的是K-Means算法。在K-Means聚类时，每个聚类簇的质心是隐含数据。我们会假设K个初始化质心，即EM算法的E步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即EM算法的M步。重复这个E步和M步，直到质心不再变化为止，这样就完成了K-Means聚类。EM算法的描述还很粗糙，我们需要用数学的语言精准描述。详见<a href="https://www.cnblogs.com/pinard/p/6912636.html" target="_blank" rel="noopener">EM算法原理总结</a></p>
<h3 id="AP-2007-Science"><a href="#AP-2007-Science" class="headerlink" title="AP (2007, Science)"></a>AP (2007, Science)</h3><p>AP（Affinity Propagation）一般翻译为近邻传播聚类<a href="https://blog.csdn.net/u010161379/article/details/51636926" target="_blank" rel="noopener">Affinity Propagation: AP聚类算法</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># basic concept</span></span><br><span class="line">* <span class="string">'Exemplar'</span>范例：即聚类簇中心点；</span><br><span class="line">* <span class="string">'similarity'</span>s(i,j)：数据点i与数据点j的<span class="string">'相似度值'</span>，一般使用欧氏距离的的负值表示，即s(i,j)值越大表示点i与j的距离越近，AP算法中理解为数据点j作为数据点i的聚类中心的能力； </span><br><span class="line">    * 相似度矩阵：作为算法的初始化矩阵，n个点就有由n乘n个相似度值组成的矩阵； </span><br><span class="line">* <span class="string">'Preference参考度'</span>s(i,i)：若按欧氏距离计算其值应为<span class="number">0</span>，但在AP聚类中其`表示数据点i作为聚类中心的程度`，因此不能为<span class="number">0</span>。迭代开始前假设所有点成为聚类中心的能力相同，因此参考度一般设为相似度矩阵中所有值得最小值或者中位数，但是参考度越大则说明个数据点成为聚类中心的能力越强，则最终聚类中心的个数则越多； </span><br><span class="line">* <span class="string">'Responsibility'</span>，r(i,k)：吸引度信息，表示数据点k`适合`作为数据点i的聚类中心的程度；(`k想做i的主公`)</span><br><span class="line">* <span class="string">'Availability'</span>，a(i,k)：归属度信息，表示数据点i选择数据点k作为其聚类中心的`合适`程度；(`i想做k的勇士`)</span><br><span class="line">* <span class="string">'Damping factor'</span>阻尼系数：为防止数据震荡，引入的衰减系数，`起到收敛作用`，每个信息值等于前一次迭代更新的信息值的λ倍加上此轮更新值得<span class="number">1</span>-λ倍，其中λ在<span class="number">0</span><span class="number">-1</span>之间，默认为<span class="number">0.5</span></span><br></pre></td></tr></table></figure>
<p>AP算法通过迭代过程不断更新每一个点的responsibility和availability,直到产生m个高质量的exemplar,同时将其余的数据点分配到相应的聚类中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 算法流程： </span></span><br><span class="line"><span class="number">1.</span> 更新相似度矩阵中每个点的吸引度信息，计算归属度信息； </span><br><span class="line"><span class="number">2.</span> 更新归属度信息，计算吸引度信息； </span><br><span class="line"><span class="number">3.</span> 对样本点的吸引度信息和归属度信息求和，检测其选择聚类中心的决策；若经过若干次迭代之后其聚类中心不变、或者迭代次数超过既定的次数、又或者一个子区域内的关于样本点的决策经过数次迭代后保持不变，则算法结束。</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关于其算法流程，知乎上kael 用户将AP聚类过程比喻为选举过程： </span></span><br><span class="line">* 所有人都参加选举（大家都是选民也都是参选人），要选出几个作为代表 </span><br><span class="line">* s(i,k)就相当于i对选k这个人的一个固有的偏好程度 </span><br><span class="line">* r(i,k)表示用s(i,k)减去最强竞争者的评分，可以理解为k在对i这个选民的竞争中的优势程度 </span><br><span class="line">* r(i,k)的更新过程对应选民i对各个参选人的挑选（越出众越有吸引力） </span><br><span class="line">* a(i,k)：从公式里可以看到，所有r(i’,k)&gt;<span class="number">0</span>的值都对a有正的加成。对应到我们这个比喻中，就相当于选民i通过网上关于k的民意调查看到：有很多人（即i’们）都觉得k不错（r(i’,k)&gt;<span class="number">0</span>），那么选民i也就会相应地觉得k不错，是个可以相信的选择 </span><br><span class="line">* a(i,k)的更新过程对应关于参选人k的民意调查对于选民i的影响（已经有了很多跟随者的人更有吸引力） </span><br><span class="line">* 两者交替的过程也就可以理解为`不断地参考各个参选人给出的民意调查`和`选民在各个参选人之间不断地比较`。 </span><br><span class="line">* r(i,k)的思想反映的是`竞争`，a(i,k)则是为了`让聚类更成功`。</span><br></pre></td></tr></table></figure>
<h3 id="Local-density-based-2014-Science"><a href="#Local-density-based-2014-Science" class="headerlink" title="Local density-based (2014, Science)"></a>Local density-based (2014, Science)</h3><blockquote>
<p>该方法假设聚类中心周围都是密度比其低的点，同时这些点到该聚类中心的距离比其到其他聚类中心更近。</p>
</blockquote>
<center><br><img src="/2019/01/01/数据分析与挖掘/LocalDensity-based.png" width="600"><br></center>

<ol>
<li>找出聚类中心:以通过给定的$δ<em>{min}$和$ρ</em>{min}$筛选出同时满足($ρ<em>i$ &gt; $ρ</em>{min}$)和($δ_ i$ &gt; $δ_{min}$)条件的点作为聚类中心点。 </li>
<li>剩余点的类别指派: 当前点的类别标签等于高于当前点密度的最近的点的标签一致。从而对所有点的类别进行了指定。</li>
<li>去除噪音：先算出类别之间的边界，然后找出边界中密度值最高的点的密度作为阈值只保留类别中大于或等于此密度值的点</li>
</ol>
<h3 id="Evaluation-of-Clustering"><a href="#Evaluation-of-Clustering" class="headerlink" title="Evaluation of Clustering"></a>Evaluation of Clustering</h3><h4 id="Clustering-evaluation"><a href="#Clustering-evaluation" class="headerlink" title="Clustering evaluation"></a>Clustering evaluation</h4><blockquote>
<p>Clustering Evaluation: Evaluating the <code>goodness</code> of clustering results (No commonly recognized best suitable measure in practice) 聚类评估：评估聚类结果的优劣（在实践中没有公认的最佳合适度量）</p>
</blockquote>
<h5 id="Three-categorization-of-measures"><a href="#Three-categorization-of-measures" class="headerlink" title="Three categorization of measures"></a>Three categorization of measures</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'External'</span>: Supervised, employ criteria <span class="keyword">not</span> inherent to the dataset</span><br><span class="line">  外部：监督，采用数据集不固有的标准 (使用<span class="string">'新数据'</span>)</span><br><span class="line"> Compare a clustering against prior <span class="keyword">or</span> expert-specified knowledge (i.e., the ground truth) using certain clustering quality measure</span><br><span class="line">  使用某些聚类质量测量将聚类与先前或专家指定的知识（即基础事实）进行比较</span><br><span class="line">- <span class="string">'Internal'</span>: Unsupervised, criteria derived <span class="keyword">from</span> data itself</span><br><span class="line">  内部：无监督，来自<span class="string">'数据本身'</span>的标准</span><br><span class="line"> Evaluate the goodness of a clustering by considering how well the clusters are separated <span class="keyword">and</span> how compact the clusters are, e.g., silhouette coefficient</span><br><span class="line">  通过考虑群集的分离程度以及群集的紧密程度（例如，轮廓系数）来评估群集的良好性</span><br><span class="line">- <span class="string">'Relative'</span>: Directly compare different clusterings, usually those obtained via different parameter settings <span class="keyword">for</span> the same algorithm</span><br><span class="line">  相对：直接比较不同的聚类，通常是通过<span class="string">'相同算法的不同参数'</span>设置获得的聚类</span><br></pre></td></tr></table></figure>
<h4 id="Clustering-stability"><a href="#Clustering-stability" class="headerlink" title="Clustering stability"></a>Clustering stability</h4><blockquote>
<p>Clustering stability : To understand the sensitivity of the clustering result to various algorithm parameters, e.g., # of clusters  聚类稳定性：理解聚类结果对各种<code>算法参数的敏感性</code>，例如聚类数</p>
</blockquote>
<h5 id="Methods-for-Finding-K-the-Number-of-Clusters"><a href="#Methods-for-Finding-K-the-Number-of-Clusters" class="headerlink" title="Methods for Finding K, the Number of Clusters"></a>Methods for Finding K, the Number of Clusters</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Empirical method <span class="string">'经验'</span>方法</span><br><span class="line"> <span class="comment"># of clusters: k ≈ sqrt(n / 2) for a dataset of n points (e.g., n = 200, k = 10)</span></span><br><span class="line">- Elbow method: Use the turning point <span class="keyword">in</span> the curve of the sum of within cluster variance <span class="keyword">with</span> respect to the <span class="comment"># of clusters</span></span><br><span class="line">  使用聚类方差之和与曲线群数之和的曲线中的<span class="string">'转折点'</span></span><br><span class="line">- Cross validation method </span><br><span class="line">  交叉验证法 (<span class="string">'试错，择优'</span>)</span><br><span class="line"> Divide a given data set into m parts</span><br><span class="line"> Use m – <span class="number">1</span> parts to obtain a clustering model</span><br><span class="line"> Use <span class="string">'the remaining part to test'</span> the quality of the clustering</span><br><span class="line">	 For example, <span class="keyword">for</span> each point <span class="keyword">in</span> the test set, find the closest centroid, <span class="keyword">and</span> use the sum of squared distance between all points <span class="keyword">in</span> the test set <span class="keyword">and</span> the closest centroids to measure how well <span class="string">'the model fits the test set'</span></span><br><span class="line"> For any k &gt; <span class="number">0</span>, <span class="string">'repeat it m times'</span>, compare the overall quality measure w.r.t. different k’s, <span class="keyword">and</span> find <span class="comment"># of clusters that fits the data the best</span></span><br></pre></td></tr></table></figure>
<h4 id="Clustering-tendency"><a href="#Clustering-tendency" class="headerlink" title="Clustering tendency"></a>Clustering tendency</h4><blockquote>
<p>Clustering tendency: Assess the suitability of clustering, i.e., whether the data has any inherent grouping structure  聚类趋势：评估聚类的<code>适用性</code>，即数据是否具有任何<code>固有的分组结构</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Still, there are some <span class="string">'clusterability assessment methods'</span>, such <span class="keyword">as</span></span><br><span class="line"> <span class="string">'Spatial histogram'</span>: Contrast the histogram of the data <span class="keyword">with</span> that generated <span class="keyword">from</span></span><br><span class="line">random samples</span><br><span class="line">  空间直方图：将数据的直方图与生成的直方图进行对比</span><br><span class="line"> <span class="string">'Distance distribution'</span>: Compare the pairwise point distance <span class="keyword">from</span> the data <span class="keyword">with</span> those <span class="keyword">from</span> the randomly generated samples</span><br><span class="line">  距离分布：将数据的成对点距离与随机生成的样本的距离进行比较</span><br><span class="line"> <span class="string">'Hopkins Statistic'</span>: A sparse sampling test <span class="keyword">for</span> spatial randomness</span><br><span class="line">  霍普金斯统计：空间随机性的稀疏抽样测试</span><br></pre></td></tr></table></figure>
<h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><center><br>    <img src="/2019/01/01/数据分析与挖掘/clustering_Summary.png" width="600"><br>    <img src="/2019/01/01/数据分析与挖掘/clustering_Summary2.png" width="600"><br></center>

<h2 id="Graph-clustering"><a href="#Graph-clustering" class="headerlink" title="Graph clustering"></a>Graph clustering</h2><ul>
<li><p>What is graph clustering </p>
<ul>
<li><p><strong>Complex network</strong></p>
<p>在我们的现实生活中，许多复杂系统都可以建模成一种复杂网络进行分析，比如常见的电力网络、航空网络、交通网络、计算机网络以及社交网络等等。复杂网络不仅是一种数据的表现形式，它同样也是一种科学研究的手段。</p>
</li>
<li><p>Graph clustering</p>
</li>
<li><p>Community </p>
</li>
<li><p>Module  </p>
</li>
</ul>
</li>
</ul>
<h3 id="Community-detection-algorithms"><a href="#Community-detection-algorithms" class="headerlink" title="Community detection_algorithms"></a>Community detection_algorithms</h3><h4 id="CPM-（Clique-Percolation-Method）"><a href="#CPM-（Clique-Percolation-Method）" class="headerlink" title="CPM （Clique Percolation Method）"></a>CPM （Clique Percolation Method）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># basic concept</span></span><br><span class="line"> <span class="string">'Clique'</span>: Complete graph  完全图(所有节点两两相连)</span><br><span class="line"> <span class="string">'k-clique'</span>: Complete graph <span class="keyword">with</span> k vertice(顶点) k-派系</span><br><span class="line"> <span class="string">'Adjacent k-cliques'</span>: Two k-cliques are adjacent when they `share k<span class="number">-1</span> nodes`</span><br><span class="line">   k-派系相邻：两个不同的k-派系共享k<span class="number">-1</span>个节点，认为他们相邻</span><br><span class="line"> <span class="string">'k-clique community'</span>: Union of all k-cliques that can be `reached <span class="keyword">from</span> each other` through a series of adjacent k-cliques.</span><br><span class="line">   k-派系连通：一个k-派系可以通过若干个相邻的k-派系到达另一个k-派系，则称这两个k-派系彼此联通</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Step1: 找到网络中大小为K的完全子图 Locate maximal cliques </span><br><span class="line">Step2: 将每个完全子图定义为一个节点，建立一个重叠矩阵</span><br><span class="line">Step3: 将重叠矩阵变成社区邻接矩阵(其中重叠矩阵中对角线小于k、非对角线小于k<span class="number">-1</span>的元素全置为<span class="number">0</span>,所有非<span class="number">0</span>项置为<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<center class="half"><br><img src="/2019/01/01/数据分析与挖掘/CPM1.png" width="300"><br><img src="/2019/01/01/数据分析与挖掘/CPM2.png" width="300"><br></center>

<p>从图中可以看出包含了两个社区{1，2，3，4}和{4，5，6，7，8}，节点4属于两个社区的重叠节点</p>
<p><a href="https://www.cnblogs.com/bethansy/p/6704712.html" target="_blank" rel="noopener">CPM（Cluster Percolation method）派系过滤算法</a></p>
<h4 id="Spectral-clustering"><a href="#Spectral-clustering" class="headerlink" title="Spectral clustering"></a>Spectral clustering</h4><center><br><img src="/2019/01/01/数据分析与挖掘/SpectralClustering.png" width="600"><br></center>

<p>谱聚类（Spectral Clustering），就是先用<code>Laplacian eigenmaps对数据降维</code>（简单地说，就是先将数据转换成邻接矩阵或相似性矩阵，再转换成Laplacian矩阵，再对Laplacian矩阵进行特征分解，把最小的K个特征向量排列在一起），然后再<code>使用k-means</code>完成聚类。谱聚类是个很好的方法，效果通常比k-means好，计算复杂度还低，这都要归功于降维的作用。 </p>
<p><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener">谱聚类（spectral clustering）原理总结_刘建平</a>、<a href="https://www.cnblogs.com/pinard/p/6235920.html" target="_blank" rel="noopener">用scikit-learn学习谱聚类</a></p>
<h4 id="Modularity-based-methods-–-G-Nand-Q"><a href="#Modularity-based-methods-–-G-Nand-Q" class="headerlink" title="Modularity based methods – G Nand Q"></a>Modularity based methods – G Nand Q</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> Calculate the betweenness <span class="keyword">for</span> all edges <span class="keyword">in</span> the network</span><br><span class="line">   计算每一条边的边介数.</span><br><span class="line"><span class="number">2.</span> Remove the edge <span class="keyword">with</span> the highest betweenness.</span><br><span class="line">   删除边界数最大的边.</span><br><span class="line"><span class="number">3.</span> Recalculate betweennesses <span class="keyword">for</span> all edges affected by the removal.</span><br><span class="line">   重新计算网络中剩下的边的边阶数.</span><br><span class="line"><span class="number">4.</span> Repeat <span class="keyword">from</span> step <span class="number">2</span> until no edges remain.</span><br><span class="line">   从步骤<span class="number">2</span>重复，直到没有边剩余.</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/aspirinvagrant/article/details/45599071" target="_blank" rel="noopener">社区发现算法（二）</a></p>
<h4 id="MCL"><a href="#MCL" class="headerlink" title="MCL"></a>MCL</h4><blockquote>
<p>MCL (Markov Cluster Algorithm) is a graph clustering algorithm. </p>
</blockquote>
<ul>
<li><p>Graph Clustering</p>
<p>和特征聚类不同，<code>图聚类</code>比较难以观察，整个算法以各点之间的距离作为突破口，可以这样形容：张三，是王五的好朋友，刚认识李四，对赵六很是反感。那么，对于该节点，我们无法直接得出他的特征，但能知道他的<code>活动圈</code>。利用图聚类，可以将同一社交范围的人聚合到一起。MCL就是属于图聚类的一种。</p>
</li>
<li><p>位于同一簇的点，其内部的联系应当紧密，而和外部的联系则比较少（惺惺相惜）</p>
<ul>
<li>如果你从一个点出发，到达其中的一个邻近点，那么你在’簇内的可能性’远大于’离开当前簇，到达新簇’的可能性——这就是MCL的核心思想。</li>
</ul>
</li>
</ul>
<h5 id="Random-walk"><a href="#Random-walk" class="headerlink" title="Random walk"></a>Random walk</h5><p>如果在一张图上进行多次的“<strong>Random Walks</strong>”，那么就有很大可能发现簇群，达到聚类的目的。而“<strong>Random Walks</strong>”的实现则是通过“<strong>Markov Chains</strong>”（马尔柯夫链）。</p>
<h5 id="Markov-chains"><a href="#Markov-chains" class="headerlink" title="Markov chains"></a>Markov chains</h5><p>Markov Chain——如果有由随机变量$X1,X2,X3$⋯组成的数列。$Xn$的值则是在时间$n$的状态，如果$X_{n+1}$对于过去状态的条件概率分布满足：$P(X_{n+1}=x|X_0,X_1,X_2,⋯,X_n)=P(X_{n+1}=x|X_n)$，则我们称其是一条Markov Chain.</p>
<ul>
<li>Markov Process——在给定当前知识或信息的情况下，过去（即当期以前的历史状态）对于预测将来（即当期以后的未来状态）是无关的。</li>
<li>下一步骤的概率仅依赖于当前概率</li>
</ul>
<h5 id="MCL-Algorithm"><a href="#MCL-Algorithm" class="headerlink" title="MCL Algorithm"></a>MCL Algorithm</h5><p>在MCL中， <strong>Expansion</strong> 和 <strong>Inflation</strong> 将不断的交替进行，<strong>Expansion</strong> 使得不同的区域之间的联系加强，而 <strong>Inflation</strong> 则不断的<code>分化</code>各点之间的联系(强者恒强，弱者恒弱)。经过多次迭代，将渐渐出现聚集现象，以此便达到了聚类的效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> 输入：一个非全连通图，Expansion 时的参数e和 Inflation 的参数r</span><br><span class="line"><span class="number">2.</span> 建立邻接矩阵</span><br><span class="line"><span class="number">3.</span> 添加自环(避免<span class="number">0</span>概率情况)</span><br><span class="line"><span class="number">4.</span> 标准化概率矩阵</span><br><span class="line"><span class="number">5.</span> Expansion操作，每次对矩阵进行e次幂方</span><br><span class="line"><span class="number">6.</span> Inflation操作，每次对矩阵内元素进行r次幂方，再进行标准化</span><br><span class="line"><span class="number">7.</span> 重复步骤<span class="number">5</span>和<span class="number">6</span>，直到达到稳定</span><br><span class="line"><span class="number">8.</span> 将结果矩阵转化为聚簇</span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/magle/p/7672957.html" target="_blank" rel="noopener">聚类算法——MCL</a></p>
<h3 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h3><center><br><img src="/2019/01/01/数据分析与挖掘/graphMining_Summary.png" width="600"><br><img src="/2019/01/01/数据分析与挖掘/CommunityDetection_Summary.png" width="600"><br></center>

<h2 id="Todo"><a href="#Todo" class="headerlink" title="Todo"></a>Todo</h2><ul>
<li><p>看相关参考书目《数据挖掘：概念与技术》《数据挖掘导论》课后例题  着重看”简单计算”</p>
</li>
<li><p>Collaborative Filtering</p>
<p><a href="https://www.cnblogs.com/pinard/p/6349233.html" target="_blank" rel="noopener">协同过滤推荐算法总结</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6351319.html" target="_blank" rel="noopener">矩阵分解在协同过滤推荐算法中的应用</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6362647.html" target="_blank" rel="noopener">SimRank协同过滤推荐算法</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/pinard/p/6912636.html" target="_blank" rel="noopener">EM算法原理总结</a></p>
</li>
<li><p>特征工程</p>
<p><a href="https://www.cnblogs.com/pinard/p/9032759.html" target="_blank" rel="noopener">特征工程之特征选择</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/9061549.html" target="_blank" rel="noopener">特征工程之特征表达</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/9093890.html" target="_blank" rel="noopener">特征工程之特征预处理</a></p>
</li>
</ul>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2019-01-08T12:24:00.099Z" itemprop="dateUpdated">2019-01-08 20:24:00</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="CaptainSE">
            CaptainSE
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            

            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/01/01/数据分析与挖掘/&title=《数据分析与挖掘》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/01/01/数据分析与挖掘/&title=《数据分析与挖掘》 — Go Further&source=【阅读时间】XXX min XXX words【内容描述】 基于《数据挖掘：概念与技术》的简述，欲了解细节，强烈建议读原书！！！" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/01/01/数据分析与挖掘/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《数据分析与挖掘》 — Go Further&url=http://yoursite.com/2019/01/01/数据分析与挖掘/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/01/01/数据分析与挖掘/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2019/01/06/数据分析与挖掘Exam/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">数据分析与挖掘Exam</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/12/31/visdom-Tutorial/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">visdom_Tutorial</h4>
      </a>
    </div>
  
</nav>



    

















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢老板~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>CaptainSE &copy; 2015 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/01/01/数据分析与挖掘/&title=《数据分析与挖掘》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/01/01/数据分析与挖掘/&title=《数据分析与挖掘》 — Go Further&source=【阅读时间】XXX min XXX words【内容描述】 基于《数据挖掘：概念与技术》的简述，欲了解细节，强烈建议读原书！！！" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/01/01/数据分析与挖掘/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《数据分析与挖掘》 — Go Further&url=http://yoursite.com/2019/01/01/数据分析与挖掘/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/01/01/数据分析与挖掘/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACJUlEQVR42u3aS27jMBAFQN//0p5tBoii95pRAJOllWHJUpcWdH/4esXH+8tx9f3Xs1ff5Hd7PXFgYGB8LOP945E84Odf5eD87DeRYGBgHMC4Cu7qmvybWeh5bBgYGBg5KU8Q85QRAwMDY51xk64FS3N7FgMD42TGLF1rC86kofZ4LY6BgfGBjLzr/vefH5lvYGBgfBTjXR7JspgvyrMYvokKAwNja0a+wCWPbzdMrCy4GBgY5zDasnN9bJlfU7wUDAyMrRnt4HClVl6H3USIgYGxKSMJdzaeHGap8XP/ezoGBsaRjDxpy4vS9vriSgwMjK0ZK42zWettliBGKSkGBsamjJXQ80Swfa81DAMDY2vGeptsNiRY+W1+TwwMjP0Y7SLYji1no9P6pWBgYGzHyDeStuA8xLYxdzMYwMDAOIDRlojtmLMdBtSbLTAwMLZjtEVsWxPnW2PzNBQDA+NkxqwEnQ1BV7qANxFiYGAcwGiTs/bxKyVuseBiYGBsx5gVmes9+TZLvflLwMDAOIAxGzfmi2bbjEvug4GBcRojHy62pHrfR1C+YmBgnMNYCSvZtpVvy8hHETf/GBgYGNsxVgrI9cSubf0PQ8fAwNiC0Ral7XaKJxZcDAyMMxn1toZfaui3BW0xg8XAwMBoR4wjPAYGBsaMkbTekrvNytrLZRoDA+MARlKOrrT+f2tgeXlPDAyMrRntTVfmD/lreiT1xMDA+DzGP7RILLo3zGEQAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            clearTimeout(titleTime);
        } else {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
