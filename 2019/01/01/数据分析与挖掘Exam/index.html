<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>数据分析与挖掘Exam | Go Further | Stay Hungry, Stay Foolish</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#142421">
    
    
    <meta name="keywords" content="">
    <meta name="description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta property="og:type" content="article">
<meta property="og:title" content="数据分析与挖掘Exam">
<meta property="og:url" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/index.html">
<meta property="og:site_name" content="Go Further">
<meta property="og:description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/NormalDistributionCurve.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/Boxplot.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/ProximityMeasureforBinaryAttributes.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/Dissimilarity%20betweenAsymmetricBinaryVariables.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/CosineSimilarityofTwoVectors.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/CalculatingCosineSimilarity.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/dataPreprocess_mainTask.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/DataReduction.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/Histogram_Clustering_Sampling.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/DataCubeAggregation.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/DataCompression.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/Normalization.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/AutomaticConceptHierarchyGeneration.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/Summary_datapreprocessing.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/SupervisedLearning.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/ConfusionMatrix.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/CorrelationAnalysis.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/CorrelationAnalysisExample.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/CorrelationBetweenTwoNumericalVariables.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/VarianceForSingleVariable.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/CovarianceForTwoVariables.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/Example_CalculationOfCovariance.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/Binning.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/BinningExample.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/BinningvsClustering.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/Example-AttributeSelection_withInformationGain.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/ComputationOfGiniIndex.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/ClassifierEvaluationMetrics.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/ClassifierEvaluationMetrics1.png">
<meta property="og:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/ClassifierEvaluationMetricsExample.png">
<meta property="og:updated_time" content="2019-01-03T10:16:54.014Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="数据分析与挖掘Exam">
<meta name="twitter:description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta name="twitter:image" content="http://yoursite.com/2019/01/01/数据分析与挖掘Exam/NormalDistributionCurve.png">
    
        <link rel="alternate" type="application/atom+xml" title="Go Further" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">CaptainSE</h5>
          <a href="mailto:841145636@qq.com" title="841145636@qq.com" class="mail">841145636@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/Captainzj" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">数据分析与挖掘Exam</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">数据分析与挖掘Exam</h1>
        <h5 class="subtitle">
            
                <time datetime="2019-01-01T10:56:52.000Z" itemprop="datePublished" class="page-time">
  2019-01-01
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/XD/">XD</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Exam"><span class="post-toc-number">1.</span> <span class="post-toc-text">Exam</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Outline"><span class="post-toc-number">2.</span> <span class="post-toc-text">Outline</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Introduction"><span class="post-toc-number">3.</span> <span class="post-toc-text">Introduction</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#What-is-datamining"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">What is datamining</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#What-is-machine-learning"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">What is machine learning</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#What-is-artificial-intelligence"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">What is artificial intelligence</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Data"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">Data</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-types"><span class="post-toc-number">3.4.1.</span> <span class="post-toc-text">Data types</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-statistics"><span class="post-toc-number">3.4.2.</span> <span class="post-toc-text">Data statistics</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Data-dispersion-分散-characteristics"><span class="post-toc-number">3.4.2.1.</span> <span class="post-toc-text">Data dispersion(分散) characteristics</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Graphic-Displays-of-Basic-Statistical-Descriptions"><span class="post-toc-number">3.4.2.2.</span> <span class="post-toc-text">Graphic Displays of Basic Statistical Descriptions</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Distance-on-Numeric-Data"><span class="post-toc-number">3.4.2.3.</span> <span class="post-toc-text">Distance on Numeric Data</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Proximity-Measure-for-Binary-Attributes"><span class="post-toc-number">3.4.2.4.</span> <span class="post-toc-text">Proximity Measure for Binary Attributes</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Dissimilarity-between-Asymmetric-Binary-Variables"><span class="post-toc-number">3.4.2.5.</span> <span class="post-toc-text">Dissimilarity between Asymmetric Binary Variables</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Cosine-Similarity-of-Two-Vectors"><span class="post-toc-number">3.4.2.6.</span> <span class="post-toc-text">Cosine Similarity of Two Vectors</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Calculating-Cosine-Similarity"><span class="post-toc-number">3.4.2.7.</span> <span class="post-toc-text">Calculating Cosine Similarity</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Data-preprocessing"><span class="post-toc-number">3.5.</span> <span class="post-toc-text">Data preprocessing</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-cleaning"><span class="post-toc-number">3.5.1.</span> <span class="post-toc-text">Data cleaning</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#How-to-Handle-Missing-Data"><span class="post-toc-number">3.5.1.1.</span> <span class="post-toc-text">How to Handle Missing Data?</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#How-to-Handle-Noisy-Data"><span class="post-toc-number">3.5.1.2.</span> <span class="post-toc-text">How to Handle Noisy Data?</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-integrating"><span class="post-toc-number">3.5.2.</span> <span class="post-toc-text">Data integrating</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Handling-Redundancy-in-Data-Integration"><span class="post-toc-number">3.5.2.1.</span> <span class="post-toc-text">Handling Redundancy in Data Integration</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-Reduction"><span class="post-toc-number">3.5.3.</span> <span class="post-toc-text">Data Reduction</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Methods-for-data-reduction"><span class="post-toc-number">3.5.3.1.</span> <span class="post-toc-text">Methods for data reduction</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-transforming"><span class="post-toc-number">3.5.4.</span> <span class="post-toc-text">Data transforming</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Normalization-归一化"><span class="post-toc-number">3.5.4.1.</span> <span class="post-toc-text">Normalization 归一化</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Concept-hierarchy-generation-概念层次生成"><span class="post-toc-number">3.5.4.2.</span> <span class="post-toc-text">Concept hierarchy generation 概念层次生成</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Dimensionality-Reduction-Techniques"><span class="post-toc-number">3.5.4.3.</span> <span class="post-toc-text">Dimensionality Reduction Techniques</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Summary"><span class="post-toc-number">3.6.</span> <span class="post-toc-text">Summary</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Classification"><span class="post-toc-number">4.</span> <span class="post-toc-text">Classification</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#What-is-classification"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">What is classification?</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Supervised-learning"><span class="post-toc-number">4.1.1.</span> <span class="post-toc-text">Supervised learning</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Steps"><span class="post-toc-number">4.1.2.</span> <span class="post-toc-text">Steps</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Model-construction-模型构建"><span class="post-toc-number">4.1.2.1.</span> <span class="post-toc-text">Model construction 模型构建</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Model-Validation-and-Testing"><span class="post-toc-number">4.1.2.2.</span> <span class="post-toc-text">Model Validation and Testing</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Model-Deployment"><span class="post-toc-number">4.1.2.3.</span> <span class="post-toc-text">Model Deployment</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Algorithms"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">Algorithms</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Decision-tree-ID3-C4-5-CART"><span class="post-toc-number">4.2.1.</span> <span class="post-toc-text">Decision tree-ID3,C4.5,CART</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#SVM"><span class="post-toc-number">4.2.2.</span> <span class="post-toc-text">SVM</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Bayes-【point】"><span class="post-toc-number">4.2.3.</span> <span class="post-toc-text">Bayes 【point】</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ANN"><span class="post-toc-number">4.2.4.</span> <span class="post-toc-text">ANN</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Model-evaluation-and-selection"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">Model evaluation and selection</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Classifier-Evaluation-Metrics"><span class="post-toc-number">4.3.1.</span> <span class="post-toc-text">Classifier Evaluation Metrics</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Confusion-matrix-and-criteria"><span class="post-toc-number">4.3.1.1.</span> <span class="post-toc-text">Confusion matrix and criteria</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Estimating-a-classifier’s-accuracy"><span class="post-toc-number">4.3.2.</span> <span class="post-toc-text">Estimating a classifier’s accuracy</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Cross-evaluation"><span class="post-toc-number">4.3.2.1.</span> <span class="post-toc-text">Cross-evaluation</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Ensemble-methods"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">Ensemble methods</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Bagging"><span class="post-toc-number">4.4.1.</span> <span class="post-toc-text">Bagging</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Boosting"><span class="post-toc-number">4.4.2.</span> <span class="post-toc-text">Boosting</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Random-Forest"><span class="post-toc-number">4.4.3.</span> <span class="post-toc-text">Random Forest</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Frequent-patterns"><span class="post-toc-number">5.</span> <span class="post-toc-text">Frequent patterns</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Clustering"><span class="post-toc-number">6.</span> <span class="post-toc-text">Clustering</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Graph-clustering"><span class="post-toc-number">6.1.</span> <span class="post-toc-text">Graph clustering</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Todo"><span class="post-toc-number">7.</span> <span class="post-toc-text">Todo</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Calculate"><span class="post-toc-number">8.</span> <span class="post-toc-text">Calculate</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Correlation-Analysis"><span class="post-toc-number">8.1.</span> <span class="post-toc-text">Correlation Analysis</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Covariance"><span class="post-toc-number">8.2.</span> <span class="post-toc-text">Covariance</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Simple-Discretization-Binning"><span class="post-toc-number">8.3.</span> <span class="post-toc-text">Simple Discretization: Binning</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Example-Attribute-Selection-with-Information-Gain"><span class="post-toc-number">8.3.1.</span> <span class="post-toc-text">Example: Attribute Selection with Information Gain</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Computation-of-Gini-Index"><span class="post-toc-number">8.3.2.</span> <span class="post-toc-text">Computation of Gini Index</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Classifier-Evaluation-Metrics-Example"><span class="post-toc-number">8.3.3.</span> <span class="post-toc-text">Classifier Evaluation Metrics: Example</span></a></li></ol></li></ol></li></ol>
        </nav>
    </aside>


<article id="post-数据分析与挖掘Exam"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">数据分析与挖掘Exam</h1>
        <div class="post-meta">
            <time class="post-time" title="2019-01-01 18:56:52" datetime="2019-01-01T10:56:52.000Z"  itemprop="datePublished">2019-01-01</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/XD/">XD</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p>
<a id="more"></a>
<h2 id="Exam"><a href="#Exam" class="headerlink" title="Exam"></a>Exam</h2><table>
<thead>
<tr>
<th style="text-align:center">题型</th>
<th style="text-align:center">题量（道）</th>
<th style="text-align:center">分值（分）</th>
<th style="text-align:center">总计（分）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">选择题</td>
<td style="text-align:center">10</td>
<td style="text-align:center">2</td>
<td style="text-align:center">20</td>
</tr>
<tr>
<td style="text-align:center">计算题</td>
<td style="text-align:center">3</td>
<td style="text-align:center">10</td>
<td style="text-align:center">30</td>
</tr>
<tr>
<td style="text-align:center">简答题</td>
<td style="text-align:center">4</td>
<td style="text-align:center">5</td>
<td style="text-align:center">20</td>
</tr>
<tr>
<td style="text-align:center">论述题</td>
<td style="text-align:center">2</td>
<td style="text-align:center">15</td>
<td style="text-align:center">30</td>
</tr>
</tbody>
</table>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><ul>
<li><a href="#Introduction">Introduction</a></li>
<li><a href="#Classification">Classification</a></li>
<li><a href="#Frequent patterns">Frequent patterns</a></li>
<li><a href="#Clustering">Clustering</a></li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="What-is-datamining"><a href="#What-is-datamining" class="headerlink" title="What is datamining"></a>What is datamining</h3><h3 id="What-is-machine-learning"><a href="#What-is-machine-learning" class="headerlink" title="What is machine learning"></a>What is machine learning</h3><h3 id="What-is-artificial-intelligence"><a href="#What-is-artificial-intelligence" class="headerlink" title="What is artificial intelligence"></a>What is artificial intelligence</h3><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><h4 id="Data-types"><a href="#Data-types" class="headerlink" title="Data types"></a>Data types</h4><h4 id="Data-statistics"><a href="#Data-statistics" class="headerlink" title="Data statistics"></a>Data statistics</h4><h5 id="Data-dispersion-分散-characteristics"><a href="#Data-dispersion-分散-characteristics" class="headerlink" title="Data dispersion(分散) characteristics"></a>Data dispersion(分散) characteristics</h5><ul>
<li><p><strong>Mean</strong></p>
<ul>
<li><p>Mean (algebraic measure) (sample vs. population):</p>
<p>Note: $n$ is sample size and $N$ is population size. </p>
<p>$\underbrace{\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i}<em>\text{sample}$    $\underbrace{\mu  = \frac{1}{N}\sum</em>{i=1}^{n}x_i}_\text{population}$</p>
</li>
<li><p>Weighted arithmetic mean: 算术平均数/加权平均数</p>
<p>$\bar{x}=\frac{\sum_{i=1}^{n}w_ix}{\sum_{i=1}^{n}w_i}$</p>
</li>
</ul>
</li>
<li><p><strong>Median</strong></p>
<p>Middle value if odd number of values, or average of the middle two values otherwise</p>
</li>
<li><p><strong>Mode</strong><br>Value that occurs most frequently in the data</p>
</li>
<li><p><strong>Properties of Normal Distribution Curve</strong></p>
<center><br>    <img src="/2019/01/01/数据分析与挖掘Exam/NormalDistributionCurve.png" style="zoom:40%"><br></center>
</li>
<li><p><strong>Variance and Standard Deviation (sample: s, population: σ)</strong></p>
<ul>
<li><p><strong>Variance</strong>: (algebraic, scalable computation)</p>
<p>$s^2=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-x)^2=\frac{1}{n-1}[\sum_{i=1}^{n}{x_i}^2-\frac{1}{n}(\sum_{i=1}^{n}x_i)^2]$</p>
<p>${\sigma}^2 =\frac{1}{N}\sum_{i=1}^{n}(x_i-\mu)^2 = \frac{1}{N}\sum_{i=1}^{n}{x_i}^2-{\mu}^2 $</p>
</li>
<li><p><strong>Standard deviation</strong> $s$ (or $σ$) is the square root of variance $s^2$ (or $σ^2$)</p>
<p>$s = \sqrt{s^2}$</p>
<p>$\sigma = \sqrt{\sigma^2} $</p>
</li>
</ul>
</li>
<li><p><strong>standardized measure (z-score)</strong></p>
<p>$z=\frac{x-\mu}{\sigma}$</p>
</li>
</ul>
<h5 id="Graphic-Displays-of-Basic-Statistical-Descriptions"><a href="#Graphic-Displays-of-Basic-Statistical-Descriptions" class="headerlink" title="Graphic Displays of Basic Statistical Descriptions"></a>Graphic Displays of Basic Statistical Descriptions</h5><ul>
<li><p><strong>Boxplot:</strong> graphic display of five-number summary  箱线图</p>
<div style="width:1000px;margin:0;padding:0"><br>    <div style="float:left;width:200px;"><img src="/2019/01/01/数据分析与挖掘Exam/Boxplot.png"></div><br>    <div style="float:right;width:800px;padding:14px"><br>        &bull; <b>Quartiles</b>: Q1 (25th percentile), Q3 (75th percentile)<br><br>        &bull; <b>Inter-quartile range</b>: IQR = Q3 – Q1 <br><br>        &bull; <b>Five number summary</b>: min, Q1, median, Q3, max<br><br>        &bull; <b>Boxplot</b>: Data is represented with a box<br><br>        &emsp; &bull; <b>Q1, Q3, IQR</b>:  The ends of the box are at the first and third quartiles, i.e., the height of the box is IQR<br><br>        &emsp; &bull; <b>Median (Q2)</b> is marked by a line within the box <br><br>        &emsp; &bull; <b>Whiskers</b>: two lines outside the box extended to Minimum and Maximum<br><br>    </div><br>    <div style="clear:both"></div><br></div>
</li>
<li><p><strong>Histogram:</strong> x-axis are values, y-axis repres. frequencies 柱状图/直方图</p>
</li>
<li><p><strong>Quantile plot:</strong>  each value $x_i$  is paired with $f_i$  indicating that approximately $100 f_i \%$ of data  are ​$\leq  x_i​$ 分位图</p>
</li>
<li><p><strong>Quantile-quantile (q-q) plot:</strong> graphs the quantiles of one univariant distribution against the corresponding quantiles of another 绘制一个单变量分布的分位数与另一个分配的相应分位数的关系图.QQPlot图是用于直观验证一组数据是否来自某个分布，或者验证某两组数据是否来自同一（族）分布。在教学和软件中<code>常用的是检验数据是否来自于正态分布</code>。</p>
</li>
<li><p><strong>Scatter plot:</strong> each pair of values is a pair of coordinates and plotted as points in the plane 每对值是一对坐标并绘制为平面中的点 </p>
</li>
</ul>
<h5 id="Distance-on-Numeric-Data"><a href="#Distance-on-Numeric-Data" class="headerlink" title="Distance on Numeric Data"></a>Distance on Numeric Data</h5><p><strong>Dissimilarity (distance) matrix</strong>:  Usually symmetric, thus a <code>triangular matrix</code><br>$$ 
\begin{pmatrix}
0 &  &  & \\ 
d(2,1) & 0  &  & \\ 
... & ... & ... & \\ 
d(n,1) & d(n,2)  & ... & 0 
\end{pmatrix}
$$ </p>
<ul>
<li><p><strong>Minkowski distance</strong><br>$$ 
  d(i,j)=\sqrt[p]{\left | x_{i1}-x_{j1} \right |^p+\left | x_{i2}-x_{j2} \right |^p+……+\left | x_{il}-x_{jl} \right |^p}
  $$ </p>
</li>
<li><p>$p = 1: (L_1 norm)$ <strong>Manhattan (or city block) distance</strong></p>
<p>E.g.,the Hamming distance: the number of bits that are different between two binary<br>vectors<br>$$ 
  d(i,j)=\left | x_{i1}-x_{j1} \right |+\left | x_{i2}-x_{j2} \right |+……+\left | x_{il}-x_{jl} \right |
  $$ </p>
</li>
<li><p>$p = 2:  (L_2 norm)$ <strong>Euclidean distance</strong></p>
<p>$$ 
  d(i,j)=\sqrt{\left | x_{i1}-x_{j1} \right |^2+\left | x_{i2}-x_{j2} \right |^2+……+\left | x_{il}-x_{jl} \right |^2}
  $$ </p>
</li>
<li><p>$p→ ∞: (L_{max} norm,L_∞ norm) $<strong>“supremum” distance</strong></p>
<p>The maximum difference between any component (attribute) of the vectors</p>
<p>$$ 
  d(i,j)=\lim_{p→ ∞}\sqrt[p]{\left | x_{i1}-x_{j1} \right |^p+\left | x_{i2}-x_{j2} \right |^p+……+\left | x_{il}-x_{jl} \right |^p}=\max_{f=1}^{l}\left|x_{if}-x_{jf}\right|
  $$ </p>
</li>
</ul>
<h5 id="Proximity-Measure-for-Binary-Attributes"><a href="#Proximity-Measure-for-Binary-Attributes" class="headerlink" title="Proximity Measure for Binary Attributes"></a>Proximity Measure for Binary Attributes</h5><center><br>    <img src="/2019/01/01/数据分析与挖掘Exam/ProximityMeasureforBinaryAttributes.png" style="zoom:50%"><br></center>

<h5 id="Dissimilarity-between-Asymmetric-Binary-Variables"><a href="#Dissimilarity-between-Asymmetric-Binary-Variables" class="headerlink" title="Dissimilarity between Asymmetric Binary Variables"></a>Dissimilarity between Asymmetric Binary Variables</h5><center><br><img src="/2019/01/01/数据分析与挖掘Exam/Dissimilarity betweenAsymmetricBinaryVariables.png" style="zoom:50%"><br></center>

<h5 id="Cosine-Similarity-of-Two-Vectors"><a href="#Cosine-Similarity-of-Two-Vectors" class="headerlink" title="Cosine Similarity of Two Vectors"></a>Cosine Similarity of Two Vectors</h5><center><br><img src="/2019/01/01/数据分析与挖掘Exam/CosineSimilarityofTwoVectors.png" style="zoom:50%"><br></center>

<h5 id="Calculating-Cosine-Similarity"><a href="#Calculating-Cosine-Similarity" class="headerlink" title="Calculating Cosine Similarity"></a>Calculating Cosine Similarity</h5><center><br><img src="/2019/01/01/数据分析与挖掘Exam/CalculatingCosineSimilarity.png" style="zoom:50%"><br></center>

<h3 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">常用方法   计算公式(零碎)</span><br><span class="line">缺失值 处理方法</span><br></pre></td></tr></table></figure>
<center><br><img src="/2019/01/01/数据分析与挖掘Exam/dataPreprocess_mainTask.png" width="600"><br></center>

<h4 id="Data-cleaning"><a href="#Data-cleaning" class="headerlink" title="Data cleaning"></a>Data cleaning</h4><p>Handle <code>missing data</code>(Incomplete), smooth <code>noisy data</code>,identify or remove <code>outliers</code>, and resolve <code>inconsistencies</code> 处理丢失的数据，平滑噪声数据，识别或删除异常值，并解决不一致问题</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Data discrepancy detection  数据差异检测</span><br><span class="line">    - Use metadata (e.g., domain, range, dependency, distribution) </span><br><span class="line">      使用元数据（例如，域，范围，依赖关系，分发）</span><br><span class="line">    - Check field overloading  检查字段重载</span><br><span class="line">    - Check uniqueness rule, consecutive rule <span class="keyword">and</span> null rule</span><br><span class="line">      检查唯一性规则，连续规则和空规则</span><br><span class="line">    - Use commercial tools 使用商业工具</span><br><span class="line">      - Data scrubbing: use simple domain knowledge (e.g., postal code, spell-check) to detect errors <span class="keyword">and</span> make corrections 数据清理：使用简单的域知识（例如，邮政编码，拼写检查）来检测错误并进行更正</span><br><span class="line">      - Data auditing: by analyzing data to discover rules <span class="keyword">and</span> relationship to detect violators (e.g., correlation <span class="keyword">and</span> clustering to find outliers)  数据审计：通过分析数据来发现规则和检测违规者的关系（例如，关联和聚类以查找异常值）</span><br><span class="line"></span><br><span class="line">- Data migration <span class="keyword">and</span> integration 数据迁移和集成</span><br><span class="line">  - Data migration tools: allow transformations to be specified 数据迁移工具：允许指定转换</span><br><span class="line">  - ETL (Extraction/Transformation/Loading) tools: allow users to specify transformations through a graphical user interface  ETL（提取/转换/加载）工具：允许用户通过图形用户界面指定转换</span><br><span class="line"></span><br><span class="line">- Integration of the two processes </span><br><span class="line">  - Iterative <span class="keyword">and</span> interactive (e.g., Potter’s Wheels)  迭代和互动（例如，波特的轮子）</span><br></pre></td></tr></table></figure>
<h5 id="How-to-Handle-Missing-Data"><a href="#How-to-Handle-Missing-Data" class="headerlink" title="How to Handle Missing Data?"></a>How to Handle Missing Data?</h5><blockquote>
<p>1.忽略元组  2.手动填充  3. 自动（以unknown/均值/最可能的值）填充</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Ignore the tuple: usually done when <span class="class"><span class="keyword">class</span> <span class="title">label</span> <span class="title">is</span> <span class="title">missing</span> <span class="params">(when doing classification)</span>—<span class="title">not</span> <span class="title">effective</span> <span class="title">when</span> <span class="title">the</span> % <span class="title">of</span> <span class="title">missing</span> <span class="title">values</span> <span class="title">per</span> <span class="title">attribute</span> <span class="title">varies</span> <span class="title">considerably</span> 忽略元组：通常在缺少类标签时（进行分类时）完成 - 当每个属性的缺失值百分比变化很大时<span class="params">(该属性)</span>无效</span></span><br><span class="line"><span class="class"></span></span><br><span class="line">- Fill in the missing value manually: tedious + infeasible? 手动填写缺失值：单调乏味+不可行？</span><br><span class="line"></span><br><span class="line">- Fill <span class="keyword">in</span> it automatically <span class="keyword">with</span></span><br><span class="line">  - a <span class="keyword">global</span> constant : e.g., “unknown”, a new <span class="class"><span class="keyword">class</span>?! </span></span><br><span class="line"><span class="class">  - <span class="title">the</span> <span class="title">attribute</span> <span class="title">mean</span>  属性平均值<span class="params">(与下一条的不同？)</span></span></span><br><span class="line"><span class="class">  - <span class="title">the</span> <span class="title">attribute</span> <span class="title">mean</span> <span class="title">for</span> <span class="title">all</span> <span class="title">samples</span> <span class="title">belonging</span> <span class="title">to</span> <span class="title">the</span> <span class="title">same</span> <span class="title">class</span>:</span> <span class="string">'smarter'</span></span><br><span class="line">  - the most probable value: inference-based such <span class="keyword">as</span> Bayesian formula <span class="keyword">or</span> decision tree</span><br></pre></td></tr></table></figure>
<h5 id="How-to-Handle-Noisy-Data"><a href="#How-to-Handle-Noisy-Data" class="headerlink" title="How to Handle Noisy Data?"></a>How to Handle Noisy Data?</h5><blockquote>
<p>1.分档  2.回归 3.聚类（无监督） 4.半监督</p>
</blockquote>
<ul>
<li><p><a href="#Simple Discretization: Binning">Binning</a> 分档</p>
<ul>
<li><p>First sort data and partition into (equal-frequency) bins 首先将数据排序并分区为（等频）箱</p>
</li>
<li><p>Then one can smooth by bin means, smooth by bin median, smooth by bin boundaries, etc.然后，可以通过bin均值平滑，通过bin中值平滑，通过bin边界平滑等。</p>
</li>
</ul>
</li>
<li><p>Regression 回归</p>
<ul>
<li>Smooth by fitting the data into regression functions 通过将数据拟合到回归函数中来平滑</li>
</ul>
</li>
<li><p>Clustering 聚类</p>
<ul>
<li>Detect and remove outliers 检测并删除异常值</li>
</ul>
</li>
<li><p>Semi-supervised: Combined computer and human inspection 半监督：计算机和人工检查相结合</p>
<ul>
<li>Detect suspicious values and check by human (e.g., deal with possible outliers) 检测可疑值并由人检查（例如，处理可能的异常值）</li>
</ul>
</li>
</ul>
<h4 id="Data-integrating"><a href="#Data-integrating" class="headerlink" title="Data integrating"></a>Data integrating</h4><p>Integration of multiple databases, data cubes, or files 集成多个数据库，数据立方体或文件</p>
<blockquote>
<p>1.数据集成 2.模式集成 3.实体识别 4.检测和解决数据值的冲突</p>
</blockquote>
<ul>
<li><p>Data integration</p>
<ul>
<li>Combining data from multiple sources into a coherent store 将来自多个来源的数据组合到一个连贯的存储中</li>
</ul>
</li>
<li><p>Schema integration: e.g., A.cust-id $\equiv $ B.cust-#  模式集成</p>
<ul>
<li>Integrate metadata from different sources 集成来自不同来源的元数据</li>
</ul>
</li>
<li><p><strong>Entity identification</strong> 实体识别</p>
<ul>
<li>Identify real world entities from multiple data sources, e.g., Bill Clinton = William Clinton  从多个数据源中识别真实世界的实体，例如Bill Clinton = William Clinton</li>
</ul>
</li>
<li><p>Detecting and resolving data value conflicts  检测和解决数据值冲突</p>
<ul>
<li><p>For the same real world entity, attribute values from different sources are different 对于相同的现实世界实体，来自不同来源的属性值是不同的</p>
</li>
<li><p>Possible reasons: different representations, different scales, e.g., metric vs. British units 可能的原因：不同的表示，不同的比例，例如，公制与英制单位</p>
</li>
</ul>
</li>
</ul>
<h5 id="Handling-Redundancy-in-Data-Integration"><a href="#Handling-Redundancy-in-Data-Integration" class="headerlink" title="Handling Redundancy in Data Integration"></a>Handling Redundancy in Data Integration</h5><blockquote>
<p>1.冗余原因（对象标识、派生数据）2.检测手段（相关性和协方差分析）3.仔细整合</p>
</blockquote>
<ul>
<li><p>Redundant data occur often when integration of multiple databases 当多个数据库集成时，通常会出现冗余数据</p>
<ul>
<li><p>Object identification:  The same attribute or object may have different names in different databases 对象标识：相同的属性或对象在不同的数据库中可能具有不同的名称</p>
</li>
<li><p>Derivable data: One attribute may be a “derived” attribute in another table, e.g., annual revenue 派生数据：一个属性可以是另一个表中的“派生”属性，例如年收入</p>
</li>
</ul>
</li>
<li><p>Redundant attributes may be able to be detected by <a href="#Correlation Analysis">correlation analysis</a> and <a href="#Covariance">covariance analysis</a> 可以通过相关性分析和协方差分析来检测冗余属性</p>
</li>
<li><p>Careful integration of the data from multiple sources may help reduce/avoid redundancies and inconsistencies and improve mining speed and quality  仔细整合来自多个来源的数据可能有助于减少/避免冗余和不一致，并提高”采矿“速度和质量</p>
</li>
</ul>
<h4 id="Data-Reduction"><a href="#Data-Reduction" class="headerlink" title="Data Reduction"></a>Data Reduction</h4><ul>
<li><p>Data reduction 数据压缩</p>
<ul>
<li>Obtain a reduced representation of the data set  获得数据集的缩减表示<ul>
<li>much smaller in volume but yet produces almost the same analytical results 体积小得多，但产生几乎相同的分析结果</li>
</ul>
</li>
</ul>
</li>
<li><p>Why data reduction?—A database/data warehouse(仓库) may store terabytes of data</p>
<ul>
<li>Complex analysis may take a very long time to run on the complete data set 复杂分析可能需要很长时间才能在完整数据集上运行</li>
</ul>
</li>
</ul>
<h5 id="Methods-for-data-reduction"><a href="#Methods-for-data-reduction" class="headerlink" title="Methods for data reduction"></a><strong>Methods for data reduction</strong></h5><center><br><img src="/2019/01/01/数据分析与挖掘Exam/DataReduction.png" width="600"><br></center>

<ul>
<li><p>Regression（”best fit“） and Log-Linear Models  （<strong>Parametric methods</strong>）</p>
</li>
<li><p>Histograms, clustering（”物以群分“）, sampling（选择具有代表性的子集；简单随机、放回、不放回、分层抽样） （<strong>Non-parametric methods</strong>）</p>
<center><br>    <img src="/2019/01/01/数据分析与挖掘Exam/Histogram_Clustering_Sampling.png" width="600"><br></center>
</li>
<li><p>Data cube aggregation 数据立方体聚合</p>
<p>The aggregated data for <strong>an individual entity of interest</strong>  感兴趣的实体聚合数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- Demographic Data 人口统计学数据（Service、Age、Gender、Job level、Workforce Segment）</span><br><span class="line">- Organisational process data 组织行为处理数据（Performance rating、Potential rating、Salary increases、Turnover、in training &amp; development）</span><br><span class="line">- Predictive attitudinal data 预测态度数据（Competencies能力、Intention to stay、AffectIve commitment、Job satisfaction、Discretionary自动支配 effort）</span><br></pre></td></tr></table></figure>
<center><br><img src="/2019/01/01/数据分析与挖掘Exam/DataCubeAggregation.png" style="zoom:40%"><br></center>
</li>
<li><p>Data compression</p>
<center><br><img src="/2019/01/01/数据分析与挖掘Exam/DataCompression.png" style="zoom:30%"><br></center>

</li>
</ul>
<h4 id="Data-transforming"><a href="#Data-transforming" class="headerlink" title="Data transforming"></a>Data transforming</h4><ul>
<li><p>A function that <strong>maps</strong> the entire set of values of a given attribute to a new set of replacement values s.t. each old value can be identified with one of the new values  一种函数，它将给定属性的整个值集映射到一组新的替换值 使得 可以使用其中一个新值标识每个旧值</p>
</li>
<li><p>Methods</p>
<ul>
<li><p>Smoothing: Remove noise from data</p>
</li>
<li><p>Attribute/feature construction  属性/特征构建</p>
<ul>
<li>New attributes constructed from the given ones</li>
</ul>
</li>
<li><p>Aggregation: Summarization, data cube construction</p>
</li>
<li><p>Normalization: Scaled to fall within a smaller, specified range</p>
<ul>
<li><p>min-max normalization</p>
</li>
<li><p>z-score normalization</p>
</li>
<li><p>normalization by decimal scaling</p>
</li>
</ul>
</li>
<li><p>Discretization 离散化: Concept hierarchy climbing</p>
</li>
</ul>
</li>
</ul>
<h5 id="Normalization-归一化"><a href="#Normalization-归一化" class="headerlink" title="Normalization 归一化"></a>Normalization 归一化</h5><center><br><img src="/2019/01/01/数据分析与挖掘Exam/Normalization.png" style="zoom:30%"><br></center>

<h5 id="Concept-hierarchy-generation-概念层次生成"><a href="#Concept-hierarchy-generation-概念层次生成" class="headerlink" title="Concept hierarchy generation 概念层次生成"></a>Concept hierarchy generation 概念层次生成</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Binning </span><br><span class="line">	- Top-down split, unsupervised</span><br><span class="line">- Histogram analysis</span><br><span class="line">	- Top-down split, unsupervised</span><br><span class="line">- Clustering analysis </span><br><span class="line">	- Unsupervised, top-down split <span class="keyword">or</span> bottom-up merge</span><br><span class="line">- Decision-tree analysis</span><br><span class="line">	- Supervised, top-down split</span><br><span class="line">- Correlation (e.g., χ<span class="number">2</span>) analysis </span><br><span class="line">	- Unsupervised, bottom-up merge</span><br><span class="line">- Note: All the methods can be applied recursively</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Concept hierarchy organizes concepts (i.e., attribute values) hierarchically <span class="keyword">and</span> <span class="keyword">is</span> usually associated <span class="keyword">with</span> each dimension <span class="keyword">in</span> a data warehouse 概念层次结构按层次组织概念（例如属性值），并且通常与数据仓库中的每个维度相关联</span><br><span class="line">- Concept hierarchies facilitate drilling <span class="keyword">and</span> rolling <span class="keyword">in</span> data warehouses to view data <span class="keyword">in</span> multiple granularity 概念层次结构有助于在数据仓库中钻取和滚动，以多种粒度查看数据</span><br><span class="line">- Concept hierarchy formation: Recursively reduce the data by collecting <span class="keyword">and</span> replacing low level concepts (such <span class="keyword">as</span> numeric values <span class="keyword">for</span> age) by higher level concepts (such <span class="keyword">as</span> youth, adult, <span class="keyword">or</span> senior) 概念层次结构：通过收集和替换更高级别概念（例如青年，成人或高级）的低级概念（例如年龄的数字值）来递归地减少数据</span><br><span class="line">- Concept hierarchies can be explicitly specified by domain experts <span class="keyword">and</span>/<span class="keyword">or</span> data warehouse designers 概念层次结构可以由域专家和/或数据仓库设计者明确指定</span><br><span class="line">- Concept hierarchy can be automatically formed <span class="keyword">for</span> both numeric <span class="keyword">and</span> nominal data—For numeric data, use discretization methods shown 可以为数字和标称数据自动形成概念层次结构 - 对于数字数据，使用显示的离散化方法</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Concept Hierarchy Generation for Nominal Data</span></span><br><span class="line">- Specification of a partial/total ordering of attributes explicitly at the schema level by users <span class="keyword">or</span> experts</span><br><span class="line">	- street &lt; city &lt; state &lt; country</span><br><span class="line">- Specification of a hierarchy <span class="keyword">for</span> a set of values by explicit data grouping</span><br><span class="line">	- &#123;Urbana, Champaign, Chicago&#125; &lt; Illinois</span><br><span class="line">- Specification of only a partial set of attributes</span><br><span class="line">	- E.g., only street &lt; city, <span class="keyword">not</span> others</span><br><span class="line">- Automatic generation of hierarchies (<span class="keyword">or</span> attribute levels) by the analysis of the number of distinct values</span><br><span class="line">	- E.g., <span class="keyword">for</span> a set of attributes: &#123;street, city, state, country&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Automatic Concept Hierarchy Generation</span></span><br><span class="line">- Some hierarchies can be automatically generated based on the analysis of the number of distinct values per attribute <span class="keyword">in</span> the data set </span><br><span class="line">	- The attribute <span class="keyword">with</span> the most distinct values <span class="keyword">is</span> placed at the lowest level of the hierarchy</span><br><span class="line">	- Exceptions, e.g., weekday, month, quarter, year</span><br></pre></td></tr></table></figure>
<center><br><img src="/2019/01/01/数据分析与挖掘Exam/AutomaticConceptHierarchyGeneration.png" width="600"><br></center>

<h5 id="Dimensionality-Reduction-Techniques"><a href="#Dimensionality-Reduction-Techniques" class="headerlink" title="Dimensionality Reduction Techniques"></a>Dimensionality Reduction Techniques</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- <span class="string">'Dimensionality reduction methodologies'</span></span><br><span class="line">	- <span class="string">'Feature selection'</span>: Find a subset of the original variables (<span class="keyword">or</span> features, attributes)  找寻合适子集(仅收集与分析任务相关的属性)</span><br><span class="line">	- <span class="string">'Feature extraction'</span>: Transform the data <span class="keyword">in</span> the high-dimensional space to a space of fewer dimensions  高维映射至低维(降维)</span><br><span class="line">- Some typical <span class="string">'dimensionality methods'</span></span><br><span class="line">	- Principal Component Analysis (<span class="string">'PCA'</span>) </span><br><span class="line">		- A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components 一种统计过程，它使用<span class="string">'正交变换'</span>将可能相关变量的一组观察值转换为一组称为主成分的线性不相关变量值</span><br><span class="line">		- The original data are projected onto a much smaller space, resulting <span class="keyword">in</span> dimensionality reduction 将原始数据投影到更小的空间，从而减少维数 (Feature extraction 降维)</span><br><span class="line">		- Method:  Find the eigenvectors of the covariance matrix, <span class="keyword">and</span> these eigenvectors define the new space 找到协方差矩阵的特征向量，这些特征向量定义新的空间</span><br><span class="line">	- <span class="string">'Supervised and nonlinear techniques'</span></span><br><span class="line">		- <span class="string">'Feature subset selection'</span></span><br><span class="line">        	- Best combined attribute selection(Best step-wise feature selection) <span class="keyword">and</span> elimination(Repeatedly eliminate the worst attribute)</span><br><span class="line">		- <span class="string">'Feature creation'</span></span><br><span class="line">			- Create new attributes (features) that can capture the important information <span class="keyword">in</span> a data set more effectively than the original ones 创建新属性（功能），可以比原始信息更有效地捕获数据集中的重要信息   (比如，从成绩单中得出平均分/绩点)</span><br><span class="line">			- <span class="string">'Three general methodologies'</span></span><br><span class="line">                - Attribute extraction 降维</span><br><span class="line">                    - Domain-specific</span><br><span class="line">                - Mapping data to new space (see: data reduction)</span><br><span class="line">                  E.g., Fourier transformation, wavelet transformation, manifold approaches (<span class="keyword">not</span> covered)</span><br><span class="line">                - Attribute construction </span><br><span class="line">                    - Combining features (see: discriminative frequent patterns <span class="keyword">in</span> Chapter on “Advanced Classification”)</span><br><span class="line">                    - Data discretization</span><br></pre></td></tr></table></figure>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><center><br>    <img src="/2019/01/01/数据分析与挖掘Exam/Summary_datapreprocessing.png" width="600"><br></center>

<h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">构建模型 应用模型(预测) 描述过程</span><br></pre></td></tr></table></figure>
<h3 id="What-is-classification"><a href="#What-is-classification" class="headerlink" title="What is classification?"></a><strong>What is classification?</strong></h3><p>根据训练集和类标签（分类属性中的值）构建模型，并将其用于分类新数据，预测其标签。【point】</p>
<h4 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h4><center><br><img src="/2019/01/01/数据分析与挖掘Exam/SupervisedLearning.png" width="600"><br></center>

<p><strong>监督学习</strong>是机器学习任务的一种。它<code>从有标记的训练数据中推导出预测标签</code>。有标记的训练数据是指每个训练实例都包括输入和期望的输出。一句话：<strong>给定数据，预测标签</strong>。(分类、回归)</p>
<p><strong>无监督学习</strong>是机器学习任务的一种。它<code>从无标记的训练数据中推断结论</code>。最典型的无监督学习就是<a href="#Clustering">聚类</a>分析，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。一句话：<strong>给定数据，寻找隐藏的结构</strong>。</p>
<h4 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h4><h5 id="Model-construction-模型构建"><a href="#Model-construction-模型构建" class="headerlink" title="Model construction 模型构建"></a>Model construction 模型构建</h5><ul>
<li>Each sample is assumed to belong to a predefined class (shown by the class label) 假设每个样本属于预定义的类（由类标签显示）</li>
<li>The set of samples used for model construction is training set  用于模型构建的样本集是训练集</li>
<li>Model: Represented as decision trees, rules, mathematical formulas, or other forms 模型：表示为决策树，规则，数学公式或其他形式</li>
</ul>
<h5 id="Model-Validation-and-Testing"><a href="#Model-Validation-and-Testing" class="headerlink" title="Model Validation and Testing"></a>Model Validation and Testing</h5><ul>
<li>Test: Estimate accuracy of the model <ul>
<li>The known label of test sample is compared with the classified result from the model  将已知的测试样品标签与模型的分类结果进行比较</li>
<li>Accuracy: % of test set samples that are correctly classified by the model 准确度：按模型正确分类的测试集样本的百分比 </li>
<li>Test set is independent of training set  测试集独立于训练集</li>
</ul>
</li>
<li>Validation: If the test set is used to select or refine models, it is called validation (development/test) set 验证：如果测试集用于选择或改进模型，则称为验证（开发/测试）集 To be better  【与测试集相较，强调refine models】</li>
</ul>
<h5 id="Model-Deployment"><a href="#Model-Deployment" class="headerlink" title="Model Deployment"></a>Model Deployment</h5><p> If the accuracy is acceptable, use the model to classify new data 【模型部署】</p>
<h3 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a><strong><a href="https://captainzj.github.io/2018/11/24/Classification-Algorithm/" target="_blank" rel="noopener">Algorithms</a></strong></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">算法 思路 优缺点 </span><br><span class="line">考 朴素贝叶斯</span><br><span class="line">解释 svm ann 大概了解DeepLearning 理论描述</span><br></pre></td></tr></table></figure>
<h4 id="Decision-tree-ID3-C4-5-CART"><a href="#Decision-tree-ID3-C4-5-CART" class="headerlink" title="Decision tree-ID3,C4.5,CART"></a>Decision tree-ID3,C4.5,CART</h4><p><a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">决策树算法原理(上)</a>、<a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener">决策树算法原理(下)</a>、<a href="https://www.cnblogs.com/pinard/p/6056319.html" target="_blank" rel="noopener">scikit-learn决策树算法类库使用小结</a> </p>
<ul>
<li><p>Basic algorithm </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- Tree <span class="keyword">is</span> constructed <span class="keyword">in</span> a top-down, recursive, divide-<span class="keyword">and</span>-conquer manner</span><br><span class="line">  树以自上而下，递归，分而治之的方式构建</span><br><span class="line">    - At start, all the training examples are at the root</span><br><span class="line">      一开始，所有的训练样例都是根源</span><br><span class="line">    - Examples are partitioned recursively based on selected attributes</span><br><span class="line">      样例基于被选定的属性递归地划分</span><br><span class="line">    - On each node, attributes are selected based on the training examples on that node, <span class="keyword">and</span> a heuristic <span class="keyword">or</span> statistical measure (e.g., information gain)</span><br><span class="line">      在每个节点上，基于<span class="string">'该节点上的训练示例'</span>以及<span class="string">'启发式或统计度量（例如，信息增益）'</span>来<span class="string">'选择属性'</span>。</span><br></pre></td></tr></table></figure>
<ul>
<li>Conditions for stopping partitioning </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">- All samples <span class="keyword">for</span> a given node belong to the same <span class="class"><span class="keyword">class</span> 给定节点的所有样本都属于同一个类</span></span><br><span class="line"><span class="class">- <span class="title">There</span> <span class="title">are</span> <span class="title">no</span> <span class="title">remaining</span> <span class="title">attributes</span> <span class="title">for</span> <span class="title">further</span> <span class="title">partitioning</span>  没有剩余属性可用于进一步分区</span></span><br><span class="line"><span class="class">- <span class="title">There</span> <span class="title">are</span> <span class="title">no</span> <span class="title">samples</span> <span class="title">left</span>  没有剩下的样例</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Prediction </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'Majority voting'</span> <span class="keyword">is</span> employed <span class="keyword">for</span> classifying the leaf</span><br></pre></td></tr></table></figure>
</li>
<li><p><a href="https://captainzj.github.io/2018/11/24/Classification-Algorithm/#%E4%BF%A1%E6%81%AF%E7%86%B5-Entropy" target="_blank" rel="noopener">Entropy</a>   <a href="#Example: Attribute Selection with Information Gain">Example: Attribute Selection with Information Gain</a>   <a href="#Computation of Gini Index">Computation of Gini Index</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- Entropy 信息熵:  表征混乱程度 </span><br><span class="line">- Conditional Entropy条件熵: 在已知随机变量X的条件下随机变量Y的不确定性(概率)</span><br><span class="line">- Mutual Information互信息/Information gain信息增益: 得知特征X的信息而使得类Y的信息的不确定性减少的程度(越大越好) -&gt;&gt; ID3决策树</span><br><span class="line">- Gain Ratio信息增益比: 解决使用信息增益存在偏向于选择取值较多的特征的问题(越大越好) -&gt;&gt; C4.5决策树 </span><br><span class="line">- GINI index基尼指数： 表征不纯度(越小越好)  -&gt;&gt;  CART分类树</span><br></pre></td></tr></table></figure>
</li>
<li><p>Advantage</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>）<span class="string">'简单直观'</span>，生成的决策树很直观。</span><br><span class="line"><span class="number">2</span>）基本<span class="string">'不需要预处理'</span>，不需要提前归一化，处理缺失值。</span><br><span class="line"><span class="number">3</span>）使用决策树预测的代价是O(log2m)。 m为样本数。</span><br><span class="line"><span class="number">4</span>）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。</span><br><span class="line"><span class="number">5</span>）可以<span class="string">'处理多维度输出'</span>的分类问题。</span><br><span class="line"><span class="number">6</span>）相比于神经网络之类的黑盒分类模型，决策树在逻辑上有很好的<span class="string">'可解释性'</span></span><br><span class="line"><span class="number">7</span>）可以交叉验证的<span class="string">'剪枝'</span>来选择模型，从而提高泛化能力。</span><br><span class="line"><span class="number">8</span>）对于异常点的<span class="string">'容错能力'</span>好，健壮性高。</span><br></pre></td></tr></table></figure>
</li>
<li><p>Disadvantage</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>）决策树算法非常容<span class="string">'易过拟合'</span>，导致<span class="string">'泛化能力不强'</span>.可以通过设置节点最少样本数量和限制决策树深度来改进.</span><br><span class="line"><span class="number">2</span>）决策树会因为样本发生一点点的改动，就会导致树<span class="string">'结构的剧烈改变'</span>。这个可以通过集成学习之类的方法解决.</span><br><span class="line"><span class="number">3</span>）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，<span class="string">'容易陷入局部最优'</span>。可以通过集成学习之类的方法来改善。</span><br><span class="line"><span class="number">4</span>）有些比较<span class="string">'复杂的关系'</span>，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</span><br><span class="line"><span class="number">5</span>）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h4><p><a href="https://www.cnblogs.com/pinard/p/6097604.html" target="_blank" rel="noopener">支持向量机原理(一) 线性支持向量机</a>、<a href="https://www.cnblogs.com/pinard/p/6100722.html" target="_blank" rel="noopener">支持向量机原理(二) 线性支持向量机的软间隔最大化模型</a>、<a href="https://www.cnblogs.com/pinard/p/6103615.html" target="_blank" rel="noopener">支持向量机原理(三)线性不可分支持向量机与核函数</a>、<a href="https://www.cnblogs.com/pinard/p/6111471.html" target="_blank" rel="noopener">支持向量机原理(四)SMO算法原理</a>、<a href="https://www.cnblogs.com/pinard/p/6113120.html" target="_blank" rel="noopener">支持向量机原理(五)线性支持回归</a>、<a href="https://www.cnblogs.com/pinard/p/6117515.html" target="_blank" rel="noopener">scikit-learn 支持向量机算法库使用小结</a>、<a href="https://www.cnblogs.com/pinard/p/6126077.html" target="_blank" rel="noopener">支持向量机高斯核调参小结</a></p>
<h4 id="Bayes-【point】"><a href="#Bayes-【point】" class="headerlink" title="Bayes 【point】"></a>Bayes 【point】</h4><p><a href="https://www.cnblogs.com/pinard/p/6069267.html" target="_blank" rel="noopener">朴素贝叶斯算法原理小结</a>、<a href="https://www.cnblogs.com/pinard/p/6074222.html" target="_blank" rel="noopener">scikit-learn 朴素贝叶斯类库使用小结</a></p>
<h4 id="ANN"><a href="#ANN" class="headerlink" title="ANN"></a>ANN</h4><h3 id="Model-evaluation-and-selection"><a href="#Model-evaluation-and-selection" class="headerlink" title="Model evaluation and selection"></a><strong>Model evaluation and selection</strong></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">混淆矩阵 准确率 错误率 等等指标</span><br><span class="line">交叉验证</span><br></pre></td></tr></table></figure>
<h4 id="Classifier-Evaluation-Metrics"><a href="#Classifier-Evaluation-Metrics" class="headerlink" title="Classifier Evaluation Metrics"></a>Classifier Evaluation Metrics</h4><p><a href="#Classifier Evaluation Metrics: Example">Classifier Evaluation Metrics: Example</a></p>
<h5 id="Confusion-matrix-and-criteria"><a href="#Confusion-matrix-and-criteria" class="headerlink" title="Confusion matrix and criteria"></a>Confusion matrix and criteria</h5><center><br>    <img src="/2019/01/01/数据分析与挖掘Exam/ConfusionMatrix.png" width="600"><br></center>

<p><a href="https://www.cnblogs.com/pinard/p/5993450.html" target="_blank" rel="noopener">精确率与召回率，RoC曲线与PR曲线</a></p>
<h4 id="Estimating-a-classifier’s-accuracy"><a href="#Estimating-a-classifier’s-accuracy" class="headerlink" title="Estimating a classifier’s accuracy"></a>Estimating a classifier’s accuracy</h4><h5 id="Cross-evaluation"><a href="#Cross-evaluation" class="headerlink" title="Cross-evaluation"></a>Cross-evaluation</h5><figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Cross-validation (k-fold, where k = 10 is most popular)</span></span><br><span class="line">- Randomly partition the data into k mutually exclusive subsets, each approximately equal size 随机将数据划分为k个互斥的子集，每个子集的大小大致相等</span><br><span class="line">- At i-th iteration, use Di <span class="keyword">as</span> test set <span class="keyword">and</span> others <span class="keyword">as</span> training set 在第i次迭代中，使用Di作为测试集，使用其他作为训练集</span><br><span class="line">- Leave-one-out: k folds where k = <span class="string">'#'</span> of tuples, <span class="keyword">for</span> small sized data 留一个：k折叠，其中k = <span class="string">'#'</span> 元组的数量，对于小尺寸数据</span><br><span class="line">- *Stratified cross-validation*: folds are stratified so that class distribution, in each fold is approximately the same as that in the initial data *分层交叉验证*：折叠是分层的，因此每个折叠中的类分布与初始数据中的类别分布大致相同</span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/pinard/p/5992719.html" target="_blank" rel="noopener">交叉验证(Cross Validation)原理小结</a></p>
<h3 id="Ensemble-methods"><a href="#Ensemble-methods" class="headerlink" title="Ensemble methods"></a>Ensemble methods</h3><p><a href="https://www.cnblogs.com/pinard/p/6131423.html" target="_blank" rel="noopener">集成学习原理小结</a></p>
<h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p><a href="https://www.cnblogs.com/pinard/p/6156009.html" target="_blank" rel="noopener">Bagging与随机森林算法原理小结</a></p>
<h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><p><a href="https://www.cnblogs.com/pinard/p/6133937.html" target="_blank" rel="noopener">集成学习之Adaboost算法原理小结</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6136914.html" target="_blank" rel="noopener">scikit-learn Adaboost类库使用小结</a></p>
<h4 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h4><p><a href="https://www.cnblogs.com/pinard/p/6160412.html" target="_blank" rel="noopener">scikit-learn随机森林调参小结</a></p>
<h2 id="Frequent-patterns"><a href="#Frequent-patterns" class="headerlink" title="Frequent patterns"></a>Frequent patterns</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">频繁模式</span><br><span class="line">频繁子图 不考</span><br><span class="line">关联规则重点掌握</span><br><span class="line">必须明白 apriori fp growth </span><br><span class="line">k -&gt; k+1</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>What is a frequent pattern</strong> </p>
<ul>
<li><strong>Association rule</strong> </li>
</ul>
</li>
<li><p>Algorithm </p>
<ul>
<li><p><strong>Apriori</strong> </p>
<p><a href="https://www.cnblogs.com/pinard/p/6293298.html" target="_blank" rel="noopener">Apriori算法原理总结</a></p>
</li>
<li><p><strong>FP-growth</strong> </p>
<p><a href="https://www.cnblogs.com/pinard/p/6307064.html" target="_blank" rel="noopener">FP Tree算法原理总结</a></p>
</li>
</ul>
</li>
</ul>
<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kmeans 相关算法</span><br><span class="line">分析比较</span><br><span class="line">ap基本概念</span><br></pre></td></tr></table></figure>
<ul>
<li><p>What is clustering</p>
<ul>
<li>Unsupervised learning </li>
</ul>
</li>
<li><p>Algorithms</p>
<ul>
<li><p>Partition-based—k-means</p>
<p><a href="https://www.cnblogs.com/pinard/p/6164214.html" target="_blank" rel="noopener">K-Means聚类算法原理</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6169370.html" target="_blank" rel="noopener">用scikit-learn学习K-Means聚类</a></p>
</li>
<li><p>Hierarchical-based—two ways</p>
</li>
<li><p>Density-based—DBSCAN</p>
<p><a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">DBSCAN密度聚类算法</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6217852.html" target="_blank" rel="noopener">用scikit-learn学习DBSCAN聚类</a></p>
</li>
<li><p>AP (2007, Science)</p>
</li>
<li><p>Local density-based (2014, Science) </p>
</li>
</ul>
</li>
</ul>
<h3 id="Graph-clustering"><a href="#Graph-clustering" class="headerlink" title="Graph clustering"></a>Graph clustering</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">谱聚类 modularity模块化 复杂网络的概念</span><br></pre></td></tr></table></figure>
<ul>
<li><p>What is graph clustering </p>
<ul>
<li><strong>Complex network</strong></li>
<li>Graph clustering</li>
<li>Community </li>
<li>Module  </li>
</ul>
</li>
<li><p>Algorithms </p>
<ul>
<li><p>CPM （Clique Percolation Method）</p>
<p>派系过滤算法</p>
</li>
<li><p>Spectral clustering </p>
<p><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="noopener">谱聚类（spectral clustering）原理总结_刘建平</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6235920.html" target="_blank" rel="noopener">用scikit-learn学习谱聚类</a></p>
</li>
<li><p>G Nand Q</p>
</li>
<li><p>MCL </p>
</li>
</ul>
</li>
</ul>
<h2 id="Todo"><a href="#Todo" class="headerlink" title="Todo"></a>Todo</h2><ul>
<li><p>看相关参考书目《数据挖掘：概念与技术》《数据挖掘导论》课后例题  着重看”简单计算”</p>
</li>
<li><p>Collaborative Filtering</p>
<p><a href="https://www.cnblogs.com/pinard/p/6349233.html" target="_blank" rel="noopener">协同过滤推荐算法总结</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6351319.html" target="_blank" rel="noopener">矩阵分解在协同过滤推荐算法中的应用</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6362647.html" target="_blank" rel="noopener">SimRank协同过滤推荐算法</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/pinard/p/6912636.html" target="_blank" rel="noopener">EM算法原理总结</a></p>
</li>
<li><p>特征工程</p>
<p><a href="https://www.cnblogs.com/pinard/p/9032759.html" target="_blank" rel="noopener">特征工程之特征选择</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/9061549.html" target="_blank" rel="noopener">特征工程之特征表达</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/9093890.html" target="_blank" rel="noopener">特征工程之特征预处理</a></p>
</li>
</ul>
<h2 id="Calculate"><a href="#Calculate" class="headerlink" title="Calculate"></a>Calculate</h2><h3 id="Correlation-Analysis"><a href="#Correlation-Analysis" class="headerlink" title="Correlation Analysis"></a>Correlation Analysis</h3><center><br>    <img src="/2019/01/01/数据分析与挖掘Exam/CorrelationAnalysis.png" width="600"><br>    <img src="/2019/01/01/数据分析与挖掘Exam/CorrelationAnalysisExample.png" width="600"><br>    <img src="/2019/01/01/数据分析与挖掘Exam/CorrelationBetweenTwoNumericalVariables.png" width="600"><br></center>

<h3 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h3><center><br>    <img src="/2019/01/01/数据分析与挖掘Exam/VarianceForSingleVariable.png" width="600"><br>    <img src="/2019/01/01/数据分析与挖掘Exam/CovarianceForTwoVariables.png" width="600"><br>    <img src="/2019/01/01/数据分析与挖掘Exam/Example_CalculationOfCovariance.png" width="600"><br></center>

<h3 id="Simple-Discretization-Binning"><a href="#Simple-Discretization-Binning" class="headerlink" title="Simple Discretization: Binning"></a>Simple Discretization: Binning</h3><center><br><img src="/2019/01/01/数据分析与挖掘Exam/Binning.png" width="600"><br><img src="/2019/01/01/数据分析与挖掘Exam/BinningExample.png" width="600"><br><img src="/2019/01/01/数据分析与挖掘Exam/BinningvsClustering.png" width="600"><br></center>

<h4 id="Example-Attribute-Selection-with-Information-Gain"><a href="#Example-Attribute-Selection-with-Information-Gain" class="headerlink" title="Example: Attribute Selection with Information Gain"></a>Example: Attribute Selection with Information Gain</h4><center><br>    <img src="/2019/01/01/数据分析与挖掘Exam/Example-AttributeSelection_withInformationGain.png" width="600"><br></center>

<h4 id="Computation-of-Gini-Index"><a href="#Computation-of-Gini-Index" class="headerlink" title="Computation of Gini Index"></a>Computation of Gini Index</h4><center><br><img src="/2019/01/01/数据分析与挖掘Exam/ComputationOfGiniIndex.png" width="600"><br></center>

<h4 id="Classifier-Evaluation-Metrics-Example"><a href="#Classifier-Evaluation-Metrics-Example" class="headerlink" title="Classifier Evaluation Metrics: Example"></a>Classifier Evaluation Metrics: Example</h4><center><br><img src="/2019/01/01/数据分析与挖掘Exam/ClassifierEvaluationMetrics.png" width="600"><br><img src="/2019/01/01/数据分析与挖掘Exam/ClassifierEvaluationMetrics1.png" width="600"><br><img src="/2019/01/01/数据分析与挖掘Exam/ClassifierEvaluationMetricsExample.png" width="600"><br></center>
        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2019-01-03T10:16:54.014Z" itemprop="dateUpdated">2019-01-03 18:16:54</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="CaptainSE">
            CaptainSE
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            

            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/01/01/数据分析与挖掘Exam/&title=《数据分析与挖掘Exam》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/01/01/数据分析与挖掘Exam/&title=《数据分析与挖掘Exam》 — Go Further&source=【阅读时间】XXX min XXX words【阅读内容】……" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/01/01/数据分析与挖掘Exam/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《数据分析与挖掘Exam》 — Go Further&url=http://yoursite.com/2019/01/01/数据分析与挖掘Exam/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/01/01/数据分析与挖掘Exam/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/12/31/visdom-Tutorial/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">visdom_Tutorial</h4>
      </a>
    </div>
  
</nav>



    

















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢老板~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>CaptainSE &copy; 2015 - 2019</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2019/01/01/数据分析与挖掘Exam/&title=《数据分析与挖掘Exam》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2019/01/01/数据分析与挖掘Exam/&title=《数据分析与挖掘Exam》 — Go Further&source=【阅读时间】XXX min XXX words【阅读内容】……" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2019/01/01/数据分析与挖掘Exam/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《数据分析与挖掘Exam》 — Go Further&url=http://yoursite.com/2019/01/01/数据分析与挖掘Exam/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2019/01/01/数据分析与挖掘Exam/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACMElEQVR42u3aQW7DMAwF0dz/0i7QrR1lPumgNTVaBY1h6WXBkiJfL7yO35V+Pq/zt+ddzn+/bcmQIeOxjGO5zkfsH7T2zLu9ZMiQsQ+DBFmyMQ/E6x+Fn02GDBkyyEHfHZH8EGukDBkyZHQCLv+Wp6EyZMiQsWb0i9I0EJNdvlKLy5Ah44GM7zUG+p//RX9DhgwZf8o4wsXfsH4yvXr7cCoZMmSMZvD0rl+CpodOL+BkyJAxlVELo+l4RO2ZYFBMhgwZoxmdsEsqSjJOwd+A3iNDhozRjP5AWKe1mV7GyZAhYzcGD6y8QE2bnWnqefGkDBkyhjI6F/o8EayNXAQkGTJkjGbw5OwbMw9B7yJNNGXIkDGO0WlYprD0Pp+HaRkyZExl8COm1/2cnfIuziNDhozRjFrDshY64+5EGIhlyJAxm9EZxqoNjfFrNf68DBkypjLS8pVTaw0DvuNFhitDhozRDH4dVgvBvOgttkhlyJAxmlFrTKZJHm828Lel7VUZMmQ8l9FpMaalZhrQg5AqQ4aMDRi1TkItgav9T/iAlCFDxmgGzxbTcYq0iOVJatDfkCFDxgjGEa605cnLVD6QgRoDMmTIGMToBLtaOsjx6b4yZMiYzehc1qdtgLvC9M09BxkyZDyEwYNsGo7vGphYh2YZMmTI4OlamjJ23lYMuDJkyNiGESRt7aoaBWsZMmRswCBFLA+anYZoGkVlyJCxA+OGAjIcj0jLY86WIUPGOMYPmG0ZBiqSgJoAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            clearTimeout(titleTime);
        } else {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
