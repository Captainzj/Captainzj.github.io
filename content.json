{"meta":{"title":"Go Further","subtitle":"Stay Hungry, Stay Foolish","description":"CaptainSE","author":"CaptainSE","url":"http://yoursite.com"},"pages":[{"title":"My Blog Name | 404","date":"2018-08-17T17:00:22.326Z","updated":"2018-08-17T17:00:18.631Z","comments":true,"path":"404.html","permalink":"http://yoursite.com/404.html","excerpt":"","text":""},{"title":"categories","date":"2018-08-15T18:36:38.000Z","updated":"2018-08-16T04:54:47.591Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-08-15T18:34:59.000Z","updated":"2018-08-16T04:53:15.699Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"数据分析与挖掘Exam","slug":"数据分析与挖掘Exam","date":"2019-01-06T10:56:52.000Z","updated":"2019-01-08T03:32:38.266Z","comments":true,"path":"2019/01/06/数据分析与挖掘Exam/","link":"","permalink":"http://yoursite.com/2019/01/06/数据分析与挖掘Exam/","excerpt":"【阅读时间】XXX min XXX words【内容描述】《数据分析与挖掘》之后篇","text":"【阅读时间】XXX min XXX words【内容描述】《数据分析与挖掘》之后篇 Exam 题型 题量（道） 分值（分） 总计（分） 选择题 10 2 20 计算题 3 10 30 简答题 4 5 20 论述题 2 15 30 Introduction What is datamining 从数据中发现知识 统计分析 + 分析报告 -&gt;&gt; 数据挖掘(数据统计) + (协同过滤关联分析) -&gt;&gt; (找寻事物的隐含规律) (描述性) + (预测性概率) -&gt;&gt; (规范性) - 分析报告一般是整个事件发生结束以后的总结（描述性）。- 统计分析能利用大量的历史样本来预测整个事件总体未来的走向（预测性概率）。- 数据挖掘则透过事件的表象发现隐藏在背后的蛛丝马迹，从而找到潜伏的规律以及看似无关事物之间背后的联系，用此来洞察未来（规范性）。 What is machine learning 计算机程序基于数据自动地学习识别复杂的模式，并作出智能的决断。 允许程序可以根据提供的数据进行自动的学习，它可以使你的程序变得更”聪明”。 - 监督学习 (分类)- 无监督学习 (聚类)- 半监督学习: 在学习模型时，它使用标记(学习模型)和未标记(改进类边界)的实例。- 主动学习: 通过主动地从用户获取知识来提高模型质量 What is artificial intelligence 在计算机科学的基础上，综合信息论、心理学、生理学、语言学、逻辑学和数学等知识，制造能模拟人类智能行为的计算机系统的边缘学科。 Data计算公式 Data types* 'Relational records' 关系记录 - Relational tables, highly structured 关系表，高度结构化* 'Data matrix', e.g., numerical matrix, crosstabs 数据矩阵，例如数值矩阵，交叉表* 'Transaction data' 交易数据* 'Document data': Term-frequency vector (matrix) of text documents 文档数据：文本文档的术语 - 频率向量（矩阵）* 'Transportation network' 交通网络* 'World Wide Web' 万维网* 'Molecular Structures' 分子结构* 'Social or information networks' 社交或信息网络 Attribute Types * 'Nominal': categories, states, or “names of things” # 标称属性* 'Binary' : Nominal attribute with only 2 states (0 and 1)# 二元属性:仅有2个状态（0和1）的标称属性* 'Ordinal' # 序数属性 - Values have a meaningful order (ranking) but magnitude between successive values is not known 值具有有意义的顺序（排名），但连续值之间的大小未知* 'Numeric' # 数值属性 - `Interval`: Measured on a scale of equal-sized units 间隔: 按相同大小的单位测量 - `Ratio` 比率(倍数) We can speak of values as being an order of magnitude larger than the unit of measurement (10 K˚ is twice as high as 5 K˚). 我们可以说价值比测量单位大一个数量级（10K˚是5K˚的两倍）* 'Discrete Attribute' # 离散属性 - Sometimes, represented as integer variables 有时，表示为`整数变量` - Note: Binary attributes are a special case of discrete attributes 注意：二进制属性是离散属性的特例* 'Continuous Attribute' # 连续属性 - Practically, real values can only be measured and represented using a finite number of digits 实际上，只能使用有限数字来测量和表示实际值 - Continuous attributes are typically represented as floating-point variables 连续属性通常表示为`浮点变量` Data statistics Motivation: To better understand the data: central tendency, variation and spread Data preprocessing常用方法 缺失值 处理方法 Data cleaning Handle missing data(Incomplete), smooth noisy data,identify or remove outliers, and resolve inconsistencies 处理丢失的数据，平滑噪声数据，识别或删除异常值，并解决不一致问题 How to Handle Missing Data? Data is not always available: many tuples have no recorded value for several attributes 1.忽略元组 2.手动填充 3. 自动（以”unknown”/“均值”/“最可能的值’’）填充 How to Handle Noisy Data? Noise: random error or variance in a measured variable 1.binning平滑 2.回归(拟合平滑) 3.聚类（无监督,检查并删除异常点） 4.半监督（检测可疑值并由人查验） Data integrating Integration of multiple databases, data cubes, or files 集成多个数据库，数据立方体或文件 1.数据集成 2.模式集成 3.实体识别 4.检测和解决数据值的冲突 Handling Redundancy in Data Integration 1.冗余原因（对象标识不同、派生数据）2.检测手段（相关性和协方差分析）3.仔细整合 Data transforming A function that maps the entire set of values of a given attribute to a new set of replacement values s.t. each old value can be identified with one of the new values 一种函数，它将给定属性的整个值集映射到一组新的替换值 使得 可以使用其中一个新值标识每个旧值 - Methods - 'Smoothing': Remove noise from data # 平滑 - 'Attribute/feature construction' # 属性/特征构建 - New attributes constructed from the given ones - 'Aggregation': Summarization, data cube construction # 聚合 - 'Normalization': Scaled to fall within a smaller, specified range # 归一化 - min-max normalization - z-score normalization - normalization by decimal scaling - 'Discretization': Concept hierarchy climbing # 离散化 Data Reduction Obtain a reduced representation of the data set 获得数据集的缩减表示 much smaller in volume but yet produces almost the same analytical results 体积小得多，但产生几乎相同的分析结果 Methods for data reduction - 'Regression and Log-Linear Models' （Parametric methods）- 'Histograms, clustering, sampling' （Non-parametric methods） &gt; sampling: 选择具有代表性的子集；简单随机、放回、不放回、分层抽样- 'Data cube aggregation' 数据立方体聚合- 'Data Compression' # Some typical dimensionality methods* Principal Component Analysis ('PCA') * Supervised and nonlinear techniques - 'Feature subset selection' 找寻合适子集(仅收集与分析任务相关的属性) - 'Feature creation' - `Attribute extraction` 高维映射至低维(降维) - `Attribute construction` Summary classification构建模型 应用模型(预测) 描述过程 What is classification? 根据训练集和类标签（分类属性中的值）构建模型，并将其用于分类新数据，预测其标签。 Supervised learning 监督学习是机器学习任务的一种。它从有标记的训练数据中推导出预测标签。有标记的训练数据是指每个训练实例都包括输入和期望的输出。一句话：给定数据，预测标签。(分类、回归) 无监督学习是机器学习任务的一种。它从无标记的训练数据中推断结论。最典型的无监督学习就是聚类分析，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。一句话：给定数据，寻找隐藏的结构。 Steps 模型构建：根据数据集特征构建合适的分类模型，并使用训练集样本进行模型训练 模型验证与测试：将已知的测试样品标签与模型使用测试集所得的分类结果进行比较，而后使用验证集改进模型准确率 模型部署：如果准确度可接受，便可使用此模型分类新数据 Algorithms 1.决策树：基于规则 2.贝叶斯：基于概率 3.ANN: 机器学习最优化 思路 优缺点 算法 思路 优缺点考 朴素贝叶斯解释 svm ann 大概了解DeepLearning 理论描述 Decision tree-ID3,C4.5,CART Algorithm Step 树以自上而下，递归，分而治之的方式构建 一开始，所有的训练样例都是根源 样例基于被选定的属性递归地划分(在每个节点上，基于该节点上的训练示例以及启发式或统计度量（例如，信息增益）来选择属性。) 停止条件 给定节点的所有样本都属于同一个类 没有剩余属性可用于进一步分区 没有剩余样例 Basic Concepts - Entropy 信息熵: 表征混乱程度 - Conditional Entropy条件熵: 在已知随机变量X的条件下随机变量Y的不确定性(概率)- Mutual Information互信息/Information gain信息增益: 得知特征X的信息而使得类Y的信息的不确定性减少的程度(越大越好) -&gt;&gt; 'ID3决策树'- Gain Ratio信息增益比: 解决使用信息增益存在偏向于选择取值较多的特征的问题(越大越好) -&gt;&gt; 'C4.5决策树' - GINI index基尼指数： 表征不纯度(越小越好) -&gt;&gt; 'CART分类树' 决策树算法比较 | 算法 | 支持模型 | 树结构 | 特征选择 | 连续值处理 | 缺失值处理 | 剪枝 || :–: | :——–: | :—-: | :————–: | :——–: | :——–: | :—-: || ID3 | 分类 | 多叉树 | 信息增益 | 不支持 | 不支持 | 不支持 || C4.5 | 分类 | 多叉树 | 信息增益比 | 支持 | 支持 | 支持 || CART | 分类，回归 | 二叉树 | 基尼系数，均方差 | 支持 | 支持 | 支持 | SVM Define: Find a linear/non-linear hyperplane (decision boundary) that will separate the data Optimize：希望所有的点都离超平面远 -&gt; 可以让离超平面比较近的点尽可能的远离超平面 kernel核函数：将数据从低维空间映射到高维空间 Bayes 贝叶斯定理： $P(H|X)=\\frac{P(X|H)P(H)}{P(X)}$ Naïve Bayesian Classifier # Advantages Easy to implement 易于实现 Good results obtained in most of the cases 大多数情况下获得了良好的结果# Disadvantages Assumption: class conditional independence, therefore loss of accuracy类条件独立-&gt;准确率缺失 Practically, dependencies exist among variables 实际上，变量之间存在'依赖'关系  E.g., hospitals: patients: Profile: age, family history, etc. Symptoms: fever, cough etc., Disease: lung cancer, diabetes,etc.  Dependencies among these cannot be modeled by Naïve Bayesian Classifier 这些依赖关系不能用朴素贝叶斯分类器建模# How to deal with these dependencies? Bayesian Belief Networks 如何处理这些依赖关系？贝叶斯置信网络 ANN 受生物神经元的启发，将多输入单输出的信息处理单元作为人工神经网络中的一个神经元。人工神经网络的基本结构如下：输入层(输入层的神经元数目对应于训练集数据的属性数目)、隐藏层、输出层(输出层的神经元数目对应于网络预测的分类数目) Backpropagation Backpropagate the error (by updating weights and biases) DeepLearning 通过更深的layers，自动提取特征（构建特征空间），以达到更”深层次”的学习效果 Model evaluation and selection混淆矩阵 准确率 错误率 等等指标 交叉验证 Confusion matrix and criteria Issues Affecting Model Selection 1.准确性 2.速度 3.鲁棒性 4.可伸缩性 5.可解释性 6.其他措施，例如规则的好处，例如决策树大小或分类规则的紧凑性 Cross-evaluation# Cross-validation (k-fold, where k = 10 is most popular)- 随机将数据划分为k个互斥的子集，每个子集的大小大致相等- 在第i次迭代中，使用Di作为测试集，使用其他作为训练集- 留一个：k折叠，其中k = '#' 元组的数量，对于小尺寸数据- *分层交叉验证*：折叠是分层的，因此每个折叠中的类分布与初始数据中的类别分布大致相同 Ensemble methods 使用模型组合来提高准确性 Bagging Bagging: 使用训练集的子集训练每个模型，并且并行学习模型 Boosting （Bagging）投票得分 -&gt; （Boosting）加权得分 Boosting：训练每个新模型实例以强调先前模型错误分类的训练实例，以及按顺序学习的模型 Random Forest Avariation of bagging for decision trees 对决策树的bagging的’变异’ Summary Frequent patterns频繁模式频繁子图 不考 hhhhh关联规则重点掌握必须明白 apriori fp-growth k -&gt; k+1 What is a frequent pattern 频繁模式是数据集中频繁出现(满足最小支持度)的项集、序列或子结构。 Association rule 关联规则就是有关联的规则，形式是这样定义的：两个不相交的非空集合X、Y，如果有X–&gt;Y，就说X–&gt;Y是一条关联规则。 AlgorithmApriori Apriori定律 1. 如果一个集合是频繁项集，则它的所有'子集'都是频繁项集。2. 如果一个集合不是频繁项集，则它的所有'超集'都不是频繁项集。 Step FP-growthFpGrowth算法通过构造一个树结构来压缩数据记录，使得挖掘频繁项集只需要扫描两次数据记录，而且该算法不需要生成候选集合，所以效率会比较高。 Method  For each frequent item, construct its conditional pattern-base, and then its conditional FP-tree 对于每个频繁项，'构造其条件模式库'，然后'构造其条件FP树' Repeat the process on 'each' newly created conditional FP-tree 对每个新创建的条件FP树重复此过程 (\"Recursion: Mining Each Conditional FP-tree\") Until the resulting FP-tree is empty, or it contains only one path—single path will generate all the combinations of its sub-paths, each of which is a frequent pattern 在生成的FP树为空之前，或者它只包含一个路径 - 单个路径将生成其子路径的所有组合，'每个路径都是一个频繁的模式' Summary Clusteringkmeans 相关算法分析比较ap基本概念 What is clustering 聚类就是按照某个特定标准(如距离准则)把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同数据尽量分离。 Unsupervised learning 聚类是一种输入数据无标签的“分类”方式（即非监督学习），通常并不需要使用训练数据进行学习，仅把相似的东西聚到一起，并不关心所得的簇具体代表什么 AlgorithmsPartition-based—k-means k-Means : 选取平均值作为新的聚类中心 k-means对初始值的设置很敏感 K-means++：选取与当前所属聚类中心距离最远的点作为新的聚类中心 k-means对噪声和离群值非常敏感 K-Medoids：选取中心点（计算该点到当前聚簇中所有点距离之和，最终距离之后最小的点）作为新的聚类中心 K-Medians：选取中位数作为新的聚类中心 k-means只用于numerical，不适用于categorical类型数据 K-Modes：选取众数作为新的聚类中心 k-means不能解决非凸non-convex数据 Kernel K-Means：使用核函数(多项式/高斯径向基/Sigmoid核函数)将数据投影到高维特征空间，然后执行K-Means聚类 Hierarchical-based—two ways- '凝聚'：从单一集群开始，一次连续合并两个集群，构建'自下而上'的集群层次结构- '分裂'：从一个庞大的宏集群开始，将其连续分成两组，生成一个'自上而下'的集群层次结构 Brich Density-based—DBSCAN DBSCAN密度聚类：由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。 AP (2007, Science)AP算法通过迭代过程不断更新每一个点的$responsibility$和$availability$,直到产生$m$个高质量的$exemplar$,同时将其余的数据点分配到相应的聚类中。 Local density-based (2014, Science) 1.找出聚类中心 2.剩余点的类别指派 3.去除噪音 Summary Graph clustering谱聚类 modularity模块化 复杂网络的概念 What is graph clustering Complex network 在我们的现实生活中，许多复杂系统都可以建模成一种复杂网络进行分析，比如常见的电力网络、航空网络、交通网络、计算机网络以及社交网络等等。复杂网络不仅是一种数据的表现形式，它同样也是一种科学研究的手段。 Graph clustering 和特征聚类不同，图聚类比较难以观察，部分算法会以各点之间的距离作为突破口，可以这样形容：张三，是王五的好朋友，刚认识李四，对赵六很是反感。那么，对于该节点，我们无法直接得出他的特征，但能知道他的活动圈。利用图聚类，可以将同一社交范围的人聚合到一起。 Community 'k-clique community': Union of all k-cliques that can be `reached from each other` through a series of adjacent k-cliques.k-派系连通：一个k-派系可以通过若干个相邻的k-派系到达另一个k-派系，则称这两个k-派系彼此联通 Module AlgorithmsCPM Step1: 找到网络中大小为K的完全子图 Locate maximal cliques Step2: 将每个完全子图定义为一个节点，建立一个重叠矩阵Step3: 将重叠矩阵变成社团邻接矩阵(其中重叠矩阵中对角线小于k、非对角线小于k-1的元素全置为0,所有非0项置为1) Spectral clustering谱聚类（Spectral Clustering），就是先用Laplacian eigenmaps对数据降维（简单地说，就是先将数据转换成邻接矩阵或相似性矩阵，再转换成Laplacian矩阵，再对Laplacian矩阵进行特征分解，把最小的K个特征向量排列在一起），然后再使用k-means完成聚类。谱聚类是个很好的方法，效果通常比k-means好，计算复杂度还低，这都要归功于降维的作用。 GNandQ1. 计算网络中所有边缘的中介性.2. 去除间隙最大的边缘.3. 重新计算受移除影响的所有边缘的间隙.4. 从步骤2重复，直到没有边缘. MCL在MCL中， Expansion 和 Inflation 将不断的交替进行，Expansion 使得不同的区域之间的联系加强，而 Inflation 则不断的分化各点之间的联系(强者恒强，弱者恒弱)。经过多次迭代，将渐渐出现聚集现象，以此便达到了聚类的效果。 Summary CalculatedataProcessingProximity Measure for Binary Attributes Correlation Analysis Covariance Normalization Simple Discretization: Binning ClassificationID3 Decision Tree：Information Gain CART Decision Tree： Gini Index Classifier Evaluation Metrics: Example Naïve Bayesian Classifier Bayesian Belief Network SVM$\\vec{w}=\\sum_{i=1}^{N}\\lambda_iy_i\\vec{x_i} $ $\\lambda_i(y_i(\\vec{w} * \\vec{x_i}+b ))=0$ Frequent patternsAssociation Rules: supprt &amp;&amp; confidence FP-growth ClusteringKernel K-Means Community detection Spectral Clustering MCL Todo 看相关参考书目《数据挖掘：概念与技术》《数据挖掘导论》课后例题 着重看”简单计算” Collaborative Filtering 协同过滤推荐算法总结 矩阵分解在协同过滤推荐算法中的应用 SimRank协同过滤推荐算法 EM算法原理总结 特征工程 特征工程之特征选择 特征工程之特征表达 特征工程之特征预处理","categories":[{"name":"XD","slug":"XD","permalink":"http://yoursite.com/categories/XD/"}],"tags":[]},{"title":"数据分析与挖掘","slug":"数据分析与挖掘","date":"2019-01-01T06:42:25.000Z","updated":"2019-01-08T06:24:05.143Z","comments":true,"path":"2019/01/01/数据分析与挖掘/","link":"","permalink":"http://yoursite.com/2019/01/01/数据分析与挖掘/","excerpt":"【阅读时间】XXX min XXX words【内容描述】 基于《数据挖掘：概念与技术》的简述，欲了解细节，强烈建议读原书！！！","text":"【阅读时间】XXX min XXX words【内容描述】 基于《数据挖掘：概念与技术》的简述，欲了解细节，强烈建议读原书！！！ IntroductionDataData types* 'Relational records' 关系记录 - Relational tables, highly structured 关系表，高度结构化* 'Data matrix', e.g., numerical matrix, crosstabs 数据矩阵，例如数值矩阵，交叉表* 'Transaction data' 交易数据* 'Document data': Term-frequency vector (matrix) of text documents 文档数据：文本文档的术语 - 频率向量（矩阵）* 'Transportation network' 交通网络* 'World Wide Web' 万维网* 'Molecular Structures' 分子结构* 'Social or information networks' 社交或信息网络 Attribute Types * 'Nominal': categories, states, or “names of things” # 标称属性: 类别，状态或“事物名称” - Hair_color = &#123;auburn, black, blond, brown, grey, red, white&#125; - marital status, occupation, ID numbers, zip codes 婚姻状况，职业，身份证号码，邮政编码* 'Binary' # 二元属性 - Nominal attribute with only 2 states (0 and 1) 仅有2个状态（0和1）的标称属性 - Symmetric binary: both outcomes equally important. e.g., gender 对称二元：两种结果同样重要. 例如，性别 - Asymmetric binary: outcomes not equally important. e.g., medical test (positive vs. negative) 不对称二元：结果不是同等重要的. 例如，医学检验（正面与负面） - Convention: assign 1 to most important outcome (e.g., HIV positive) 公约：为最重要的结果指定1 （例如艾滋病毒阳性）* 'Ordinal' # 序数属性 - Values have a meaningful order (ranking) but magnitude between successive values is not known 值具有有意义的顺序（排名），但连续值之间的大小未知 - Size = &#123;small, medium, large&#125;, grades, army rankings 大小= &#123;小，中，大&#125;，成绩，军队排名* 'Numeric' # 数值属性 - `Interval` 间隔 - Measured on a scale of equal-sized units 按相同大小的单位测量 - Values have order 数值有序 E.g., temperature in C˚or F˚, calendar dates - No true zero-point 无真正\"零点\" - `Ratio` 比率(倍数) - Inherent zero-point 固有零点 - We can speak of values as being an order of magnitude larger than the unit of measurement (10 K˚ is twice as high as 5 K˚). 我们可以说价值比测量单位大一个数量级（10K˚是5K˚的两倍） - e.g., temperature in Kelvin, length, counts, monetary quantities 例如，以开尔文为单位的温度，长度，计数，货币数量* 'Discrete Attribute' # 离散属性 - Has only a finite or countably infinite set of values 只有一组有限或可数无限的值 - E.g., zip codes, profession, or the set of words in a collection of documents 例如，邮政编码，专业或文档集合中的单词集 - Sometimes, represented as integer variables 有时，表示为`整数变量` - Note: Binary attributes are a special case of discrete attributes 注意：二进制属性是离散属性的特例* 'Continuous Attribute' # 连续属性 - Has real numbers as attribute values 将实数作为属性值 - E.g., temperature, height, or weight 例如，温度，高度或重量 - Practically, real values can only be measured and represented using a finite number of digits 实际上，只能使用有限数字来测量和表示实际值 - Continuous attributes are typically represented as floating-point variables 连续属性通常表示为`浮点变量` Data statisticsData dispersion(分散) characteristics Mean Mean (algebraic measure) (sample vs. population): Note: $n$ is sample size and $N$ is population size. $\\underbrace{\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i}_\\text{sample}$ $\\underbrace{\\mu = \\frac{1}{N}\\sum_{i=1}^{n}x_i}_\\text{population}$ Weighted arithmetic mean: 算术平均数/加权平均数 $\\bar{x}=\\frac{\\sum_{i=1}^{n}w_ix}{\\sum_{i=1}^{n}w_i}$ Median Middle value if odd number of values, or average of the middle two values otherwise ModeValue that occurs most frequently in the data Properties of Normal Distribution Curve Variance and Standard Deviation (sample: s, population: σ) Variance: (algebraic, scalable computation) $s^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-x)^2=\\frac{1}{n-1}[\\sum_{i=1}^{n}{x_i}^2-\\frac{1}{n}(\\sum_{i=1}^{n}x_i)^2]$ ${\\sigma}^2 =\\frac{1}{N}\\sum_{i=1}^{n}(x_i-\\mu)^2 = \\frac{1}{N}\\sum_{i=1}^{n}{x_i}^2-{\\mu}^2 $ Standard deviation $s$ (or $σ$) is the square root of variance $s^2$ (or $σ^2$) $s = \\sqrt{s^2}$ $\\sigma = \\sqrt{\\sigma^2} $ standardized measure (z-score) $z=\\frac{x-\\mu}{\\sigma}$ Graphic Displays of Basic Statistical Descriptions Boxplot: graphic display of five-number summary 箱线图 &bull; Quartiles: Q1 (25th percentile), Q3 (75th percentile) &bull; Inter-quartile range: IQR = Q3 – Q1 &bull; Five number summary: min, Q1, median, Q3, max &bull; Boxplot: Data is represented with a box &emsp; &bull; Q1, Q3, IQR: The ends of the box are at the first and third quartiles, i.e., the height of the box is IQR &emsp; &bull; Median (Q2) is marked by a line within the box &emsp; &bull; Whiskers: two lines outside the box extended to Minimum and Maximum Histogram: x-axis are values, y-axis repres. frequencies 柱状图/直方图 Quantile plot: each value $x_i$ is paired with $f_i$ indicating that approximately $100 f_i \\%$ of data are ​$\\leq x_i$ 分位图 Quantile-quantile (q-q) plot: graphs the quantiles of one univariant distribution against the corresponding quantiles of another 绘制一个单变量分布的分位数与另一个分配的相应分位数的关系图.QQPlot图是用于直观验证一组数据是否来自某个分布，或者验证某两组数据是否来自同一（族）分布。在教学和软件中常用的是检验数据是否来自于正态分布。 Scatter plot: each pair of values is a pair of coordinates and plotted as points in the plane 每对值是一对坐标并绘制为平面中的点 Distance on Numeric DataDissimilarity (distance) matrix: Usually symmetric, thus a triangular matrix$$\\begin{pmatrix}0 &amp; &amp; &amp; \\d(2,1) &amp; 0 &amp; &amp; \\… &amp; … &amp; … &amp; \\d(n,1) &amp; d(n,2) &amp; … &amp; 0\\end{pmatrix}$$ Minkowski distance$$d(i,j)=\\sqrt[p]{\\left | x_{i1}-x_{j1} \\right |^p+\\left | x_{i2}-x_{j2} \\right |^p+……+\\left | x_{il}-x_{jl} \\right |^p}$$ $p = 1: (L_1 norm)$ Manhattan (or city block) distance E.g.,the Hamming distance: the number of bits that are different between two binaryvectors$$d(i,j)=\\left | x_{i1}-x_{j1} \\right |+\\left | x_{i2}-x_{j2} \\right |+……+\\left | x_{il}-x_{jl} \\right |$$ $p = 2: (L_2 norm)$ Euclidean distance$$d(i,j)=\\sqrt{\\left | x_{i1}-x_{j1} \\right |^2+\\left | x_{i2}-x_{j2} \\right |^2+……+\\left | x_{il}-x_{jl} \\right |^2}$$ $p→ ∞: (L_{max} norm,L_∞ norm) $“supremum” distance The maximum difference between any component (attribute) of the vectors$$d(i,j)=\\lim_{p→ ∞}\\sqrt[p]{\\left | x_{i1}-x_{j1} \\right |^p+\\left | x_{i2}-x_{j2} \\right |^p+……+\\left | x_{il}-x_{jl} \\right |^p}=\\max_{f=1}^{l}\\left|x_{if}-x_{jf}\\right|$$ Proximity 邻近 Measure for Binary Attributes Data preprocessing Data cleaning Handle missing data(Incomplete), smooth noisy data,identify or remove outliers, and resolve inconsistencies 处理丢失的数据，平滑噪声数据，识别或删除异常值，并解决不一致问题 * 'Data discrepancy detection' 数据差异检测 - Use metadata (e.g., domain, range, dependency, distribution) 使用元数据（例如，域，范围，依赖关系，分发） - Check field overloading 检查字段重载 - Check uniqueness rule, consecutive rule and null rule 检查唯一性规则，连续规则和空规则 - Use commercial tools 使用商业工具 - Data scrubbing: use simple domain knowledge (e.g., postal code, spell-check) to detect errors and make corrections 数据清理：使用简单的域知识（例如，邮政编码，拼写检查）来检测错误并进行更正 - Data auditing: by analyzing data to discover rules and relationship to detect violators (e.g., correlation and clustering to find outliers) 数据审计：通过分析数据来发现规则和检测违规者的关系（例如，关联和聚类以查找异常值）* 'Data migration and integration' 数据迁移和集成 - Data migration tools: allow transformations to be specified 数据迁移工具：允许指定转换 - ETL (Extraction/Transformation/Loading) tools: allow users to specify transformations through a graphical user interface ETL（提取/转换/加载）工具：允许用户通过图形用户界面指定转换* 'Integration of the two processes' - Iterative and interactive (e.g., Potter’s Wheels) 迭代和互动（例如，波特的轮子） How to Handle Missing Data? 1.忽略元组 2.手动填充 3. 自动（以unknown/均值/最可能的值）填充 * 'Ignore the tuple': usually done when class label is missing (when doing classification)—not effective when the % of missing values per attribute varies considerably 忽略元组：通常在缺少类标签时（进行分类时）完成 - 当每个属性的缺失值百分比变化很大时(该属性)无效* Fill in the missing value 'manually': tedious + infeasible? 手动填写缺失值：单调乏味+不可行？* Fill in it 'automatically' with - a global constant : e.g., “unknown”, a new class?! - the attribute mean 属性平均值(与下一条的不同？) - the attribute mean for all samples belonging to the same class: 'smarter' - the most probable value: inference-based such as Bayesian formula or decision tree How to Handle Noisy Data? 1.分档 2.回归 3.聚类（无监督） 4.半监督 * 'Binning' 分档 - First sort data and partition into (equal-frequency) bins 首先将数据排序并分区为（等频）箱 - Then one can smooth by bin means, smooth by bin median, smooth by bin boundaries, etc.然后，可以通过bin均值平滑，通过bin中值平滑，通过bin边界平滑等。* 'Regression' 回归 - Smooth by fitting the data into regression functions 通过将数据拟合到回归函数中来平滑* 'Clustering' 聚类 - Detect and remove outliers 检测并删除异常值* 'Semi-supervised': Combined computer and human inspection 半监督：计算机和人工检查相结合 - Detect suspicious values and check by human (e.g., deal with possible outliers) 检测可疑值并由人查验（例如，处理可能的异常值） Data integratingIntegration of multiple databases, data cubes, or files 集成多个数据库，数据立方体或文件 1.数据集成 2.模式集成 3.实体识别 4.检测和解决数据值的冲突 - 'Data integration' - Combining data from multiple sources into a coherent store 将来自多个来源的数据组合到一个连贯的存储中- 'Schema integration': e.g., A.cust-id \\equiv B.cust-# 模式集成 - Integrate metadata from different sources 集成来自不同来源的元数据- 'Entity identification' 实体识别 - Identify real world entities from multiple data sources, e.g., Bill Clinton = William Clinton 从多个数据源中识别真实世界的实体，例如Bill Clinton = William Clinton- 'Detecting and resolving data value conflicts' 检测和解决数据值冲突 - For the same real world entity, attribute values from different sources are different 对于相同的现实世界实体，来自不同来源的属性值是不同的 - Possible reasons: different representations, different scales, e.g., metric vs. British units 可能的原因：不同的表示，不同的比例，例如，公制与英制单位 Handling Redundancy in Data Integration 1.冗余原因（对象标识、派生数据）2.检测手段（相关性和协方差分析）3.仔细整合 Redundant data occur often when integration of multiple databases 当多个数据库集成时，通常会出现冗余数据 - 'Object identification': The same attribute or object may have different names in different databases 对象标识：相同的属性或对象在不同的数据库中可能具有不同的名称- 'Derivable data:' One attribute may be a “derived” attribute in another table, e.g., annual revenue 派生数据：一个属性可以是另一个表中的“派生”属性，例如年收入 Redundant attributes may be able to be detected by correlation analysis and covariance analysis 可以通过相关性分析和协方差分析来检测冗余属性 Careful integration of the data from multiple sources may help reduce/avoid redundancies and inconsistencies and improve mining speed and quality 仔细整合来自多个来源的数据可能有助于减少/避免冗余和不一致，并提高”采矿“速度和质量 Data Reduction- 'Data reduction' 数据压缩 - Obtain a reduced representation of the data set 获得数据集的缩减表示 - much smaller in volume but yet produces almost the same analytical results 体积小得多，但产生几乎相同的分析结果- 'Why data reduction ?'—A database/data warehouse(仓库) may store terabytes of data - Complex analysis may take a very long time to run on the complete data set 复杂分析可能需要很长时间才能在完整数据集上运行 Methods for data reduction- 'Regression and Log-Linear Models' （Parametric methods）- 'Histograms, clustering, sampling' （Non-parametric methods） &gt; sampling: 选择具有代表性的子集；简单随机、放回、不放回、分层抽样- 'Data cube aggregation' 数据立方体聚合- 'Data Compression' Data cube aggregation 数据立方体聚合 The aggregated data for an individual entity of interest 感兴趣的实体聚合数据 - Demographic Data &apos;人口统计学数据&apos;（Service、Age、Gender、Job level、Workforce Segment）- Organisational process data &apos;组织行为处理数据&apos;（Performance rating、Potential rating、Salary increases、Turnover、in training &amp; development）- Predictive attitudinal data &apos;预测态度数据&apos;（Competencies能力、Intention to stay、AffectIve commitment、Job satisfaction、Discretionary自动支配 effort） Data compression Dimensionality Reduction Techniques# Dimensionality reduction methodologies- `Feature selection`: Find a subset of the original variables (or features, attributes) 找寻合适子集(仅收集与分析任务相关的属性)- `Feature extraction`: Transform the data in the high-dimensional space to a space of fewer dimensions 高维映射至低维(降维) # Some typical dimensionality methods* Principal Component Analysis ('PCA') - A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components 一种统计过程，它使用'正交变换'将可能相关变量的一组观察值转换为一组称为主成分的线性不相关变量值 - The original data are projected onto a much smaller space, resulting in dimensionality reduction 将原始数据投影到更小的空间，从而减少维数 (Feature extraction 降维) - Method: Find the eigenvectors of the covariance matrix, and these eigenvectors define the new space 找到协方差矩阵的特征向量，这些特征向量定义新的空间* 'Supervised and nonlinear techniques'- `Feature subset selection` - Best combined attribute selection(Best step-wise feature selection) and elimination(Repeatedly eliminate the worst attribute)- `Feature creation` - Create new attributes (features) that can capture the important information in a data set more effectively than the original ones 创建新属性（功能），可以比原始信息更有效地捕获数据集中的重要信息 (比如，从成绩单中得出平均分/绩点) - 'Three general methodologies' - `Attribute extraction` 降维 - Domain-specific - Mapping data to new space (see: data reduction) E.g., Fourier transformation, wavelet transformation, manifold approaches (not covered) - `Attribute construction` - Combining features (see: discriminative frequent patterns in Chapter on “Advanced Classification”) - Data discretization Data transforming- A function that maps the entire set of values of a given attribute to a new set of replacement values s.t. each old value can be identified with one of the new values 一种函数，它将给定属性的整个值集映射到一组新的替换值 使得 可以使用其中一个新值标识每个旧值- Methods - 'Smoothing': Remove noise from data - 'Attribute/feature construction' 属性/特征构建 - New attributes constructed from the given ones - 'Aggregation': Summarization, data cube construction - 'Normalization': Scaled to fall within a smaller, specified range - min-max normalization - z-score normalization - normalization by decimal scaling - 'Discretization' 离散化: Concept hierarchy climbing Normalization 归一化 Concept hierarchy generation 概念层次生成 # Data Discretization Methods- 'Binning' - Top-down split, unsupervised- 'Histogram' analysis - Top-down split, unsupervised- 'Clustering' analysis - Unsupervised, top-down split or bottom-up merge- 'Decision-tree' analysis - Supervised, top-down split- 'Correlation' (e.g., χ2) analysis - Unsupervised, bottom-up merge- Note: All the methods can be applied `recursively` - Concept hierarchy organizes concepts (i.e., attribute values) hierarchically and is usually associated with each dimension in a data warehouse 概念层次结构按层次组织概念（例如属性值），并且通常与数据仓库中的每个维度相关联- Concept hierarchies facilitate drilling and rolling in data warehouses to view data in multiple granularity 概念层次结构有助于在数据仓库中钻取和滚动，以多种粒度查看数据- Concept hierarchy formation: Recursively reduce the data by collecting and replacing low level concepts (such as numeric values for age) by higher level concepts (such as youth, adult, or senior) 概念层次结构：通过收集和替换更高级别概念（例如青年，成人或高级）的低级概念（例如年龄的数字值）来递归地减少数据- Concept hierarchies can be explicitly specified by domain experts and/or data warehouse designers 概念层次结构可以由域专家和/或数据仓库设计者明确指定- Concept hierarchy can be automatically formed for both numeric and nominal data—For numeric data, use discretization methods shown 可以为数字和标称数据自动形成概念层次结构 - 对于数字数据，使用显示的离散化方法 # Concept Hierarchy Generation for Nominal Data- Specification of a partial/total ordering of attributes explicitly at the schema level by users or experts - 'street &lt; city &lt; state &lt; country'- Specification of a hierarchy for a set of values by explicit data grouping - '&#123;Urbana, Champaign, Chicago&#125; &lt; Illinois'- Specification of only a partial set of attributes - E.g., only 'street &lt; city', not others- Automatic generation of hierarchies (or attribute levels) by the analysis of the number of distinct values - E.g., for a set of attributes:'&#123;street, city, state, country&#125;'# Automatic Concept Hierarchy Generation- Some hierarchies can be automatically generated based on 'the analysis of the number' of distinct values per attribute in the data set - The attribute with the most distinct values is placed at the lowest level of the hierarchy - Exceptions, e.g., weekday, month, quarter, year Summary Classification What is classification? 根据训练集和类标签（分类属性中的值）构建模型，并将其用于分类新数据，预测其标签。 Supervised learning **监督学习**是机器学习任务的一种。它`从有标记的训练数据中推导出预测标签`。有标记的训练数据是指每个训练实例都包括输入和期望的输出。一句话：**给定数据，预测标签**。(分类、回归) **无监督学习**是机器学习任务的一种。它`从无标记的训练数据中推断结论`。最典型的无监督学习就是聚类分析，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。一句话：**给定数据，寻找隐藏的结构**。 Classification Steps Model construction 模型构建 - Each sample is assumed to belong to a predefined class (shown by the class label) 假设每个样本属于预定义的类（由类标签显示）- The set of samples used for model construction is training set 用于模型构建的样本集是训练集- 'Model': Represented as decision trees, rules, mathematical formulas, or other forms 模型：表示为决策树，规则，数学公式或其他形式 Model Validation and Testing - 'Test': Estimate accuracy of the model - The known label of test sample is compared with the classified result from the model 将已知的测试样品标签与模型的分类结果进行比较 - Accuracy: % of test set samples that are correctly classified by the model 准确度：按模型正确分类的测试集样本的百分比 - Test set is independent of training set 测试集独立于训练集- 'Validation': If the test set is used to select or refine models, it is called validation (development/test) set 验证：如果测试集用于选择或改进模型，则称为验证（开发/测试）集 To be better【与测试集相较，强调refine models】 Model Deployment If the accuracy is acceptable, use the model to classify new data 【模型部署】 AlgorithmsDecision tree-ID3,C4.5,CART决策树算法原理(上)、决策树算法原理(下)、scikit-learn决策树算法类库使用小结 Basic algorithm - Tree is constructed in a top-down, recursive, divide-and-conquer manner 树以自上而下，递归，分而治之的方式构建 - At start, all the training examples are at the root 一开始，所有的训练样例都是根源 - Examples are partitioned recursively based on selected attributes 样例基于被选定的属性递归地划分 - On each node, attributes are selected based on the training examples on that node, and a heuristic or statistical measure (e.g., information gain) 在每个节点上，基于'该节点上的训练示例'以及'启发式或统计度量（例如，信息增益）'来'选择属性'。 Conditions for stopping partitioning - All samples for a given node belong to the same class 给定节点的所有样本都属于同一个类- There are no remaining attributes for further partitioning 没有剩余属性可用于进一步分区- There are no samples left 没有剩下的样例 Prediction 'Majority voting' is employed for classifying the leaf Entropy Example: Attribute Selection with Information Gain Computation of Gini Index - Entropy 信息熵: 表征混乱程度 - Conditional Entropy条件熵: 在已知随机变量X的条件下随机变量Y的不确定性(概率)- Mutual Information互信息/Information gain信息增益: 得知特征X的信息而使得类Y的信息的不确定性减少的程度(越大越好) -&gt;&gt; ID3决策树- Gain Ratio信息增益比: 解决使用信息增益存在偏向于选择取值较多的特征的问题(越大越好) -&gt;&gt; C4.5决策树 - GINI index基尼指数： 表征不纯度(越小越好) -&gt;&gt; CART分类树 剪枝 * 预剪枝(prepruning),通过提前停止树的构建(例如，通过决定在给定的结点`不再分裂或划分`训练元组的子集)而对树\"剪枝\"。如果划分一个结点的元组导致低于预定义(信息增益、基尼指数等度量方式)阈值的划分，则给定子集的进一步划分将停止。然而，选取一个适当的阈值是困难的。高阈值可能导致过分简化的树，而低阈值可能使得树的简化不足。* 后剪枝(postpruning)，它由\"完全生长\"的树剪去子树。通过删除结点的分支并用树叶替换它而剪掉给定结点上的子树。 - E.g. CART使用代价复杂度剪枝算法* 组合方法：预剪枝和后剪枝交叉使用。后剪枝所需要的计算比预剪枝多，但是通常产生更可靠的树。 Advantage 1）&apos;简单直观&apos;，生成的决策树很直观。2）基本&apos;不需要预处理&apos;，不需要提前归一化，处理缺失值。3）使用决策树预测的代价是O(log2m)。 m为样本数。4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。5）可以&apos;处理多维度输出&apos;的分类问题。6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上有很好的&apos;可解释性&apos;7）可以交叉验证的&apos;剪枝&apos;来选择模型，从而提高泛化能力。8）对于异常点的&apos;容错能力&apos;好，健壮性高。 Disadvantage 1）决策树算法非常容'易过拟合'，导致'泛化能力不强'.可以通过设置节点最少样本数量和限制决策树深度来改进.2）决策树会因为样本发生一点点的改动，就会导致树'结构的剧烈改变'。这个可以通过集成学习之类的方法解决.3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，'容易陷入局部最优'。可以通过集成学习之类的方法来改善。4）有些比较'复杂的关系'，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。6）处理大数据集，决策树的构造可能变得效率低下('可伸缩问题') -&gt;&gt; RainForest(雨林)，能适应可用的内存量，并应用于任意决策树归纳算法 决策树算法比较 | 算法 | 支持模型 | 树结构 | 特征选择 | 连续值处理 | 缺失值处理 | 剪枝 || :—-: | :——–: | :—-: | :—————-: | :——–: | :——–: | :—-: || ID3 | 分类 | 多叉树 | 信息增益 | 不支持 | 不支持 | 不支持 || C4.5 | 分类 | 多叉树 | 信息增益比 | 支持 | 支持 | 支持 || CART | 分类，回归 | 二叉树 | 基尼系数，均方差 | 支持 | 支持 | 支持 | SVM Define: Find a linear/non-linear hyperplane (decision boundary) that will separate the data Optimize：希望所有的点都离超平面远 -&gt; 可以让离超平面比较近的点尽可能的远离超平面 kernel核函数：将数据从低维空间映射到高维空间 支持向量机原理(一) 线性支持向量机、支持向量机原理(二) 线性支持向量机的软间隔最大化模型、支持向量机原理(三)线性不可分支持向量机与核函数、支持向量机原理(四)SMO算法原理、支持向量机原理(五)线性支持回归、scikit-learn 支持向量机算法库使用小结、支持向量机高斯核调参小结 Bayes Basic Concepts * P(H|X): 后验概率. 基于相关性描述(证据)X, '假设H'成立的概率* P(H): 先验概率. 一般情况下(没有相关约束)，'假设H'成立的概率在分类问题中,希望确定 给定“证据”X，假设H成立的概率P(H|X). 贝叶斯定理： $P(H|X)=\\frac{P(X|H)P(H)}{P(X)}$ Naïve Bayesian Classifier 后验概率最大化来判断分类 1) 如果没有Y的先验概率，则计算Y的K个先验概率：$P(Y=C_k)$ 2) 分别计算第k个类别的第j维特征的第l个个取值条件概率：$P(X_j=x_{jl}|Y=C_k)$ 3）对于实例$X^{(test)}$，分别计算：$P(Y=C_k)\\prod_{j=1}^{n}P(X_j=x_j^{(test)}|Y=C_k)$ 4) 确定实例$X^{(test)}$的分类$C_{result} = \\underbrace{argmax}_{C_k}P(Y=C_k)\\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k) $ # Advantages Easy to implement 易于实现 Good results obtained in most of the cases 大多数情况下获得了良好的结果# Disadvantages Assumption: class conditional independence, therefore loss of accuracy类条件独立-&gt;准确率缺失 Practically, dependencies exist among variables 实际上，变量之间存在依赖关系  E.g., hospitals: patients: Profile: age, family history, etc. Symptoms: fever, cough etc., Disease: lung cancer, diabetes,etc.  Dependencies among these cannot be modeled by Naïve Bayesian Classifier 这些依赖关系不能用朴素贝叶斯分类器建模# How to deal with these dependencies? Bayesian Belief Networks 如何处理这些依赖关系？贝叶斯置信网络 朴素贝叶斯算法原理小结、scikit-learn 朴素贝叶斯类库使用小结 ANN 受生物神经元的启发，将多输入单输出的信息处理单元作为人工神经网络中的一个神经元。人工神经网络的基本结构如下：输入层(输入层的神经元数目对应于训练集数据的属性数目)、隐藏层、输出层(输出层的神经元数目对应于网络预测的分类数目) Backpropagation Backpropagate the error (by updating weights and biases) # Terminating condition (when error is very small, etc.) Small changes in weights 已接近\"最优\" Small errors 结果可接受 Number of predefined iterations # more 反向传播可能会停留在局部最小值，但实际上它通常表现良好 DeepLearning 通过更深的layers，自动提取特征（构建特征空间），以达到更”深层次”的学习效果 - Train networks with many layers (vs. shallow nets with just a couple of layers) '训练具有多层的网络'（与仅有几层的浅网络） More neurons than previous networks 与以前的网络相比，有更多的神经元 More complex ways to connect layers 更复杂的连接层的方法 Tremendous computing power to train networks 以巨大计算能力训练网络 Automatic feature extraction '自动特征提取'- Multiple layers work to build 'an improved feature space' 多个层用于构建改进的特征空间 Analogy: Signals passing through regions of the visual cortex 类比：信号通过视觉皮层的区域  Example: For face recognition: edge → nose → face, layer-by-layer 示例：用于面部识别：边缘→鼻子→面部，逐层- 'Popular Deep Learning Frameworks' for Classification  Deep Feedforward Neural Networks 深度前馈神经网络 Convolutional Neural Networks 卷积神经网络 Recurrent Neural Networks 回归神经网络# More 为解决梯度消失/梯度爆炸、训练退化的问题，提出ResNet 为实现特征的复用，提出DenseNet Model evaluation and selectionClassifier Evaluation MetricsClassifier Evaluation Metrics: Example Confusion matrix and criteria 精确率与召回率，RoC曲线与PR曲线 Estimating a classifier’s accuracyCross-evaluation# Cross-validation (k-fold, where k = 10 is most popular)- Randomly partition the data into k mutually exclusive subsets, each approximately equal size 随机将数据划分为k个互斥的子集，每个子集的大小大致相等- At i-th iteration, use Di as test set and others as training set 在第i次迭代中，使用Di作为测试集，使用其他作为训练集- Leave-one-out: k folds where k = '#' of tuples, for small sized data 留一个：k折叠，其中k = '#' 元组的数量，对于小尺寸数据- *Stratified cross-validation*: folds are stratified so that class distribution, in each fold is approximately the same as that in the initial data *分层交叉验证*：折叠是分层的，因此每个折叠中的类分布与初始数据中的类别分布大致相同# Holdout method- Given data is randomly partitioned into two independent sets 给定数据被随机分成两个独立的集合 - Training set (e.g., 2/3) for model construction 模型构建的训练集（例如，2/3） - Test set (e.g., 1/3) for accuracy estimation 测试集（例如，1/3）用于准确度估计- Repeated random sub-sampling validation: a variation of holdout 重复随机子采样验证：保持的变化 - Repeat holdout k times, accuracy = avg. of the accuracies obtained 重复k次，精度= 所得准确性的平均值 # Bootstrap 引导程序- Works well with small data sets '适用于小型数据集'- Samples the given training tuples uniformly with replacement 均匀地对给定的训练元组进行'取样' - Each time a tuple is selected, it is equally likely to be selected again and re-added to the training set 每次选择元组时，同样可能再次选择并重新添加到训练集- Several bootstrap methods, and a common one is .632 bootstrap 有几种引导方法，常见的方法是.632引导程序 (大约63.2％的原始数据最终在bootstrap中，其余36.8％形成测试集) 交叉验证(Cross Validation)原理小结 Issues Affecting Model Selection- 'Accuracy' 准确性 - classifier accuracy: predicting class label- 'Speed' 速度 - time to construct the model (training time) 构建模型的时间（训练时间） - time to use the model (classification/prediction time) 使用模型的时间（分类/预测时间）- 'Robustness' 鲁棒性(稳健性): handling noise and missing values- 'Scalability' 可伸缩性: efficiency in disk-resident databases 磁盘驻留数据库的效率- 'Interpretability' 可解释性 - understanding and insight provided by the model 模型提供的理解和洞察力- 'Other measures', e.g., goodness of rules, such as decision tree size or compactness of classification rules 其他措施，例如规则的好处，例如决策树大小或分类规则的紧凑性 Ensemble methods# Ensemble Methods: Increasing the AccuracyEnsemble methods - Use a combination of models to increase accuracy 使用模型组合来提高准确性 - Combine a series of k learned models, M1, M2, ..., Mk, with the aim of creating an improved model M* 结合一系列k学习模型，M1，M2，...，Mk，旨在创建一个改进的模型M*Popular ensemble methods - 'Bagging': Trains each model using a subset of the training set, and models learned in parallel Bagging: 使用训练集的子集训练每个模型，并且并行学习模型 - 'Boosting': Trains each new model instance to emphasize the training instances that previous models mis-classified, and models learned in order Boosting：训练每个新模型实例以强调先前模型错误分类的训练实例，以及按顺序学习的模型 集成学习原理小结 Bagging: Bootstrap Aggregation - 'Analogy': Diagnosis based on multiple doctors’ 'majority vote'- 'Training' - Given a set D of d tuples, at each iteration i, a training set Di of d tuples is sampled with replacement from D (i.e., bootstrap) 给定d个元组的D组，在每次迭代i中，对D元组的训练集Di进行'采样'，并用D替换（即'自举'） - A classifier model 'Mi' is learned for each training set 'Di' 为每个训练集Di学习分类器模型Mi.- 'Classification': classify an 'unknown sample X' - Each classifier Mi returns its class prediction - The bagged classifier M* 'counts the votes' and assigns the class with the mostvotes to X - 'Prediction': It can be applied to the prediction of 'continuous values' by 'taking theaverage value' of each prediction for a given test tuple 预测: 它可以应用于连续值的预测 by 给定测试元组的每个预测的平均值- 'Accuracy': Improved accuracy in prediction - Often significantly better than a single classifier derived from D 通常明显优于源自D的单一分类器 - For noise data: Not considerably worse, more robust 对于噪声数据：不会更糟，而是更健壮 Bagging与随机森林算法原理小结 Boosting （Bagging）投票得分 -&gt; （Boosting）加权得分 - Analogy: Consult several doctors, based on 'a combination of weighted' diagnoses—weight assigned based on the previous diagnosis accuracy- How boosting works? - Weights are assigned to each training tuple '权重'分配给每个训练元组 - A series of k classifiers is iteratively learned 迭代学习一系列k个分类器 - After a classifier Mi is learned, the weights are updated to allow the subsequent classifier, Mi+1, to 'pay more attention to the training tuples that were misclassified' by Mi 在学习分类器Mi之后，权重被更新以允许随后的分类器Mi+1'更多地关注被Mi错误分类的训练元组' - The final M* 'combines the votes' of each individual classifier, where the weight of each classifier's vote is a function of its accuracy 最终的M*结合了每个分类器的投票，每个分类器的投票'权重'是其'准确性'的函数- Boosting algorithm can be extended for numeric prediction 可以扩展Boosting算法进行'数值预测'- Comparing with bagging: Boosting tends to have greater accuracy, but it also risks overfitting the model to misclassified data 与装袋相比：boosting往往具有更高的'准确性'，但也存在过度拟合模型错误分类数据的'风险' 集成学习之Adaboost算法原理小结、scikit-learn Adaboost类库使用小结 Random Forest Avariation of bagging for decision trees 对决策树的bagging的’变异’ # Random Forest (first proposed by L. Breiman in 2001)- Avariation of bagging for decision trees 对决策树的bagging的'变异'- Data bagging 数据装袋 - Use a subset of training data by sampling with replacement for each tree 通过'采样'为每棵树'替换'使用训练数据的子集- Feature bagging - At each node use a random selection of attributes as candidates and split by the best attribute among them 在每个节点使用'随机'选择的属性作为'候选'并且以它们中的'最佳'属性进行'划分'- Compared to original bagging,increases the diversity among generated trees 与原始套袋相比，增加了生成树的'多样性'- During classification, each tree 'votes' and the 'most popular' class is returned# Two Methods to construct Random Forest - Forest-RI (random input selection): Randomly select, at each node, F attributes as candidates for the split at the node. The CART methodology is used to grow the trees to maximum size Forest-RI（'随机输入'选择）：在每个节点上随机选择F属性作为节点分割的候选者。 CART方法用于将树增长到最大尺寸 - Forest-RC (random linear combinations): Creates new attributes (or features) that are a linear combination of the existing attributes (reduces the correlation between individual classifiers) Forest-RC（'随机线性组合'）：创建属于现有属性的线性组合的新属性（或特征）（减少各个分类器之间的相关性）- Comparable in accuracy to Adaboost, but more robust to errors and outliers 与Adaboost相比具有可比性，但对错误和异常值'更具鲁棒性'- Insensitive to the number of attributes selected for consideration at each split, and faster than typical bagging or boosting 对每次拆分时选择的'属性数量不敏感'，并且比典型的bagging或boosting'速度更快' scikit-learn随机森林调参小结 Summary Frequent patterns What is a frequent pattern # basic concept 满足最小支持度的项集F -&gt;&gt; F为'频繁项集'(frequent pattern) 项集L的任意超集均为非频繁项集 -&gt;&gt; L为'最大频繁模式'(Max-Pattern)/最大频繁项集(Maximal Frequent Itemset) 项集X的直接超集(最小的严格超集)的支持度计数都不等于(小于)ta本身的支持度计数 -&gt;&gt; X为'闭合频繁项集'(closed-pattern) Association rule 关联规则就是有关联的规则，形式是这样定义的：两个不相交的非空集合X、Y，如果有X–&gt;Y，就说X–&gt;Y是一条关联规则。举个例子，在上面的表中，我们发现购买啤酒就一定会购买尿布，{啤酒}–&gt;{尿布}就是一条关联规则。关联规则的强度用支持度$Support$和置信度$Confidence$等描述 一般来说，要选择一个数据集合中的频繁数据集，则需要自定义评估标准。最常用的评估标准是用自定义的支持度，或者是自定义支持度和置信度的一个组合。 关联规则挖掘步骤 1. '生成频繁项集' 这一阶段找出所有满足最小支持度的项集，找出的这些项集称为频繁项集2. '生成规则' 在上一步产生的频繁项集的基础上生成满足最小置信度的规则，产生的规则称为强规则 AlgorithmApriori为了减少频繁项集的生成时间，我们应该尽早的消除一些完全不可能是频繁项集的集合，故而引出Apriori的两条定律： 如果一个集合是频繁项集，则它的所有子集都是频繁项集。 如果一个集合不是频繁项集，则它的所有超集都不是频繁项集。 利用这两条定律，我们抛掉很多的候选项集，Apriori算法就是利用这两个定理来实现快速挖掘频繁项集的。Apriori算法原理总结、关联规则挖掘基本概念与Aprior算法 Apriori算法属于候选消除算法，是一个生成候选集、消除不满足条件的候选集、并不断循环直到不再产生候选集的过程。 上面的图演示了Apriori算法的过程，注意看由二级频繁项集生成三级候选项集时，没有{牛奶,面包,啤酒}，那是因为{面包,啤酒}不是二级频繁项集，这里利用了Apriori定理。最后生成三级频繁项集后，没有更高一级的候选项集，因此整个算法结束，{牛奶,面包,尿布}是最大频繁子集 # How to generate candidates?  Step 1: self-joining Lk Step 2: pruning# Example of Candidate-generation L3=&#123;abc, abd, acd, ace, bcd&#125; # INPUT Self-joining: L3*L3 # Step1  abcd from abc and abd  acde from acd and ace Pruning: # Step2  acde is removed because ade is not in L3  C4 = &#123;abcd&#125; # OUTPUT 我们发现Apriori算法是一个候选消除算法，每一次消除都需要扫描一次所有数据记录，造成整个算法在面临大数据集时显得效率低下（多次扫描事务数据库、产生大量的候选集、对候选集的支持度计算产生了繁琐的工作量）. 故而，我们需要了解$Fp-Growth$算法（如下） FP-growthFpGrowth算法通过构造一个树结构来压缩数据记录，使得挖掘频繁项集只需要扫描两次数据记录，而且该算法不需要生成候选集合，所以效率会比较高。 Step 1：扫描数据记录，生成一级频繁项集，并按出现次数由多到少排序Step 2：再次扫描数据记录，对每条记录中出现在Step 1产生的表中的项，按表中的顺序排序。初始时，新建一个根结点，标记为null； # Method For each frequent item, construct its conditional pattern-base, and then its conditional FP-tree 对于每个频繁项，'构造其条件模式库'，然后'构造其条件FP树' Repeat the process on 'each' newly created conditional FP-tree 对每个新创建的条件FP树重复此过程 (\"Recursion: Mining Each Conditional FP-tree\") Until the resulting FP-tree is empty, or it contains only one path—single path will generate all the combinations of its sub-paths, each of which is a frequent pattern 在生成的FP树为空之前，或者它只包含一个路径 - 单个路径将生成其子路径的所有组合，'每个路径都是一个频繁的模式' FP Tree算法原理总结、关联规则FpGrowth算法 Summary Clustering What is clustering 聚类就是按照某个特定标准(如距离准则)把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同数据尽量分离。 Unsupervised learning 聚类是一种输入数据无标签的“分类”方式（即非监督学习），通常并不需要使用训练数据进行学习，仅把相似的东西聚到一起，并不关心所得的簇具体代表什么 Partition-based—k-meansK-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。 - There are many variants of the K-Means method, varying in different aspects K-Means方法有许多变体，在不同方面有所不同- Choosing better initial centroid estimates 选择更好的初始质心估计 'k-means对初始值的设置很敏感': K-means++, Intelligent K-Means, Genetic K-Means- Choosing different representative prototypes for the clusters 为集群选择不同的代表性原型 'k-means对噪声和离群值非常敏感': K-Medoids, K-Medians 'k-means只用于numerical，不适用于categorical类型数据': K-Modes- Applying feature transformation techniques 应用特征转换技术  Weighted K-Means 'k-means不能解决非凸non-convex数据': Kernel K-Means K-means++：选取新数据点作为新的聚类中心时，遵循的原则是与当前所属聚类中心距离最远的点，被选取作为聚类中心的概率较大 K-Medoids：Instead of taking the mean value of the object in a cluster as a referencepoint, medoids can be used, which is the most centrally located object in a cluster 不使用聚类中对象的平均值作为参考点，而是可以使用中心点medoids，它是集群中最集中的对象（计算该点到当前聚簇中所有点距离之和，最终距离之后最小的点，则视为新的中心点） K-Medians: Instead of taking the mean value of the object in a cluster as a reference point, medians are used (L1-norm as the distance measure) 不使用聚类中对象的平均值作为参考点，而是使用中位数（L1范数作为距离度量） K-Modes: An extension to K-Means by replacing means of clusters with modes 通过用众数替换簇的平均值来扩展K-Means Kernel K-Means : Project data onto the high-dimensional feature space using the kernel function, and then perform K-Means clustering 使用核函数将数据投影到高维特征空间，然后执行K-Means聚类 - Typical kernel functions: Polynomial kernel of degree '多项式核函数' Gaussian radial basis function (RBF) kernel '高斯径向基核函数' Sigmoid kernel 'Sigmoid核函数' K-Means聚类算法原理、用scikit-learn学习K-Means聚类 Hierarchical-based—two ways- Hierarchical clustering Generate a clustering hierarchy(drawn as a dendrogram) 生成聚类层次结构（绘制为树形图） 'Not required to specify K', the number of clusters 不需要指定聚类簇数 More deterministic 更具确定性 No iterative refinement 无迭代校准 Two categories of algorithms - 'Agglomerative': Start with singleton clusters, continuously merge two clusters at a time to build a bottom-up hierarchy of clusters '凝聚'：从单一集群开始，一次连续合并两个集群，构建'自下而上'的集群层次结构  Agglomerative clustering varies on 'different similarity measures' among clusters  Single link ('nearest' neighbor)  Average link (group 'average')  Complete link ('diameter'直径)  Centroid link ('centroid'重心 similarity)- 'Divisive': Start with a huge macro-cluster, split it continuously into two groups, generating a top-down hierarchy of clusters '分裂'：从一个庞大的宏集群开始，将其连续分成两组，生成一个'自上而下'的集群层次结构 BIRCH BIRCH (Balanced Iterative Reducing and Clustering Using Hierarchies): Use CF-tree and incrementally adjust the quality of sub-clusters 利用层次方法的平衡迭代规约和聚类：使用CF树并逐步调整子集群的质量 将所有的训练集样本建立了CF Tree，一个基本的BIRCH算法就完成了，对应的输出就是若干个CF节点，每个节点里的样本点就是一个聚类的簇。BIRCH聚类算法原理、用scikit-learn学习BIRCH聚类 CURE CURE (Clustering Using REpresentatives): Represent a cluster using a set of well-scattered representative points 使用一组分散的代表点来表示聚类 CHAMELEON Hierarchical Clustering Using Dynamic Modeling, A graph partitioning approach Probabilistic Hierarchical Clustering Use probabilistic models to measure distances between clusters 使用概率模型来测量簇之间的距离 Density-based—DBSCAN Clustering based on density (a local cluster criterion), such as density-connected points 基于密度的聚类（局部聚类标准），例如密度连接点 DBSCAN 图中MinPts=5，红色的点都是核心对象，因为其ϵ-邻域至少有5个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列。在这些密度可达的样本序列的ϵ-邻域内所有的样本相互都是密度相连(对称性)的。 DBSCAN密度聚类：由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。DBSCAN密度聚类、用scikit-learn学习DBSCAN聚类 E-M algorithm K-Means （距离） -&gt;&gt; E-M algorithm (概率分布) 假设需要调查某校的男生和女生的身高分布。假设在校园里随机抽样100个男生和100个女生，共200个人。不知道抽取的这200个人里面的每一个人到底是从男生的那个身高分布里面抽取的，还是女生的那个身高分布抽取的。即就是，抽取得到的每个样本都不知道是从哪个分布抽取的。 EM的意思是“Expectation Maximization”，在上述问题中，先随便猜一下男生(身高)的正态分布的参数:如均值和方差是多少。例如男生的均值是1米7，方差是0.1米(当然了，刚开始肯定没那么准)，然后计算出每个人更可能属于第一个还是第二个正态分布中的(例如，这个人的身高是1米8，那很明显，他最大可能属于男生的那个分布)，这个是属于Expectation一步。有了每个人的归属，或者说已经大概地按上面的方法将这200个人分为男生和女生两部分，就可以根据之前说的最大似然那样，通过这些被大概分为男生的n个人来重新估计第一个分布的参数，女生的那个分布同样方法重新估计。这个是Maximization。然后，当更新了这两个分布的时候，每一个属于这两个分布的概率又变了，那么就再需要调整E步……如此往复，直到参数基本不再发生变化为止。 一个最直观了解EM算法思路的是K-Means算法。在K-Means聚类时，每个聚类簇的质心是隐含数据。我们会假设K个初始化质心，即EM算法的E步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即EM算法的M步。重复这个E步和M步，直到质心不再变化为止，这样就完成了K-Means聚类。EM算法的描述还很粗糙，我们需要用数学的语言精准描述。详见EM算法原理总结 AP (2007, Science)AP（Affinity Propagation）一般翻译为近邻传播聚类Affinity Propagation: AP聚类算法 # basic concept* 'Exemplar'范例：即聚类簇中心点；* 'similarity's(i,j)：数据点i与数据点j的'相似度值'，一般使用欧氏距离的的负值表示，即s(i,j)值越大表示点i与j的距离越近，AP算法中理解为数据点j作为数据点i的聚类中心的能力； * 相似度矩阵：作为算法的初始化矩阵，n个点就有由n乘n个相似度值组成的矩阵； * 'Preference参考度's(i,i)：若按欧氏距离计算其值应为0，但在AP聚类中其`表示数据点i作为聚类中心的程度`，因此不能为0。迭代开始前假设所有点成为聚类中心的能力相同，因此参考度一般设为相似度矩阵中所有值得最小值或者中位数，但是参考度越大则说明个数据点成为聚类中心的能力越强，则最终聚类中心的个数则越多； * 'Responsibility'，r(i,k)：吸引度信息，表示数据点k`适合`作为数据点i的聚类中心的程度；(`k想做i的主公`)* 'Availability'，a(i,k)：归属度信息，表示数据点i选择数据点k作为其聚类中心的`合适`程度；(`i想做k的勇士`)* 'Damping factor'阻尼系数：为防止数据震荡，引入的衰减系数，`起到收敛作用`，每个信息值等于前一次迭代更新的信息值的λ倍加上此轮更新值得1-λ倍，其中λ在0-1之间，默认为0.5 AP算法通过迭代过程不断更新每一个点的responsibility和availability,直到产生m个高质量的exemplar,同时将其余的数据点分配到相应的聚类中。 # 算法流程： 1. 更新相似度矩阵中每个点的吸引度信息，计算归属度信息； 2. 更新归属度信息，计算吸引度信息； 3. 对样本点的吸引度信息和归属度信息求和，检测其选择聚类中心的决策；若经过若干次迭代之后其聚类中心不变、或者迭代次数超过既定的次数、又或者一个子区域内的关于样本点的决策经过数次迭代后保持不变，则算法结束。# 关于其算法流程，知乎上kael 用户将AP聚类过程比喻为选举过程： * 所有人都参加选举（大家都是选民也都是参选人），要选出几个作为代表 * s(i,k)就相当于i对选k这个人的一个固有的偏好程度 * r(i,k)表示用s(i,k)减去最强竞争者的评分，可以理解为k在对i这个选民的竞争中的优势程度 * r(i,k)的更新过程对应选民i对各个参选人的挑选（越出众越有吸引力） * a(i,k)：从公式里可以看到，所有r(i’,k)&gt;0的值都对a有正的加成。对应到我们这个比喻中，就相当于选民i通过网上关于k的民意调查看到：有很多人（即i’们）都觉得k不错（r(i’,k)&gt;0），那么选民i也就会相应地觉得k不错，是个可以相信的选择 * a(i,k)的更新过程对应关于参选人k的民意调查对于选民i的影响（已经有了很多跟随者的人更有吸引力） * 两者交替的过程也就可以理解为`不断地参考各个参选人给出的民意调查`和`选民在各个参选人之间不断地比较`。 * r(i,k)的思想反映的是`竞争`，a(i,k)则是为了`让聚类更成功`。 Local density-based (2014, Science) 该方法假设聚类中心周围都是密度比其低的点，同时这些点到该聚类中心的距离比其到其他聚类中心更近。 找出聚类中心:以通过给定的$δ{min}$和$ρ{min}$筛选出同时满足($ρi$ &gt; $ρ{min}$)和($δ_ i$ &gt; $δ_{min}$)条件的点作为聚类中心点。 剩余点的类别指派: 当前点的类别标签等于高于当前点密度的最近的点的标签一致。从而对所有点的类别进行了指定。 去除噪音：先算出类别之间的边界，然后找出边界中密度值最高的点的密度作为阈值只保留类别中大于或等于此密度值的点 Evaluation of ClusteringClustering evaluation Clustering Evaluation: Evaluating the goodness of clustering results (No commonly recognized best suitable measure in practice) 聚类评估：评估聚类结果的优劣（在实践中没有公认的最佳合适度量） Three categorization of measures- 'External': Supervised, employ criteria not inherent to the dataset 外部：监督，采用数据集不固有的标准 (使用'新数据') Compare a clustering against prior or expert-specified knowledge (i.e., the ground truth) using certain clustering quality measure 使用某些聚类质量测量将聚类与先前或专家指定的知识（即基础事实）进行比较- 'Internal': Unsupervised, criteria derived from data itself 内部：无监督，来自'数据本身'的标准 Evaluate the goodness of a clustering by considering how well the clusters are separated and how compact the clusters are, e.g., silhouette coefficient 通过考虑群集的分离程度以及群集的紧密程度（例如，轮廓系数）来评估群集的良好性- 'Relative': Directly compare different clusterings, usually those obtained via different parameter settings for the same algorithm 相对：直接比较不同的聚类，通常是通过'相同算法的不同参数'设置获得的聚类 Clustering stability Clustering stability : To understand the sensitivity of the clustering result to various algorithm parameters, e.g., # of clusters 聚类稳定性：理解聚类结果对各种算法参数的敏感性，例如聚类数 Methods for Finding K, the Number of Clusters- Empirical method '经验'方法 # of clusters: k ≈ sqrt(n / 2) for a dataset of n points (e.g., n = 200, k = 10)- Elbow method: Use the turning point in the curve of the sum of within cluster variance with respect to the # of clusters 使用聚类方差之和与曲线群数之和的曲线中的'转折点'- Cross validation method 交叉验证法 ('试错，择优') Divide a given data set into m parts Use m – 1 parts to obtain a clustering model Use 'the remaining part to test' the quality of the clustering  For example, for each point in the test set, find the closest centroid, and use the sum of squared distance between all points in the test set and the closest centroids to measure how well 'the model fits the test set' For any k &gt; 0, 'repeat it m times', compare the overall quality measure w.r.t. different k’s, and find # of clusters that fits the data the best Clustering tendency Clustering tendency: Assess the suitability of clustering, i.e., whether the data has any inherent grouping structure 聚类趋势：评估聚类的适用性，即数据是否具有任何固有的分组结构 - Still, there are some 'clusterability assessment methods', such as 'Spatial histogram': Contrast the histogram of the data with that generated fromrandom samples 空间直方图：将数据的直方图与生成的直方图进行对比 'Distance distribution': Compare the pairwise point distance from the data with those from the randomly generated samples 距离分布：将数据的成对点距离与随机生成的样本的距离进行比较 'Hopkins Statistic': A sparse sampling test for spatial randomness 霍普金斯统计：空间随机性的稀疏抽样测试 Summary Graph clustering What is graph clustering Complex network 在我们的现实生活中，许多复杂系统都可以建模成一种复杂网络进行分析，比如常见的电力网络、航空网络、交通网络、计算机网络以及社交网络等等。复杂网络不仅是一种数据的表现形式，它同样也是一种科学研究的手段。 Graph clustering Community Module Community detection_algorithmsCPM （Clique Percolation Method）# basic concept 'Clique': Complete graph 完全图(所有节点两两相连) 'k-clique': Complete graph with k vertice(顶点) k-派系 'Adjacent k-cliques': Two k-cliques are adjacent when they `share k-1 nodes` k-派系相邻：两个不同的k-派系共享k-1个节点，认为他们相邻 'k-clique community': Union of all k-cliques that can be `reached from each other` through a series of adjacent k-cliques. k-派系连通：一个k-派系可以通过若干个相邻的k-派系到达另一个k-派系，则称这两个k-派系彼此联通 Step1: 找到网络中大小为K的完全子图 Locate maximal cliques Step2: 将每个完全子图定义为一个节点，建立一个重叠矩阵Step3: 将重叠矩阵变成社团邻接矩阵(其中重叠矩阵中对角线小于k、非对角线小于k-1的元素全置为0,所有非0项置为1) 从图中可以看出包含了两个社区{1，2，3，4}和{4，5，6，7，8}，节点4属于两个社区的重叠节点 CPM（Cluster Percolation method）派系过滤算法 Spectral clustering 谱聚类（Spectral Clustering），就是先用Laplacian eigenmaps对数据降维（简单地说，就是先将数据转换成邻接矩阵或相似性矩阵，再转换成Laplacian矩阵，再对Laplacian矩阵进行特征分解，把最小的K个特征向量排列在一起），然后再使用k-means完成聚类。谱聚类是个很好的方法，效果通常比k-means好，计算复杂度还低，这都要归功于降维的作用。 谱聚类（spectral clustering）原理总结_刘建平、用scikit-learn学习谱聚类 Modularity based methods – G Nand Q1. Calculate the betweenness for all edges in the network 计算每一条边的边介数.2. Remove the edge with the highest betweenness. 删除边界数最大的边.3. Recalculate betweennesses for all edges affected by the removal. 重新计算网络中剩下的边的边阶数.4. Repeat from step 2 until no edges remain. 从步骤2重复，直到没有边剩余. 社区发现算法（二） MCL MCL (Markov Cluster Algorithm) is a graph clustering algorithm. Graph Clustering 和特征聚类不同，图聚类比较难以观察，整个算法以各点之间的距离作为突破口，可以这样形容：张三，是王五的好朋友，刚认识李四，对赵六很是反感。那么，对于该节点，我们无法直接得出他的特征，但能知道他的活动圈。利用图聚类，可以将同一社交范围的人聚合到一起。MCL就是属于图聚类的一种。 位于同一簇的点，其内部的联系应当紧密，而和外部的联系则比较少（惺惺相惜） 如果你从一个点出发，到达其中的一个邻近点，那么你在’簇内的可能性’远大于’离开当前簇，到达新簇’的可能性——这就是MCL的核心思想。 Random walk如果在一张图上进行多次的“Random Walks”，那么就有很大可能发现簇群，达到聚类的目的。而“Random Walks”的实现则是通过“Markov Chains”（马尔柯夫链）。 Markov chainsMarkov Chain——如果有由随机变量$X1,X2,X3$⋯组成的数列。$Xn$的值则是在时间$n$的状态，如果$X_{n+1}$对于过去状态的条件概率分布满足：$P(X_{n+1}=x|X_0,X_1,X_2,⋯,X_n)=P(X_{n+1}=x|X_n)$，则我们称其是一条Markov Chain. Markov Process——在给定当前知识或信息的情况下，过去（即当期以前的历史状态）对于预测将来（即当期以后的未来状态）是无关的。 下一步骤的概率仅依赖于当前概率 MCL Algorithm在MCL中， Expansion 和 Inflation 将不断的交替进行，Expansion 使得不同的区域之间的联系加强，而 Inflation 则不断的分化各点之间的联系(强者恒强，弱者恒弱)。经过多次迭代，将渐渐出现聚集现象，以此便达到了聚类的效果。 1. 输入：一个非全连通图，Expansion 时的参数e和 Inflation 的参数r2. 建立邻接矩阵3. 添加自环()4. 标准化概率矩阵5. Expansion操作，每次对矩阵进行e次幂方6. Inflation操作，每次对矩阵内元素进行r次幂方，再进行标准化7. 重复步骤5和6，直到达到稳定8. 将结果矩阵转化为聚簇 聚类算法——MCL Summary Todo 看相关参考书目《数据挖掘：概念与技术》《数据挖掘导论》课后例题 着重看”简单计算” Collaborative Filtering 协同过滤推荐算法总结 矩阵分解在协同过滤推荐算法中的应用 SimRank协同过滤推荐算法 EM算法原理总结 特征工程 特征工程之特征选择 特征工程之特征表达 特征工程之特征预处理","categories":[{"name":"XD","slug":"XD","permalink":"http://yoursite.com/categories/XD/"}],"tags":[]},{"title":"visdom_Tutorial","slug":"visdom-Tutorial","date":"2018-12-31T12:32:33.000Z","updated":"2019-01-01T03:27:39.605Z","comments":true,"path":"2018/12/31/visdom-Tutorial/","link":"","permalink":"http://yoursite.com/2018/12/31/visdom-Tutorial/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… Github linkage: facebookresearch/visdom SetupInstall# Install Python server and client，如果您使用python的话，装这一个就可以了。$ pip install visdom Startup$ visdom# or (The visdom command is equivalent to running python -m visdom.server)$ python -m visdom.server If the above does not work, try using an SSH tunnel to your server by adding the following line to your local ~/.ssh/config: LocalForward 127.0.0.1:8097 127.0.0.1:8097 Exampleimport visdomimport numpy as npvis = visdom.Visdom()vis.text('Hello, world!')vis.image(np.ones((3, 10, 10))) &gt;&gt;&gt; import visdom&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; vis = visdom.Visdom()&gt;&gt;&gt; vis.text('Hello, world!')'window_36ef496c02a28c'&gt;&gt;&gt; vis.image(np.ones((3, 10, 10)))'window_36ef496f76bad6' Demopython example/demo.pyth example/demo1.luath example/demo2.lua ErrorSSL: CERTIFICATE_VERIFY_FAILED➜ ~ visdomDownloading scripts. It might take a while.ERROR:root:Error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777) while downloading https://unpkg.com/jquery@3.1.1/dist/jquery.min.jsERROR:root:Error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777) while downloading https://unpkg.com/bootstrap@3.3.7/dist/js/bootstrap.min.jsERROR:root:Error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777) while downloading https://unpkg.com/react@16.2.0/umd/react.production.min.jsERROR:root:Error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777) while downloading https://unpkg.com/react-dom@16.2.0/umd/react-dom.production.min.jsERROR:root:Error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777) while downloading https://unpkg.com/react-modal@3.1.10/dist/react-modal.min.jsERROR:root:Error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777) while downloading https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVGERROR:root:Error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777) while downloading https://cdn.plot.ly/plotly-latest.min.jsERROR:root:Error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777) while downloading https://unpkg.com/sjcl@1.0.7/sjcl.js... Solution: Your best option is probably to try and do a manual install of the dependencies as described in CONTRIBUTING.md. Alternatively, you could try to bypass the SSL certification by adding the following at the top of py/server.py before start the server: import sslssl._create_default_https_context = ssl._create_unverified_context Note that this is very dangerous, so immediately remove this monkey-patch after the server started successfully the first time, and restart the server. (The server only downloads the dependencies the first time you start it.) I’m closing the issue because #242 fixed the issue in the error logging. Thanks for reporting! After adding the codes, please re-install the visdom &gt;$ pip3 uninstall visdom&gt;$ pip3 install -e .&gt;$ python3 setup.py install&gt; Details : ➜ 'visdom-master' pip3 uninstall visdomUninstalling visdom-0.1.8.5: Would remove: /Library/Frameworks/Python.framework/Versions/3.6/bin/visdom /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/visdom.egg-linkProceed (y/n)? y Successfully uninstalled visdom-0.1.8.5➜ 'visdom-master' pip uninstall visdomSkipping visdom as it is not installed.➜ 'visdom-master' pip3 install -e .Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simpleObtaining file:///Users/Captain/Desktop/visdom-masterRequirement already satisfied: numpy&gt;=1.8 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from visdom==0.1.8.5) (1.14.5)Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from visdom==0.1.8.5) (1.1.0)Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from visdom==0.1.8.5) (2.21.0)Requirement already satisfied: tornado in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from visdom==0.1.8.5) (5.1.1)Requirement already satisfied: pyzmq in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from visdom==0.1.8.5) (17.1.2)Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from visdom==0.1.8.5) (1.11.0)Requirement already satisfied: torchfile in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from visdom==0.1.8.5) (0.1.0)Requirement already satisfied: websocket-client in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from visdom==0.1.8.5) (0.54.0)Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from visdom==0.1.8.5) (5.3.0)Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests-&gt;visdom==0.1.8.5) (1.24.1)Requirement already satisfied: certifi&gt;=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests-&gt;visdom==0.1.8.5) (2018.11.29)Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests-&gt;visdom==0.1.8.5) (3.0.4)Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests-&gt;visdom==0.1.8.5) (2.8)Installing collected packages: visdom Running setup.py develop for visdomSuccessfully installed visdom➜ 'visdom-master' python3 setup.py installrunning installrunning bdist_eggrunning egg_infowriting py/visdom.egg-info/PKG-INFOwriting dependency_links to py/visdom.egg-info/dependency_links.txtwriting entry points to py/visdom.egg-info/entry_points.txtwriting requirements to py/visdom.egg-info/requires.txtwriting top-level names to py/visdom.egg-info/top_level.txtreading manifest file 'py/visdom.egg-info/SOURCES.txt'reading manifest template 'MANIFEST.in'warning: no previously-included files matching '__pycache__' found under directory '*'warning: no previously-included files matching '*.py[co]' found under directory '*'writing manifest file 'py/visdom.egg-info/SOURCES.txt'installing library code to build/bdist.macosx-10.6-intel/eggrunning install_librunning build_pycreating buildcreating build/libcreating build/lib/visdomcopying py/visdom/server.py -&gt; build/lib/visdomcopying py/visdom/__init__.py -&gt; build/lib/visdomcopying py/visdom/VERSION -&gt; build/lib/visdomcopying py/visdom/__init__.pyi -&gt; build/lib/visdomcopying py/visdom/py.typed -&gt; build/lib/visdomcreating build/lib/visdom/staticcopying py/visdom/static/index.html -&gt; build/lib/visdom/staticcopying py/visdom/static/login.html -&gt; build/lib/visdom/staticcopying py/visdom/static/version.built -&gt; build/lib/visdom/staticcreating build/lib/visdom/static/csscopying py/visdom/static/css/login.css -&gt; build/lib/visdom/static/csscopying py/visdom/static/css/style.css -&gt; build/lib/visdom/static/csscreating build/lib/visdom/static/jscopying py/visdom/static/js/main.js -&gt; build/lib/visdom/static/jscreating build/bdist.macosx-10.6-intelcreating build/bdist.macosx-10.6-intel/eggcreating build/bdist.macosx-10.6-intel/egg/visdomcopying build/lib/visdom/server.py -&gt; build/bdist.macosx-10.6-intel/egg/visdomcopying build/lib/visdom/__init__.pyi -&gt; build/bdist.macosx-10.6-intel/egg/visdomcopying build/lib/visdom/__init__.py -&gt; build/bdist.macosx-10.6-intel/egg/visdomcopying build/lib/visdom/VERSION -&gt; build/bdist.macosx-10.6-intel/egg/visdomcreating build/bdist.macosx-10.6-intel/egg/visdom/staticcopying build/lib/visdom/static/index.html -&gt; build/bdist.macosx-10.6-intel/egg/visdom/staticcreating build/bdist.macosx-10.6-intel/egg/visdom/static/csscopying build/lib/visdom/static/css/login.css -&gt; build/bdist.macosx-10.6-intel/egg/visdom/static/csscopying build/lib/visdom/static/css/style.css -&gt; build/bdist.macosx-10.6-intel/egg/visdom/static/csscreating build/bdist.macosx-10.6-intel/egg/visdom/static/jscopying build/lib/visdom/static/js/main.js -&gt; build/bdist.macosx-10.6-intel/egg/visdom/static/jscopying build/lib/visdom/static/login.html -&gt; build/bdist.macosx-10.6-intel/egg/visdom/staticcopying build/lib/visdom/static/version.built -&gt; build/bdist.macosx-10.6-intel/egg/visdom/staticcopying build/lib/visdom/py.typed -&gt; build/bdist.macosx-10.6-intel/egg/visdombyte-compiling build/bdist.macosx-10.6-intel/egg/visdom/server.py to server.cpython-36.pycbyte-compiling build/bdist.macosx-10.6-intel/egg/visdom/__init__.py to __init__.cpython-36.pyccreating build/bdist.macosx-10.6-intel/egg/EGG-INFOcopying py/visdom.egg-info/PKG-INFO -&gt; build/bdist.macosx-10.6-intel/egg/EGG-INFOcopying py/visdom.egg-info/SOURCES.txt -&gt; build/bdist.macosx-10.6-intel/egg/EGG-INFOcopying py/visdom.egg-info/dependency_links.txt -&gt; build/bdist.macosx-10.6-intel/egg/EGG-INFOcopying py/visdom.egg-info/entry_points.txt -&gt; build/bdist.macosx-10.6-intel/egg/EGG-INFOcopying py/visdom.egg-info/not-zip-safe -&gt; build/bdist.macosx-10.6-intel/egg/EGG-INFOcopying py/visdom.egg-info/requires.txt -&gt; build/bdist.macosx-10.6-intel/egg/EGG-INFOcopying py/visdom.egg-info/top_level.txt -&gt; build/bdist.macosx-10.6-intel/egg/EGG-INFOcreating distcreating 'dist/visdom-0.1.8.5-py3.6.egg' and adding 'build/bdist.macosx-10.6-intel/egg' to itremoving 'build/bdist.macosx-10.6-intel/egg' (and everything under it)Processing visdom-0.1.8.5-py3.6.eggcreating /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/visdom-0.1.8.5-py3.6.eggExtracting visdom-0.1.8.5-py3.6.egg to /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesRemoving visdom 0.1.8.5 from easy-install.pth fileAdding visdom 0.1.8.5 to easy-install.pth fileInstalling visdom script to /Library/Frameworks/Python.framework/Versions/3.6/binInstalled /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/visdom-0.1.8.5-py3.6.eggProcessing dependencies for visdom==0.1.8.5Searching for Pillow==5.3.0Best match: Pillow 5.3.0Adding Pillow 5.3.0 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for websocket-client==0.54.0Best match: websocket-client 0.54.0Adding websocket-client 0.54.0 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for torchfile==0.1.0Best match: torchfile 0.1.0Adding torchfile 0.1.0 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for six==1.11.0Best match: six 1.11.0Adding six 1.11.0 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for pyzmq==17.1.2Best match: pyzmq 17.1.2Adding pyzmq 17.1.2 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for tornado==5.1.1Best match: tornado 5.1.1Adding tornado 5.1.1 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for requests==2.21.0Best match: requests 2.21.0Adding requests 2.21.0 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for scipy==1.1.0Best match: scipy 1.1.0Adding scipy 1.1.0 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for numpy==1.14.5Best match: numpy 1.14.5Adding numpy 1.14.5 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for chardet==3.0.4Best match: chardet 3.0.4Adding chardet 3.0.4 to easy-install.pth fileInstalling chardetect script to /Library/Frameworks/Python.framework/Versions/3.6/binUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for certifi==2018.11.29Best match: certifi 2018.11.29Adding certifi 2018.11.29 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for urllib3==1.24.1Best match: urllib3 1.24.1Adding urllib3 1.24.1 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesSearching for idna==2.8Best match: idna 2.8Adding idna 2.8 to easy-install.pth fileUsing /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packagesFinished processing dependencies for visdom==0.1.8.5➜ 'visdom-master' No LuaRocks module found for mnist➜ 'visdom-master' th example/demo2.lua| running on CPU.../Users/Captain/torch/install/bin/luajit: /Users/Captain/torch/install/share/lua/5.1/trepl/init.lua:389: module 'mnist' not found:No LuaRocks module found for mnist no field package.preload['mnist'] no file '/Users/Captain/.luarocks/share/lua/5.1/mnist.lua' no file '/Users/Captain/.luarocks/share/lua/5.1/mnist/init.lua' no file '/Users/Captain/torch/install/share/lua/5.1/mnist.lua' no file '/Users/Captain/torch/install/share/lua/5.1/mnist/init.lua' no file './mnist.lua' no file '/Users/Captain/torch/install/share/luajit-2.1.0-beta1/mnist.lua' no file '/usr/local/share/lua/5.1/mnist.lua' no file '/usr/local/share/lua/5.1/mnist/init.lua' no file '/Users/Captain/.luarocks/lib/lua/5.1/mnist.so' no file '/Users/Captain/torch/install/lib/lua/5.1/mnist.so' no file '/Users/Captain/torch/install/lib/mnist.dylib' no file './mnist.so' no file '/usr/local/lib/lua/5.1/mnist.so' no file '/usr/local/lib/lua/5.1/loadall.so'stack traceback: [C]: in function 'error' /Users/Captain/torch/install/share/lua/5.1/trepl/init.lua:389: in function 'require' example/demo2.lua:29: in function 'getIterator' example/demo2.lua:53: in main chunk [C]: in function 'dofile' ...tain/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk [C]: at 0x0109dd2360 Solution: $ luarocks install mnist","categories":[],"tags":[]},{"title":"Classification with Pre-trained Model","slug":"Classification-with-Pretrained-Model","date":"2018-12-31T06:53:44.000Z","updated":"2018-12-31T07:19:37.924Z","comments":true,"path":"2018/12/31/Classification-with-Pretrained-Model/","link":"","permalink":"http://yoursite.com/2018/12/31/Classification-with-Pretrained-Model/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 参考： 使用pytorch预训练模型分类与特征提取 Pytorch：利用预训练好的VGG16网络提取图片特征 #encoding=utf-8 import osimport numpy as np import torchimport torch.nnimport torchvision.models as modelsfrom torch.autograd import Variable import torch.cudaimport torchvision.transforms as transforms from PIL import Image img_to_tensor = transforms.ToTensor() def make_model(): resmodel=models.resnet34(pretrained=True) resmodel.cuda()#将模型从CPU发送到GPU,如果没有GPU则删除该行 return resmodel #分类def inference(resmodel,imgpath): resmodel.eval()#必需，否则预测结果是错误的 img=Image.open(imgpath) img=img.resize((224,224)) tensor=img_to_tensor(img) tensor=tensor.resize_(1,3,224,224) tensor=tensor.cuda()#将数据发送到GPU，数据和模型在同一个设备上运行 result=resmodel(Variable(tensor)) result_npy=result.data.cpu().numpy()#将结果传到CPU，并转换为numpy格式 max_index=np.argmax(result_npy[0]) return max_index #特征提取def extract_feature(resmodel,imgpath): resmodel.fc=torch.nn.LeakyReLU(0.1) resmodel.eval() img=Image.open(imgpath) img=img.resize((224,224)) tensor=img_to_tensor(img) tensor=tensor.resize_(1,3,224,224) tensor=tensor.cuda() result=resmodel(Variable(tensor)) result_npy=result.data.cpu().numpy() return result_npy[0] if __name__==\"__main__\": model=make_model() imgpath='path_to_img/xxx.jpg' # imgpath='ILSVRC2012_val_00001101.JPEG' 此时图片与该代码文件放在同一目录下 print inference(model,imgpath) print extract_feature(model, imgpath) 注： 关于使用的img img.jpg是随便找的一张图，要求是3通道的（RGB，不能是黑白的，如果是黑白的要自己拓展成3通道）.建议先看下这里的文章：https://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c。另外，建议不要用pytorch提供的vgg模型来提取特征，效果不是很好，最好是通过别的框架的，例如keras提供的vgg模型来提取特征，保存为文件之后再调用pytorch #分类 的返回值max_index index为0～999的值，0对应n01440764，1对应n01443537，是一个映射表参考：https://blog.csdn.net/u010165147/article/details/72848497","categories":[],"tags":[]},{"title":"torchvision_pretrainedModel","slug":"torchvision-pretrainedModel","date":"2018-12-28T08:09:59.000Z","updated":"2018-12-31T07:16:32.881Z","comments":true,"path":"2018/12/28/torchvision-pretrainedModel/","link":"","permalink":"http://yoursite.com/2018/12/28/torchvision-pretrainedModel/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 关于“Save &amp;&amp; Load Model” 可参见说明. 直接加载预训练模型import torchvisionmodel = torchvision.models.densenet169(pretrained=True) Save model import torchtorch.save(model.state_dict(),'model.pth') View model params pretrained_dict = model.state_dict() # densenet169for k,v in pretrained_dict.items(): print(k,v.size()) #打印网络中的变量名 Out: features.conv0.weight torch.Size([64, 3, 7, 7])features.norm0.weight torch.Size([64])features.norm0.bias torch.Size([64])features.norm0.running_mean torch.Size([64])features.norm0.running_var torch.Size([64])features.norm0.num_batches_tracked torch.Size([])features.denseblock1.denselayer1.norm1.weight torch.Size([64])features.denseblock1.denselayer1.norm1.bias torch.Size([64])features.denseblock1.denselayer1.norm1.running_mean torch.Size([64])features.denseblock1.denselayer1.norm1.running_var torch.Size([64])features.denseblock1.denselayer1.norm1.num_batches_tracked torch.Size([])features.denseblock1.denselayer1.conv1.weight torch.Size([128, 64, 1, 1])features.denseblock1.denselayer1.norm2.weight torch.Size([128])features.denseblock1.denselayer1.norm2.bias torch.Size([128])features.denseblock1.denselayer1.norm2.running_mean torch.Size([128])features.denseblock1.denselayer1.norm2.running_var torch.Size([128])features.denseblock1.denselayer1.norm2.num_batches_tracked torch.Size([])features.denseblock1.denselayer1.conv2.weight torch.Size([32, 128, 3, 3])features.denseblock1.denselayer2.norm1.weight torch.Size([96])features.denseblock1.denselayer2.norm1.bias torch.Size([96])features.denseblock1.denselayer2.norm1.running_mean torch.Size([96])features.denseblock1.denselayer2.norm1.running_var torch.Size([96])features.denseblock1.denselayer2.norm1.num_batches_tracked torch.Size([])features.denseblock1.denselayer2.conv1.weight torch.Size([128, 96, 1, 1])features.denseblock1.denselayer2.norm2.weight torch.Size([128])features.denseblock1.denselayer2.norm2.bias torch.Size([128])features.denseblock1.denselayer2.norm2.running_mean torch.Size([128])features.denseblock1.denselayer2.norm2.running_var torch.Size([128])features.denseblock1.denselayer2.norm2.num_batches_tracked torch.Size([])features.denseblock1.denselayer2.conv2.weight torch.Size([32, 128, 3, 3])features.denseblock1.denselayer3.norm1.weight torch.Size([128])features.denseblock1.denselayer3.norm1.bias torch.Size([128])features.denseblock1.denselayer3.norm1.running_mean torch.Size([128])features.denseblock1.denselayer3.norm1.running_var torch.Size([128])features.denseblock1.denselayer3.norm1.num_batches_tracked torch.Size([])features.denseblock1.denselayer3.conv1.weight torch.Size([128, 128, 1, 1])features.denseblock1.denselayer3.norm2.weight torch.Size([128])features.denseblock1.denselayer3.norm2.bias torch.Size([128])features.denseblock1.denselayer3.norm2.running_mean torch.Size([128])features.denseblock1.denselayer3.norm2.running_var torch.Size([128])features.denseblock1.denselayer3.norm2.num_batches_tracked torch.Size([])features.denseblock1.denselayer3.conv2.weight torch.Size([32, 128, 3, 3])features.denseblock1.denselayer4.norm1.weight torch.Size([160])features.denseblock1.denselayer4.norm1.bias torch.Size([160])features.denseblock1.denselayer4.norm1.running_mean torch.Size([160])features.denseblock1.denselayer4.norm1.running_var torch.Size([160])features.denseblock1.denselayer4.norm1.num_batches_tracked torch.Size([])features.denseblock1.denselayer4.conv1.weight torch.Size([128, 160, 1, 1])features.denseblock1.denselayer4.norm2.weight torch.Size([128])features.denseblock1.denselayer4.norm2.bias torch.Size([128])features.denseblock1.denselayer4.norm2.running_mean torch.Size([128])features.denseblock1.denselayer4.norm2.running_var torch.Size([128])features.denseblock1.denselayer4.norm2.num_batches_tracked torch.Size([])features.denseblock1.denselayer4.conv2.weight torch.Size([32, 128, 3, 3])features.denseblock1.denselayer5.norm1.weight torch.Size([192])features.denseblock1.denselayer5.norm1.bias torch.Size([192])features.denseblock1.denselayer5.norm1.running_mean torch.Size([192])features.denseblock1.denselayer5.norm1.running_var torch.Size([192])features.denseblock1.denselayer5.norm1.num_batches_tracked torch.Size([])features.denseblock1.denselayer5.conv1.weight torch.Size([128, 192, 1, 1])features.denseblock1.denselayer5.norm2.weight torch.Size([128])features.denseblock1.denselayer5.norm2.bias torch.Size([128])features.denseblock1.denselayer5.norm2.running_mean torch.Size([128])features.denseblock1.denselayer5.norm2.running_var torch.Size([128])features.denseblock1.denselayer5.norm2.num_batches_tracked torch.Size([])features.denseblock1.denselayer5.conv2.weight torch.Size([32, 128, 3, 3])features.denseblock1.denselayer6.norm1.weight torch.Size([224])features.denseblock1.denselayer6.norm1.bias torch.Size([224])features.denseblock1.denselayer6.norm1.running_mean torch.Size([224])features.denseblock1.denselayer6.norm1.running_var torch.Size([224])features.denseblock1.denselayer6.norm1.num_batches_tracked torch.Size([])features.denseblock1.denselayer6.conv1.weight torch.Size([128, 224, 1, 1])features.denseblock1.denselayer6.norm2.weight torch.Size([128])features.denseblock1.denselayer6.norm2.bias torch.Size([128])features.denseblock1.denselayer6.norm2.running_mean torch.Size([128])features.denseblock1.denselayer6.norm2.running_var torch.Size([128])features.denseblock1.denselayer6.norm2.num_batches_tracked torch.Size([])features.denseblock1.denselayer6.conv2.weight torch.Size([32, 128, 3, 3])features.transition1.norm.weight torch.Size([256])features.transition1.norm.bias torch.Size([256])features.transition1.norm.running_mean torch.Size([256])features.transition1.norm.running_var torch.Size([256])features.transition1.norm.num_batches_tracked torch.Size([])features.transition1.conv.weight torch.Size([128, 256, 1, 1]).... features.denseblock4.denselayer32.conv2.weight torch.Size([32, 128, 3, 3])features.norm5.weight torch.Size([1664])features.norm5.bias torch.Size([1664])features.norm5.running_mean torch.Size([1664])features.norm5.running_var torch.Size([1664])features.norm5.num_batches_tracked torch.Size([])classifier.weight torch.Size([1000, 1664])classifier.bias torch.Size([1000]) View model structurefeature = torch.nn.Sequential(*list(model.children())[:]) # densenet169print(feature) Out: Sequential( (0): Sequential( (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu0): ReLU(inplace) (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (denseblock1): _DenseBlock( (denselayer1): _DenseLayer( (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) (denselayer2): _DenseLayer( (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) ... ... (denselayer32): _DenseLayer( (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU(inplace) (conv1): Conv2d(1632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu2): ReLU(inplace) (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) ) (norm5): BatchNorm2d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Linear(in_features=1664, out_features=1000, bias=True)) 基本上可以看出总体分为两个部分，这两个部分对应的名字可以用print(model._modules.keys())查询到： # 查询结果odict_keys(['features', 'classifier']) 之后可以直接用model.features直接只调用features部分，直接将分类部分抛弃掉。 加载部分预训练模型其实大多数时候我们需要根据我们的任务调节我们的模型，所以很难保证模型和公开的模型完全一样，但是预训练模型的参数确实有助于提高训练的准确率，为了结合二者的优点，就需要我们加载部分预训练模型。 #加载model，model是自己定义好的模型resnet50 = models.resnet50(pretrained=True) model =Net(...) #读取参数 pretrained_dict =resnet50.state_dict() model_dict = model.state_dict() #将pretrained_dict里不属于model_dict的键剔除掉,筛选符合自定义模型的键pretrained_dict = &#123;k: v for k, v in pretrained_dict.items() if k in model_dict&#125; # 更新现有的model_dict model_dict.update(pretrained_dict) # update # 加载我们真正需要的state_dict model.load_state_dict(model_dict) 因为需要剔除原模型中不匹配的键，也就是层的名字，所以我们的新模型改变了的层需要和原模型对应层的名字不一样，比如：resnet最后一层的名字是fc(PyTorch中)，那么我们修改过的resnet的最后一层就不能取这个名字，可以叫fc_ 简单预训练我们先从torchvision中调用基本模型，加载预训练模型，然后，重点来了，将其中的层直接替换为我们需要的层即可： resnet = torchvision.models.resnet152(pretrained=True)# 原本为1000类，改为10类resnet.fc = torch.nn.Linear(2048, 10) 其中使用了pretrained参数，会直接加载预训练模型，内部实现和前文提到的加载预训练的方法一样。因为是先加载的预训练参数，相当于模型中已经有参数了，所以替换掉最后一层即可。OK！ 使用预训练模型分类与特征提取# To Completed Reference： PyTorch预训练[知乎] 使用pytorch预训练模型分类与特征提取 Pytorch：利用预训练好的VGG16网络提取图片特征","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"}]},{"title":"Pytorch_Save&Load_Model","slug":"Pytorch-Save-Load-Model","date":"2018-12-28T07:33:53.000Z","updated":"2018-12-28T08:10:49.283Z","comments":true,"path":"2018/12/28/Pytorch-Save-Load-Model/","link":"","permalink":"http://yoursite.com/2018/12/28/Pytorch-Save-Load-Model/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 方法一(推荐)第一种方法也是官方推荐的方法，只保存和恢复模型中的参数。 保存torch.save(the_model.state_dict(), PATH) 恢复the_model = TheModelClass(*args, **kwargs)the_model.load_state_dict(torch.load(PATH)) 使用这种方法，我们需要自己导入模型的结构信息。 方法二使用这种方法，将会保存模型的参数和结构信息。 保存torch.save(the_model, PATH) 恢复the_model = torch.load(PATH) 一个相对完整的例子savetorch.save(&#123; 'epoch': epoch + 1, 'arch': args.arch, 'state_dict': model.state_dict(), 'best_prec1': best_prec1, &#125;, 'checkpoint.tar' ) loadif args.resume: if os.path.isfile(args.resume): print(\"=&gt; loading checkpoint '&#123;&#125;'\".format(args.resume)) checkpoint = torch.load(args.resume) args.start_epoch = checkpoint['epoch'] best_prec1 = checkpoint['best_prec1'] model.load_state_dict(checkpoint['state_dict']) print(\"=&gt; loaded checkpoint '&#123;&#125;' (epoch &#123;&#125;)\" .format(args.evaluate, checkpoint['epoch'])) 获取模型中某些层的参数model.state_dict()对于恢复的模型，如果我们想查看某些层的参数，可以： # 定义一个网络from collections import OrderedDictmodel = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ]))# 打印网络的结构print(model) Out: Sequential ( (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1)) (relu1): ReLU () (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1)) (relu2): ReLU ()) 如果我们想获取conv1的weight和bias： params=model.state_dict() for k,v in params.items(): print(k) #打印网络中的变量名print(params['conv1.weight']) #打印conv1的weight.size()print(params['conv1.bias']) #打印conv1的bias.size() Out: conv1.weightconv1.biasconv2.weightconv2.biastorch.Size([20, 1, 5, 5])torch.Size([20]) model.named_parameters()params = list(model.named_parameters())(name, param) = params[0]print(name)print(param.grad)print('-------------------------------------------------') (name1, param1) = params[1]print(name1)print(param1.grad)print('----------------------------------------------------')(name2, param2) = params[2]print(name2) print(param2.grad)print('----------------------------------------------------')(name3, param3) = params[3]print(name3) print(param3.grad) Out: conv1.weightNone-------------------------------------------------conv1.biasNone----------------------------------------------------conv2.weightNone----------------------------------------------------conv2.biasNone 参考： PyTorch 学习笔记（五）：存储和恢复模型并查看参数 pytorch查看网络中的参数","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"}]},{"title":"Spectral_Clustering","slug":"Spectral-Clustering","date":"2018-12-28T03:50:46.000Z","updated":"2018-12-28T03:54:40.120Z","comments":true,"path":"2018/12/28/Spectral-Clustering/","link":"","permalink":"http://yoursite.com/2018/12/28/Spectral-Clustering/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】……","categories":[],"tags":[]},{"title":"聚类综述","slug":"聚类综述","date":"2018-12-28T03:23:11.000Z","updated":"2018-12-28T03:37:04.165Z","comments":true,"path":"2018/12/28/聚类综述/","link":"","permalink":"http://yoursite.com/2018/12/28/聚类综述/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】……","categories":[],"tags":[]},{"title":"医学数据格式","slug":"Medical-Data-Format","date":"2018-12-27T09:08:13.000Z","updated":"2018-12-27T11:56:24.036Z","comments":true,"path":"2018/12/27/Medical-Data-Format/","link":"","permalink":"http://yoursite.com/2018/12/27/Medical-Data-Format/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… ToRead： Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery Unsupervised Domain Adaptation in Brain Lesion Segmentation with Adversarial Networks Paper： A Survey on Deep Learning in Medical Image Analysis 数据格式 含义（字段） 头部-&gt; 内容 DICOM nii 数据 Kind knowledge point + theory MRI T1\\T2\\Flair 区别， 各自针对的内容 CT 腹部CT\\胸部\\全身CT 扫描方式 PET 医学图像数据类型MRICTPET医学图像数据格式DICOM简介医学图像采用数字成像和通信（DICOM）作为存储和交换医学图像数据的标准解决方案。这个标准的第一个版本是在1985年发布的。发展到现在，该方案有了一些改变。该标准使用文件格式和通信协议。 文件格式 - 所有患者医疗图像都以DICOM文件格式保存。除了其他图像相关数据（例如用于拍摄图像的设备以及医疗处理的一些背景）之外，该格式具有关于患者的PHI（受保护的健康信息），例如姓名，性别，年龄。医学影像设备创建DICOM文件。医生使用DICOM查看器，可显示DICOM图像的计算机软件应用程序，读取和诊断图像中的发现。 通信协议 - DICOM通信协议用于搜索档案中的成像研究，并将成像研究恢复到工作站以显示。连接到医院网络的所有医疗成像应用程序都使用DICOM协议来交换信息，主要是DICOM图像，还包括患者和手术信息。还有更先进的网络命令，用于控制和跟踪治疗，调度程序，报告状态，分担医生和成像设备之间的工作量。关于DICOM标准细节，在这里推荐一个很好的博客http://dicomiseasy.blogspot.com 分析DICOM图像 了解DICOM 格式数据 CT扫描的测量单位是Hounsfield单位（HU），它是放射性强度的量度。 仔细校准CT扫描仪以准确测量。 关于这方面的详细了解可以在这里到。Introduction to CT physics 每个像素被分配一个数值（CT值），它是相应体素中所有衰减值的平均值。 将这个数字与水的衰减值进行比较，并在戈弗雷·豪斯菲尔德爵士（Sir Godfrey Hounsfield）之后以胡恩斯菲尔德单位（Hounsfield units，HU）的任意单位的比例显示。 获得DICOM 数据库 kaggle competitions and Datasets Dicom Library Osirix Datasets Visible Human Datasets The Zubal Phantom 杜克大学他们有公开的数据集 niiMatlab/python 医学工具包DICOMpython package pydiocm 浏览image数据工具参考医学图像处理与深度学习[知乎]","categories":[],"tags":[]},{"title":"深度学习之PyTorch实战计算机视觉","slug":"深度学习之PyTorch实战计算机视觉","date":"2018-12-17T03:46:57.000Z","updated":"2018-12-17T05:50:00.292Z","comments":true,"path":"2018/12/17/深度学习之PyTorch实战计算机视觉/","link":"","permalink":"http://yoursite.com/2018/12/17/深度学习之PyTorch实战计算机视觉/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 第6章 PyTorch基础 Why PyTorch pytorch可以说是torch的python版，然后增加了很多新的特性。pytorch在编写模型的时候最大的特点就是利用autograd技术来实现自动求导，也就是不需要我们再去麻烦地写一些反向的计算函数，这点上继承了torch。浅谈Pytorch与Torch的关系 pytorch document Link：https://pytorch.org/docs/stable/index.html 第7章 迁移学习如果我们用这么多资源训练的模型能够解决同一类问题，那么模型的性价比会提高很多，这就促使使用迁移模型解决同 一类问题的方法出现 。因为该方法的出现，我们通过对 一个训练好的模型进行细微调整，就能将其应用到相似的问题中，最后还能取得很好的效果 ; 另外，对于原始数据较少的问题，我们也能够通过采用迁移模型进行有效解决 ，所以，如果能够选取合适的迁移学习方法，则会对解决我们所面临的问题有很大的帮助 。 第8章 图像风格迁移实战 第9章 多模型融合 “集百家之所长” 结果融合法 通用理论：各个模型的输出结果的差异性越高 ， 多模型融合的效果就会越好 。 结果多数表决 结果直接平均 融合各模型的平均预测水平，弥补个别模型的明显劣势，预防过拟合和欠拟合的发生 评价：虽然在总体的准确率上有所提升，但在单个数据的预测能力上并不优秀 结果加权平均 评价：调节各个模型的权重参数对最后的融合模型的结果影响较大。 所以在使用权重平均的过程中，我们需要不断尝试使用不同的权重值组合，以达到多模型融合的最优解决方案 。 第10章 循环神经网络在循环神经网络中循环单元可以随意控制输入数据及输出数据的数量，具有非常大的灵活性。 Image result for rnn 循环神经网络的网络简化模型，通过不断地对自身的网络结构进行复制来构造不同的循环神经网络模型。 Image result for rnn 将上图展开， Image result for rnn 虽然循环神经网络已经能够很好地对输入的序列数据进行处理 ，是不能进行长期记忆，其带来的影响就是如果近期输入的数据发生了变化，则会对当前的输出结果产生重大影响 。 为了避免这种情况的出现，研究者开发了 LSTM ( Long Short Term Memory)类型 的循环神经网络模型 。 Image result for rnn lstm 第11章 自动编码器 Image result for autoEncoder 自动编码器模型的最大用途就是实现输入数据的清洗，比如去除输入数据中的噪声数据、对输入数据的某些关键特征进行增强和放大，等等 。 举一个比较简单的例子，假设我们现在有一些被打上了马赛克的图片需要进行除码处理，这时就可以通过自动编码器模型来解决这个 问题 。 其实可以将这个除码的过程看作对数据进行除噪的过程。","categories":[],"tags":[]},{"title":"PyTorch_Docs","slug":"PyTorch-Package","date":"2018-12-15T12:22:24.000Z","updated":"2018-12-24T05:01:31.838Z","comments":true,"path":"2018/12/15/PyTorch-Package/","link":"","permalink":"http://yoursite.com/2018/12/15/PyTorch-Package/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… torchTensortorch.Tensor.viewview(*shape) → Tensor Returns a new tensor with the same data as the self tensor but of a different shape. Example: &gt;&gt;&gt; x = torch.randn(4, 4)&gt;&gt;&gt; x.size()torch.Size([4, 4])&gt;&gt;&gt; y = x.view(16) &gt;&gt;&gt; y.size()torch.Size([16])&gt;&gt;&gt; z = x.view(-1, 8) # the size -1 is inferred(推导) from other dimensions&gt;&gt;&gt; z.size()torch.Size([2, 8]) # 2 = (4*4)/8&gt;&gt;&gt; a=x.view(2,-1) &gt;&gt;&gt; a.size()torch.Size([2, 8]) # 8 = (4*4)/2&gt;&gt;&gt; b=x.view(4,-1)&gt;&gt;&gt; b.size()torch.Size([4, 4]) # 4 = (4*4)/4 Parameters: shape (torch.Size or int…) – the desired size torch.catDocs Example: &gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 0) # dim:0 按列级联(排成一列)tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 1) # dim:1 按行级联(排成一行)tensor([[ 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497]]) torch.nnContainerstorch.nn.Moduleforward(*input)forward(*input) Defines the computation performed at every call. torch.nn.Sequentialtorch.nn.Sequential 类是 torch.nn 中的一种序列容器，通过在容器中嵌套各种实现神经网络中具体功能相关的类，来完成对神 经网络模型 的搭建，最主要的是，参数会按照我们定义好的序列自动传递下去。我们可以将嵌套在容器中的各个部分看作各种不同的模块，这些模块可以自由组合。模块的加入 一般有两种方式 ， 一种是直接嵌套，另 一 种是以 orderdict 有序字典的方式进行传入，这两种方式的唯一区别是，使用后者搭建的模型的每个模块都有我们自定义的名字 ， 而前者默认使用从零开始的数字序列作为每个模块的名字。 Convolution Layers PyTorch Lecture 10: Basic CNN torch.nn.Conv1dDocs Examples: &gt;&gt;&gt; m = nn.Conv1d(16, , 3, stride=2) # (in_channels, out_channels, kernel_size, stride)&gt;&gt;&gt; input = torch.randn(20, 16, 50)&gt;&gt;&gt; output = m(input) Output： &gt;&gt;&gt; input.size()torch.Size([20, 16, 50])&gt;&gt;&gt; output.size()torch.Size([20, 33, 24])&gt;&gt;&gt; m.weight.size()torch.Size([33, 16, 3])&gt;&gt;&gt; m.bias.size()torch.Size([33]) $ (N, C_{\\text{in}}, L)$ — $\\text{input.size :} (20, 16, 50)$ $ (N, C_{\\text{out}}, L_{\\text{out}})$ — $\\text{output.size :} (20, 33, 24)$ $C_{\\text{out}} \\text{ : out_channels}$ $L_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\\times (\\text{kernel_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor \\Rightarrow 24 = \\left\\lfloor\\frac{50 + 2 \\times 0 - 1\\times (3 - 1) - 1}{2} + 1\\right\\rfloor$ weight (Tensor) – the learnable weights of the module of shape $\\text{(out_channels, in_channels, kernel_size)}$. bias (Tensor) –the learnable bias of the module of shape $\\text{(out_channels)}$. torch.nn.Conv2dDocs 文档说明 Examples: &gt;&gt;&gt; # With square kernels and equal stride&gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2) # (in_channels, out_channels, kernel_size, stride)&gt;&gt;&gt; # non-square kernels and unequal stride and with padding&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))&gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))&gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)&gt;&gt;&gt; output = m(input) Output: &gt;&gt;&gt; input.size()torch.Size([20, 16, 50, 100])&gt;&gt;&gt; output.size()torch.Size([20, 33, 26, 100])&gt;&gt;&gt; m.weight.size()torch.Size([33, 16, 3, 5])&gt;&gt;&gt; m.bias.size()torch.Size([33]) $(N, C_{in}, H_{in}, W_{in})$ — $\\text{input.size :} (20, 16, 50, 100)$ $(N, C_{out}, H_{out}, W_{out})$ — $\\text{output.size :} (20, 33, 26, 100)$ $C_{\\text{out}} \\text{ : out_channels}$ $H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\\times (\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor \\Rightarrow 26 = \\left\\lfloor\\frac{50 + 2 \\times 4- 3\\times (3 - 1) - 1}{2} + 1\\right\\rfloor$ $W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\\times (\\text{kernel_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor \\Rightarrow 100 = \\left\\lfloor\\frac{100 + 2 \\times 2 - 1\\times (5- 1) - 1}{1} + 1\\right\\rfloor$ weight (Tensor) – the learnable weights of the module of shape $\\text{(out_channels, in_channels, kernel_size[0], kernel_size[1])}$. bias (Tensor) – the learnable bias of the module of shape $\\text{(out_channels)}$. torch.nn.Conv3dDocs Examples: &gt;&gt;&gt; # With square kernels and equal stride&gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2)&gt;&gt;&gt; # non-square kernels and unequal stride and with padding&gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))&gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)&gt;&gt;&gt; output = m(input) Output: &gt;&gt;&gt; input.size()torch.Size([20, 16, 10, 50, 100])&gt;&gt;&gt; output.size()torch.Size([20, 33, 8, 50, 99])&gt;&gt;&gt; m.weight.size()torch.Size([33, 16, 3, 5, 2])&gt;&gt;&gt; m.bias.size()torch.Size([33]) $(N, C_{in}, D_{in}, H_{in}, W_{in})$ — $\\text{input.size :}20, 16, 10, 50, 100)$ $(N, C_{out}, D_{out}, H_{out}, W_{out})$ — $\\text{output.size :} (20, 33, 8, 50, 99)$ $H_{out} = \\left\\lfloor\\frac{H{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\\times (\\text{kernel_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor \\\\Rightarrow 50 = \\left\\lfloor\\frac{50 + 2 \\times 2 - 1\\times (5- 1) - 1}{1} + 1\\right\\rfloor$ $D_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\\times (\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor \\\\Rightarrow 8 = \\left\\lfloor\\frac{10 + 2 \\times 4 - 1\\times (3 - 1) - 1}{2} + 1\\right\\rfloor$ $W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]\\times (\\text{kernel_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor \\\\Rightarrow 99 = \\left\\lfloor\\frac{100 + 2 \\times 0 - 1\\times (2 - 1) - 1}{1} + 1\\right\\rfloor$ weight (Tensor) – the learnable weights of the module of shape$\\text{ (out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2])}$ bias (Tensor) – the learnable bias of the module of shape $\\text{(out_channels)}$. Pooling Layerstorch.nn.MaxPool2dDocs 文档说明 Applies a 2D max pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size$(N, C, H, W)​$, output $(N, C, H_{out}, W_{out})​$ and kernel_size $(kH, kW)​$can be precisely described as: $\\begin{aligned}out(N_i, C_j, h, w) ={} &amp; \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\text{input}(N_i, C_j, \\text{stride[0]} \\times h + m,\\text{stride[1]} \\times w + n)\\end{aligned}$ Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2&gt;&gt;&gt; m = nn.MaxPool2d(3, stride=2)&gt;&gt;&gt; # pool of non-square window&gt;&gt;&gt; m = nn.MaxPool2d((3, 2), stride=(2, 1))&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)&gt;&gt;&gt; output = m(input) Output: &gt;&gt;&gt; input.size()torch.Size([20, 16, 50, 32])&gt;&gt;&gt; output.size()torch.Size([20, 16, 24, 31]) $(N, C, H_{in}, W_{in})$ — $\\text{input.size:}(20, 16, 50, 32) $ $(N, C, H_{out}, W_{out})$ — $\\text{output.size: } (20, 16, 24, 31)$ $H_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}\\times (\\text{kernel_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor \\\\Rightarrow 24 = \\left\\lfloor\\frac{50 + 2 \\times 0 - 1\\times (3 - 1) - 1}{2} + 1\\right\\rfloor $ $W_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}\\times (\\text{kernel_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor \\ \\Rightarrow 31 = \\left\\lfloor\\frac{32 + 2 \\times 0 - 1\\times (2 - 1) - 1}{1} + 1\\right\\rfloor$ torch.nn.AvgPool2dDocs Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2&gt;&gt;&gt; m = nn.AvgPool2d(3, stride=2)&gt;&gt;&gt; # pool of non-square window&gt;&gt;&gt; m = nn.AvgPool2d((3, 2), stride=(2, 1))&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)&gt;&gt;&gt; output = m(input) Output: &gt;&gt;&gt; input.size()torch.Size([20, 16, 50, 32])&gt;&gt;&gt; output.size()torch.Size([20, 16, 24, 31]) $(N, C, H_{in}, W_{in})$ — $\\text{input.size: } (20, 16, 50, 32)$ $(N, C, H_{out}, W_{out})$ — $\\text{output.size: } (20, 16, 24, 31)$ $H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[0] -\\text{kernel_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor \\Rightarrow 24 = \\left\\lfloor\\frac{50 + 2 \\times 0 -3}{2} + 1\\right\\rfloor$ $W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[1] -\\text{kernel_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor \\Rightarrow 31 = \\left\\lfloor\\frac{32 + 2 \\times 0 -2}{1} + 1\\right\\rfloor$ torch.nn.Dropout torch.nn.Dropout 类用于防止卷积神经网络在训练的过程中发生过拟合 ， 其工作原理简单来说就是在模型训练的过程中，以一定的随机概率将卷积神经网络模型的部分参数归零(“丢弃”)， 以达到减少相邻两层神经连接的目的。 Non-Linear ActivationsReLUDocs Example: &gt;&gt;&gt; m = nn.ReLU()&gt;&gt;&gt; input = torch.randn(2)&gt;&gt;&gt; output = m(input) Output: &gt;&gt;&gt; inputtensor([0.7526, 0.9017])&gt;&gt;&gt; outputtensor([0.7526, 0.9017])&gt;&gt;&gt; inputtensor([-0.5070, 0.4540])&gt;&gt;&gt; outputtensor([0.0000, 0.4540]) Input:$(N, )$ where means, any number of additional dimensions Output: $(N, *)$, same shape as the input Lineartorch.nn.LinearDocs Applies a linear transformation to the incoming data: $y = xA^T + b$ Examples: &gt;&gt;&gt; m = nn.Linear(20, 30)&gt;&gt;&gt; input = torch.randn(128, 20) # (N,∗,in_features)&gt;&gt;&gt; output = m(input)&gt;&gt;&gt; print(output.size())torch.Size([128, 30]) # (N,∗,out_features) Output: &gt;&gt;&gt; inputtensor([[-0.8833, 0.1130, 0.0446, ..., -0.2786, 2.0762, 0.6139], [-0.9236, 3.1720, -0.9857, ..., 0.1017, -0.4042, -0.1072], [ 0.0153, 0.8800, -0.6031, ..., -0.2836, 0.7584, -2.3324], ..., [ 0.9628, -0.3177, -0.7577, ..., 0.2819, 0.9684, -1.8474], [ 0.3380, 1.0946, -1.3399, ..., -0.0043, -0.9811, 0.3067], [ 0.6834, 0.5804, -0.8192, ..., 1.2167, 1.3583, -1.5123]])&gt;&gt;&gt; outputtensor([[ 1.0747, 1.2864, 0.6512, ..., 0.1053, -0.0487, -0.1705], [ 0.2707, -0.7018, -0.4553, ..., -0.2100, -0.3003, 1.0038], [ 0.1943, -0.3070, -0.3651, ..., 1.1940, -0.2991, 0.0455], ..., [ 0.4118, -0.3984, 0.2089, ..., 0.7984, -0.6598, 0.2150], [-0.1639, -1.5081, -0.4011, ..., 0.9673, 0.3524, 0.0993], [ 0.5961, -0.4150, -0.1207, ..., 0.3189, -0.0829, 0.5195]], grad_fn=&lt;AddmmBackward&gt;)&gt;&gt;&gt; m.weight.size()torch.Size([30, 20]) # (out_features,in_features)&gt;&gt;&gt; m.bias.size()torch.Size([30]) # (out_features) NormalizationLayersBatch Normalization 批标准化 (PyTorch tutorial 神经网络 教学) 什么是批标准化 (Batch Normalization)_Morvan让数值保持在激活函数的有效区间，可避免梯度消失。Batch Normalization (BN) 就被添加在每一个全连接和激励函数之间. 深度学习中 Batch Normalization为什么效果好？ BatchNorm2dDocs Examples: &gt;&gt;&gt; # With Learnable Parameters&gt;&gt;&gt; m = nn.BatchNorm2d(100)&gt;&gt;&gt; # Without Learnable Parameters&gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False)&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)&gt;&gt;&gt; output = m(input) Output: &gt;&gt;&gt; output.size()torch.Size([20, 100, 35, 45]) # (same shape as input） Dropout Layerstorch.nn.DropoutDocs Examples: &gt;&gt;&gt; m = nn.Dropout(p=0.2)&gt;&gt;&gt; input = torch.randn(20, 16)&gt;&gt;&gt; output = m(input) Output: &gt;&gt;&gt; output = m(input)&gt;&gt;&gt; input.size()torch.Size([20, 16])&gt;&gt;&gt; output.size()torch.Size([20, 16]) # Output is of the same shape as input &gt;&gt;&gt; m = nn.Dropout(p=0.2)&gt;&gt;&gt; input = torch.randn(5, 3)&gt;&gt;&gt; output = m(input)&gt;&gt;&gt; inputtensor([[ 1.7224, 1.3201, -0.8480], [ 1.8960, -0.1245, 0.6991], [ 1.1756, 0.2378, 1.4059], [ 0.2427, 0.2278, -0.7612], [-0.8882, 0.2088, 1.5004]])&gt;&gt;&gt; outputtensor([[ 2.1530, 1.6501, -1.0601], [ 2.3700, -0.1556, 0.0000], [ 1.4694, 0.2972, 0.0000], [ 0.3034, 0.0000, -0.9515], [-0.0000, 0.2610, 1.8755]]) torch.nn.inittorch.nn.init.kaiming_uniform_Docs torch.nn.functionalPooling functionsadaptive_avg_pool2dDocs Examples: &gt;&gt;&gt; # target output size of 5x7&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((5,7)) # output_size – (H, W) &gt;&gt;&gt; input = torch.randn(1, 64, 8, 9)&gt;&gt;&gt; output = m(input)&gt;&gt;&gt; output.size()torch.Size([1, 64, 5, 7])&gt;&gt;&gt; # target output size of 7x7 (square)&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d(7) # a single H for a square image H x H&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)&gt;&gt;&gt; output = m(input)&gt;&gt;&gt; output.size()torch.Size([1, 64, 7, 7])&gt;&gt;&gt; # target output size of 10x7&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((None, 7)) # None, which means the size will be the same as that of the input.&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)&gt;&gt;&gt; output = m(input)&gt;&gt;&gt; output.size()torch.Size([1, 64, 10, 7]) torchvision.transforms torchvision.transforms.Resize:用于对载入的图片数据按我们需求的大小进行缩放。 传递给这个类的参数可以是一个整型数据，也可以是一个类似于$ (h,w)$的序列， 其中 ， h 代表高度， w 代表宽度，但是如果使用的是 一 个整型数据，那么表示缩放的宽度和高度都是这个整型数据的值 。 torchvision.transforms.Scale: 用于对载入的图片数据按我 们 需求的大小进 行缩 放，用法和 torchvision.transforms.Resize类似。 torchvision.transforms.CenterCrop:用 于对载入的图片以图片中心为参考点 ， 按我们需要的大小进行裁剪 。传递给这个类 的参数可以是一个整型数据 ，也可以 是一个类似 于( h,w)的序列 。 torchvision.transforms.RandomCrop: 用于对载入的图片按我们需要的大小进行随机裁剪。传递给这个类的参数可以是一个整型数据，也可以是一个类似于$ (h,w)$的序列。 torchvision.transforms.RandomHorizontaIFlip: 用于对载入的图片按随机概率进行水平翻转。我们可以通过传递给这个类的参数自定义随机概率 ，如果没有定义 ，则使用 默认的概率值 0.5。 torchvision.transforms.RandomVerticalFlip: 用于对载入的图片按随机概率进行垂直翻转。 我们可以通过传递给这个类的参数自定义随机概率 ，如果没有定义 ，则 使用默 认的概率值 0.5。 torchvision.transforms.ToTensor: 用于对载入的图片数据进行类型转换 ， 将之前构成 PIL 图片的数据转换成 Tensor数据类型的变量 ，让 PyTorch 能够对其进行计算和处理。 torchvision.transforms.ToPILlmage: 用于将 Tensor变量的数据转换成 PIL 图片数据， 主要是为了方便图片内容的显示。 torch.utils.model_zootorch.utils.model_zoo.load_url Example: &gt;&gt;&gt; state_dict = torch.utils.model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')","categories":[],"tags":[]},{"title":"GraduationDesign","slug":"GraduationDesign","date":"2018-12-13T09:51:23.000Z","updated":"2018-12-21T07:04:46.434Z","comments":true,"path":"2018/12/13/GraduationDesign/","link":"","permalink":"http://yoursite.com/2018/12/13/GraduationDesign/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 毕设要求 课题名称 基于densenet的图像分类算法研究与实现 课题简介 随着深度学习网络越来越深，架构越来越复杂，解决反向传播时梯度消失的问题也越来越严重。densenetdensenet的每一层都和损失函数的梯度以及原始输入直接相连，用更少的参数减轻了反向梯度消失，可以令网络规模更大。该课题研究基于densenet的图像分类算法，提高分类效率。 代码调研 DenseNet的使用 Github link: https://github.com/liuzhuang13/DenseNetCaffe Star 243 DenseNet算法详解 代码的github链接：https://github.com/liuzhuang13/DenseNet Star 3090MXNet版本代码（有ImageNet预训练模型）: https://github.com/miraclewkf/DenseNet Star 41 Densely Connected Convolutional Networks》论文笔记 https://github.com/liuzhuang13/DenseNet Star 3090 pytorch官方已经提供了Resnet、densenet的实现代码及权重文件 CVPR 2017最佳论文作者解读 https://github.com/gaohuang/MSDNet Star 351 代码参见： Torch implementation: https://github.com/liuzhuang13/DenseNet/tree/master/models Star 3090 PyTorch implementation: https://github.com/gpleiss/efficient_densenet_pytorch Star 771 MxNet implementation: https://github.com/taineleau/efficient_densenet_mxnet Caffe implementation: https://github.com/Tongcheng/DN_CaffeScript Star 123 代码测试 Torch implementation: https://github.com/liuzhuang13/DenseNet/tree/master/models Star 3090 PyTorch https://blog.csdn.net/u014380165/article/details/79119664 https://pytorch.org/docs/master/torchvision/models.html DenseNet的一个PyTorch实现 Pytorch 使用预训练模型 Q: pytorch.model.densenet 使用 Question _DenseLayer 命名方式_ $\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_t$ model version num_init_features growth_rate block_config densenet121 64 32 (6, 12, 24, 16) densenet169 64 32 (6, 12, 32, 32) densenet201 64 32 (6, 12, 48, 32) densenet161 96 48 (6, 12, 36, 24)","categories":[],"tags":[]},{"title":"白话深度学习与TensorFlow_笔记","slug":"白话深度学习与TensorFlow-笔记","date":"2018-12-12T09:32:42.000Z","updated":"2018-12-17T06:24:01.592Z","comments":true,"path":"2018/12/12/白话深度学习与TensorFlow-笔记/","link":"","permalink":"http://yoursite.com/2018/12/12/白话深度学习与TensorFlow-笔记/","excerpt":"【阅读时间】XXX min XXX words【阅读说明】这本书适合零基础的初学者","text":"【阅读时间】XXX min XXX words【阅读说明】这本书适合零基础的初学者 第5章 手写板功能 手写识别 Github Link：https://github.com/tensorflow/tensorflow 文件目录：https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist 第6章 卷积神经网络 卷积 卷积核 池化层 Pooling Layer 池化层的作用实际上对 Feature Map 所做的数据处理又进行了一次所谓的池化处理。常见的池化层处理有两种方式:一种叫 Max Pooling，一种叫 Mean Pooling (也叫 Average Pooling) 第7章 综合问题 ReLU 函数 Activation Function 优点： 其一 ，在第一象限中不会有明显的梯度消失问题，因为导数恒为 1，而 w 在初始化的 时候也是有大有小，连乘的时候不会轻易出现很小或者很大的数值，这就是一个非常好的 特性了 。 其二，由于导数为 1，所以求解它的导数要比求解 Sigmoid 函数的导数时间代价要小 一些. 因而现在的工程人员在近几年的网络中都喜欢大量使用 ReLU 函数 。 在笔者的工程经验中也是至少 80% 以上的工程都是倾向于优先使用 ReLU 函数作为激励函数的 。 归一化 normalization 统一“单位”，让模型对各参数数值变化的敏感程度保持一致，“没有偏见” 正则化 regulization 防止过拟合，提高泛化能力 超参数 hyper parameter 不能通过算法学习到的参数，例如 K-Means 算法中的簇数 N，还有就是像在深度学习中涉及的学习率 $\\eta $ Dropout 在一轮训练阶段丢弃一部分网络节点 。 # tensorflowkeep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob) 第8章 循环神经网络 隐马尔可夫模型 马尔可夫链的核心是说，在给定当前知识或信息的情况下，观察对象过去的历史状态对于将来的预测来说预测是无关的 。 也可以说，在观察一个系统变化的时候，它下一个状态(第n+l个状态)如何的概率只需要观察和统计当前状态(第n个状态)即可以正确得出。 RNN 和 BPTT 算法 LSTM算法 聊天机器人 Github链接：https://github.com/Conchylicultor/DeepQA issue: OK Error about BasicLSTMCell After using Nicholas C.’s pre-trained model ,still not work. To resolve WARNING: Restoring previous model I think the pretrained model is not work N C 第9章 深度残差网络Github Link： https://github.com/ry/tensorflow-resnet https://github.com/raghakot/keras-resnet https://github.com/KaimingHe/deep-residual-networks 第10章 受限玻尔兹曼机https://github.com/meownoid/tensorfow-rbm 解码器(autodecoder) https://github.com/Cospel/rbm-ae-tf 降维工具 第11章 强化学习 OpenAI Gym Github Link：https://github.com/openai/gym Playing Atari with Deep Reinforcement Learning Github Link：https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner 第12章 对抗学习 DCGAN Github Link：https://github.com/carpedm20/DCGAN-tensorflow 第13章 有趣的深度学习应用 人脸识别 GIthub link：https://github.com/davidsandberg/facenet LFW datasets: http://vis-www.cs.umass.edu/lfw/ 作诗姬 Github link：https://github.com/XingxingZhang/rnnpg VGG Github link：https://github.com/anishathalye/neural-style python neural_style.py --content &lt;content file&gt; --styles &lt;style file&gt; --output &lt;output file&gt;Example:python neural_style.py –content examples/cat.jpg –styles examples/2-style1.jpg –output y-output.jpg (tensorflow) ➜ neural-style-master python neural_style.py --content examples/cat.jpg --styles examples/2-style1.jpg --output y-output.jpg --network imagenet-vgg-verydeep-19.matOptimization started...Iteration 1/1000Iteration 2/1000 (16 sec elapsed, 4 hr 26 min remaining)Iteration 3/1000 (30 sec elapsed, 4 hr 10 min remaining)Iteration 4/1000 (43 sec elapsed, 4 hr 0 min remaining)Iteration 5/1000 (55 sec elapsed, 3 hr 50 min remaining)...Iteration 999/1000 (3 hr 22 min elapsed, 24 sec remaining)Iteration 1000/1000 (3 hr 23 min elapsed, 12 sec remaining)content loss: 796114 style loss: 234885 tv loss: 44175.5 total loss: 1.07517e+06 Other Concepts VC维 Vapnik-Chervonenkis Dimension H的VC维表示为VC(H) ，指能够被H分散的最大集合的大小。若H能分散任意大小的集合，那么VC(H)为无穷大。","categories":[],"tags":[]},{"title":"软件过程与项目管理","slug":"软件过程与项目管理","date":"2018-12-10T11:02:40.000Z","updated":"2018-12-10T12:23:08.557Z","comments":true,"path":"2018/12/10/软件过程与项目管理/","link":"","permalink":"http://yoursite.com/2018/12/10/软件过程与项目管理/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… CoursesIntroduction to Project Management The Project Management and Information Technology ContextThe Project Management Process GroupsInitiating processes - 启动过程Planning processes - 计划过程Executing processes - 执行过程组Monitoring and controlling processes- 监控过程Closing processes -闭合过程 Project Integration Management Project Scope Management Project Schedule Management Project Cost Management Project Quality Management Project Resource Management Project Communications Management Project Risk Managementprocesses Planning risk management: deciding how to approach and plan the risk management activities for the project 决定如何处理和规划项目的风险管理活动 The project team should review project documents as well as corporate risk management policies, risk categories,lessons-learned reports from past projects, andtemplates for creating a risk management plan Identifying risks: determining which risks are likely to affect a project and documenting the characteristics of each 确定哪些风险可能影响项目并记录每个风险的特征 tools and techniques Brainstorming The Delphi Technique: 专家小组；匿名输入；书面答复 Interviewing SWOT analysis: Strengths,weaknesses, opportunities, and threats Output Risk Register Performing qualitative risk analysis: prioritizing risks based on their probability and impact of occurrence 根据风险的概率和发生的影响确定风险的优先级 tools and techniques Probability/impact matrixes The Top Ten Risk Item Tracking Expert judgment Performing quantitative risk analysis: numerically estimating the effects of risks on project objectives 数字估算风险对项目目标的影响 Main techniques Decision tree analysis 决策树分析 EMV(Expected Monetary Value ) Simulation 模拟 Monte Carlo analysis Sensitivity analysis 敏感性分析 Planning risk responses: taking steps to enhance opportunities and reduce threats to meeting project objectives 采取措施增加机会并减少对实现项目目标的威胁 Negative Positive Implementing risk responses: implementing the risk response plans 实施风险应对计划 Monitoring risk: monitoring identified and residual risks, identifying new risks, carrying out risk response plans, and evaluating the effectiveness of risk strategies throughout the life of the project 监控已识别和剩余风险，识别新风险，执行风险应对计划，并在项目的整个生命周期内评估风险策略的有效性 Main output of this process is a risk management plan PROJECT PROCUREMENT MANAGEMENT Types of outsourcing- Local outsourcing- Offshore outsourcing- Nearshore outsourcing Types of Contracts- 'Fixed price' or lump sum contracts: involve a fixed total price for a well-defined product or service #卖家不利 - `Point of Total Assumption` (PTA): cost at which the contractor assumes total responsibility for each additional dollar of contract cost - 'Fixed Price' / Lump Sum / Firm Fixed Price : Risk is on the seller - Fixed Price with Economic Price Adjustment Contracts ('FP-EPA'): 旨在保护买方和卖方免受其无法控制的外部条件的影响。 如`通货膨胀变化`，或成本增加。 - Fixed Price Incentive Fee ('FPIF') 固定价格激励: 根据卖家表现支付额外奖励，例如更快/更便宜/更好,比如 项目早期每月完成一次，向卖方支付额外的10,000美元 - Fixed Price Award Fee ('FPAF')固定价格奖励: 买方根据业绩支付固定价格和奖励金额（奖金） - 'Cost-reimbursable' contracts: involve payment to the seller for direct and indirect costs #买家不利 - Cost plus incentive fee, cost plus fixed fee, and cost plus percentage of costs - Cost + Fee (CPF)/ Cost Plus Percentage of Costs ('CPPC') :涉及向卖方支付已完成工作所产生的`所有合法实际费用`，以及费用的百分比。 - Cost Plus Fixed Fee.('CPFF'). 成本加固定费用 - Cost Plus Incentive Fee ('CPIF'). 成本加奖励 - Cost Plus Award Fee ('CPAF'). 成本加奖励费 - 'Time and material' contracts: hybrid of both fixed price and cost reimbursable contracts 用于在授予合同时`无法确定工作量`的服务工作 例如: 合同=每天1美元加上每线性木材5美元的材料- 'Unit price' contracts: require the buyer to pay the seller a predetermined amount per unit of service ContractRisk.png PROJECT STAKEHOLDER MANAGEMENTProcess Identifying stakeholders: identifying everyone involved in the project or affected by it, and determining the best ways to manage relationships with them. output: stakeholder register includes basic information on stakeholders Identification information: stakeholders’ names, positions, locations, roles in the project, and contact information Assessment information: stakeholders’ major requirements and expectations, potential influences, and phases of the project in which stakeholders have the most interest Stakeholder classification: is the stakeholder internal or external to the organization? Is the stakeholder a supporter of the project or resistant to it? Planning stakeholder management: determining strategies to effectively engage stakeholders in project decisions and activities based on their needs, interests, and potential impact. output: stakeholder engagement plan Managing stakeholder engagement: communicating and working with project stakeholders to satisfy their needs and expectations, resolving issues, and fostering engagement in project decisions and activities output: stakeholder engagement plan Monitoring stakeholder engagement:monitoring stakeholder relationships and adjusting plans and strategies for engaging stakeholders as needed Outputs: work performance information, change requests, project management plan updates, and project documents updates. Exam考后说明 True or False： 只要理解知识点部分的内容，就够了，不必去记忆。考卷的”False”很明显 Single choice：刷一遍题库，对题目有印象就行。考试都是原题，选项次序都不变 Fill in the blanks：刷一遍题库，考试都是原题 Writing：still 背题库，默写式答题 Summary： 此门课程难点还是 前期完成的项目本身 笔试部分则是”记到便是赚到“ (2018.12.10) 知识点啊~朋友们项目管理简介1.项目与运营不同，因为它们在达到目标或项目终止时结束。2.使用渐进式细化开发项目。项目通常在开始时被广泛定义，随着时间的推移，项目的具体细节变得更加清晰。因此，应该逐步开发项目。 3.一个项目涉及不确定性。每个项目都是独一无二的，因此有时很难明确定义目标，估计完成所需的时间，或确定需要多少费用。这种不确定性是项目管理如此具有挑战性的主要原因之一。4.管理三重约束涉及在项目的范围，时间和成本目标之间进行权衡。经验丰富的项目经理知道必须决定三重约束的哪个方面最重要。5.利益相关者是参与或受项目活动影响的人，包括项目发起人，项目团队，支持人员，客户，用户，供应商，甚至是项目的反对者。6.项目管理知识领域描述了项目经理必须发展的关键能力。项目采购管理涉及从执行组织外部为项目获取或采购商品和服务。7.项目经理不仅负责项目成果的交付。他们是负责这些项目开发的产品和流程成功的变革推动者。 8.IT项目经理必须愿意发展不仅仅是他们的技术技能，才能成为富有成效的团队成员和成功的项目经理。每个人，无论他们多么技术，都应该培养商业和软技能。 项目管理和信息技术背景1.使用系统方法对于成功的项目管理至关重要。如果高层管理人员和项目经理要了解项目与整个组织的关系，他们必须遵循系统理念。2.矩阵组织的项目经理有来自各个职能领域的工作人员从事项目工作。3.组织文化非常强大，许多人认为许多公司问题的根本原因不在于组织结构或员工;他们在文化中。4.在围绕团体或团队而不是个人组织工作活动的组织中，项目工作最为成功。强调团队工作的组织文化最适合管理项目。5.在项目生命周期的早期阶段，资源需求通常最低，不确定性水平最高。在后期阶段对项目进行重大改变要昂贵得多。6.由于组织通常会在项目持续时投入更多资金，因此应在每个阶段之后进行管理评审，以评估进度，潜在成功以及与组织目标的持续兼容性。7.虚拟团队是一群使用通信技术在时间和空间边界上一起工作的人。团队成员可能都在同一个国家的同一家公司工作，或者他们可能包括员工以及独立顾问，供应商，甚至志愿者，他们提供来自全球的专业知识。 项目管理流程组1.启动流程包括定义和授权项目或项目阶段。启动过程在项目的每个阶段进行。2.启动和关闭任务通常是最短的（分别在项目或阶段的开始和结束时），并且它们需要最少的资源和时间。3.监控和控制流程与所有其他项目管理流程组重叠，因为可以随时进行更改。4.敏捷是一种适应性产品生命周期，当可交付成果具有高度变化和高交付频率时使用。5.六西格玛项目使用两种主要方法：DMAIC（定义，测量，分析，改进和控制）用于改进现有业务流程，并使用DMADV（定义，测量，分析，设计和验证）创建新产品或流程设计，以实现可预测的，无缺陷的性能。6.启动会议是在项目开始时举行的会议，以便利益相关者可以相互见面，审查项目的目标，并讨论未来的计划。启动会议通常在业务案例和项目章程完成后举行，但可以根据需要提前举行。 7.WBS是项目管理中非常重要的工具，因为它为决定如何开展工作提供了基础。 WBS还为创建项目进度表和执行挣值管理提供了基础，用于衡量和预测项目绩效。8.因为Scrum暗示团队成员是由ScrumMaster指导的自我导向组，所以团队合同不是必需的。9.燃尽图表(burndown chart)显示了每天冲刺中剩余的累积工作量。10.冲刺审查是团队向产品所有者展示冲刺期间完成的内容的会议。 11.Scrum框架中监控和控制的两个主要项目是每日Scrum和冲刺审查。12.结账流程包括正式接受项目或项目阶段并有效结束。作为阶段或项目的一部分，管理活动（例如归档项目文件，结束合同，记录经验教训以及接受正式接受交付的工作）通常涉及此流程组。 项目集成管理1.界面管理涉及识别和管理项目各个元素之间的交互点。2.项目集成管理包括界面管理，涉及识别和管理项目各个元素之间的交互点。随着项目涉及的人数增加，接口数量可能呈指数级增长。3.有些人喜欢使用思维导图进行SWOT分析，这种技术使用从核心思想辐射出来的分支来构建思想和想法。4.许多信息系统被归类为“战略性”，因为它们直接支持关键业务战略。例如，信息系统可以帮助组织支持作为低成本生产者的战略。5.随着项目的进展，组织必须重新评估每个项目的需求，资金和意愿，以确定是否应该继续，重新定义或终止项目。6.由于要求和期望不明确，许多项目都失败了，所以从项目章程开始就很有意义。7.项目管理计划不仅仅是甘特图。 项目范围管理1.范围基准包括已批准的项目范围声明及其关联的WBS和WBS字典。 2.WBS的主要目的是定义完成项目所需的所有工作。3.工作包是WBS最低级别的任务。它表示项目经理监视和控制的工作级别。4.创建一个好的WBS非常困难。为此，您必须了解项目及其范围，并纳入利益相关方的需求和知识。5.执行任务executing task在项目之间变化最大，但其他项目管理过程组下的许多任务对于所有项目都是类似的。6.创建一个好的WBS及其WBS字典的基本原则是，一个工作单元应该只出现在WBS中的一个地方。7.即使项目范围相当明确，许多IT项目仍然存在范围蔓延 - 项目范围越来越大的趋势。许多IT项目因范围蔓延而失败。 项目进度管理1.活动或任务是通常在工作分解结构（WBS）上找到的工作要素，其具有预期的持续时间，成本和资源要求。2.在项目进度管理中，定义活动的主要输出是活动列表，活动属性，里程碑列表和项目管理计划更新。3.项目进度表从发起项目的基本文件中发展而来。项目章程经常提到计划的项目开始和结束日期，作为更详细的计划的起点。4.时间表管理计划包括有关报告格式的信息。此信息描述了项目所需的计划报告的格式和频率。此外，它还包括有关流程描述的信息，并描述了如何执行所有计划管理流程。5.项目的里程碑是一个重要事件，通常没有持续时间。通常需要多次活动和大量工作来完成里程碑，但里程碑本身就像是帮助识别必要活动的标记。6.依赖关系或关系涉及项目活动或任务的顺序。确定活动之间的这些关系或依赖关系对于开发和管理项目进度表具有重大影响。7.网络图是显示活动排序的首选技术。网络图是项目活动及其排序之间逻辑关系的示意图。8.网络图是项目活动及其排序之间逻辑关系的示意图。网络图中的箭头表示活动顺序或任务之间的关系。9.当两个或多个节点在单个节点之前时发生合并。另一方面，当两个或多个活动跟随单个节点时发生突发。10.甘特图提供了一种标准格式，用于通过以日历形式列出项目活动及其相应的开始和结束日期来显示项目进度信息。在甘特图中，黑色菱形符号代表了一个里程碑11.跟踪甘特图基于项目任务完成的工作百分比或实际开始和结束日期。它允许项目经理监控各个任务和整个项目的进度。12.在关键路径分析中，几个任务在项目上并行完成，大多数项目通过网络图有多条路径。包含关键任务的最长路径或路径是驱动项目完成日期的原因。13.crashing的主要优点是缩短了完成项目所需的时间。主要缺点是它通常会增加项目总成本。14.关键链调度是一种在创建项目计划时考虑有限资源的方法，并包含用于保护项目完成日期的缓冲区。它假设资源不是多任务或至少最小化多任务处理。 项目成本管理1.超支是实际成本超过估计值的额外百分比或金额。2.现金流量分析是确定项目的估计年度成本和收益以及由此产生的年度现金流量的一种方法。项目经理必须进行现金流量分析以确定净现值。3.沉没成本是过去花费的钱。在决定投资或继续投资哪些项目时，不应包括沉没成本。4.管理储备允许未来不可预测的情况。例如，如果项目经理生病了两周或者一个重要的供应商停业，可以留出管理储备来支付由此产生的费用。5.估算通常在项目的不同阶段进行，随着时间的推移应该变得更加准确。6.类似的估计需要大量的专家判断，并且通常比其他技术成本更低。但是，它也不太准确。7.项目成本估算不准确的原因之一是人类偏向于低估。因此，项目经理和高层管理人员必须审核估算并提出重要问题，以确保估算不会有偏差8.方差和指数的公式以EV（赢得值）开头。通过从EV中减去实际成本或计划值来计算差异，并且通过将EV除以实际成本或计划值来计算指数。9.如果CPI小于1或小于100％，则该项目超出预算。另一方面，如果CPI大于1％或超过100％，则项目预算可控。 项目质量管理1.实验设计是一种有助于确定哪些变量对过程总体结果影响最大的技术。您还可以将实验设计应用于项目管理问题，例如成本和进度权衡。2.可靠性是指产品或服务在正常条件下按预期运行的能力。3.所有项目利益相关方必须共同努力，以平衡项目的质量，范围，时间和成本方面。但是，项目经理最终负责其项目的质量管理。4.接受决定确定作为项目一部分生产的产品或服务是否将被接受或拒绝。如果他们被接受，他们被认为是经过验证的可交付成果。5.运行图表显示过程随时间变化的历史和模式。它是一个折线图，显示按发生顺序绘制的数据点。6.测试需要在系统开发生命周期的几乎每个阶段进行，而不仅仅是在组织发布或将产品交给客户之前。7.在全面质量控制中，产品质量比生产率更重要，工人可以在出现质量问题时停止生产。8.符合要求意味着项目的流程和产品符合书面规范。例如，如果项目范围声明要求交付100台具有特定处理器和内存的计算机，则可以轻松检查是否已交付合适的计算机。 项目人力资源管理1.马斯洛的需求层次表明人们的行为受到一系列需求的指导或激励。2.根据赫兹伯格的说法，激励者，如更高的工资，更多的监督，或更有吸引力的工作环境，将激励工人做更多的工作。他提到导致工作满意度的因素作为激励因素和可能导致不满意的因素作为卫生因素。3.需要制度权力或社会权力的人希望组织其他人来推进组织的目标。4.相信X理论的人认为工人在可能的情况下不喜欢和避免工作，因此管理者必须使用强制，威胁和各种控制方案让工人做出足够的努力来实现目标。他们认为普通工人希望被指导并且更愿意避免责任，没有什么野心，并且希望安全高于一切。 5.Thamhain和Wilemon发现，当项目经理过于依赖权力，金钱或惩罚来影响人们时，项目更有可能失败。当项目经理使用工作挑战和专业知识来影响人们时，项目更有可能成功。6.合法的权力使人们根据权威的地位做事。这种权力类似于权威的影响基础。7.如OBS中所述，责任分配矩阵（RAM）将WBS中描述的项目工作映射到负责执行工作的人员。8.资源调配可以减少项目人员和会计部门的问题。劳动力水平和人力资源的增加和减少往往会产生额外的工作和混乱。9.平滑模式是项目经理强调或避免差异领域并强调协议领域的模式。这种方法也称为适应性，当关系具有高度重要性且任务不重要时，最好使用这种方法。 项目沟通管理1.举行会议的准则之一是确定是否可以避免会议。如果有更好的方法来实现手头的目标，就不要开会。2.项目经理经常将所有经验教训报告中的信息合并到项目总结报告中。3.提高组织的沟通能力需要组织中的文化变革，这需要花费大量时间，努力工作和耐心。4.要使项目取得成功，每个项目团队成员都需要这两种技能，并需要通过正规教育和在职培训不断开发这些技能。5.地理位置和文化背景影响项目沟通的复杂性。如果项目利益相关者在不同的国家，通常很难或不可能在正常工作时间内安排双向沟通的时间。6.项目沟通管理涉及包含影响项目中开发的产品或服务的关键性能特征的详细技术信息。记录可能影响产品性能的技术规范的任何变更甚至更为重要。7.沟通的一个重要方面是参与项目的人数。随着数量的增加，通信的复杂性也会增加，因为人们可以通过更多的渠道或途径进行交流。随着团队规模的增加，沟通变得更加复杂。 项目风险管理1.项目风险管理涉及了解项目可能出现的潜在问题以及它们如何阻碍项目成功。但是，也有积极的风险或机会，可以为项目带来良好的结果。2.寻求风险的人更喜欢更不确定的结果，并且通常愿意支付惩罚来承担风险。3.项目风险管理的第一步是通过执行风险管理计划来决定如何处理特定项目的知识领域。4.应急计划是预定义的行动，如果发生已识别的风险事件，项目团队将采取这些行动。5.头脑风暴是一种技术，通过这种技术，一个小组试图通过自发地和没有判断地积累思想来产生想法或找到特定问题的解决方案。德尔菲技术是一种基于对未来事件的独立和匿名输入的系统化交互式预测程序。6.项目经理可以绘制风险对概率/影响矩阵或图表的概率和影响，其中列出了风险发生的相对概率和风险发生的相对影响。7.十大风险项目跟踪是一种定性风险分析工具。8.已识别的风险可能无法实现，或者其发生或丧失的可能性可能会减少。 项目采购管理1.提供采购服务的组织或个人称为供应商。供应商也称为供应商，承包商，分包商或销售商。2.在外包时，组织应谨慎保护可能在供应商手中易受攻击的战略信息。3.计划采购涉及通过使用组织外部的产品或服务来确定最佳地满足哪些项目需求。如果不需要从组织外部购买产品或服务，则不需要进一步的采购管理。4.成本可偿还合同通常包括费用，例如利润百分比或达到或超过选定项目目标的激励。与固定价格合同相比，买方通过成本可偿还合同承担更多风险。5.固定价格（FFP）合约对买方的风险最小，其次是固定价格激励费（FPIF）合约。6.制造或购买分析涉及估算提供产品或服务的内部成本，并将估算与外包成本进行比较。7.如果公司使用设备20天，他们最好租赁，总费用为10,000美元（20 x 500美元）。 10,000美元的购买成本将增加2,000美元的运营成本（20 x 100美元）。8.评估投标的关键因素，特别是涉及IT的项目，是投标人过去的业绩记录。检查性能记录和参考可以降低选择跟踪记录不佳的供应商的风险。9.控制采购可确保卖方的业绩符合合同要求。合同关系是一种法律关系，这意味着它受州和联邦合同法的约束。 项目利益相关者管理1.与通信和人力资源管理相关的许多概念也适用于利益相关者管理，但需要开展独特的活动以实现良好的利益相关者管理。2.识别利益相关者涉及识别参与项目或受其影响的每个人，并确定管理与他们之间关系的最佳方式。该过程的主要输出是利益相关者登记。3.内部项目利益相关者通常包括项目发起人，项目团队，支持人员和项目的内部客户。其他内部利益相关者包括高层管理人员，其他职能经理和其他项目经理，因为组织资源有限。4.由于员工流动，合作伙伴关系和其他事件，利益相关者可能在项目期间发生变化。5.领先的利益相关者是了解项目及其潜在影响并积极参与帮助项目成功的人6.项目经理必须了解并与各利益相关方合作;因此，他们应该专门讨论如何使用各种沟通方法及其人际关系和管理技能来吸引利益相关者。7.您无法控制利益相关者，但您可以监控干系人的参与程度。参与涉及对话，人们寻求理解和解决共同关心的问题。8.应邀请主要利益攸关方积极参加启动会议，而不仅仅是参加会议。项目经理应该强调，会议期望进行对话，包括利益相关者喜欢的文本或任何沟通方式。 填空题第4章 - 项目集成管理 ____涉及通过分析优势和劣势，研究机会和威胁，预测未来趋势以及预测新产品和服务的需求来确定长期目标。答案：战略规划 ____涉及分析公司的优势，劣势，机会和威胁，并用于协助战略规划。答案：SWOT分析 ____是一种技术，它使用从核心思想辐射出来的分支来构建思想和想法。答案：思维导图 ____指的是改善组织的机会。答案：机遇 ____是从效益中减去项目成本然后除以成本的结果。答案：投资回报率（ROI） ____是一种工具，它提供了一个基于许多标准选择项目的系统过程。答案：加权评分模型 ____是记录的起点，测量或观察，以便可以用于将来的比较。变化。答案：基线 ____涉及在整个项目生命周期中识别，评估和管理变更。答案：综合变更控制 第5章 - 项目范围管理 ____创建涉及将主要项目可交付成果细分为更小，更易于管理的组件。答案：工作分解结构（WBS） ____是指“项目必须满足的条件或能力，或者在产品，服务或结果中出现以满足协议或其他正式规定的条件或能力。”答案：要求 ____包括已批准的项目范围声明及其关联的WBS和WBS字典。答案：范围基线 工作包是WBS的____级任务。答案：最低 ____在创建WBS的方法中，团队成员首先确定尽可能多的与项目相关的特定任务。答案：自下而上 ____是一种技术，它使用从核心思想中散发出来的分支来构建创建WBS时的思想和想法。答案：思维导图 ____是项目范围越来越大的趋势。答案：范围蔓延 ____执行范围验证的主要工具是和小组决策制定技术。答案：检查 ____涉及开发系统的工作副本或系统的某些方面。答案：原型设计 第6章 - 项目进度管理 ____是完成任务所需的工作日或工作小时数。答案：努力 在项目进度表中，灵活性最小的变量是____。答案：时间 ____涉及确保及时完成项目所需的过程。答案：项目进度管理 ____是要列入项目进度表的活动的表格。答案：活动清单 ____是项目活动及其排序之间逻辑关系的示意图。答案：网络图 在____关系中，“from”活动必须在“to”活动完成之前开始。答案：从头到尾 ____没有持续时间和资源，但偶尔需要在AOA网络图上显示活动之间的逻辑关系。答案：虚拟活动 第7章 - 项目成本管理 ____过程的主要成果是活动成本估算，估算基础和项目文件更新。答案：成本估算 ____流程的主要成果是成本绩效基准，项目资金要求和项目文件更新。答案：成本预算 ____理论指出，当重复生产许多物品时，随着生产更多单位，这些物品的单位成本会以规律的方式减少。答案：学习曲线 ____估算是在项目的早期阶段或甚至在项目正式启动之前完成的。答案：粗略的数量级（ROM） ____是项目经理用来衡量和监控成本绩效的分阶段预算。答案：成本基准 第8章 - 项目质量管理 ____这个词意味着产品可以按照预期使用。答案：适合使用 ____是一种质量计划技术，有助于确定哪些变量对过程的总体结果影响最大。答案：实验设计 ____图表将有关质量问题的投诉追溯到负责任的生产操作。答案：因果关系鱼刺石川 Watts S. Humphrey将____定义为在交付程序之前必须更改的任何内容。答案：软件缺陷 ____是一个公司部门的非监督人员和工作领导小组，他们自愿组织如何提高部门工作效率的小组研究。答案：质量圈子 ____意味着对失败负责或不满足质量期望。答案：不合格的成本 Genichi Taguchi的____方法着重于通过用科学探究替代试错法来消除缺陷。答案：稳健的设计 第9章 - 项目资源管理 根据马斯洛的说法，只有满足____需求后，个人才能满足增长需求。答案：缺陷 赫茨伯格称之为导致工作满意度的因素____。答案：激励者 ____是整体等于其各部分之和的概念。答案：协同作用 ____正在倾听，意图理解。答案：移情倾听 ____是和谐，一致，一致或亲和的关系，对沟通很重要。答案：交流 ____根据所需的详细程度将工作分配给负责任和执行的组织，团队或个人。答案：责任分配矩阵（RAM） ____是一种特定类型的组织结构图，显示哪些组织单位负责哪些工作项。答案：OBS（组织分解结构） 第10章 - 项目沟通管理 许多信息技术专业人员在_项目中工作，他们从未与项目赞助商，其他团队成员或其他项目利益相关者会面。答案：虚拟 ____分析包括信息的联系人，信息到期时以及信息的首选格式等信息。答案：利益相关方沟通 在试图评估项目利益相关者的承诺时，_会议或网络会议可能是最合适的媒介。答案：面对面 控制通信的主要目标是确保整个____的最佳信息流。答案：项目生命周期 所有会议必须有____和预期结果。答案：目的 ____强制会议组织者计划会议，并让潜在参与者有机会决定是否需要参加。答案：议程 第11章 - 项目风险管理 项目_是一种不确定性，可能对实现项目目标产生负面或正面影响。答案：风险 是从潜在收益中获得的满足或愉悦的数量。答案：风险效用 是项目潜在风险类别的等级。答案：风险分解结构 是一种技术，通过这种技术，一个小组试图通过自发地和没有判断地积累思想来产生想法或找到特定问题的解决方案。答案：头脑风暴 是包含各种风险管理流程结果的文件。答案：风险登记 是风险事件概率和风险事件货币价值的乘积。答案：EMV预期的货币价值预期货币价值（EMV）EMV（预期货币价值） 风险是在实施所有应对策略后仍然存在的风险。答案：剩余 第12章-项目采购管理 ____是指从外部来源获取商品和/或服务的过程。答案：采购 ____是一项具有相互约束力的协议，规定卖方有义务提供指定的产品或服务，并规定买方有义务支付这些产品或服务。答案：合同 ____决定是指组织决定在组织内部制造某些产品或执行某些服务是否符合其最佳利益，或者是否最好从外部组织购买。答案：制造或购买 ____合同包括由于通货膨胀等条件的变化而对合同价格进行预定义的最终调整的特殊规定。答案：固定价格与经济价格调整（FP-EPA），固定价格与经济，价格调整，FP-EPA ____合同是固定价格和成本可偿还合同的混合体。答案：时间和材料（T＆M），时间和材料，T＆M ____是允许买方或供应商终止合同的合同条款。答案：终止条款 ____是卖方在满足买方需求时采用不同方法编制的文件。答案：提案","categories":[],"tags":[]},{"title":"GoogLeNet","slug":"GoogLeNet","date":"2018-12-07T02:44:09.000Z","updated":"2019-01-01T14:00:24.759Z","comments":true,"path":"2018/12/07/GoogLeNet/","link":"","permalink":"http://yoursite.com/2018/12/07/GoogLeNet/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】未完待续","text":"【阅读时间】XXX min XXX words【阅读内容】未完待续 参考链接：GoogLeNet系列解读、深度学习之GoogLeNet解读、GoogLeNet的心路历程 ToRead：深入理解GoogLeNet结构 GoogLeNet Incepetion V1GoogLeNet, 一个22层的深度网络，2014年ILSVRC挑战赛冠军，将Top5 的错误率降低到6.67%。这是一种 类似于 网中网（Network In Network）的结构，即原来的结点也是一个网络。 这是GoogLeNet的最早版本，出现在2014年的《Going deeper with convolutions》。之所以名为“GoogLeNet”而非“GoogleNet”,文章说是为了向早期的LeNet致敬。 Motivation深度学习以及神经网络快速发展，人们不再只关注更给力的硬件、更大的数据集、更大的模型，而是更在意新的idea、新的算法以及模型的改进。 一般来说，提升网络性能最直接的办法就是增加网络深度和宽度，这也就意味着巨量的参数。但是，巨量参数容易产生过拟合也会大大增加计算量。 文章认为解决上述两个缺点的根本方法是将全连接甚至一般的卷积都转化为稀疏连接。一方面现实生物神经系统的连接也是稀疏的，另一方面有文献1表明：对于大规模稀疏的神经网络，可以通过分析激活值的统计特性和对高度相关的输出进行聚类来逐层构建出一个最优网络。这点表明臃肿的稀疏网络可能被不失性能地简化。 虽然数学证明有着严格的条件限制，但Hebbian准则有力地支持了这一点：fire together,wire together。 早些的时候，为了打破网络对称性和提高学习能力，传统的网络都使用了随机稀疏连接。但是，计算机软硬件对非均匀稀疏数据的计算效率很差，所以在AlexNet中又重新启用了全连接层，目的是为了更好地优化并行运算。 所以，现在的问题是有没有一种方法，既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能。大量的文献表明可以将稀疏矩阵聚类为较为密集的子矩阵来提高计算性能，据此论文提出了名为Inception 的结构来实现此目的。 Architectural DetailsInception 结构的主要思路是怎样用密集成分来近似最优的局部稀疏结构。作者首先提出下图这样的基本结构： 对上图做以下说明：1 . 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合；2 . 之所以卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定pad=0、1、2，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了；3 . 文章说很多地方都表明pooling挺有效，所以Inception里面也嵌入了。4 . 网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。 但是，使用5x5的卷积核仍然会带来巨大的计算量。 为此，文章借鉴NIN，采用1x1卷积核来进行降维。例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256。其中，卷积层的参数为128x5x5x256。假如上一层输出先经过具有32个输出的1x1卷积层，再经过具有256个输出的5x5卷积层，那么最终的输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256，大约减少了4倍。详细的降维计算过程 具体改进后的Inception Module如下图： GoogLeNetGoogLeNet的整体结构如下图： 对上图做如下说明：1 . 显然GoogLeNet采用了模块化的结构，方便增添和修改；2 . 网络最后采用了average pooling来代替全连接层，想法来自NIN,事实证明可以将TOP1 accuracy提高0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便以后大家finetune；3 . 虽然移除了全连接，但是网络中依然使用了Dropout ;4 . 为了避免梯度消失，网络额外增加了2个辅助的softmax（average pooling）用于向前传导梯度。文章中说这两个辅助的分类器的loss应该加一个衰减系数，但看caffe中的model也没有加任何衰减。此外，实际测试的时候，这两个额外的softmax会被去掉。 下图是一个比较清晰的结构图： ConclusionGoogLeNet是谷歌团队为了参加ILSVRC 2014比赛而精心准备的，为了达到最佳的性能，除了使用上述的网络结构外，还做了大量的辅助工作：包括训练多个model求平均、裁剪不同尺度的图像做多次验证等等。详细的这些可以参看文章的实验部分。 本文的主要想法其实是想通过构建密集的块结构来近似最优的稀疏结构，从而达到提高性能而又不大量增加计算量的目的。GoogleNet的caffemodel大小约50M，但性能却很优异。 GoogLeNet Inception V2 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift，top5 error 4.8% 这篇文章做出的贡献不是一般的大，它提出了Batch Normalization（BN），以至于网上关于它的介绍铺天盖地，但中文优秀原创没几个，都是转载来转载去，挑几个好的比如：这个、这个、这个。 GoogLeNet Inception v3GoogLeNet凭借其优秀的表现，得到了很多研究人员的学习和使用，因此Google团队又对其进行了进一步发掘改进，产生了升级版本的GoogLeNet。这一节介绍的版本记为V2，文章为：《Rethinking the Inception Architecture for Computer Vision》。 Introduction14年以来，构建更深的网络逐渐成为主流，但是模型的变大也使计算效率越来越低。这里，文章试图找到一种方法在扩大网络的同时又尽可能地发挥计算性能。 首先，GoogLeNet V1出现的同期，性能与之接近的大概只有VGGNet了，并且二者在图像分类之外的很多领域都得到了成功的应用。但是相比之下，GoogLeNet的计算效率明显高于VGGNet，大约只有500万参数，只相当于Alexnet的1/12(GoogLeNet的caffemodel大约50M，VGGNet的caffemodel则要超过600M)。 GoogLeNet的表现很好，但是，如果想要通过简单地放大Inception结构来构建更大的网络，则会立即提高计算消耗。此外，在V1版本中，文章也没给出有关构建Inception结构注意事项的清晰描述。因此，在文章中作者首先给出了一些已经被证明有效的用于放大网络的通用准则和优化方法。这些准则和方法适用但不局限于Inception结构。 General Design Principles下面的准则来源于大量的实验，因此包含一定的推测，但实际证明基本都是有效的。 1 . 避免表达瓶颈，特别是在网络靠前的地方。 信息流前向传播过程中显然不能经过高度压缩的层，即表达瓶颈。从input到output，feature map的宽和高基本都会逐渐变小，但是不能一下子就变得很小。比如你上来就来个kernel = 7, stride = 5 ,这样显然不合适。另外输出的维度channel，一般来说会逐渐增多(每层的num_output)，否则网络会很难训练。（特征维度并不代表信息的多少，只是作为一种估计的手段） 2 . 高维特征更易处理。 高维特征更易区分，会加快训练。 3. 可以在低维嵌入上进行空间汇聚而无需担心丢失很多信息。 比如在进行3x3卷积之前，可以对输入先进行降维而不会产生严重的后果。假设信息可以被简单压缩，那么训练就会加快。 4 . 平衡网络的宽度与深度。 上述的这些并不能直接用来提高网络质量，而仅用来在大环境下作指导。 Factorizing Convolutions with Large Filter Size大尺寸的卷积核可以带来更大的感受野，但也意味着更多的参数，比如5x5卷积核参数是3x3卷积核的25/9=2.78倍。为此，作者提出可以用2个连续的3x3卷积层(stride=1)组成的小网络来代替单个的5x5卷积层，(保持感受野范围的同时又减少了参数量)如下图： 然后就会有2个疑问： 1 . 这种替代会造成表达能力的下降吗？后面有大量实验可以表明不会造成表达缺失； 2 . 3x3卷积之后还要再加激活吗？ 作者也做了对比试验，表明添加非线性激活会提高性能。 从上面来看，大卷积核完全可以由一系列的3x3卷积核来替代，那能不能分解的更小一点呢。文章考虑了 nx1 卷积核。如下图所示的取代3x3卷积： 于是，任意nxn的卷积都可以通过1xn卷积后接nx1卷积来替代。实际上，作者发现在网络的前期使用这种分解效果并不好，还有在中度大小的feature map上使用效果才会更好。（对于mxm大小的feature map,建议m在12到20之间）。 总结如下图： (1) 图4是GoogLeNet V1中使用的Inception结构； (2) 图5是用3x3卷积序列来代替大卷积核； (3) 图6是用nx1卷积来代替大卷积核，这里设定n=7来应对17x17大小的feature map。该结构被正式用在GoogLeNet V2中。 GoogLeNet Inception v4 Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning，top5 error 3.08% Szegedy读了此论文后，蹦出了结合GoogLeNet与Residual Connections的奇思妙想，于是就有了上面那篇论文，主要贡献如下： 1、在Inception v3的基础上发明了Inception v4，v4比v3更加复杂，复杂到不可思议 2、结合ResNet与GoogLeNet，发明了Inception-ResNet-v1、Inception-ResNet-v2，其中Inception-ResNet-v2效果非常好，但相比ResNet，Inception-ResNet-v2的复杂度非常惊人，跟Inception v4差不多 3、加入了Residual Connections以后，网络的训练速度加快了 4、在网络复杂度相近的情况下，Inception-ResNet-v2略优于Inception-v4 5、Residual Connections貌似只能加速网络收敛，真正提高网络精度的是“更大的网络规模” 以上就是Inception v4论文的主要贡献了，没有什么创新，只是在前人的基础上修修补补、移花接木，但这篇文章工作量不小，需要花费大量时间训练作者提出的3种网络。 至此，GoogLeNet四篇相关论文就介绍完了，纵观GoogLeNet的发展历程，Szegedy为我们提供了许多可以借鉴的网络设计方法，比如Inception结构、非对称卷积、Batch Normalization、取消全连层……等等。就连Szegedy本人，也汲取了ResNet的精髓，合体两种网络设计出了Inception-ResNet。所以多读论文，多学习别人的idea，是非常重要的。","categories":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://yoursite.com/categories/DeepLearning/"}],"tags":[{"name":"Paper","slug":"Paper","permalink":"http://yoursite.com/tags/Paper/"}]},{"title":"DenseNet_Introduction","slug":"DenseNet-Introduction","date":"2018-12-03T08:39:14.000Z","updated":"2018-12-03T15:26:59.184Z","comments":true,"path":"2018/12/03/DenseNet-Introduction/","link":"","permalink":"http://yoursite.com/2018/12/03/DenseNet-Introduction/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 原文链接：Densely Connected Convolutional Netwroks 译文链接：DenseNet论文翻译及pytorch实现解析（上）、Densely Connected Convolutional Networks翻译 参考链接：DenseNet算法详解(ToRead 评价最高)、《Densely Connected Convolutional Networks》论文笔记（精而简）、残差系列网络（简明）、DenseNet 的“what”、“why”和“how”(思路清晰)、DenseNet详解(图解清晰) ToRead：DenseNet 简介(数学说明)、DenseNet：比ResNet更优的CNN模型(形象细致) DenseNet的优点： 减轻梯度消失的问题； 加强了特征的传导和利用； 减少了参数量（与ResNet相比，在实现同等准确率的条件下，DenseNet的参数量要小于ResNet） 减少了计算量 概念介绍Dense block​ 网络的每一层都直接与其前面层相连（可以直接将梯度从后层传向前层），实现特征的重复利用，这就使得网络更加“参数高效”。 Transition layer​ 该层位于两个dense block之间，由conv层和pooling层组成。之前说过，在一个dense block里，空间维度是保持不变的，为了能进行下采样，故而在两个dense block之间插入transition layer，进一步减少特征图的数量，提升模型的紧凑程度。 Growth rate​ 论文里涉及到growth rate这个概念。它指的是一个dense block里各个层输出feature maps的通道数，在同一个dense block里bn-relu-conv输出的通道数都是一样的。如上图，它的growth rate=4。一般，为了不使网络变得太宽，以及增加参数的利用效率，growth rate一般不会设得太大。 Bottleneck layers ​ 在一个dense block里，尽管每层输出的通道数并不大（growth rate一般不会设得很大），但是输入是由前面层的feature maps串接起来的，所以输入的通道数会很大。为了提高计算效率，作者引进1*1 conv层作为bottleneck layer，放置在每层的前面，用来降低通道数(减小参数量)。带有bottleneck layer的DenseNet被称为DenseNet-B。 Compression​ 如果transition layer对上一个dense block进行了通道数的降维（即压缩），则称这类DenseNet为 DenseNet-C。同时使用了bottleneck layer和compression的DenseNet称为DenseNet-BC。 上图是DenseNet和它的几种变体进行parameter efficiency 的比较，可以看出在实现同等accuracy的条件下，DenseNet-BC所用的参数量最少，实现最大的parameter efficiency。 完整的DenseNet网络结构： Figure 2 上图是由三个dense block组成的，两个block之间的conv+pool为transition layer。dense block3后面的pooling是global average pooling，然后再接一个全连接层+softmax。 算法分析Model compactness由于DenseNet对输入进行cat操作,一个直观的影响就是每一层学到的feature map都能被之后所有层直接使用,这使得特征可以在整个网络中重用,也使得模型更加简洁. 从上图中我们可以看出DenseNet的参数效率:左图包含了对多种DenseNet结构参数和最终性能的统计,我们可以看出当模型实现相同的test error时,原始的DenseNet往往要比DenseNet-BC拥有2-3倍的参数量.中间图为DenseNet-BC与ResNet的对比,在相同的模型精度下,DenseNet-BC只需要ResNet约三分之一的参数数量.右图为1001层超过10M参数量的ResNet与100层只有0.8M参数量的DenseNet-BC在训练时的对比,虽然他们在约相同的训练epoch时收敛,但DenseNet-BC却只需要ResNet不足十分之一的参数量. Implicit Deep Supervision解释DenseNet为何拥有如此高性能的另一个原因是网络中的每一层不仅接受了原始网络中来自loss的监督,同时由于存在多个bypass与shortcut,网络的监督是多样的.Deep supervision的优势同样在deeply-supervised nets (DSN)中也被证实.(DSN中每一个Hidden layer都有一个分类器,强迫其学习一些有区分度的特征).与DSN不同的是,DenseNet拥有单一的loss function, 模型构造和梯度计算更加简易. Feature Reuse在设计初,DenseNet便被设计成让一层网络可以使用所有之前层网络feature map的网络结构,为了探索feature的复用情况,作者进行了相关实验.作者训练的L=40,K=12的DenseNet,对于任意Denseblock中的所有卷积层,计算之前某层feature map在该层权重的绝对值平均数.这一平均数表明了这一层对于之前某一层feature的利用率,下图为由该平均数绘制出的热力图: 从图中我们可以得出以下结论: a) 一些较早层提取出的特征仍可能被较深层直接使用b) 即使是Transition layer也会使用到之前Denseblock中所有层的特征c) 第2-3个Denseblock中的层对之前Transition layer利用率很低,说明transition layer输出大量冗余特征.这也为DenseNet-BC提供了证据支持,即Compression的必要性.d) 最后的分类层虽然使用了之前Denseblock中的多层信息,但更偏向于使用最后几个feature map的特征,说明在网络的最后几层,某些high-level的特征可能被产生. 实现结果作者在多个benchmark数据集上训练了多种DenseNet模型,并与state-of-art的模型(主要是ResNet和其变种)对比: 由上表我们可以看出,DenseNet只需要较小的Growth rate(12,24)便可以实现state-of-art的性能,结合了Bottleneck和Compression的DenseNet-BC具有远小于ResNet及其变种的参数数量,且无论DenseNet或者DenseNet-BC,都在原始数据集和增广数据集上实现了超越ResNet的性能.","categories":[],"tags":[]},{"title":"ResNet_Introduction","slug":"ResNet-Introduction","date":"2018-12-02T11:03:11.000Z","updated":"2018-12-03T14:35:38.115Z","comments":true,"path":"2018/12/02/ResNet-Introduction/","link":"","permalink":"http://yoursite.com/2018/12/02/ResNet-Introduction/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 论文出处：Deep Residual Learning for Image Recognition 译文链接：http://blog.csdn.net/wspba/article/details/57074389 参考链接：ResNet学习笔记、深度详解ResNet及其六大变体、残差网络ResNet笔记、残差系列网络 引入随着CNN网络的发展，尤其的VGG网络的提出，大家发现网络的层数是一个关键因素，貌似越深的网络效果越好。但是随着网络层数的增加，问题也随之而来。 首先出现的问题是梯度消失/梯度爆炸，这就导致训练难以收敛。归一初始化（normalized initialization）和中间归一化（intermediate normalization）在很大程度上解决了这一问题，它使得数十层的网络在反向传播的随机梯度下降（SGD）上能够收敛。 当深层网络能够收敛时，一个退化问题又出现了：随着网络深度的增加，准确率达到饱和然后迅速退化。意外的是，这种退化并不是由过拟合造成的，并且在一个合理的深度模型中增加更多的层却导致了更高的错误率。Fig.1展示了一个典型的例子： 对于更深的模型，这有一种通过构建的解决方案：恒等映射identity mapping(所谓的恒等映射，即输入$x$经过某一个函数（设为$G(x)$）作用输出还是$x$本身，即$G(x)=x$)，来构建增加的层，而其它层直接从浅层模型中复制而来。这个构建的解决方案也表明了，一个更深的模型不应当产生比它的浅层版本更高的训练错误率。但是，相关的实验结果说明传统的网络（”plain” networks）很难去学习恒等映射，即会出现因为深度增加而导致性能下降的问题（即使采用了恒等映射方法）。#### Identity Mapping by Shortcuts本文中，我们提出了一种深度残差学习框架来解决这种因为深度增加而导致性能下降的问题。我们假设$F(x)$代表某个只包含有两三层的block的映射函数， $x$ 是block的输入，$F(x)$ 是block的输出。假设他们具有相同的维度。在训练的过程中我们希望能够通过修改网络中的 $w$ 和 $b$ 去拟合一个理想的 $\\mathcal{H}(x)$ (从输入到输出的一个理想的映射函数)。也就是我们的目标是修改 $\\mathcal{F}(x)$ 中的 $w$ 和 $b$ 逼近 $\\mathcal{H}(x)$ 。如果我们改变思路，用$\\mathcal{F}(x)$ 来逼近$\\mathcal{H}(x)-x$，那么我们最终得到bolck的输出$\\mathcal{H}(x)$就由 $\\mathcal{F}(x)$ 变为$\\mathcal{F}(x)+x$ （这里的加指的是对应位置上的元素相加，也就是element-wise addition），这里的直接将输入连接到输出的结构也称为shortcut。 这里我们假设优化残差映射$\\mathcal{F}({x})$比优化原来的映射 $\\mathcal{H}({x})$容易。- 改变前目标： 训练$\\mathcal{F}(x)$ 逼近 $\\mathcal{H}(x)$- 改变后目标：训练 $\\mathcal{F}(x)$ 逼近$\\mathcal{H}(x)-x$ (即 $\\mathcal{F}(x)+x$ 逼近$\\mathcal{H}(x)$)左边的original block需要调整其内部参数，使得输入的x经过卷积操作后最终输出的F(x)等于x，即实现了恒等映射F(x)=x，等号左边是block的输出，右边是block的输入。但是这种结构的卷积网络很难调整其参数完美地实现F(x)=x。再看右边的Res block。因为shortcut的引入，整个block的输出变成了F(x)+x，block的输入还是x。此时网络需要调整其内部参数使得F(x)+x=x，也就是直接令其内部的所有参数为0，使得F(x)=0，F(x)+x=x就变成了0+x = x，等号左边是block的输出，右边是block的输入。输出等于输入，即完美地完成了恒等映射。因此使用ResNet结构搭建的深度网络至少与浅层网络具有相同的拟合能力，不会出现之前的网络退化问题。—— We consider a building block defined as： $\\begin{equation} {y}= \\mathcal{F}({x}, {W_{i}}) + W_{s}{x}. \\end{equation}$#### Shortcut的三种方式由于ResNet要求 $F(x)$与 $x$ 的维度大小要一致才能够相加，因此在$F(x)$ 与$x$ 维度不相同时就需要对$x$的维度做调整，文章中提出了三种调整的方式：1. 如果 $x$ 的维度增加，就使用0来填充增加出来的维度（A方式）2. 如果 $x$ 的维度增加，使用线性变换来增加多出来的维度，在程序里表现为使用一个1x1的卷积核进行调整维度（B方式）3. 对于所有的shortcut，都使用线性变换，也就是1x1的卷积（C方式）由下面的实验结果可以，分析ABC这三种方式。A方式采用0填充.，完全不存在任何的残差学习能力。B方式与C方式相比，错误率略高。但是B方式的模型复杂度要远低于C方式，因此，作者最终在所有的网络中采用方式B。B方式在 $x$ 的维度与$F(x)$的维度相同时，直接用 $x$ 加上 $F(x)$，在 $x$ 的维度与 $F(x)$的维度不同时，才采用1x1的卷积层对 $x$ 的维度进行调整。——#### 对ResNet的解读 ResNet架构有很多独立有效路径（或者说残差网络其实是很多并行子网络的组合），而且大部分路径在移除了部分层之后会保持完整无损。相反，VGG网络只有一个有效路径，因此移除一个层都会对它的唯一路径的性能产生极大的影响。 图(左)大部分的路径都流经了19到35个残差块。为了得到路径长度k的梯度幅度，作者们首先向网络输入了一批数据，然后任意采样了k个残差块。当反向传递梯度时，他们仅将采样的残差块通过权重层进行传递。图(中)表示随着路径长度的增加，梯度幅度会迅速下降。我们现在可以将每一路径长度与其期望的梯度大小相乘，看每一路径长度在训练中起到多大的作用，就像图(右)。 令人惊讶的是，大多分布都来自于9到18的路径长度，但它们都只包含少量的总路径（或者说ResNet是由大多数中度网络和一小部分浅度网络和深度网络组成的，说明虽然表面上ResNet网络很深，但是其实起实际作用的网络层数并没有很深。），如图(左)。这是一个非常有趣的发现，因为这暗示着ResNet无法解决过长路径的梯度消失问题，ResNet的成功实际上源自于它缩短了它的有效路径(effective path)的长度。 Stochastic depth通过在训练期间随机丢弃层来改善深度残留网络的训练。这表明并不是所有的层都是需要的，并且强调在深度（剩余）网络中存在大量的冗余。 Network ArchitecturesPlain Network 主要是受 VGG 网络启发，主要采用3*3滤波器，遵循两个设计原则：1）对于相同输出特征图尺寸，卷积层有相同个数的滤波器，2）如果特征图尺寸缩小一半，滤波器个数加倍以保持每个层的计算复杂度。通过步长为2的卷积来进行降采样。一共34个权重层。需要指出，我们这个网络与VGG相比，滤波器要少，复杂度要小。Residual Network 主要是在 上述的 plain network上加入 shortcut connections。ResNet的结构使得网络具有与学习恒等映射的能力，同时也具有学习其他映射的能力。因此ResNet的结构要优于传统的卷积网络（plain networks）结构。#### Experiments","categories":[],"tags":[]},{"title":"Tensorboard_Morvan","slug":"Tensorboard-Morvan","date":"2018-12-01T14:08:52.000Z","updated":"2018-12-01T14:15:06.483Z","comments":true,"path":"2018/12/01/Tensorboard-Morvan/","link":"","permalink":"http://yoursite.com/2018/12/01/Tensorboard-Morvan/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 链接：https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-1-tensorboard1/ (tensorflow) ➜ Morvan_Tensorflow tensorboard --logdir logsTensorBoard 1.11.0 at http://MacBook-Pro:6006 (Press CTRL+C to quit) Chrome http://0.0.0.0:6006","categories":[],"tags":[{"name":"Morvan","slug":"Morvan","permalink":"http://yoursite.com/tags/Morvan/"}]},{"title":"Matplotlib_Python3_Morvan","slug":"Matplotlib-Python3-Morvan","date":"2018-12-01T13:18:30.000Z","updated":"2018-12-01T14:15:38.172Z","comments":true,"path":"2018/12/01/Matplotlib-Python3-Morvan/","link":"","permalink":"http://yoursite.com/2018/12/01/Matplotlib-Python3-Morvan/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】Not Completed…… Matplotlib 是一个非常强大的 Python 画图工具，它能帮你画出美丽的线图、散点图、等高线图、条形图、柱状图、3D图形，甚至是图形动画等等。","text":"【阅读时间】XXX min XXX words【阅读内容】Not Completed…… Matplotlib 是一个非常强大的 Python 画图工具，它能帮你画出美丽的线图、散点图、等高线图、条形图、柱状图、3D图形，甚至是图形动画等等。 链接：https://morvanzhou.github.io/tutorials/data-manipulation/plt/ Matplotlib简介Matplotlib 安装基本使用基本用法figure 图像设置坐标轴Legend图例Annotation标注tick能见度画图种类Scatter 散点图Bar 柱状图Contours 等高线图Image 图片3D图片多图合并显示Subplot 多合一显示Subplot 分格显示图中图次坐标轴动画Animation动画","categories":[],"tags":[{"name":"Morvan","slug":"Morvan","permalink":"http://yoursite.com/tags/Morvan/"}]},{"title":"numpy&pandas_Python3_Morvan","slug":"numpy-pandas-Python3-Morvan","date":"2018-12-01T12:10:48.000Z","updated":"2018-12-01T14:16:13.563Z","comments":true,"path":"2018/12/01/numpy-pandas-Python3-Morvan/","link":"","permalink":"http://yoursite.com/2018/12/01/numpy-pandas-Python3-Morvan/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】Not Completed……","text":"【阅读时间】XXX min XXX words【阅读内容】Not Completed…… 链接：https://morvanzhou.github.io/tutorials/data-manipulation/np-pd/ Numpy 和 Pandas 安装numpy 安装# 使用 python 3+:$ pip3 install numpy# 使用 python 2+:$ pip install numpy pandas安装# 使用 python 3+:$ pip3 install pandas# 使用 python 2+:$ pip install pandas Numpy属性 ndim：维度 shape：行数和列数 size：元素个数 import numpy as np #为了方便使用numpy 采用np简写array = np.array([[1,2,3],[2,3,4]]) #列表转化为矩阵print(array)\"\"\"array([[1, 2, 3], [2, 3, 4]])\"\"\"print('number of dim:',array.ndim) # 维度# number of dim: 2print('shape :',array.shape) # 行数和列数# shape : (2, 3)print('size:',array.size) # 元素个数# size: 6 Numpy 的创建array 创建 array 有很多 形式 关键字 array：创建数组 dtype：指定数据类型 zeros：创建数据全为0 ones：创建数据全为1 empty：创建数据接近0 arrange：按指定范围创建数据 linspace：创建线段 创建数组a = np.array([2,23,4]) # list 1dprint(a)# [2 23 4] 指定数据 dtypea = np.array([2,23,4],dtype=np.int)print(a.dtype)# int 64a = np.array([2,23,4],dtype=np.int32)print(a.dtype)# int32a = np.array([2,23,4],dtype=np.float)print(a.dtype)# float64a = np.array([2,23,4],dtype=np.float32)print(a.dtype)# float32 创建特定数据a = np.array([[2,23,4],[2,32,4]]) # 2d 矩阵 2行3列print(a)\"\"\"[[ 2 23 4] [ 2 32 4]]\"\"\" 创建全零数组 a = np.zeros((3,4)) # 数据全为0，3行4列\"\"\"array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]])\"\"\" 创建全一数组, 同时也能指定这些特定数据的 dtype: a = np.ones((3,4),dtype = np.int) # 数据为1，3行4列\"\"\"array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]])\"\"\" 创建全空数组, 其实每个值都是接近于零的数: a = np.empty((3,4)) # 数据为empty，3行4列\"\"\"array([[ 0.00000000e+000, 4.94065646e-324, 9.88131292e-324, 1.48219694e-323], [ 1.97626258e-323, 2.47032823e-323, 2.96439388e-323, 3.45845952e-323], [ 3.95252517e-323, 4.44659081e-323, 4.94065646e-323, 5.43472210e-323]])\"\"\" 用 arange 创建连续数组: a = np.arange(10,20,2) # 10-19 的数据，2步长\"\"\"array([10, 12, 14, 16, 18])\"\"\" 使用 reshape 改变数据的形状 a = np.arange(12).reshape((3,4)) # 3行4列，0到11\"\"\"array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])\"\"\" 用 linspace 创建线段型数据: a = np.linspace(1,10,20) # 开始端1，结束端10，且分割成20个数据，生成线段\"\"\"array([ 1. , 1.47368421, 1.94736842, 2.42105263, 2.89473684, 3.36842105, 3.84210526, 4.31578947, 4.78947368, 5.26315789, 5.73684211, 6.21052632, 6.68421053, 7.15789474, 7.63157895, 8.10526316, 8.57894737, 9.05263158, 9.52631579, 10. ])\"\"\" 同样也能进行 reshape 工作: a = np.linspace(1,10,20).reshape((5,4)) # 更改shape\"\"\"array([[ 1. , 1.47368421, 1.94736842, 2.42105263], [ 2.89473684, 3.36842105, 3.84210526, 4.31578947], [ 4.78947368, 5.26315789, 5.73684211, 6.21052632], [ 6.68421053, 7.15789474, 7.63157895, 8.10526316], [ 8.57894737, 9.05263158, 9.52631579, 10. ]])\"\"\" Numpy 基础运算import numpy as npa=np.array([10,20,30,40]) # array([10, 20, 30, 40])b=np.arange(4) # array([0, 1, 2, 3])c=a-b # array([10, 19, 28, 37]) 矩阵的减法c=a+b # array([10, 21, 32, 43]) 矩阵的加法c=a*b # array([ 0, 20, 60, 120]) 矩阵的乘法c=b**2 # array([0, 1, 4, 9]) 乘方c=10*np.sin(a) # array([-5.44021111, 9.12945251, -9.88031624, 7.4511316 ]) 三角函数print(b&lt;3) # array([ True, True, True, False], dtype=bool) print()逻辑判断 对多行多维度的矩阵进行操作 a=np.array([[1,1],[0,1]])b=np.arange(4).reshape((2,2))print(a)# array([[1, 1],# [0, 1]])print(b)# array([[0, 1],# [2, 3]])c_dot = np.dot(a,b) # 矩阵乘法，即对应行乘对应列得到相应元素# array([[2, 4],# [2, 3]])c_dot_2 = a.dot(b) # 矩阵乘法(另一写法)# array([[2, 4],# [2, 3]]) import numpy as npa=np.random.random((2,4))print(a)# array([[ 0.94692159, 0.20821798, 0.35339414, 0.2805278 ],# [ 0.04836775, 0.04023552, 0.44091941, 0.21665268]])np.sum(a) # 4.4043622002745959 对矩阵中所有元素进行求和np.min(a) # 0.23651223533671784 对矩阵中所有元素寻找最小值np.max(a) # 0.90438450240606416 对矩阵中所有元素寻找最大值a=np.random.random((2,4))print(\"a =\",a)# a = [[ 0.23651224 0.41900661 0.84869417 0.46456022]# [ 0.60771087 0.9043845 0.36603285 0.55746074]]print(\"sum =\",np.sum(a,axis=1)) # 当axis的值为1的时候，将会以行作为查找单元# sum = [ 1.96877324 2.43558896]print(\"min =\",np.min(a,axis=0)) # 当axis的值为0的时候，将会以列作为查找单元# min = [ 0.23651224 0.41900661 0.36603285 0.46456022]print(\"max =\",np.max(a,axis=1))# max = [ 0.84869417 0.9043845 ] 对应元素的索引也是非常重要的 import numpy as npA = np.arange(2,14).reshape((3,4)) # array([[ 2, 3, 4, 5]# [ 6, 7, 8, 9]# [10,11,12,13]]) print(np.argmin(A)) # 0 求矩阵中最小元素的索引print(np.argmax(A)) # 11 求矩阵中最大元素的索引print(np.mean(A)) # 7.5 计算均值 # print(A.mean()) # 7.5print(np.average(A)) # 7.5print(A.median()) # 7.5 求解中位数print(np.cumsum(A)) # [2 5 9 14 20 27 35 44 54 65 77 90] 累加函数print(np.diff(A)) # 累差运算# [[1 1 1]# [1 1 1]# [1 1 1]]print(np.nonzero(A)) # 这个函数将所有非零元素的行与列坐标分割开，重构成两个分别关于行和列的矩阵。# (array([0,0,0,0,1,1,1,1,2,2,2,2]),array([0,1,2,3,0,1,2,3,0,1,2,3]))B = np.array([[0, 11, 22, 33],[11, 0, 22, 33],[11, 22, 0, 33]])print(np.nonzero(B)) #遍历每一个元素，若其非0，则返回其行/列索引 第一(二)个array返回行(列)索引# (array([0, 0, 0, 1, 1, 1, 2, 2, 2]), array([1, 2, 3, 0, 2, 3, 0, 1, 3])) 排序操作 import numpy as npA = np.arange(14,2, -1).reshape((3,4)) # array([[14, 13, 12, 11],# [10, 9, 8, 7],# [ 6, 5, 4, 3]])print(np.sort(A)) # 仅针对每一行进行从小到大排序操作# array([[11,12,13,14]# [ 7, 8, 9,10]# [ 3, 4, 5, 6]]) 矩阵的转置有两种表示方法： print(np.transpose(A)) print(A.T)# array([[14,10, 6]# [13, 9, 5]# [12, 8, 4]# [11, 7, 3]])# array([[14,10, 6]# [13, 9, 5]# [12, 8, 4]# [11, 7, 3]]) 在Numpy中具有clip(Array,Array_min,Array_max)函数：将Array中大于Array_max的元素转换成Array_max，将Array中小于Array_min的元素转换成Array_min print(A)# array([[14,13,12,11]# [10, 9, 8, 7]# [ 6, 5, 4, 3]])print(np.clip(A,5,9)) # array([[ 9, 9, 9, 9]# [ 9, 9, 8, 7]# [ 6, 5, 5, 5]]) Numpy索引https://morvanzhou.github.io/tutorials/data-manipulation/np-pd/2-5-np-indexing/","categories":[],"tags":[{"name":"Morvan","slug":"Morvan","permalink":"http://yoursite.com/tags/Morvan/"}]},{"title":"Communication-Technology-Basics-Experiment","slug":"Communication-Technology-Basics-Experiment","date":"2018-12-01T06:43:44.788Z","updated":"2018-12-01T08:02:06.310Z","comments":true,"path":"2018/12/01/Communication-Technology-Basics-Experiment/","link":"","permalink":"http://yoursite.com/2018/12/01/Communication-Technology-Basics-Experiment/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】通信技术基础上机","text":"【阅读时间】XXX min XXX words【阅读内容】通信技术基础上机 基带编码examplexn=[0 1 1 0 1 1 1 0 0 0 0 1];num=0;yn = xn;for i=1:length(xn) if xn(i)==1 num = num+1; yn(i) = yn(i)+ num; endendsubplot(2,1,1);stairs([0:length(xn)-1],xn);axis([0 length(xn) -2 2]);grid onsubplot(2,1,2);stairs([0:length(xn)-1],yn);axis([0 length(xn) -1 8]);grid on Result: AMI码xn=[1 0 1 1 0 0 0 1 1];% 输入单极性码yn=xn;% 输出yn初始化num=0;% 计数器初始化for k=1:length(xn) if xn(k)==1 num=num+1; % \"1\"计数器 if mod(num,2)==1 % 奇数个1时输出-1,进行极性交替 yn(k)=-1; else yn(k)=+1; end endend % 以上部分完成AMI码编码subplot(2,1,1);stairs([0:length(xn)-1],xn);axis([0 length(xn) -2 2]);grid on;subplot(2,1,2);stairs([0:length(xn)-1],yn);axis([0 length(xn) -2 2]);grid on; Result: HDB3xn=[1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0];% 输入单极性码% xn = [1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1];yn=xn;% 输出yn初始化num=0;% 计数器初始化for k=1:length(xn) if xn(k)==1 num=num+1; % \"1\"计数器 if mod(num,2)==1 % 奇数个1时输出-1,进行极性交替 yn(k)=-1; else yn(k)=1; end endend subplot(3,1,1);stairs([0:length(xn)-1],xn);axis([0 length(xn) -2 2]); grid on;subplot(3,1,2);stairs([0:length(xn)-1],yn);axis([0 length(xn) -2 2]); grid on;% HDB3编码 num=0; % 连零计数器初始化 yh=yn; % 输出初始化 sign=0; % 极性标志初始化为0 nonzero=0;for k=1:length(yn) if yn(k)==0 num=num+1; % 连“0”个数计数 if num==4 % 如果4连“0” num=0; % 计数器清零 yh(k)= nonzero; % 让0000的最后一个0改变为与前一个非零符号相同极性的符号 if yh(k)==sign % 如果当前V符号与前一个V符号的极性相同 yh(k)=-1*yh(k); % 则让当前V符号极性反转,以满足V符号间相互极性反转要求 yh(k-3)=yh(k); % 添加B符号,与V符号同极性 yh(k+1:length(yn))=-1*yh(k+1:length(yn)); % 并让后面的非零符号从V符号开始再交替变化 end sign=yh(k); % 记录前一个V符号的极性 end else num=0; % 当前输入为“1”则连“0”计数器清零 nonzero = yn(k); endend % 编码完成subplot(3,1,3);stairs([0:length(xn)-1],yh);axis([0 length(xn) -2 2]); grid on; Result: Manchesterclear allclose allxn=[0 1 0 0 1 1 0 0 0 1 1];t=0:1:2*length(xn)-1;for i=1:length(xn) if(xn(i)==1) %manchester code \"1\" yn(2*i-1)=-1; yn(2*i)=1; else %manchester code \"0\" yn(2*i-1)=1; yn(2*i)=-1; endendsubplot(2,1,1);stairs([0:length(xn)-1],xn);axis([0 length(xn) -2 2]); grid on;subplot(2,1,2);stairs(t,yn);axis([0 length(yn) -2 2]); grid on; Result： 数字调制技术DPSK参数设置Block Parameters DPSK // DPSK_1(PPt) - Bernoulli Binary Generator - Source of initial seed: Parameter - Initial seed: 50 - Output data type: single - Differential Encoder - Initial conditions: 0.05 - Unipolar to Bipolar Converter - M-ary number:2 - Output data type:Same as input - Sine Wave - Amplitude: 2 // 1 - Frequency(rad/sec):4*2*pi // 40*pi - Sample time:1/500 // 1/1000- Analog Filter Design - Design method:Butterworth - Filter order: 8 // 4 - Lower passband edge frequency (rad/s): 2*pi // 2*2*pi - Upper passband edge frequency (rad/s): 10*pi // 7*2*pi- Transport Delay - Time delay: 0.318*pi - Analog Filter Design1 - Design method:Butterworth - Filter type: Lowpass - Filter order: 4 - Passband edge frequency (rad/s): 2*2*pi // 3*2*pi - Switch - Criteria for passing first input: u2 &gt;= Threshold - Threshold: 0.3 - Enable zero crossing detection (√) 2FSK参数设置Block Parameters (文件参数) //(PPT参数) - Analog Filter Design1 - Design method:Butterworth - Filter type: Lowpass - Filter order: 8 - Passband edge frequency (rad/s): 10*pi (与结果图Scope11一致) // 80 - Analog Filter Design2 - Design method:Butterworth - Filter type: Lowpass - Filter order: 8 - Passband edge frequency (rad/s): 20*pi // 20 - Sine Wave3 - Frequency(rad/sec):20*pi // 80*pi - Sample time:1/500 // 1/1000 - Subtract - Icon shape:rectangular // round - List of signs:+- - Switch - Criteria for passing first input: u2 &gt;= Threshold - Threshold: 0 // 0.3(与结果图Scope15一致) - Enable zero crossing detection (√) PCM_DMExampleclear;t=0:0.01:4;a=sin(2*pi*t)+sin(2*pi*5*t); subplot(4,1,1),plot(t,a);title('Original signal');ts=0.05;t=0:ts:4;a=sin(2*pi*t)+sin(2*pi*5*t); subplot(4,1,2),stem(t,a);title('Sampling signal');[sqnr8_u,aquan8_u,code8_u]=u_pcm1(a,8); subplot(4,1,3),stem(t,aquan8_u);title('Uniformly quantized signal');[sqnr8_A,aquan8_A,code8_A]=A_pcm1(a,8); axis([0 4,-2 2]);subplot(4,1,4),stem(t,aquan8_A);title('A-law quantized signal'); Result: u_pcm1function [sqnr,aq,code]=u_pcm1(a,n) amax=max(a);amin=min(a);delta=(amax-amin)/n; for i=1:n+1 m(i)=amin+(i-1)*delta;end%%量化间隔for i=1:n q(i)=(m(i)+m(i+1))/2;end %量化值的计算 for i=1:n index=find((q(i)-delta/2 &lt;= a) &amp; (a &lt;= q(i)+delta/2)); %%找到处于某个量化间隔的所有抽样点 aq(index)=q(i).*ones(1,length(index)); %%利用qi作为该量化间隔的抽样值的量化值 q_index(find((aq==q(i))))=(i-1).*ones(1,length(find(aq==q(i)))); %%得到量化索引end% %PCM编码——二进制编码code=dec2bin(q_index); %SQNR的计算sqnr=20*log10(norm(a)/norm(a-aq)); %norm(a)求a的均方根值 A_pcm1function [sqnr,aq,code]=A_pcm1(x,n) A=87.6;[amax,amin,y]=A_compress(x,A);[sqnr,y_q,code]=u_pcm1(y,n);aq=A_expand(y,A);aq=aq*(amax-amin)/2; %SQNR的计算sqnr=20*log10(norm(x)/norm(x-aq)); %norm(a)求a的均方根值 A_compressfunction [amax,amin,y] = A_compress(x,A)amax=max(x);amin=min(x);x=2*x/(amax-amin);y=zeros(1,length(x));%%A律压缩for i=1:length(x) if abs(x(i))&lt;=1/A y(i)=sign(x(i))*A*abs(x(i))/(1+log(A)); else y(i)=sign(x(i))*(1+log(A*abs(x(i))))/(1+log(A)); endend A_expandfunction x=A_expand(y,A)for i=1:length(y) if abs(y(i))&lt;=1/(1+log(A)); x(i)=sign(y(i))*(1+log(A))/A; else x(i)=sign(y(i)).*exp(abs(y(i))*(1+log(A))-1)/A; endend DM1 % ch6example13prog1.m clc;clear allTs=1e-3; %采样间隔t=0:Ts:20*1e-3; %仿真时间序列x=sin(2*pi*50*t)+0.5*sin(2*pi*150*t); %信号delta=0.7; %量化阶距D(1+length(t))=0; %预测器初始状态for k=1:length(t) e(k)=x(k)-D(k); %误差信号 if e(k)&gt;=0 e_q(k)=delta; else e_q(k)=-delta; end %量化器输出 D(k+1)=e_q(k)+D(k); %预测器输出 codeout(k)=(e_q(k)&gt;0); %编码输出endsubplot(3,1,1);plot(t,x,'-o');axis([0 20*1e-3,-2 2]);hold on;subplot(3,1,2);stairs(t,codeout);axis([0 20*1e-3,-2 2]); %解码端Dr(1+length(t))=0; %解码端预测器初始状态for k=1:length(t) if codeout(k)==0 eq(k)=-delta; else eq(k)=delta; end xr(k)=eq(k)+Dr(k); Dr(k+1)=xr(k); %延迟器状态更新endsubplot(3,1,3);stairs(t,xr);hold on; %解码输出subplot(3,1,3);plot(t,x); %原信号 Result： 差错控制CRC16% CRC 编码主程序clear;clc;close all;uncode_sequence=[1 0 0 1 1 0 1 1 0 0 1]sequence_length = length(uncode_sequence); % 得到原始信号长度crc_ccitt=[1 0 0 1 1];crc_length=length(crc_ccitt)-1;add_bit = zeros(1,crc_length); % 添加冗余比特位crc_coded_sequence = [uncode_sequence add_bit]; % 初始化输出检错码序列remainder_bits = [uncode_sequence add_bit]; % 初始化余数数组for k = 1:sequence_length % 开始循环计算长除得到最终余数 add_zeros = zeros(1,sequence_length-k); % 加入冗余位参与模2运算 register_bits = [crc_ccitt add_zeros]; % 构造除数数组 if remainder_bits(1) == 0 % 被除数第一位为0则将除数所有位置0 register_bits = zeros(1,length(register_bits)); end remainder_bits = bitxor(register_bits,remainder_bits); % 将除数与被除数进行异或操作register_bits = crc_ccitt; % 将寄存器恢复为除数数组remainder_bits(1) = []; % 去除模2后得到的被除数的第1位endcrc_coded_sequence = [uncode_sequence remainder_bits] % 生成余数序列的冗余位以叠加到编码序列%%CRC解码error=randint(1,length(crc_coded_sequence));%%信道误码% error=round(1*rand(1,length(crc_coded_sequence)));%%信道误码 %若matlab版本不支持randint()函数，则以此行替换crc_coded_sequence=bitxor(crc_coded_sequence,error);%%接收码组 sequence_length = length(crc_coded_sequence); % 得到编码的长度original_sequence = crc_coded_sequence; % 初始化输出序列 crc_ccitt=[1 0 0 1 1]; remainder_bits = crc_coded_sequence; % 初始化余数数组 cycle_length = sequence_length-length(crc_ccitt)+1; % 计算长除法的循环周期 for k = 1:cycle_length % 开始循环计算长除得到最终余数 add_zeros = zeros(1,cycle_length-k); % 加入冗余位参与模2运算 register_bits = [crc_ccitt add_zeros]; % 构造除数数组 if remainder_bits(1) == 0 % 被除数第一位为0则将除数所有位置0 register_bits = zeros(1,length(register_bits)); end remainder_bits = bitxor(register_bits,remainder_bits);% 将除数与被除数进行异或操作 register_bits = crc_ccitt; % 将寄存器恢复为除数数组 remainder_bits(1) = []; % 去除模2后得到的被除数的第1位 end if sum(remainder_bits) == 0 % 传输码元中没有发生个错误 original_sequence = crc_coded_sequence(1:cycle_length) else err = 1 % 码元传输发生错误 end Result： uncode_sequence = 1 0 0 1 1 0 1 1 0 0 1crc_coded_sequence = 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1err = 1 若注释掉”信道误码”和”接受码组”，Result： uncode_sequence = 1 0 0 1 1 0 1 1 0 0 1crc_coded_sequence = 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1original_sequence = 1 0 0 1 1 0 1 1 0 0 1 汉明码clc;clear all;K=4;N=7;msg=randint(1,K) %%生成随机信息位 % msg=round(1*rand(1,K)) %%生成随机信息位 %若matlab版本不支持randint()函数，则以此行替换[H,G] = hammgen(N-K) %%生成汉明码的生成矩阵和校验矩阵code=encode(msg,N,K,'linear/binary',G) %%汉明码编码noise=[0 1 0 0 0 0 0];code_noise=bitxor(code,noise)rcv=decode(code_noise,N,K,'linear/binary',G) Result： msg = 1 0 0 1H = 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1G = 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1code = 0 1 1 1 0 0 1code_noise = 0 0 1 1 0 0 1rcv = 1 0 0 1 # msg 与 rcv 保持一致即可","categories":[],"tags":[]},{"title":"ADNI模态数据概念整理","slug":"ADNI模态数据概念整理","date":"2018-11-30T13:32:50.000Z","updated":"2018-11-30T13:57:42.874Z","comments":true,"path":"2018/11/30/ADNI模态数据概念整理/","link":"","permalink":"http://yoursite.com/2018/11/30/ADNI模态数据概念整理/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 最近需要了解ADNI（Alzheimer’s Disease Neuroimaging Initiative）数据集，刚在网站上注册和提交了申请（审核通过了才能下载数据集），审核时间大概是一周。暂时无法查看数据集内容，亦无法下载。无奈只好在再次回顾ADNI Data Type 相关说明。同时从同学处获取部分Nifti离线文件，并进行python读取文件数据。 Data TypesClinical DataADNI临床数据集包括关于每个受试者的临床信息，包括招募，人口统计学，身体检查和认知评估数据。可以将整套临床数据作为逗号分隔值（CSV）文件批量下载。 Demographics 人口统计学、Neurological Exam 神经系统检查、Screening Labs 筛选实验室 、Vital Signs 生命体征、Cognitive Assessments 认知评估、Biospecimen Collections 生物样本收集、 Medications 药物、Diagnostic Summary 诊断摘要、Lumbar Puncture 腰椎穿刺 Screening 筛选、Baseline 基线 、Month 3 、Month 6 、Month 12 、 Month 18 、Month 24 、Month36 、Month48 、Ongoing Annual Follow-up 当前进行的年度跟进 Gennetic Data遗传因素在阿尔茨海默病中起重要作用。全基因组关联研究（GWAS）采用标记之间关联的测试，称为单核苷酸多态性（SNP）和感兴趣的表型。来自病例对照GWAS和其他类型的遗传关联研究的发现可以提供用于检查源自ADNI成像和其他生物标志物数据集的定量表型的目标。 APOE的4等位基因是已知的AD最强大的遗传风险因素，如果拥有一个4等位基因的人患AD的风险增加了2- 3倍，那么如果有两个等位基因的人患AD的风险增加了12倍。 MR Image DataMRI – 核磁共振成像是根据有磁矩的原子核在磁场作用下，能产生能级间的跃迁的原理采用的技术。MRI对脑内低度星形胶质细胞瘤、神经节、神经胶质瘤、动静脉畸形和血肿等的诊断确认率极高。MRI能清楚地显示癫痫患者的脑萎缩，对脑实质和脑脊液的显示度极好。 原始，预处理和后处理图像文件，FMRI和DTI 这些图像的收集对于满足ADNI开发生物标记物以追踪阿尔茨海默病的进展和潜在病理学变化的目标至关重要。 该项目将收集MRI（结构，扩散加权成像，灌注和静息状态序列）; 使用florbetapir F18（florbetapir）或florbetaben F18（florbetaben）的淀粉样蛋白PET; 18F-FDG-PET（FDG-PET）; CSF用于Aβ，tau，磷酸化tau（AKA磷酸化酶）和其他蛋白质; AV-1451 PET; 和遗传和尸检数据，以确定这些生物标志物与基线临床状态和认知下降的关系。 PET Image Data正电子发射计算机断层扫描的大致方法是，将某种物质，一般是生物生命代谢中必须的物质，如：葡萄糖、蛋白质、核酸、脂肪酸，标记上短寿命的放射性核素（如18F，11C等），注入人体后，通过对于该物质在代谢中的聚集，来反映生命代谢活动的情况，从而达到诊断的目的。 其中：18F-FDG是指氟代脱氧葡萄糖，其完整的化学名称为2-氟-2-脱氧-D-葡萄糖，通常简称为FDG。葡萄糖是人体三大能源物质之一，将可以被PET探测并形成影像的的正电子核素18F标记在葡萄糖上。 原始，预处理和后处理图像文件，PIB（ADNI1），FDG（ADNI1 / GO / 2），FLORBETAPIR（ADNI GO / 2/3），FLORBETABEN（ADNI3）和TAU IMAGING（ADNI3）这些图像的收集对于满足ADNI开发生物标记物以追踪阿尔茨海默病的进展和潜在病理学变化的目标至关重要。 an overview of the PET data collected throughout the ADNI study AVAILABLE IMAGE DATA Biospecimen DataADNI的目标之一是收集参与者的血液，尿液和脑脊液（CSF）等生物样本(Biospecimen Data)。鼓励有兴趣的调查员，无论是否与ADNI网站相关联，都可以申请使用这种有限的资源。但是，除非初步数据显示出明显优越的性能，否则不建议将ADNI样本用于技术开发或不同技术之间的比较。 此外，adni生物标记核心所执行的几项分析结果将在数据存档中提供如下： - Homocysteine- Species of isoprostanes- CSF tau, sAPPβ levels, BACE levels, and enzyme activity- Plasma Aβ 40 and Aβ 42- Other promising CSF and plasma based on ongoing multiplex immunoassay studies and mass - spectrometry MRM studies python 读取nifti数据 使用nifti数据 23.6 MB import nibabel as nibimport numpy as npimport matplotlib.pyplot as plt# show the nii_data.shapenii_file = \"ADNI_011_S_0010_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061208114538147_S8800_I32270.nii\"data = nib.load(nii_file)img = data.get_fdata()img = np.squeeze(img)print(\"img.shape\",img.shape) # img.shape (192, 192, 160)print(\"data.shape\",data.shape) # data.shape (192, 192, 160)print(\"data.affine.shape\",data.affine.shape) # data.affine.shape (4, 4)# print(data.header) #数据头信息 输出信息于Result列出# 获取slice信息生成图像# 把slice数据生成图片的方法def show_img(slices): fig, axes = plt.subplots(1, len(slices)) for i, slice in enumerate(slices): axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")#读取nifti文件中的slice数据data = nib.load(nii_file)img = data.get_fdata()#获取单张slice数据slice_0 = img[26, :, :]slice_1 = img[:, 30, :]slice_2 = img[:, :, 16]#生成图表show_img([slice_0, slice_1, slice_2])plt.suptitle(\"show slice image\")plt.show() Result： img.shape (192, 192, 160)data.shape (192, 192, 160)data.affine.shape (4, 4)&lt;class 'nibabel.nifti1.Nifti1Header'&gt; object, endian='&gt;'sizeof_hdr : 348data_type : b''db_name : b'011_S_0010'extents : 0session_error : 0regular : b'r'dim_info : 0dim : [ 3 192 192 160 1 1 1 1]intent_p1 : 0.0intent_p2 : 0.0intent_p3 : 0.0intent_code : nonedatatype : float32bitpix : 32slice_start : 0pixdim : [1. 1.2447063 1.2507237 1.2010667 1. 1. 1. 1. ]vox_offset : 0.0scl_slope : nanscl_inter : nanslice_end : 0slice_code : unknownxyzt_units : 2cal_max : 0.0cal_min : 0.0slice_duration : 0.0toffset : 0.0glmax : 1721glmin : 0descrip : b'MPR; GradWarp; B1 Correction; N3; Scaled'aux_file : b'none'qform_code : scannersform_code : unknownquatern_b : 0.70710677quatern_c : -1.0713779e-09quatern_d : -0.70710677qoffset_x : 94.87749qoffset_y : 165.8339qoffset_z : 115.27711srow_x : [0. 0. 0. 0.]srow_y : [0. 0. 0. 0.]srow_z : [0. 0. 0. 0.]intent_name : b''magic : b'n+1' slice","categories":[],"tags":[]},{"title":"VGG_Introduction","slug":"VGG_Introduction","date":"2018-11-30T07:13:36.000Z","updated":"2018-11-30T09:12:26.958Z","comments":true,"path":"2018/11/30/VGG_Introduction/","link":"","permalink":"http://yoursite.com/2018/11/30/VGG_Introduction/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 论文出处：Very Deep Convolutional Networks for Large-Scale Image Recognition 参考链接：一文读懂VGG网络\\VGG模型论文译文(上)(下) 简述原理VGG16相比AlexNet的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）。对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。 简单来说，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。 比如，3个步长为1的3x3卷积核的一层层叠加作用可看成一个大小为7的感受野（其实就表示3个3x3连续卷积相当于一个7x7卷积），其参数总量为 $3\\times(3^2C^2)= 27C^2$，如果直接使用7x7卷积核，其参数总量为 $7^2C^2 = 49C^2$，这里 C 指的是输入和输出的通道数。很明显，$27C^2$小于$49C^2$，即减少了参数；而且3x3卷积核有利于更好地保持图像性质。 这里解释一下为什么使用2个3x3卷积核可以来代替5*5卷积核： 5x5卷积看做一个小的全连接网络在5x5区域滑动，我们可以先用一个3x3的卷积滤波器卷积，然后再用一个全连接层连接这个3x3卷积输出，这个全连接层我们也可以看做一个3x3卷积层。这样我们就可以用两个3x3卷积级联（叠加）起来代替一个 5x5卷积。 Mini-network replacing the 5x5 convolutions VGG优缺点VGG优点 VGGNet的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。 几个小滤波器（3x3）卷积层的组合比一个大滤波器（5x5或7x7）卷积层好： 验证了通过不断加深网络结构可以提升性能。 VGG缺点 VGG耗费更多计算资源，并且使用了更多的参数（这里不是3x3卷积的锅），导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。VGG可是有3个全连接层啊！ PS：有的文章称：发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。 注：很多pretrained的方法就是使用VGG的model（主要是16和19），VGG相对其他的方法，参数空间很大，最终的model有500多m，AlexNet只有200m，GoogLeNet更少，所以train一个vgg模型通常要花费更长的时间，所幸有公开的pretrained model让我们很方便的使用。 以下是论文的详细介绍，深入了解细节，有助于对其进行实现。 摘要在本文中，我们研究了大规模图像识别任务下卷积网络深度对其预测准确率的影响。 我们的主要贡献是使用具有非常小的（3×3）卷积滤波器的架构对深度不断递增的网络进行全面评估，结果表明通过将权重层深度推到16-19层可以在现有技术配置下（使准确率）实现显著提升。 介绍在本文中，我们解决了ConvNet架构设计的另一个重要方面 - 它的深度。 为此，我们修复了架构的其他参数，并通过添加更多卷积层来稳定增加网络深度，由于在所有层中都使用了非常小的（3×3）卷积滤波器，这是可行的。 卷积网络配置架构在训练期间，我们ConvNets的输入是固定尺寸的224×224 RGB图像。我们所做的唯一预处理是从每个像素中减去在训练集上计算的RGB均值。图像通过一叠卷积层，我们使用了感受野非常小的卷积核：3×3（这是左/右，上/下，中心点概念可捕获的最小尺寸）。在其中一种配置中，我们还使用1×1卷积滤波器，这可以看作是输入通道的线性变换（随后是非线性）。卷积步长固定为1个像素；卷积层的空间填充是指使得在卷积操作后保留原空间的分辨率，比如使用3×3 卷积核，就填充1个像素。空间池化是由五个最大池化层完成的，每个池化层前面都会有若干个卷积层（并非所有的卷积层后都使用最大池化层）。 最大池化是以2×2像素窗口上执行，步幅为2。 一堆卷积层（在不同的体系结构中具有不同的深度）之后是三个完全连接（FC）层：前两个具有4096个通道，第三个执行1000路ILSVRC分类，因此包含1000个通道（一个 为每个类）。 最后一层是soft-max层。 全连接层的配置在所有网络中都是相同的。 所有隐藏层都配备了ReLU激活函数。 我们注意到我们的网络（除了一个网络）都没有包含局部响应归一化层（LRN）标准化（Krizhevsky et al。，2012）。 如第4部分所示，这种标准化不会提升ILSVRC数据集的性能，但会导致内存消耗和计算时间的增加。 在使用的情况下，LRN层的参数是（Krizhevsky et al。，2012）的参数 配置 本文中评估的ConvNet配置在表1中列出，每列一个。 我们将以他们的名字（A-E）来提及。 所有的配置都遵循2.1节中提到的通用设计，并且仅在深度上有所不同：从网络A中的11个权重层（8个卷积层和3个全连接层）到网络E中的19个权重层（16个卷积层和3个全连接层）。卷积层的宽度（通道数量）相当小，从第一层的64开始，然后在每个最大池层后增加1倍，直到达到512。在表2中，我们报告了每个配置的参数数目。 尽管深度很大，但我们网络的权重数量不会超过那些深度较浅、但卷积核和感受野宽度更大的网络。 VGG16包含了16个隐藏层（13个卷积层和3个全连接层），如上图中的D列所示 VGG19包含了19个隐藏层（16个卷积层和3个全连接层），如上图中的E列所示 VGG网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的max pooling。 讨论我们整体都使用了非常小的3x3卷积核配合步幅1。显而易见的是，用两层的3x3卷积层组合（中间不包含池化层）所得到的感受野相当于一层的5x5卷积层的感受野；而三层这样的卷积层组合所得到的感受野相当于一层的7x7卷积核的感受野。那么，如果我们用三层3x3的卷积层组合来代替一层7x7卷积层，我们会得到什么呢？首先，我们并入了三个ReLU激活函数，而不是一个，这使决策功能的分辨力更强。 其次，我们减少参数的数量：假设三层3×3卷积层相叠的输入和输出都具有C个通道，则该叠层参数化为$3\\times(3^2C^2)= 27C^2$个权重; 同时，一个7×7 卷积层需要$7^2C^2 = 49C^2$ 参数，参数增加81％。 这可以被看作是在7×7卷积中实施正规化， 迫使他们通过3×3卷积核进行分解（两者之间注入非线性）。 纳入1×1卷积核（配置C，表1）是一种增加决策函数的非线性而不影响卷积层感受野的方法。 Goodfellow等人（2014）将深度ConvNets（11个权重层）应用于街道号识别任务，并表明增加深度能获得更好的性能。 分类框架在本节中，我们将介绍ConvNet培训和评估的分类细节。 训练ConvNet的训练过程基本上参照Krizhevsky等人（2012）（除了从多尺度训练图像中采集输入裁剪图像，如后文所述）。也就是说，训练是通过使用小批量梯度下降（基于反向传播（LeCun et al。，1989））的动量优化多项逻辑回归目标来实现的。 批量大小设置为256，动量为0.9。 训练通过权值衰减（L2惩罚系数设置为 $5 · 10^{−4}$ ）和前两个完全连接层（dropout设置为0.5）的dropout正则化来调整。 学习率最初设置为$10^{−2}$ ，然后在验证集精度停止改进时再降低10倍。 总的来说，学习率一共降低了3次，并且在370K个迭代（74代）后停止了学习。 我们推测，尽管与（Krizhevsky et al.，2012）相比，网络的参数数量更多，网络深度也更大，但能用更少的迭代次数来实现收敛，由于：（a）更大深度和更小卷积核所带来的隐式正则化；（b）某些图层的预初始化。 网络权重的初始化很重要，因为由于深度网络中的梯度不稳定，初始化不好可能会导致学习停滞。 为了避免这个问题，我们从训练配置A（表1）开始，这个网络足够浅，可以随机初始化进行训练。 然后，当训练更深的体系结构时，我们使用了网络A的权值来初始化了前四个卷积层和最后三个完全连接的层，（中间层随机初始化）。 我们没有降低预初始化图层的学习速率，允许它们在学习期间改变。 对于随机初始化（如有），我们从具有零均值和 $10^{−2}$ 方差的正态分布采样权重。 偏差初始化为零。 值得注意的是，在提交论文后，我们发现可以使用Glorot＆Bengio（2010）的随机初始化程序在没有预先训练的情况下初始化权重。 为了获得224×224固定大小的 ConvNet输入图像，他们从重新缩放的训练图像中随机裁剪（每个SGD迭代每个图像裁剪一次）。 为了进一步增强训练集，被裁剪的图像经过随机水平翻转和随机RGB颜色偏移处理（Krizhevsky et al.，2012）。 下面将介绍训练图像缩放。 训练图像尺寸。 设S(the smallest side)是等比例缩放的训练图像的最小边，ConvNet基于这些图像的裁剪作为输入（我们也称S为训练尺度）。 虽然裁剪大小固定为224×224，但原则上S可以取不小于224的任何值：对于S = 224，裁剪图将捕获整幅图像统计数据，完全跨越训练图像的最小边; 对于S&gt;&gt;224，裁剪图将对应于图像的一小部分，包含一个小物体或一个物体部分。 我们考虑设定训练尺度S的两种方法。第一种方法是固定S，这对应于单尺度训练（注意采样作物中的图像内容仍然可以表示多尺度图像统计）。 在我们的实验中，我们评估了以两个固定尺度训练的模型：S = 256（已被广泛用于现有技术（Krizhevsky等，2012; Zeiler＆Fergus，2013; Sermanet等，2014））和S = 384。给定一个ConvNet配置，我们首先使用S = 256来训练网络。为了加速S = 384网络的训练，它被初始化为具有S = 256的预训练权重，并且我们使用较小的学习率初始值为 $10^{−3}$ 。 设定S的第二种方法是多尺度训练，其中通过从特定范围[Smin，Smax]（我们使用Smin = 256和Smax = 512）随机采样S来单独重新调整每个训练图像。 由于图像中的物体可能具有不同的大小，因此在训练时考虑到这一点是有益的。 这也可以看作是通过缩放抖动来增强训练集，其中单个模型被训练以识别多种类别的物体。 出于速度的原因，我们通过对具有相同配置的单尺度模型的所有层进行微调来训练多尺度模型，并使用固定的S = 384进行预训练。 测试在测试时，给定一个训练有素的ConvNet和一个输入图像，它按以下方式分类。首先，将其等比例缩放到预定义的最小边，表示为Q（我们也将其称为测试尺度）。我们注意到，Q不一定等于训练尺度S（如我们将在第4部分中所示，对每个S使用几个Q值可获得性能改进）。然后，网络以类似于（Sermanet等人，2014）的方式被密集地应用在重新缩放的测试图像上。也就是说，完全连接的层首先被转换成卷积层（第一个FC层转为7×7的卷积层，后两个FC层转为1×1 卷积层）。然后将所得的全卷积网络应用于整个（未裁剪的）图像。其结果是一个类别得分映射，其类别数等于任务的目标分类数，以及一个可变的空间分辨率，取决于输入图像的大小。最后，为了获得固定大小的图像类别分数的向量，类别得分映射会被空间平均（加总池化）。我们还通过水平翻转图像来增强测试集；对原始图像和翻转图像的softmax分类概率进行平均以获得图像的最终分数。 由于卷积边界条件不同，多裁剪图像评估与密集评估是互补的：将ConvNet应用于裁剪图像时，卷积后的特征映射用零填充，而在密集评估的情况下，同一裁切图像的填充天然地来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获更多的上下文信息。 实现细节实现源自公开发布的C ++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含许多重大修改，使得我们能在装有多个GPU的单系统中执行训练和评估，以及能够对多种规模的全尺寸（未裁剪）图像（如上所述）进行训练和评估。与使用单个GPU相比，我们概念更简单的方案在现成的4 GPU系统上已经提供了3.75倍的加速。在配备四个NVIDIA Titan Black GPU的系统上，根据架构的不同，训练一个网络需要2-3周的时间。 分类实验该数据集包括1000种分类的图像，并且被分成三组：训练集（1.3M张图像），验证集（50K张图像）和测试集（100K张标签被去除的图像）。 单尺度评估 虽然额外的非线性确实有帮助（C比B好），但使用感受野范围不少的卷积核（D比C好）捕获空间上下文也很重要。 带有小型卷积核的深网优于具有更大卷积核的浅网。 通过尺度抖动来增强训练集确实有助于捕获多尺度图像统计信息。 多尺度评估 多裁切图像评估 多裁切图像与密集评估，这两种方法确实是互补，因为它们的组合优于其中的每一种。 卷积网络融合 与当前最先进的技术相比较 结论在这项工作中，我们评估了用于大规模图像分类的深层卷积网络（多达19个权值层）。 已经证明，表示层的深度有利于分类准确性，并且通过大幅增加网络深度便可以使用传统的ConvNet架构来实现ImageNet挑战数据集上的最新性能（LeCun等，1989; Krizhevsky等， 2012）。 在附录中，我们还展示了我们的模型能很好地泛化应用于其他的任务和数据集，不亚于甚至性能优于那些深度略浅、更复杂的识别流水线。 我们的结果再一次证实了视觉表示中深度的重要性。","categories":[],"tags":[]},{"title":"MachineLearningInAction_Code","slug":"MachineLearningInAction-Code","date":"2018-11-26T09:54:11.000Z","updated":"2018-11-27T11:23:20.893Z","comments":true,"path":"2018/11/26/MachineLearningInAction-Code/","link":"","permalink":"http://yoursite.com/2018/11/26/MachineLearningInAction-Code/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… Chapter13# datArr = [map(float,line) for line in stringArr] #only support python2.xdatArr = [[float(x) for x in line] for line in stringArr] #support python3.x # 在shell里输入python指令so stupid !! Please use IDE,example 'PyCharm' &gt;&gt;&gt; import pca&gt;&gt;&gt; dataMat = pca.loadDataSet('testSet.txt')&gt;&gt;&gt; lowDMat,reconMat=pca.pca(dataMat,1)# &gt;&gt;&gt; shape(lowDMat) # support python2.x&gt;&gt;&gt; lowDMat.shape(1000, 1) # (1000, 1)&gt;&gt;&gt; reconMat.shape(1000, 2)&gt;&gt;&gt; dataMat.shape(1000, 2)&gt;&gt;&gt; lowDMat,reconMat=pca.pca(dataMat,2)&gt;&gt;&gt; lowDMat.shape(1000, 2) # (1000, 2)&gt;&gt;&gt; reconMat.shape(1000, 2)&gt;&gt;&gt; dataMat.shape(1000, 2)&gt;&gt;&gt; lowDMat,reconMat=pca.pca(dataMat,3)&gt;&gt;&gt; lowDMat.shape(1000, 2) # (1000, 2)&gt;&gt;&gt; reconMat.shape(1000, 2)&gt;&gt;&gt; dataMat.shape(1000, 2)... ...# 绘制降维后的数据reconMat和原始数据dataMat&gt;&gt;&gt; import matplotlib&gt;&gt;&gt; import matplotlib.pyplot as plt&gt;&gt;&gt; fig = plt.figure()&gt;&gt;&gt; ax = fig.add_subplot(111)&gt;&gt;&gt; ax.scatter(dataMat[:,0].flatten().A[0], dataMat[:,1].flatten().A[0],marker='^',s=90)&lt;matplotlib.collections.PathCollection object at 0x11a142160&gt;&gt;&gt;&gt; ax.scatter(reconMat[:,0].flatten().A[0], reconMat[:,1].flatten().A[0],marker='o',s=50,c='r')&lt;matplotlib.collections.PathCollection object at 0x11a1424e0&gt;&gt;&gt;&gt; plt.show()&gt;&gt;&gt; lowDMat, reconMat = pca.pca(dataMat, 2)&gt;&gt;&gt; lowDMatmatrix([[-2.51033597, 0.15840394], [-2.86915379, 0.5092619 ], [ 0.09741085, -0.20728318], ..., [-0.50166225, -0.62056456], [-0.05898712, -0.02335614], [-0.18978714, -1.37276015]])&gt;&gt;&gt; reconMatmatrix([[10.235186, 11.321997], [10.122339, 11.810993], [ 9.190236, 8.904943], ..., [ 9.854922, 9.201393], [ 9.11458 , 9.134215], [10.334899, 8.543604]])&gt;&gt;&gt; dataMatmatrix([[10.235186, 11.321997], [10.122339, 11.810993], [ 9.190236, 8.904943], ..., [ 9.854922, 9.201393], [ 9.11458 , 9.134215], [10.334899, 8.543604]])&gt;&gt;&gt;# 没有剔除任何特征，那么重构之后的数据reconMat会和原始的数据dataMat重合 &gt;&gt;&gt; import pca&gt;&gt;&gt; from numpy import *&gt;&gt;&gt; dataMat = pca.replaceNanWithMean()&gt;&gt;&gt; meanVals = mean(dataMat, axis = 0) # np.mean(dataMat, axis = 0)&gt;&gt;&gt; meanRemoved = dataMat - meanVals&gt;&gt;&gt; covMat = cov(meanRemoved, rowvar = 0) # covMat = np.cov(meanRemoved, rowvar = 0)&gt;&gt;&gt; eigVals,eigVects = linalg.eig(mat(covMat)) # np.linalg.eig(np.mat(covMat))&gt;&gt;&gt; eigValsarray([ 5.34151979e+07, 2.17466719e+07, 8.24837662e+06, 2.07388086e+06, 1.31540439e+06, 4.67693557e+05, 2.90863555e+05, 2.83668601e+05, 2.37155830e+05, 2.08513836e+05, 1.96098849e+05, 1.86856549e+05, 1.52422354e+05, 1.13215032e+05, 1.08493848e+05, 1.02849533e+05, 1.00166164e+05, 8.33473762e+04, 8.15850591e+04, 7.76560524e+04, 6.66060410e+04, 6.52620058e+04, 5.96776503e+04, 5.16269933e+04, 5.03324580e+04, 4.54661746e+04, 4.41914029e+04, 4.15532551e+04, 3.55294040e+04, 3.31436743e+04, 2.67385181e+04, 1.47123429e+04, 1.44089194e+04, 1.09321187e+04, 1.04841308e+04, 9.48876548e+03, 8.34665462e+03, 7.22765535e+03, 5.34196392e+03, 4.95614671e+03, 4.23060022e+03, 4.10673182e+03, 3.41199406e+03, 3.24193522e+03, 2.74523635e+03, 2.35027999e+03, 2.16835314e+03, 1.86414157e+03, 1.76741826e+03, 1.70492093e+03, 1.66199683e+03, 1.53948465e+03, 1.33096008e+03, 1.25591691e+03, 1.15509389e+03, 1.12410108e+03, 1.03213798e+03, 1.00972093e+03, 9.50542179e+02, 9.09791361e+02, 8.32001551e+02, 8.08898242e+02, 7.37343627e+02, 6.87596830e+02, 5.64452104e+02, 5.51812250e+02, 5.37209115e+02, 4.93029995e+02, 4.13720573e+02, 3.90222119e+02, 3.37288784e+02, 3.27558605e+02, 3.08869553e+02, 2.46285839e+02, 2.28893093e+02, 1.96447852e+02, 1.75559820e+02, 1.65795169e+02, 1.56428052e+02, 1.39671194e+02, 1.28662864e+02, 1.15624070e+02, 1.10318239e+02, 1.08663541e+02, 1.00695416e+02, 9.80687852e+01, 8.34968275e+01, 7.53025397e+01, 6.89260158e+01, 6.67786503e+01, 6.09412873e+01, 5.30974002e+01, 4.71797825e+01, 4.50701108e+01, 4.41349593e+01, 4.03313416e+01, 3.95741636e+01, 3.74000035e+01, 3.44211326e+01, 3.30031584e+01, 3.03317756e+01, 2.88994580e+01, 2.76478754e+01, 2.57708695e+01, 2.44506430e+01, 2.31640106e+01, 2.26956957e+01, 2.16925102e+01, 2.10114869e+01, 2.00984697e+01, 1.86489543e+01, 1.83733216e+01, 1.72517802e+01, 1.60481189e+01, 1.54406997e+01, 1.48356499e+01, 1.44273357e+01, 1.42318192e+01, 1.35592064e+01, 1.30696836e+01, 1.28193512e+01, 1.22093626e+01, 1.15228376e+01, 1.12141738e+01, 1.02585936e+01, 9.86906139e+00, 9.58794460e+00, 9.41686288e+00, 9.20276340e+00, 8.63791398e+00, 8.20622561e+00, 8.01020114e+00, 7.53391290e+00, 7.33168361e+00, 7.09960245e+00, 7.02149364e+00, 6.76557324e+00, 6.34504733e+00, 6.01919292e+00, 5.81680918e+00, 5.44653788e+00, 5.12338463e+00, 4.79593185e+00, 4.47851795e+00, 4.50369987e+00, 4.27479386e+00, 3.89124198e+00, 3.56466892e+00, 3.32248982e+00, 2.97665360e+00, 2.61425544e+00, 2.31802829e+00, 2.17171124e+00, 1.99239284e+00, 1.96616566e+00, 1.88149281e+00, 1.79228288e+00, 1.71378363e+00, 1.68028783e+00, 1.60686268e+00, 1.47158244e+00, 1.40656712e+00, 1.37808906e+00, 1.27967672e+00, 1.22803716e+00, 1.18531109e+00, 9.38857180e-01, 9.18222054e-01, 8.26265393e-01, 7.96585842e-01, 7.74597255e-01, 7.14002770e-01, 6.79457797e-01, 6.37928310e-01, 6.24646758e-01, 5.34605353e-01, 4.60658687e-01, 4.24265893e-01, 4.08634622e-01, 3.70321764e-01, 3.67016386e-01, 3.35858033e-01, 3.29780397e-01, 2.94348753e-01, 2.84154176e-01, 2.72703994e-01, 2.63265991e-01, 2.45227786e-01, 2.25805135e-01, 2.22331919e-01, 2.13514673e-01, 1.93961935e-01, 1.91647269e-01, 1.83668491e-01, 1.82518017e-01, 1.65310922e-01, 1.57447909e-01, 1.51263974e-01, 1.39427297e-01, 1.32638882e-01, 1.28000027e-01, 1.13559952e-01, 1.12576237e-01, 1.08809771e-01, 1.07136355e-01, 8.60839655e-02, 8.50467792e-02, 8.29254355e-02, 7.03701660e-02, 6.44475619e-02, 6.09866327e-02, 6.05709478e-02, 5.93963958e-02, 5.22163549e-02, 4.92729703e-02, 4.80022983e-02, 4.51487439e-02, 4.30180504e-02, 4.13368324e-02, 4.03281604e-02, 3.91576587e-02, 3.54198873e-02, 3.31199510e-02, 3.13547234e-02, 3.07226509e-02, 2.98354196e-02, 2.81949091e-02, 2.49158051e-02, 2.36374781e-02, 2.28360210e-02, 2.19602047e-02, 2.00166957e-02, 1.86597535e-02, 1.80415918e-02, 1.72261012e-02, 1.60703860e-02, 1.49566735e-02, 1.40165444e-02, 1.31296856e-02, 1.21358005e-02, 1.07166503e-02, 1.01045695e-02, 9.76055340e-03, 9.16740926e-03, 8.78108857e-03, 8.67465278e-03, 8.30918514e-03, 8.05104488e-03, 7.56152126e-03, 7.31508852e-03, 7.26347037e-03, 6.65728354e-03, 6.50769617e-03, 6.28009879e-03, 6.19160730e-03, 5.64130272e-03, 5.30195373e-03, 5.07453702e-03, 4.47372286e-03, 4.32543895e-03, 4.22006582e-03, 3.97065729e-03, 3.75292740e-03, 3.64861290e-03, 3.38915810e-03, 3.27965962e-03, 3.06633825e-03, 2.99206786e-03, 2.83586784e-03, 2.74987243e-03, 2.31066313e-03, 2.26782347e-03, 1.82206662e-03, 1.74955624e-03, 1.69305161e-03, 1.66624597e-03, 1.55346749e-03, 1.51278404e-03, 1.47296800e-03, 1.33617458e-03, 1.30517592e-03, 1.24056353e-03, 1.19823961e-03, 1.14381059e-03, 1.13027458e-03, 1.11081803e-03, 1.08359152e-03, 1.03517496e-03, 1.00164593e-03, 9.50024604e-04, 8.94981182e-04, 8.74363843e-04, 7.98497545e-04, 7.51612220e-04, 6.63964302e-04, 6.21097646e-04, 6.18098604e-04, 5.72611403e-04, 5.57509231e-04, 5.47002381e-04, 5.27195077e-04, 5.11487997e-04, 4.87787872e-04, 4.74249071e-04, 4.52367689e-04, 4.24431101e-04, 4.19119024e-04, 3.72489906e-04, 3.38125455e-04, 3.34002144e-04, 2.97951371e-04, 2.84845901e-04, 2.79038288e-04, 2.77054476e-04, 2.67962797e-04, 2.54815126e-04, 2.29230595e-04, 1.99245436e-04, 1.90381389e-04, 1.84497913e-04, 1.77415682e-04, 1.68160613e-04, 1.63992031e-04, 1.58025553e-04, 1.54226003e-04, 1.40079892e-04, 1.46097434e-04, 1.46890640e-04, 1.35736724e-04, 9.90265098e-05, 1.04252870e-04, 1.16752515e-04, 1.14080847e-04, 1.22704035e-04, 9.66039062e-05, 9.60766570e-05, 9.16166335e-05, 9.07003478e-05, 8.60212633e-05, 8.32654024e-05, 7.70526077e-05, 7.36470020e-05, 7.24998305e-05, 6.80209910e-05, 6.68682698e-05, 6.14500420e-05, 5.99843174e-05, 5.49918003e-05, 5.24646955e-05, 5.13403849e-05, 5.02336264e-05, 4.89288507e-05, 4.51104475e-05, 4.29823765e-05, 4.18869715e-05, 4.14341562e-05, 3.94822843e-05, 3.80307292e-05, 3.57776535e-05, 3.43901591e-05, 2.98089203e-05, 2.72388358e-05, 2.42608885e-05, 2.30962279e-05, 2.27807559e-05, 2.14440814e-05, 1.96208174e-05, 1.91217363e-05, 1.88276186e-05, 1.66549051e-05, 1.46846459e-05, 1.39779892e-05, 1.43753346e-05, 1.21760519e-05, 1.20295835e-05, 1.13426750e-05, 1.09258905e-05, 1.02782991e-05, 1.01021808e-05, 9.72678794e-06, 9.64538296e-06, 9.23630205e-06, 8.93991858e-06, 8.34247982e-06, 7.36188590e-06, 7.20354827e-06, 6.69282813e-06, 6.49477814e-06, 4.45482134e-06, 4.65422046e-06, 5.09342483e-06, 5.31392220e-06, 5.67034892e-06, 5.91044556e-06, 6.00244889e-06, 4.11265577e-06, 3.77558985e-06, 3.65202836e-06, 3.48065950e-06, 2.78847699e-06, 2.66299628e-06, 2.57492503e-06, 2.39210233e-06, 2.06298821e-06, 2.00824521e-06, 1.76373602e-06, 1.58273269e-06, 1.32211395e-06, 1.49813697e-06, 1.42489429e-06, 1.44003524e-06, 1.10002716e-06, 9.01008863e-07, 8.49881106e-07, 7.62521870e-07, 6.57641103e-07, 5.85636641e-07, 5.33937361e-07, 4.16077215e-07, 3.33765858e-07, 2.95575265e-07, 2.54744632e-07, 2.20144574e-07, 1.86314525e-07, 1.77370967e-07, 1.54794344e-07, 1.47331687e-07, 1.39738552e-07, 1.04110968e-07, 1.00786519e-07, 9.38635094e-08, 9.10853310e-08, 8.71546325e-08, 7.48338889e-08, 6.06817435e-08, 5.66479200e-08, 5.24576913e-08, 4.57020648e-08, 2.89942624e-08, 2.60449421e-08, 2.10987990e-08, 2.17618741e-08, 1.75542294e-08, 1.34637025e-08, 1.27167435e-08, 1.23258201e-08, 9.86367963e-09, 1.04987513e-08, 8.49423161e-09, 9.33428155e-09, 7.42190962e-09, 6.84633796e-09, 6.46870806e-09, 5.76455817e-09, 5.01138098e-09, 3.48686453e-09, 2.91267177e-09, 2.77880628e-09, 1.73093438e-09, 1.42391194e-09, 9.24975774e-10, 1.16454971e-09, 6.95073614e-10, 1.11815884e-09, 1.80003518e-10, 1.97062415e-10, 2.61936054e-10, 6.13219223e-10, 5.27584239e-10, -2.16417104e-15, 2.10627686e-15, 6.25652286e-16, -1.69155643e-17, 5.08498479e-19, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])# 发现超过20％的特征值都是0，意味着这些特征都是其他特征的副本，也就是说它们可以通过其他特征来表示，而本身并没有提供额外的信息# dataMat.shape (1567, 590)# meanVals.shape (1, 590)# meanRemoved.shape (1567, 590)# covMat.shape (590, 590)# eigVects.shape (590, 590)# eigVals.shape (590,)&gt;&gt;&gt;&gt;&gt;&gt; # 半导体制造数据http://archive.ics.uci.edu/ml/machine-learning-databases/secom/","categories":[],"tags":[]},{"title":"DeepLearningwithKeras_Code","slug":"DeepLearningwithKeras-Code","date":"2018-11-26T02:44:25.000Z","updated":"2018-11-29T07:23:56.248Z","comments":true,"path":"2018/11/26/DeepLearningwithKeras-Code/","link":"","permalink":"http://yoursite.com/2018/11/26/DeepLearningwithKeras-Code/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… keras_CIFAR10_simple keras_CIFAR10_V1","categories":[],"tags":[]},{"title":"MachineLearning_Environment configuration","slug":"MachineLearning_EnvironmentConfiguration","date":"2018-11-25T11:13:41.000Z","updated":"2018-12-31T02:58:37.153Z","comments":true,"path":"2018/11/25/MachineLearning_EnvironmentConfiguration/","link":"","permalink":"http://yoursite.com/2018/11/25/MachineLearning_EnvironmentConfiguration/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… AnacondaAnaconda多环境多版本python配置指导 更换镜像（Mac下通过Anaconda安装Tensorflow） # 添加Anaconda的TUNA镜像$ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ # 设置搜索时显示通道地址$ conda config --set show_channel_urls yes$ conda install numpy #测试是否添加成功# 之后会自动在用户根目录生成“.condarc”文件，可以在终端用 $ ls -a # 命令查看该文件，如果要删除镜像，直接删除“.condarc”文件即可： $ rm .condarc Mac Pycharm配置Anaconda环境 - Command + ,- Project → Project Interpreter - 齿轮 → Add → System Interpreter - 齿轮 → '/Users/Captain/anaconda3/python.app/Contents/MacOS/python' shell 使用Anaconda的python $ vim ~/.bash_profile#Anaconda3export PATH=~/anaconda3/bin:$PATH #若想使用系统自带的python版本，将此行注释即可 Anaconda/env下 安装包路径 $ ~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/ #keras anaconda_downgrade Failed download low_version Anaconda Anaconda installer archive Anaconda2-5.3.0-MacOSX-x86_64.pkg # 对应python3.6 Anaconda_Jupyter $ conda install ipykernel Install Anaconda on Linux TensorFlowTensorFlow on MacOS 暂不支持python3.7 查看TensorFlow版本 &gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; tf.__version__'1.12.0'&gt;&gt;&gt; 测试是否安装成功 &gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')&gt;&gt;&gt; sess = tf.Session()&gt;&gt;&gt; print(sess.run(hello))Hello,TensorFlow!- I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA Therno 测试Therno &gt;&gt;&gt; import theano&gt;&gt;&gt; import theano.tensor as T&gt;&gt;&gt; x = T.dmatrix('x')&gt;&gt;&gt; s = 1/(1+T.exp(-x))&gt;&gt;&gt; logistic = theano.function([x],s)&gt;&gt;&gt; logistic([[0,1],[-1,-2]])array([[0.5 , 0.73105858], [0.26894142, 0.11920292]])&gt;&gt;&gt; Keras 安装Keras $ pip install keras 查看Keras版本号 &gt;&gt;&gt; import kerasUsing TensorFlow backend.&gt;&gt;&gt; print(keras.__version__)2.2.4&gt;&gt;&gt; 切换Keras后端backend # 创建或打开如下Keras配置文件$ ~/.keras/keras.json# 默认配置如下： (将backend字段的值改为theano或者tensorflow，即可切换到相应的后端)&#123; \"floatx\": \"float32\", \"epsilon\": 1e-07, \"backend\": \"tensorflow\", # \"backend\": \"theano\" \"image_data_format\": \"channels_last\"&#125; 离线下载cifar数据集 1.通过源码\\keras\\datasets\\cifar10.py可以看到文件下载地址:&apos;https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz &apos;2. 通过源码keras\\utils\\data_utils.py可以看到下载后的文件保存至&apos;~/.keras/datasets/&quot;fname&quot;.tar.gz&apos; (Anaconda 的env环境下的keras文件 保存路径亦如此)- 小结：手动下载数据集，然后移动到 ~\\.keras\\datasets目录下，并改名（包括后缀名）为cifar-10-batches-py.tar.gz，并且用到其他时依次类推。 注：&quot;改后缀名!!!&quot; 测试 from keras.datasets import cifar10(x_train, y_train), (x_test, y_test) = cifar10.load_data()print(y_train[:4]) downgrade Keras/upgrade keras # you can downgrade your keras with pip install keras==1.2.2 if you don't want to bother about keras 2 much. Otherwise, you have to write keras 2 go through the release notes and check the fresh API.$ pip install keras==1.2.2# upgrade keras$ pip install --upgrade keras RunTime Error - TypeError: softmax() got an unexpected keyword argument 'axis' - pip install --upgrade keras==2.1.3 - 'python matplotlib framework under macosx'ImportError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of 'python' with 'pythonw'. See 'Working with Matplotlib on OSX' in the Matplotlib FAQ for more information. - Create a file ~/.matplotlib/matplotlibrc there and add the following code: backend: TkAgg pip 下载速度慢 临时使用： pip install -i https://pypi.tuna.tsinghua.edu.cn/simple &#39;packageName&#39; 永久修改，一劳永逸：Linux下，修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”表示隐藏文件夹) 内容如下: [global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host=mirrors.aliyun.com 列出所有可升级的包 $ pip list --outdate Jupyter notebook 安装jupyter $ pip install jupyter 运行jupyter $ jupyter notebook kernel 配置文件 $ python -m ipykernel install --userInstalled kernelspec python2 in /Users/Captain/Library/Jupyter/kernels/python2$ python3 -m ipykernel install --userInstalled kernelspec python3 in /Users/Captain/Library/Jupyter/kernels/python3 $ cat /Users/Captain/Library/Jupyter/kernels/python2/kernel.json&#123; \"display_name\": \"Python 2\", \"language\": \"python\", \"argv\": [ \"/Users/Captain/anaconda2/bin/python\", \"-m\", \"ipykernel_launcher\", \"-f\", \"&#123;connection_file&#125;\" ]&#125;%$ cat /Users/Captain/Library/Jupyter/kernels/python3/kernel.json&#123; \"display_name\": \"Python 3\", \"language\": \"python\" \"argv\": [ \"/Library/Frameworks/Python.framework/Versions/3.6/bin/python3\", \"-m\", \"ipykernel_launcher\", \"-f\", \"&#123;connection_file&#125;\" ],&#125;% 查看jupyter notebook内核列表 $ jupyter kernelspec listAvailable kernels: python2 /Users/Captain/Library/Jupyter/kernels/python2 python3 /Users/Captain/Library/Jupyter/kernels/python3 安装或删除其他内核 $ python kernel install --name python2 #安装python2 $ jupyter kernelspec uninstall python2 #删除python2 查看使用的pyhton版本 import sysprint(sys.executable)","categories":[],"tags":[]},{"title":"Classification_Algorithm","slug":"Classification-Algorithm","date":"2018-11-24T06:40:05.000Z","updated":"2019-01-04T05:42:05.753Z","comments":true,"path":"2018/11/24/Classification-Algorithm/","link":"","permalink":"http://yoursite.com/2018/11/24/Classification-Algorithm/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 决策树​ 决策树是一种机器学习的方法。决策树的生成算法有ID3, C4.5和C5.0等。决策树是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果。决策树是一种十分常用的监督学习分类方法，监管学习就是给出一堆样本，每个样本都有一组属性和一个分类结果，也就是分类结果已知，那么通过学习这些样本得到一个决策树，这个决策树能够对新的数据给出正确的分类。 ​ 这里通过一个简单的例子来说明决策树的构成思路：给出如下的一组数据，一共有五个样本，每个样本有’不浮出水面是否可以生存’，’是否有脚蹼’属性，最后判断这些样本是否是鱼类。最后一列给出了人工分类结果。 序号ID 不浮出水面是否可以生存No Surfacing? 是否有脚蹼Flippers？ 是否为鱼类Fish？ 1 是 Yes 是 Yes 是 Yes 2 是 Yes 是 Yes 是 Yes 3 是 Yes 否 No 否 No 4 否 No 是 Yes 否 No 5 否 No 是 Yes 否 No 构建决策树决策树生成过程 特征选择：特征选择是指从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。 决策树生成： 根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。 树结构来说，递归结构是最容易理解的方式。 剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合.剪枝技术有预剪枝和后剪枝两种. ​ 在非结束的条件下，首先选择出合适的特征，然后根据其分类。分类开始时，记录分类的特征到决策树中，然后在特征标签集中删除该特征，表示已经使用过该特征。根据选中的特征将数据集分为若干个子数据集，然后将子数据集作为参数递归创建决策树，最终生成一棵完整的决策树。 注：理想条件下，任何到达叶子节点的数据必然属于叶子结点的分类 递归结束的条件 每个分支下的所有实例都具有相同的分类 程序遍历完所有划分数据集的属性 遍历完全部属性，划分的数据有可能不全属于一个类，这个时候需要根据多数表决准则确定该子数据集的分类 决策树优缺点 优点：决策树计算复杂度不高、便于使用、而且高效，决策树可处理具有不相关特征的数据、可很容易地构造出易于理解的规则，而规则通常易于解释和理解。 缺点：存在处理缺失数据时的困难、过度拟合以及忽略数据集中属性之间的相关性等问题。 决策树算法ID3算法特点：使用信息增益来选择特征，信息增益大的优先选择 算法步骤： 1）初始化信息增益的阈值$ϵ$ 2）判断样本是否为同一类输出$D_i$，如果是则返回单节点树$T$。标记类别为$D_i$ 3）判断特征是否为空，如果是则返回单节点树🌲$T$，标记类别为样本中输出类别$D$实例数最多的类别 4）计算$A$中的各个特征（一共$n$个）对输出$D$的信息增益，选择信息增益最大的特征$A_g$ 5）如果$A_g$的信息增益小于阈值$ϵ$，则返回单节点树🌲$T$，标记类别为样本中输出类别$D$实例数最多的类别 6）否则，按特征$A_g$的不同取值$A_{gi}$将对应的样本输出$D$分成不同的类别$D_i$。每个类别产生一个子节点。对应特征值为$A_{gi}$。返回增加了节点的树🌲$T$ 7）对于所有的子节点，令$D=D_i$,$A=A−{A_g}$递归调用2-6步，得到子树🌲$T_i$并返回 不足： a）ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 b）ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大（即用信息增益作为标准容易偏向于取值较多的特征）。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。如果校正这个问题呢？ c）ID3算法对于缺失值的情况没有做考虑 d) 没有考虑过拟合的问题 C4.5算法特点：采用信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题 算法思路：针对ID3算法的不足，加以改进 a）对于第一个问题，不能处理连续特征， C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为$a_1,a_2,…,a_m$,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点$T_i$表示为：$T_i=\\frac{a_i+a_{i+1}}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为$类别1$，大于$a_t$的值为$类别2$，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 b）对于第二个问题，信息增益作为标准容易偏向于取值较多的特征的问题。我们引入一个信息增益比的变量$I_R(D,A)$ (下文”相关概念”中有该变量的详细说明)。特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 c）对于第三个缺失值处理的问题，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本处理。 ​ 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据$D_1$，另一部分是没有特征A的数据D2。然后对于没有缺失特征A的数据集$D_1$来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 ​ 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。 d）对于第4个问题，C4.5引入了正则化系数进行初步的剪枝。下文介绍CART算法时会详细讨论剪枝的思路。 不足： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。一般地，常采用”后剪枝加上交叉验证”选择最合适的决策树。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。 CART算法特点：使用基尼系数来选择特征，简化模型（减少计算量）同时也不至于完全丢失熵模型的优点。 对于连续特征和离散特征处理的改进： 对于CART分类树连续值的处理问题，其思想和C4.5是相同的，都是将连续的特征离散化。唯一的区别在于在选择划分点时的度量方式不同，C4.5使用的是信息增益比，则CART分类树使用的是基尼系数。 对于CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。如果某个特征A被选取建立决策树节点，它有A1,A2,A3三种类别，CART分类树会考虑把A分成${A_1}$和${A_2,A_3}$, ${A_2}$和${A_1,A_3}$,${A_3}$和${A_1,A_2}$三种情况，找到基尼系数最小的组合，比如${A_2}$和${A_1,A_3}$，然后建立二叉树节点，一个节点是$A2$对应的样本，另一个节点是${A1,A3}$对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3。这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立。 构建CART分类树的算法步骤：算法输入是训练集D，基尼系数的阈值，样本个数阈值；输出是决策树T；从根节点开始，用训练集递归的建立CART树。 1）对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。 2）计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。 3）计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数，缺失值的处理和C4.5算法里描述的相同。 4）在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2. 5）对左右的子节点递归的调用1-4步，生成决策树。 对于生成的决策树做预测的时候，假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本。则对于A的类别预测采用的是这个叶子节点里概率最大的类别（多数表决准则）。 构建CART回归树算法： ​ CART回归树和CART分类树的建立算法大部分是类似的，所以这里我们只讨论CART回归树和CART分类树的建立算法不同的地方。 ​ 首先，我们要明白，什么是回归树，什么是分类树。两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。 ​ 除了概念的不同，CART回归树和CART分类树的建立和预测的区别主要有下面两点： ​ 1)连续值的处理方法不同 ​ 2)决策树建立后做预测的方式不同。 ​ 对于连续值的处理，我们知道CART分类树采用的是用基尼系数的大小来度量特征的各个划分点的优劣情况。这比较适合分类模型，但是对于回归模型，我们使用了常见的和方差的度量方式，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。表达式为：$$ \\underbrace{min}_{A,s}\\Bigg[\\underbrace{min}_{c_1}\\sum\\limits_{x_i \\in D_1(A,s)}(y_i - c_1)^2 + \\underbrace{min}_{c_2}\\sum\\limits_{x_i \\in D_2(A,s)}(y_i - c_2)^2\\Bigg] $$ 其中，c1c1为D1数据集的样本输出均值，c2c2为D2数据集的样本输出均值。 ​ 对于决策树建立后做预测的方式，上面讲到了CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。 ​ 除了上面提到了以外，CART回归树和CART分类树的建立算法和预测没有什么区别。 CART树算法的剪枝​ 由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的泛化能力。但是，有很多的剪枝方法，我们应该这么选择呢？CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。 ​ 也就是说，CART树的剪枝算法可以概括为两步，第一步是从原始决策树生成各种剪枝效果的决策树，第二步是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的、剪枝后的树作为最终的CART树🌲。 ​ 首先我们看看剪枝的损失函数度量，在剪枝的过程中，对于任意的一刻子树$T$,其损失函数为：$$ C_{\\alpha}(T_t) = C(T_t) + \\alpha |T_t| $$ 其中，$α$为正则化参数，这和线性回归的正则化一样。$C(T_t)$为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。$|T_t|$是子树T的叶子节点的数量。 ​ 当$α=0$时，即没有正则化，原始的生成的CART树即为最优子树。当$α=∞$时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，$α$越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的$α$，一定存在使损失函数$C_α(T)$最小的唯一子树。 ​ 看过剪枝的损失函数度量后，我们再来看看剪枝的思路，对于位于节点t的任意一颗子树$T_t$，如果没有剪枝，它的损失是$$ C_{\\alpha}(T_t) = C(T_t) + \\alpha |T_t| $$ ​ 如果将其剪掉，仅仅保留根节点，则损失是$$ C_{\\alpha}(T) = C(T) + \\alpha $$ ​ 当$α=0$或者$α$很小时，$C_{\\alpha}(T_t) &lt; C_{\\alpha}(T)$ . 当$α$增大到一定的程度时，$C_{\\alpha}(T_t) = C_{\\alpha}(T)$ 。当$α$继续增大时不等式反向，也就是说，如果满足下式：$\\alpha = \\frac{C(T)-C(T_t)}{|T_t|-1}$，$T_t$和$T$有相同的损失函数，但是$T$节点更少，因此可以对子树$T_t$进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点$T$。 ​ 最后我们看看CART树的交叉验证策略。上面我们讲到，可以计算出每个子树是否剪枝的阈值$α$，如果我们把所有的节点是否剪枝的值$α$都计算出来，然后分别针对不同的$α$所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的$α$，有了这个$α$，我们就可以用对应的最优子树作为最终结果。 CART树的剪枝算法： - 输入是CART树建立算法得到的原始决策树$T$ - 输出是最优决策子树$T_α$ 1）初始化$α_{min}=∞$， 最优子树集合$ω={T}$。 2）从叶子节点开始自下而上计算各内部节点t的训练误差损失函数$C_α(T_t)$（回归树为均方差，分类树为基尼系数）, 叶子节点数$|T_t|$，以及正则化阈值$α=min{\\frac{C(T)−C(T_t)}{|Tt|−1},α{min}}$, 更新$α{min}=α$ 3) 得到所有节点的$α$值的集合M。 4）从M中选择最大的值$α_k$，自上而下的访问子树t的内部节点，如果$\\frac{C(T)−C(T_t)}{|T_t|−1}≤α_k$时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到$α_k$对应的最优子树$T_k$ 5）最优子树集合$ω=ω∪T_k$， $M=M−{α_k}$。 6）如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合$ω$. 7）采用交叉验证在$ω$选择最优子树$T_α$ 不足： 无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1，这里不多介绍。 如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。 算法小结 算法 支持模型 树结构 特征选择 连续值处理 缺失值处理 剪枝 ID3 分类 多叉树 信息增益 不支持 不支持 不支持 C4.5 分类 多叉树 信息增益比 支持 支持 支持 CART 分类，回归 二叉树 基尼系数，均方差 支持 支持 支持 决策树相关概念信息熵 Entropy 概念说明：熵表示混乱的程度，熵越大，越混乱，比如一杯浑浊水的熵就比一杯纯净的水熵大. 在信息论和概率统计中，设X是一个取有限个值的离散随机变量，其概率分布为：$$ P(X=x_i)=p_i,i=1,2,3,..,n \\tag{1} $$ 则随机变量X的熵定义为：$$ H(X)=-\\sum _{i=1}^n p_i\\log _2p_i\\tag{2} $$ 若$pi=0$，则规定$0log0=0$。需要说明的是，熵只依赖于$X$的分布，而不依赖于$X$的值. 若$D:p_1=\\frac{2}{5} \\quad p_2=\\frac{3}{5} $，则 $H(D)=-\\left (-\\frac {2}{5}\\log _2 \\frac {2}{5}-\\frac {3}{5}\\log _2\\frac {3}{5}\\right )=0.971$ 推广：多个变量的联合熵，这里给出两个变量X和Y的联合熵表达式：$$ H(X,Y) = -\\sum\\limits_{i=1}^{n}p(x_i,y_i)logp(x_i,y_i) $$ 条件熵 Conditional Entropy 概念说明：条件熵 $H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，定义为$X$给定条件下Y的条件概率分布的熵对X的数学期望。 条件熵的计算公式如下：$$ H(Y|X)= -\\sum\\limits_{i=1}^{n}p(x_i,y_i)logp(y_i|x_i)=\\sum _{i=1}^np(x_i)H(Y|X=x_i)\\tag {3} $$ 当熵和条件熵中的概率由数据估计得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵。 信息增益 Information gain 概念说明：信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。换一个角度解释一下，一杯浑浊的水$Y$，其熵为$H1$，现在将其中悬浮的一类物质$X$去除，这杯水的熵下降为$H2$，则物质$X$对于这杯水的信息增益就为$H1−H2$。 特征$X$对数据集$D$的信息增益记为$I(X,Y)$，计算公式如下：$$ I(X,Y)=H(X)-H(X|Y) \\tag {4} $$ 其中$H(X|Y)$为特征X给定条件下$Y$的经验条件熵。 比较各个特征的信息增益，选择信息增益最大的作为分类的最优特征。 ID3决策树在生成的过程中，根据信息增益来选择特征。 关系梳理：$H(X)$度量了$X$的不确定性，条件熵$H(X|Y)$度量了我们在知道$Y$以后$X$剩下的不确定性，$H(X)-H(X|Y)$为信息增益$I(X,Y)$ 左边的椭圆代表$H(X)$,右边的椭圆代表$H(Y)$,中间重合的部分就是我们的互信息或者信息增益$I(X,Y)$, 左边的椭圆去掉重合部分就是$H(X|Y)$,右边的椭圆去掉重合部分就是$H(Y|X)$。两个椭圆的并就是$H(X,Y)$。 信息增益比 Information gain Ratio 引入目的：以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以对这一问题进行校正。 概念说明：信息增益比是信息增益和特征熵的比值。 信息增益比计算公式如下：$$ I_R(D,A)=\\frac {I(A,D)}{H_A(D)} \\tag {5} $$ 其中$D$为样本特征输出的集合，$A$为样本特征，对于特征熵$H_A(D)$, 表达式如下：$$ H_A(D)=-\\sum _{i=1}^n\\frac {|D_i|}{|D|}\\log _2 \\frac {|D_i|}{|D|}\\tag {6} $$ 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 根据信息增益比，选择数值最大的作为分类的最优特征。 C4.5决策树在生成的过程中，根据信息增益比来选择特征。 基尼指数 GINI index 基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。 在分类问题中，假设有$K$个类别，第$k$个类别的概率为$p_k$, 则基尼系数的表达式为：$$ Gini(p) = \\sum\\limits_{k=1}^{K}p_k(1-p_k) = 1- \\sum\\limits_{k=1}^{K}p_k^2 $$ 如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：$$ Gini(p) = 2p(1-p) $$ 对于个给定的样本$D$,假设有$k$个类别, 第k个类别的数量为$C_k$,则样本$D$的基尼系数表达式为：$$ Gini(D) = 1-\\sum\\limits_{k=1}^{K}(\\frac{|C_k|}{|D|})^2 $$ 特别的，对于样本$D$,如果根据特征$A$的某个值$a$,把$D$分成$D_1$和$D_2$两部分，则在特征$A$的条件下，$D$的基尼系数表达式为：$$ Gini(D,A) = \\frac{|D_1|}{|D|}Gini(D_1) + \\frac{|D_2|}{|D|}Gini(D_2) $$ 和熵模型的度量方式比，基尼系数对应的误差有多大呢？对于二类分类，基尼系数和熵之半的曲线如下： 从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代。而CART分类树算法就是使用的基尼系数来选择决策树的特征。同时，为了进一步简化，CART分类树算法每次仅仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。这样一可以进一步简化基尼系数的计算，二可以建立一个更加优雅的二叉树模型。 参考：手把手生成决策树(dicision tree)\\决策树算法原理(上)、(下) 阅读链接：决策树(Decision Tree)：通俗易懂之介绍","categories":[{"name":"XD","slug":"XD","permalink":"http://yoursite.com/categories/XD/"}],"tags":[{"name":"数据分析与挖掘","slug":"数据分析与挖掘","permalink":"http://yoursite.com/tags/数据分析与挖掘/"}]},{"title":"Arxiv_AD_with_Code","slug":"Arxiv_AD_with_Code","date":"2018-11-23T14:04:50.000Z","updated":"2018-11-26T08:52:33.914Z","comments":true,"path":"2018/11/23/Arxiv_AD_with_Code/","link":"","permalink":"http://yoursite.com/2018/11/23/Arxiv_AD_with_Code/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… Alzheimer’s DiseaseAlzheimer’s Disease CNN Visualizing Convolutional Networks for MRI-based Diagnosis of Alzheimer’s Disease - We downloaded T1-weighted MPRAGE scans and non- linearly registered all images to a 1 mm isotropic ICBM template using 'ANTs (http://stnava.github.io/ANTs/)', resulting in volumes of 193 × 229 × 193.- PyTorch implementations of all visualization methods will be made 'available' at http://github.com/jrieke/cnn-interpretability. Predicting Cognitive Decline with Deep Learning of Brain Metabolism and Amyloid Imaging - This supervised learning are conducted by stochastic gradient descent (SGD) algorithm, the &apos;source code&apos; of which is distributed by MatConvNet (Version 1.0-beta 16). - Goodfellow IJ, Vinyals O, Saxe AM. Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:14126544 2014. - Vedaldi A, Lenc K. MatConvNet: Convolutional neural networks for matlab. Proceedings of the 23rd Annual ACM Conference on Multimedia Conference; 2015: ACM: 689-692. Alzheimer’s Disease DeepLearning A multi-contrast MRI approach to thalamus segmentation - The supervised learning and convex segmentation steps of the algorithm were implemented in MATLAB R2017b (The Mathworks Inc., Natick, MA, USA) and are available at 'https://github.com/veronicacorona/multicontrastSegmentation.git'.- The dataset used in this work and the proposed supervised learning and convex segmentation imple- mentations are available at 'https://github.com/veronicacorona/multicontrastSegmentation.git'.- Radio-frequency (RF) bias corrected [33] T2∗-weighted magnitude images were affine co-registered to their corresponding bias-corrected MPRAGE volume using 'ANTs (http://stnava.github.io/ ANTs/)' [34]. Multi-modal Disease Classification in Incomplete Datasets Using Geometric Matrix Completion - Hyperparameters were optimized using Hyperopt('http://hyperopt.github.io/hyperopt/'), through nested cross-validation, targeting classification loss (binary cross-entropy) on a hold-out validation set (10% in each fold of training data). Deep convolutional neural networks for segmenting 3D in vivo multiphoton images of vasculature in Alzheimer disease mouse models - DeepVess is freely available at 'https://github.com/mhaft/DeepVess' and can be used immediately by researchers who use MPM for vasculature imaging. - We hope the availability of our open source code and reported results will facilitate and motivate the adoption of this method by researchers and practitioners. Towards Alzheimer’s Disease Classification through Transfer Learning - Architecture models for Tensor- Flow and pre-trained weights were downloaded from open source repositories of the models 1['https://github.com/flyyufelix/cnn finetune'].- For the Inception V4 model, stochastic gradient descent optimization with a learning rate of 0.0001 was used 2[Models, weights, dataset, and code available at 'https://github.com/ marciahon29/Ryerson MRP'].- Keeping up with the spirit of reproducible research, all our models, dataset, and code can be accessed through the repository at: 'https://github.com/marciahon29/Ryerson MRP' .","categories":[],"tags":[]},{"title":"Numpy_Function","slug":"Numpy_Function","date":"2018-11-22T14:25:41.000Z","updated":"2018-11-23T12:15:38.819Z","comments":true,"path":"2018/11/22/Numpy_Function/","link":"","permalink":"http://yoursite.com/2018/11/22/Numpy_Function/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… numpy.random.choice()- numpy.random.choice(a, size=None, replace=True, p=None) - a: If an ndarray, a random sample is generated from its elements. If an int, the random sample is generated as if a was np.arange(n) 'a:[0..a)' - size : int or tuple of ints, optional \"随机生成size个数\" If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. - replace : boolean, optional '生成数是否可重复 True:可重复 False:不可重复' If you want only 'unique' samples then this should be false. - p : 1-D array-like, optional '[0..a)中每个出现的概率[p_0 p_1 ...p_a)' The probabilities associated with each entry in a. If not given the sample assumes a uniform distribution('均匀分布') over all entries in a. example:&gt;&gt;&gt; np.random.choice(1, 3) #从[0]中随机生成3个数(默认replace=True 可重复)array([0, 0, 0])&gt;&gt;&gt; np.random.choice(2, 3) #从[0,1]中随机生成3个数(默认replace=True 可重复)array([0, 1, 0])&gt;&gt;&gt; np.random.choice(3, 3) #从[0,1,2]中随机生成3个数(默认replace=True 可重复)array([1, 1, 0])&gt;&gt;&gt; np.random.choice(3, 3, replace=False) #从[0，1，2]中随机生成3个不同(不重复)的数array([2, 1, 0])&gt;&gt;&gt; np.random.choice(5, 3, replace=False) #从[0，1，2，3，4]中随机生成3个不同(不重复)的数array([1, 2, 3])&gt;&gt;&gt; np.random.choice(5, 3, p=[0.1,0,0.3,0.6,0]) #生成[0,1,2,3,4]的概率分别为[0.1,0,0.3,0.6,0]array([3, 2, 3])... numpy.argmax()- numpy.argmax(a, axis=None, out=None) - a : (array_like) Input array. - axis : (int, optional) By default, the index is into the flattened array, otherwise along the specified axis. '0为列 1为行' - out : (array, optional) If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype. - Returns the 'indices' of the maximum values along an axis. 返回的是索引，并非值 examplpe:&gt;&gt;&gt; a = np.arange(6).reshape(2,3)&gt;&gt;&gt; aarray([[0, 1, 2], [3, 4, 5]])&gt;&gt;&gt; np.argmax(a) #flattened array[0,1,2,3,4,5]中最大值(5)索引为55&gt;&gt;&gt; np.argmax(a, axis=0) #第0列[0,3]中最大值(3)索引为1 第1列[1,4]中最大值(4)索引为1 第2列[2,5]中最大值(5)索引为1array([1, 1, 1])&gt;&gt;&gt; np.argmax(a, axis=1) #第0行[0,1,2]最大值[2]索引为2，第1行[3,4,5]最大值(5)索引为2array([2, 2])&gt;&gt;&gt; b = np.arange(6)&gt;&gt;&gt; b[1] = 5&gt;&gt;&gt; barray([0, 5, 2, 3, 4, 5])&gt;&gt;&gt; np.argmax(b) # Only the first occurrence is returned.1&gt;&gt;&gt; c = np.array([[9,8,7],[1,2,3],[5,6,4]])&gt;&gt;&gt; carray([[9, 8, 7], [1, 2, 3], [5, 6, 4]])&gt;&gt;&gt; np.argmax(c)0&gt;&gt;&gt; np.argmax(c,axis=0)array([0, 0, 0])&gt;&gt;&gt; np.argmax(c,axis=1)array([0, 2, 1])","categories":[],"tags":[]},{"title":"Assignment_Implement_Note","slug":"Assignment-Implement-Note","date":"2018-11-20T11:10:14.000Z","updated":"2018-11-23T12:02:27.921Z","comments":true,"path":"2018/11/20/Assignment-Implement-Note/","link":"","permalink":"http://yoursite.com/2018/11/20/Assignment-Implement-Note/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… SVM 梯度计算SVM原理说明/效果演示/代码实现$$L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\right]$$ $$ \\left\\{\\begin{aligned} \\nabla_{w_{y_i}} L_i = & -\\left(\\sum_{j \\ne y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0)\\right)x_i & j = y_i \\\\ \\nabla_{w_j} L_i = & 1(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0) x_i & j \\ne y_i \\end{aligned}\\right. $$ 其中$\\mathbb{1}$是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。 def svm_loss_naive(W, X, y, reg): dW = np.zeros(W.shape) #(3073, 10) num_classes = W.shape[1] #10 num_train = X.shape[0] #500 loss = 0.0 for i in range(num_train): #[0,500) scores = X[i].dot(W) #矩阵乘法 (1,3073)*(3073,10) correct_class_score = scores[y[i]] #S_yi 该图像在正确标签上的得分 for j in range(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note: delta = 1 if margin &gt; 0: loss += margin dW[:, y[i]] -= X[i, :].T # this is really a sum over j != y_i dW[:, j] += X[i, :].T # sums each contribution of the x_i's loss /= num_train dW /= num_train loss += 0.5 *reg * np.sum(W * W) dW += reg*W return loss, dWdef svm_loss_vectorized(W, X, y, reg): loss = 0.0 dW = np.zeros(W.shape) # dW.shape==(3073,500) num_classes=W.shape[1] num_train=X.shape[0] scores=X.dot(W) #(500,10) = (500,3073)*(3073,10) scores_correct = scores[np.arange(num_train), y] #(500,) scores_correct[i]=scores[i,y[i]] scores_correct=np.reshape(scores_correct,(num_train,-1)) #(500,1) = (500,500*1/500) margins=scores-scores_correct+1 #delta=1 #scores.shape=(500,10) margins=np.maximum(0,margins) margins[np.arange(num_train),y]=0 loss=np.sum(margins)/num_train loss += 0.5 * reg * np.sum(W * W) # compute the gradient margins[margins &gt; 0] = 1 #margins中大于0的元素，数值赋为1;其余数值不变 shape==(500,10) row_sum = np.sum(margins, axis=1) # 1 by N (1行N列) margins[np.arange(num_train), y] = -row_sum #margins[np.arange(num_train), y] 赋-row_sum前，值为0 shape==(500,) #print(margins) ##necessary to understand dW += np.dot(X.T, margins)/num_train + reg * W # D by C dW.shape==(3073，10) X.T.shape==(3073,500) margins.shape==(500,10) return loss, dW 此处解释仅关注dW(即对权重的梯度计算) np.dot(X.T, margins) $$ \\underbrace{\\underbrace{\\begin{bmatrix}\\overrightarrow{W_C}&\\overrightarrow{W_D} &\\overrightarrow{W_A}& \\overrightarrow{W_B}\\end{bmatrix}}_\\text{可看作dW (D,C)} =\\underbrace{\\begin{bmatrix}\\overrightarrow{X_c}&\\overrightarrow{X_d}&\\overrightarrow{X_a}&\\overrightarrow{X_b}\\end{bmatrix}}_\\text{可看作X.T (D,N)} \\underbrace{\\begin{bmatrix} 1&1 &-9 &1 \\\\ 1 & 1 & 1&-9\\\\-9& 1& 1& 1\\\\ 1& -9&1 &1 \\end{bmatrix}}_\\text{可看作margins (N,C)}}_\\text{可看作svm_loss_vectorized dW计算过程} =\\underbrace{\\begin{bmatrix} \\overrightarrow{X_a}+\\overrightarrow{X_b}-9\\overrightarrow{X_c}+\\overrightarrow{X_d}&\\overrightarrow{X_a}+\\overrightarrow{X_b}+\\overrightarrow{X_c}-9\\overrightarrow{X_d} &-9\\overrightarrow{X_a}+\\overrightarrow{X_b}+\\overrightarrow{X_c}+\\overrightarrow{X_d}&\\overrightarrow{X_a}-9\\overrightarrow{X_b}+\\overrightarrow{X_c}+\\overrightarrow{X_d} \\end{bmatrix}}_\\text{可看作svm_loss_naive dW计算过程} $$ $margins.shape==scores.shape$ $-row_sum=-9 $ 矩阵基本知识：$$ (1)\\begin{bmatrix}\\overrightarrow{A}&\\overrightarrow{B} &\\overrightarrow{C} & \\overrightarrow{D}\\end{bmatrix} =\\begin{bmatrix}\\overrightarrow{a}&\\overrightarrow{b} &\\overrightarrow{c}&\\overrightarrow{d}\\end{bmatrix}\\begin{bmatrix} -9& 1& 1& 1\\\\ 1& -9&1 &1 \\\\ 1&1 &-9 &1 \\\\ 1 & 1 & 1&-9 \\end{bmatrix} =\\begin{bmatrix} -9\\overrightarrow{a}+\\overrightarrow{b}+\\overrightarrow{c}+\\overrightarrow{d}&\\overrightarrow{a}-9\\overrightarrow{b}+\\overrightarrow{c}+\\overrightarrow{d} & \\overrightarrow{a}+\\overrightarrow{b}-9\\overrightarrow{c}+\\overrightarrow{d}&\\overrightarrow{a}+\\overrightarrow{b}+\\overrightarrow{c}-9\\overrightarrow{d} \\end{bmatrix} $$ $$ (2)\\begin{bmatrix}\\overrightarrow{C}&\\overrightarrow{D} &\\overrightarrow{A}& \\overrightarrow{B}\\end{bmatrix} =\\begin{bmatrix}\\overrightarrow{c}&\\overrightarrow{d}&\\overrightarrow{a}&\\overrightarrow{b}\\end{bmatrix} \\begin{bmatrix} 1&1 &-9 &1 \\\\ 1 & 1 & 1&-9\\\\-9& 1& 1& 1\\\\ 1& -9&1 &1 \\end{bmatrix} =\\begin{bmatrix} \\overrightarrow{a}+\\overrightarrow{b}-9\\overrightarrow{c}+\\overrightarrow{d}&\\overrightarrow{a}+\\overrightarrow{b}+\\overrightarrow{c}-9\\overrightarrow{d} &-9\\overrightarrow{a}+\\overrightarrow{b}+\\overrightarrow{c}+\\overrightarrow{d}&\\overrightarrow{a}-9\\overrightarrow{b}+\\overrightarrow{c}+\\overrightarrow{d} \\end{bmatrix} $$ $$ (3) \\begin{bmatrix}\\overrightarrow{A}\\\\\\overrightarrow{B}\\\\\\overrightarrow{C}\\\\\\overrightarrow{D}\\end{bmatrix} =\\begin{bmatrix} -9& 1& 1& 1\\\\ 1& -9&1 &1 \\\\ 1&1 &-9 &1 \\\\ 1 & 1 & 1&-9 \\end{bmatrix}\\begin{bmatrix}\\overrightarrow{a}\\\\\\overrightarrow{b}\\\\\\overrightarrow{c}\\\\\\overrightarrow{d}\\end{bmatrix} =\\begin{bmatrix}-9\\overrightarrow{a}+\\overrightarrow{b}+\\overrightarrow{c}+\\overrightarrow{d}\\\\\\overrightarrow{a}-9\\overrightarrow{b}+\\overrightarrow{c}+\\overrightarrow{d}\\\\ \\overrightarrow{a}+\\overrightarrow{b}-9\\overrightarrow{c}+\\overrightarrow{d}\\\\\\overrightarrow{a}+\\overrightarrow{b}+\\overrightarrow{c}-9\\overrightarrow{d} \\end{bmatrix} $$ $$ (4) \\begin{bmatrix}\\overrightarrow{C}\\\\\\overrightarrow{D}\\\\\\overrightarrow{A}\\\\\\overrightarrow{B}\\end{bmatrix} =\\begin{bmatrix} 1&1 &-9 &1 \\\\ 1 & 1 & 1&-9 \\\\-9& 1& 1& 1\\\\ 1& -9&1 &1\\end{bmatrix} \\begin{bmatrix}\\overrightarrow{c}\\\\\\overrightarrow{d}\\\\\\overrightarrow{a}\\\\\\overrightarrow{b}\\end{bmatrix} =\\begin{bmatrix} \\overrightarrow{a}+\\overrightarrow{b}-9\\overrightarrow{c}+\\overrightarrow{d}\\\\\\overrightarrow{a}+\\overrightarrow{b}+\\overrightarrow{c}-9\\overrightarrow{d}\\\\-9\\overrightarrow{a}+\\overrightarrow{b}+\\overrightarrow{c}+\\overrightarrow{d}\\\\\\overrightarrow{a}-9\\overrightarrow{b}+\\overrightarrow{c}+\\overrightarrow{d} \\end{bmatrix} $$ $C = A B $ 等价于 $C^T = B^T A^T$","categories":[],"tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://yoursite.com/tags/cs231n/"}]},{"title":"DataMining","slug":"dataMining","date":"2018-11-19T17:02:45.000Z","updated":"2018-11-23T11:31:01.119Z","comments":true,"path":"2018/11/20/dataMining/","link":"","permalink":"http://yoursite.com/2018/11/20/dataMining/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… Data MiningIntroduction to R R语言概述 R语言数据类型 R语言数据管理 R语言绘图 R语言高级数据管理 Introduction to Data Mining Know your data Data types Statistical description of the data Data visualization Data similarity and dissimilarity Preprocess the data Cleaning the data Integration the data Reduction the data Dimensionality reduction Some public datasets available Classification Classification: Basic Concepts - Supervision vs. Unsupervised Learning - Supervision (监督学习)：我们对输入样本经过模型训练后有明确的预期输出 - Classification(分类)：想要预测的是离散值discrete(即标称型) - Regression(回归)：想要预测的是连续值nominal(即数值型) - Unsupervised(非监督学习)：我们对输入样本经过模型训练后得到什么输出完全没有预期 Decision Tree Induction - Basic algorithm - 树以自上而下，递归，分而治之的方式构建 - 所有的训练样例从根源开始分类 - 根据所选属性递归分区示例 - 在每个节点上，根据该节点上的训练样例以及启发式或统计度量（例如，信息增益）选择属性- Conditions for stopping partitioning - 给定节点的所有样本都属于同一个类 - 没有剩余属性可用于进一步分区 - 没有剩下的样例- Prediction - 采用多数投票对叶子进行分类 - 信息熵(Entropy) - 定义：考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望 - 不确定性越大，熵越大- 条件熵(Conditional Entropy) - 定义：另一个变量Y熵对X（条件）的期望- 信息增益(Information gain)- 增益率(Gain ratio)- 吉尼系数(Giniindex) - Overfitting and Tree Pruning - Overfitting(过拟合):An induced tree may overfit the training data - Too many branches, some may reflect anomalies due to noise or outliers - Poor accuracy for unseen samples - Two approaches to avoid overfitting - Prepruning(预剪枝): Halt tree construction early ̵ do not split a node if thiswould result in the goodness measure falling below a threshold - Difficult to choose an appropriate threshold - Postpruning(后剪枝): Remove branches from a “fully grown” tree—get a sequence of progressively pruned trees - Use a set of data different from the training data to decide which is the “best pruned tree” Model Evaluation and Selection - Evaluation metrics - How can we measure accuracy? - Other metrics to consider?- Use validation test set of class-labeled tuples instead of training set when assessing accuracy- Methods for estimating a classifier’s accuracy - Holdout method - Cross-validation - Bootstrap- Comparing classifiers: - ROC Curves Issues Affecting Model Selection- Accuracy - classifier accuracy: predicting class label- Speed - time to construct the model (training time) - time to use the model (classification/prediction time)- Robustness: handling noise and missing values- Scalability(可伸缩性): efficiency in disk-resident databases- Interpretability - understanding and insight provided by the model- Other measures, e.g., goodness of rules, such as decision tree size or compactness(紧凑性) of classification rules Techniques to Improve Classification Accuracy: Ensemble Methods Bayes Classification Methods Support Vector Machine (SVM) Artificial Neural Network (ANN) Summary - Classification is a form of data analysis that extracts models describing important data classes.- Effective and scalable(可扩展) methods have been developed for decision tree induction(归纳), Naive Bayesian classification, rule-based classification, and many other classification methods.- Evaluation metrics(指标) include: accuracy, sensitivity, specificity(特异性), precision(精确性), recall(召回率), F measure(度量), and Fß measure.- Stratified(分层) k-fold cross-validation is recommended for accuracy estimation. Bagging() and boosting() can be used to increase overall accuracy by learning and combining a series of individual(单独的) models.- Significance tests and ROC curves are useful for model selection. - There have been numerous comparisons of the different classification methods; the matter remains a research topic- No single method has been found to be superior over all others for all data sets(各方法各有千秋)- Issues such as accuracy, training time, robustness, scalability, and interpretability must be considered and can involve trade- offs(权衡), further complicating(复杂化) the quest for an overall superior method- Effective and advanced classification methods - Bayesian belief network (probabilistic概率 networks) - Backpropagation反向传播 (Neural networks) - Support Vector Machine (SVM) - Pattern-based classification - Other classification methods: lazy learners (KNN, case-based reasoning推理), genetic algorithms(遗传算法), rough set(粗糙集) and fuzzy set(模糊集) approaches- Additional Topics on Classification - Multiclass classification - Semi-supervised(半监督) classification - Active learning(主动学习) - Transfer learning(迁移学习)","categories":[],"tags":[]},{"title":"ADNI_Publications_digest","slug":"ADNI-Publications-digest","date":"2018-11-12T06:16:06.000Z","updated":"2018-11-22T03:10:49.573Z","comments":true,"path":"2018/11/12/ADNI-Publications-digest/","link":"","permalink":"http://yoursite.com/2018/11/12/ADNI-Publications-digest/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 11 Bayesian longitudinal low-rank regression models for imaging genetic data from longitudinal studiesZ. H. Lu, Z. Khondker, J. G. Ibrahim, Y. Wang, H. Zhu and I. Alzheimer’s Disease NeuroimagingFeature：开发了一个贝叶斯L2R2模型来确定纵向成像响应和协变量与成像遗传数据的关系 Result：我们应用L2R2模型来研究前10位单核苷酸多态性(SNPs)和前40位老年痴呆症相关基因的影响 Structured and Sparse Canonical Correlation Analysis as a Brain-Wide Multi-Modal Data Fusion Approach A. R. Mohammadi-Nejad, G. A. Hossein-Zadeh and H. Soltanian-Zadeh Feature：提出了一种结构稀疏CCA (ssCCA)技术作为一种脑域多模态数据融合方法。 Result：ssCCA优于现有标准和正规化的基于CCA的融合方法。结果表明，所提出的无监督技术区分了AD患者的受试者过程与HC受试者之间的过渡模式。此外，我们还绘制了与AD患者相对于HC患者的解剖变化最相关的功能区域的脑图。 Normative morphometric data for cerebral cortical areas over the lifetime of the adult human brainO. Potvin, L. Dieumegarde, S. Duchesne and I. Alzheimer’s Disease Neuroimaging Feature：制定可以量化大脑异常的健康成人大脑皮层区域的规范数据，填补神经成像在此方面的不足。在健康成人的独立样本中验证了预测规范性数值的模型，显示了令人满意的验证R2。在轻度阿尔茨海默氏病和精神分裂症患者中，测量标准样本的偏差，并观察偏差的预期模式。 High-resolution magnetic resonance imaging reveals nuclei of the human amygdala: manual segmentation to automatic atlasZ. M. Saygin, D. Kliemann, J. E. Iglesias, A. J. W. van der Kouwe, E. Boyd, M. Reuter, A. Stevens, K. Van Leemput, A. McKee, M. P. Frosch, B. Fischl, J. C. Augustinack and I. Alzheimer’s Disease Neuroimaging Feature：使用基于贝叶斯推理的atlas构建算法，使用高分辨率离体MRI数据自动分割可视化9个amygdala nuclei 的边界。提供了标准体内神经成像工具，能够自动将杏仁核划分成多个核，为神经成像研究人员提供探索人类杏仁核功能和连接的能力。 Independent value added by diffusion MRI for prediction of cognitive function in older adults J. A. Scott, D. Tosun, M. N. Braskie, P. Maillard, P. M. Thompson, M. Weiner, C. DeCarli, O. T. Carmichael and Adni Feature：预测老年人的认知功能. 确定通过扩散磁共振成像（dMRI）测量的白质微观结构能否提供关于认知障碍的老年人的基线水平或执行功能（EF）或记忆（MEM）变化的独立信息 Opposing effects of progranulin deficiency on amyloid and tau pathologies via microglial TYROBP networkH. Takahashi, Z. A. Klein, S. M. Bhagat, A. C. Kaufman, M. A. Kostylev, T. Ikezu, S. M. Strittmatter and I. Alzheimer’s Disease Neuroimaging Feature：探究缺乏颗粒体蛋白前体(PGRN)关于淀粉样蛋白和tau病理的反作用.揭示了GRN与AD的多维相互作用。PGRN缺乏会增加对Aβ的积累 On the complexity of human neuroanatomy at the millimeter morphome scale: Developing codes and characterizing entropy indexed to spatial scaleD. J. Tward and M. I. Miller Feature：我们通过使用来自阿尔茨海默病神经影像学计划的数据来训练多变量高斯先验模型，通过将它们与模板相关的微分变换来研究人脑中皮质下灰质结构的形状。这项工作代表了量化神经影像学研究可以提供疾病状态的信息量的第一步。 Interpreting Biomarker Results in Individual Patients With Mild Cognitive Impairment in the Alzheimer’s Biomarkers in Daily Practice (ABIDE) ProjectI. S. van Maurik, M. D. Zwan, B. M. Tijms, F. H. Bouwman, C. E. Teunissen, P. Scheltens, M. P. Wattjes, F. Barkhof, J. Berkhof, W. M. van der Flier and I. Alzheimer’s Disease Neuroimaging Feature：建立基于生物标志物的MCI患者未来AD痴呆预后模型。解释患有轻度认知障碍的个体患者的生物标志物结果，构建基于生物标记物的预测模型，帮助临床医生解释生物标记值并提供个性化的预测信息。 Generalized Scalar-on-Image Regression Models via Total VariationX. Wang and H. Zhu Feature：构建了一类基于总变差的广义标量图像回归模型(GSIRM-TV)，用于标量响应和存在标量协变量的成像预测器，可应用于从ADNI数据集获得的海马数据的分析。 Imaging-wide association study: Integrating imaging endophenotypes in GWASZ. Xu, C. Wu, W. Pan and I. Alzheimer’s Disease Neuroimaging Feature：提出了一种新的、强大的方法，称为全成像关联研究(image -wide association study, IWAS)，将成像内表型与GWAS相结合，以增强统计能力，并增强GWAS发现的生物学解释。所提出的IWAS是通用的，可以应用于其他的成像内表型，以及GWAS的个体水平或汇总关联数据。 12. A Novel Early Diagnosis System for Mild Cognitive Impairment Based on Local Region Analysis: A Pilot StudyF. E. A. El-Gamal, M. M. Elmogy, M. Ghazal, A. Atwan, M. F. Casanova, G. N. Barnes, R. Keynton, A. S. El-Baz and A. Khalil Feature：讨论了个性化MCI诊断系统，提出一种计算机辅助诊断（CAD）系统，其主要目标是提高诊断AD的准确性，特异性和敏感性 SIENA-XL for improving the assessment of gray and white matter volume changes on brain MRIM. Battaglini, M. Jenkinson, N. De Stefano and I. Alzheimer’s Disease Neuroimaging Feature：介绍了一种新的基于分段的纵向流水线SIENA-XL，可提高脑MRI上灰色和白色物质体积变化评估的精度 Factors influencing accuracy of cortical thickness in the diagnosis of Alzheimer’s disease.pdf)M. Belathur Suresh, B. Fischl, D. H. Salat and I. Alzheimer’s Disease Neuroimaging Feature：阐述影响诊断AD皮质厚度准确性的因素，有助于提高诊断分类的准确性 Result：通过检查一系列人口统计学，生物学和神经心理学数据来确定导致结构分类和临床诊断之间不匹配的因素，继发性病变（如WMH）影响与典型AD病理学重叠的区域的厚度值，从而影响分类的准确性。WMH体积增加，表明血管条件可能有助于皮质厚度测量的分类准确性。 A spatio-temporal reference model of the aging brainW. Huizinga, D. H. J. Poot, M. W. Vernooij, G. V. Roshchupkin, E. E. Bron, M. A. Ikram, D. Rueckert, W. J. Niessen, S. Klein and I. Alzheimer’s Disease Neuroimaging Feature：开发由于正常衰老导致的大脑形态差异的时空模型，区分正常衰老和神经退行性疾病(AD)所产生的形态间差异 Result：AD受试者和健康受试者之间的形态学差异可部分地通过加速衰老来解释 Time-to-event data with time-varying biomarkers measured only at study entry, with applications to Alzheimer’s diseaseC. Lee, R. A. Betensky and I. Alzheimer’s Disease Neuroimaging Feature：使用Cox回归模型对这些生物标记物轨迹关联到时间-事件将允许预测疾病进展 Result：使用研究条目作为时间来源并将在研究进入时测量的时变协变量作为固定基线协变量进行处理，可以简化分析 Cortical Atrophy is Associated with Accelerated Cognitive Decline in Mild Cognitive Impairment with Subsyndromal DepressionM. M. Gonzales, P. S. Insel, C. Nelson, D. Tosun, N. Mattsson, S. G. Mueller, Sacuiu, Bickford, W. Weiner and R. S. Mackin Feature：研究轻度认知障碍（MCI）和慢性亚综合征性抑郁症（SSD）患者认知功能下降与皮质萎缩之间的关系 Result：患有慢性SSD的个体可能代表MCI亚群，其非常容易受到加速认知衰退的影响，这种影响可能受额叶和前扣带萎缩的影响 A Functional Varying-Coefficient Single-Index Model for Functional Response DataJ. Li, C. Huang and H. Zhu Feature：提出了一种新的功能变系数单指标模型（FVCSIM），可对一组感兴趣的协变量进行功能反应数据的回归分析，用于量化成像数据和感兴趣的临床变量之间的复杂关系 Result：我们应用FVCSIM研究从阿尔茨海默病神经影像学倡议（ADNI）中获得的胼call体骨架上白质扩散的发展 Adaptive Testing of SNP-Brain Functional Connectivity Association via a Modular Network AnalysisC. Gao, J. Kim and W. Pan Feature：通过模块化网络分析自适应测试SNP-BRAIN功能连接关联，以识别脑功能网络中的模块结构 Result：我们将我们提出的方法应用于ADNI数据，以使用各种连通性测量来测试遗传变体与整个脑功能网络或其各种子组件之间的关联.发现了几个网络模块和APOE4基因变体之间联系的证据，APOE4基因变体是迄今为止阿尔茨海默病最重要的遗传风险因素. Biomarkers and Functional Decline in Prodromal Alzheimer’s Disease.C. Robb, C. Udeh-Momoh, S. Wagenpfeil, J. Schope, P. Alexopoulos, R. Perneczky and I. Alzheimer’s Disease Neuroimaging Feature：检测ADNI中生物标志物阳性和生物标志物阴性参与者之间的差异，以及这些差异是否代表了系统偏差 Result：基于ADNI的选择标准以及未来研究的设计，必须考虑生物标志物状态和疾病严重程度之间的潜在混淆，以确保前者(而非后者)是预测准确性的真正决定因素。 The prevalence and biomarkers’ characteristic of rapidly progressive Alzheimer’s disease from the Alzheimer’s Disease Neuroimaging Initiative databaseM. Ba, X. Li, K. P. Ng, T. A. Pascoal, S. Mathotaarachchi, P. Rosa-Neto, S. Gauthier and I. Alzheimer’s Disease Neuroimaging Feature：探究快速进展性阿尔茨海默病(rpAD)的患病率和生物标志物特征，对识别临床高危rpAD具有预测价值 Result：发现rpAD通常存在于轻度AD中。脑代谢减退和p-tau/tau比值的降低可能在短期随访期间为rpAD提供潜在的临床差异值。 13 A Bayesian Group Sparse Multi-Task Regression Model for Imaging GeneticsK. Greenlaw, E. Szefer, J. Graham, M. Lesperance, F. S. Nathoo and I. Alzheimer’s Disease Neuroimaging Feature：用于成像遗传学的贝叶斯群稀疏多任务回归模型，提供了可用于进行统计推断的技术，应用于神经影像学和遗传数据的分析，促使研究检查遗传变异对大脑结构的影响 Result：在将单核苷酸多态性与大脑影像学内表型联系起来时，将区间估计纳入单点估计之外的附加价值 Age at injury is associated with the long-term cognitive outcome of traumatic brain injuriesW. Li, S. L. Risacher, T. W. McAllister, A. J. Saykin and A. s. D. N. Initiative Feature：阐述 受伤年龄与创伤性脑损伤的长期认知结果有关 的回顾性调查报告 A Comparison of Accelerated and Non-accelerated MRI Scans for Brain Volume and Boundary Shift Integral Measures of Volume Change: Evidence from the ADNI Dataset E. N. Manning, K. K. Leung, J. M. Nicholas, I. B. Malone, M. J. Cardoso, J. M. Schott, N. C. Fox, J. Barnes and I. Alzheimer’s Disease Neuroimaging Feature：评估加速MRI扫描代替非加速扫描是否影响对照组和轻度认知障碍和阿尔茨海默病患者的脑容量和萎缩率测量 Result：与非加速协议相比，使用加速扫描协议可以缩短采集时间，从而导致更少的扫描对由于运动伪影（可能影响BSI）而被排除在后续分析中，而不会显着改变绝对变化率或临床试验所需的样本量。 Autotaxin is Related to Metabolic Dysfunction and Predicts Alzheimer’s Disease OutcomesK. E. McLimans, A. A. Willette and I. Alzheimer’s Disease Neuroimaging Feature：自体分类素与代谢紊乱有关，可预测阿尔茨海默病的结果 Result： 自分泌运动因子水平在MCI和AD中显着升高，CSF自分泌运动因子可能是用于检查AD结果和风险的有用的代谢障碍生物标志物。 14 Diagnostic performance of an automated analysis software for the diagnosis of Alzheimer’s dementia with 18F FDG PET S. Partovi, R. Yuh, S. Pirozzi, Z. Lu, S. Couturier, U. Grosse, M. D. Schluchter, A. Nelson, R. Jones, J. K. O’Donnell and P. Faulhaber Feature：评估一种定量软件辅助方法的能力，以提高18F FDG PET对阿尔茨海默氏症的诊断准确性 Result：基于定量体素的软件可能有助于经验丰富的18F FDG PET读者分析早发性AD Altered functional brain networks in amnestic mild cognitive impairment: a resting-state fMRI studyS. Cai, T. Chong, Y. Peng, W. Shen, J. Li, K. M. von Deneen and L. Huang Feature：我们的目的是使用RS-fMRI技术探索aMCI患者中与记忆缺陷相关的异常静息状态网络RSN，用于研究aMCI的发病机制 Result：本研究的目的是探讨网络中这些roi之间的功能连通性，并探讨网络间的连通性。 Application of Haralick texture features in brain (18)F-florbetapir positron emission tomography without reference region normalization F]-florbetapir%20positron%20emission%20tomography%20without%20reference%20region%20normalization.pdf)D. L. Campbell, H. Kang and S. Shokouhi Feature：Haralick特征可量化淀粉样蛋白PET放射性示踪剂摄取的空间特征，本研究的目的是计算不同诊断组中的几种HF并确定组间差异 Result：该技术可以改善AD药物试验中的受试者分层，并有助于纵向评估疾病进展和治疗效果，而没有与强度归一化相关的缺点 Sparse shared structure based multi-task learning for MRI based Cognitive Performance prediction of Alzheimer’s disease P. Cao, X. Shan, D. Zhao, M. Huang and O. Zaiane Feature：基于稀疏共享结构的多任务学习探索磁共振成像（MRI）和认知测量中存在的相关性 Result：证明了所提出的方法具有优于多种现有技术可比方法的优越性能，而且还识别与先验知识一致的认知相关MRI生物标志物 Application of concordance probability estimate to predict conversion from mild cognitive impairment to Alzheimer’s diseaseX. Han, Y. Zhang, Y. Shao and A. s. D. N. Initiative Feature：建立了Cox PH模型来预测从MCI到AD的转换，其中使用K指数评估预估准确性。 Amyloidosis and neurodegeneration result in distinct structural connectivity patterns in mild cognitive impairmentT. Jacquemont, F. De Vico Fallani, A. Bertrand, S. Epelbaum, A. Routier, B. Dubois, H. Hampel, S. Durrleman, O. Colliot and I. Alzheimer’s Disease Neuroimaging Feature：研究由淀粉样蛋白和神经变性生物标记物分层的MCI受试者亚组的结构连接体 Result：MCI亚组的连接组崩解模式在脑淀粉样蛋白和神经变性方面不同，展示了取决于生物标志物轮廓的网络改变的差异和相似之处 Adaptive testing for multiple traits in a proportional odds model with applications to detect SNP-brain network associations J. Kim, W. Pan and I. Alzheimer’s Disease Neuroimaging Feature：提出一种在POM中结合检测SNP-brain网络的自适应关联测试模型，是一种灵活的统计检验来检测遗传由神经影像遗传学研究引起的多种性状的关联 An Optimal Transportation based Univariate Neuroimaging IndexL. Mi, W. Zhang, J. Zhang, Y. Fan, D. Goradia, K. Chen, E. M. Reiman, X. Gu and Y. Wang Feature：一种基于单变量神经影像学指标的最优传输方法，用于数据集分类 Result：在阿尔茨海默病患者和健康对照组之间的分类中，该方法在阿尔茨海默病疾病神经影像学倡议（ADNI）基线sMRI数据集上达到了82.30％的准确度，并且优于其他几个指数 Predictive modelling using neuroimaging data in the presence of confoundsA. Rao, J. M. Monteiro, J. Mourao-Miranda and I. Alzheimer’s Disease Feature：讨论并评估了在神经影像学预测建模的背景下处理混杂的不同方法 Result：基线“仅图像”模型处理混淆时能给出更准确的预测 Targeted metabolomics and medication classification data from participants in the ADNI1 cohortL. St John-Williams, C. Blach, J. B. Toledo, D. M. Rotroff, S. Kim, K. Klavins, R. Baillie, X. Han, S. Mahmoudiandehkordi, J. Jack, T. J. Massaro, J. E. Lucas, G. Louie, A. A. Motsinger-Reif, S. L. Risacher, I. Alzheimer’s Disease Neuroimaging, C. Alzheimer’s Disease Metabolomics, A. J. Saykin, G. Kastenmuller, M. Arnold, T. Koal, M. A. Moseley, L. M. Mangravite, M. A. Peters, J. D. Tenenbaum, J. W. Thompson and R. Kaddurah-Daouk Feature：我们提供了使用AbsoluteIDQ-p180平台从199名对照组、356名轻度认知障碍和175名ADNI1受试者的血清中生成的定量代谢组学数据，以及用于数据预处理和药物分类以进行混淆纠正的管道。 Result：帮助发现与疾病和进展相关的代谢失败以及AD中一系列重要生理过程的生物标志物 15 151Deep ensemble learning of sparse regression models for brain disease diagnosisH. I. Suk, S. W. Lee, D. Shen and I. Alzheimer’s Disease Neuroimaging Feature：脑疾病诊断的稀疏回归模型的深度集成学习，用于阿尔茨海默病/轻度认知障碍的诊断和预测.关于脑成像分析的研究见证了机器学习技术在计算机辅助干预脑疾病诊断中的核心作用 152Ventricular and Periventricular Anomalies in the Aging and Cognitively Impaired BrainK. L. Todd, T. Brighton, E. S. Norton, S. Schick, W. Elkins, O. Pletnikova, R. H. Fortinsky, J. C. Troncoso, P. J. Molfese, S. M. Resnick, J. C. Conover and I. Alzheimer’s Disease Neuroimaging Feature：老化和认知受损大脑的心室和心室周异常.基于MRI的纵向研究为LV体积分析的使用提供支持，结合FLAIR PVH分析，用于识别和监测认知障碍与衰老 Result：分析了纵向结构磁共振成像（MRI）和受试者匹配的液体衰减反转恢复（FLAIR）MRI和脑室周围生物样本，以便在时间上映射心室扩张和相关的脑室周围水肿和房室管膜丢失的进展。实验结果揭示了与正常脑老化和认知障碍相关的病理生理学结果，并表明多因素分析最适合预测和监测认知衰退。 153Val66Met Polymorphism in BDNF Has No Sexual and APOE ε4 Status-Based Dimorphic Effects on Susceptibility to Alzheimer’s Disease: Evidence From an Updated Meta-Analysis of Case–Control Studies and High-Throughput Genotyping CohortsQ. Zhao, Shen, Zhao, L. Si, S. Jiang, Y. Qiu and A. s. D. N. Initiative Feature：本次元分析的目的是通过引入年龄，性别和APOE e4作为混杂因素来重新检验BDNF的Val66Met与AD之间的关联，验证Val66Met是否仅在女性大脑衍生神经营养因子(BDNF)多态性表达对阿尔茨海默病(AD)的易感性 Result：我们显示Val66Met多态性与AD的易感性无关，并且没有基于性别或APOE e4状态的二态效应。我们的研究表明，混杂调整对于研究Val66Met甚至AD或AD相关性状的其他多态性是必要的 154Discriminative self-representation sparse regression for neuroimaging-based alzheimer’s disease diagnosisX. Zhu, H. I. Suk, S. W. Lee and D. Shen Feature：基于神经影像的阿尔茨海默病诊断的判别自我表征稀疏回归，所选特征用于训练支持向量机以进行分类 155Ensemble of random forests One vs. Rest classifiers for MCI and AD prediction using ANOVA cortical and subcortical feature selection and partial least squares J. Ramirez, J. M. Gorriz, A. Ortiz, F. J. Martinez-Murcia, F. Segovia, D. Salas-Gonzalez, D. Castillo-Barnes, I. A. Illan, C. G. Puntonet and I. Alzheimer’s Disease Neuroimaging Feature：使用ANOVA皮层和皮质下特征选择和偏最小二乘法的随机森林与Rest分类的集成，用于MCI和AD预测 156Affect of APOE on information processing speed in non-demented elderly population: a preliminary structural MRI studyX. Luo, Y. Jiaerken, X. Yu, P. Huang, T. Qiu, Y. Jia, J. Sun, J. Zhou, M. Zhang and I. Alzheimer’s Disease Neuroimaging Feature：我们通过测量白质高信号（WMH），皮质灰质体积（GMV）和厚度的叶状分布来探索APOE相关IPS(信息处理速度)改变的神经基质 Result：（1）ε4携带者的WMH体积大于对照组，特别是额叶和顶叶; （2）顶叶WMH体积与IPS相关，尤其是ε4携带者。 157Evidence for benefit of statins to modify cognitive decline and risk in Alzheimer’s disease N. Geifman, R. D. Brinton, R. P. Kennedy, L. S. Schneider and A. J. Butte Feature：通过分析综合临床试验和前瞻性观察研究的数据集，研究他汀类药物在AD中可能的保护和治疗作用 Result：他汀类药物的使用可能使所有AD患者受益，这些患者在ApoE4纯合子中具有潜在更大的治疗效果 158Cognitive Composites Domain Scores Related to Neuroimaging Biomarkers within Probable-Amnestic Mild Cognitive Impairment-Storage Subtype. A. Espinosa, M. Alegret, P. Pesini, S. Valero, A. Lafuente, M. Buendia, I. San Jose, M. Ibarria, M. A. Tejero, J. Gimenez, S. Ruiz, I. Hernandez, F. Pujadas, P. Martinez-Lage, J. Munuera, J. Arbizu, L. Tarraga, S. B. Hendrix, A. Ruiz, J. T. Becker, S. M. Landau, O. Sotolongo-Grau, M. Sarasa, M. Boada, A. B. S. Group and I. Alzheimer’s Disease Neuroimaging Feature：这项研究的目的是在Pr-aMCI-storage亚型患者中发现与神经影像学生物标志物最相关的优化认知复合(CCs)域得分 Result：延迟回忆是与前驱AD诊断相关的神经成像生物标志物最佳相关的CC域得分。 159White matter hyperintensities are associated with disproportionate progressive hippocampal atrophy: ASSOCIATION OF WMH WITH HIPPOCAMPAL ATROPHYC. M. Fiford, E. N. Manning, J. W. Bartlett, D. M. Cash, I. B. Malone, G. R. Ridgway, M. Lehmann, K. K. Leung, C. H. Sudre, S. Ourselin, G. J. Biessels, O. T. Carmichael, N. C. Fox, M. J. Cardoso, J. Barnes and I. Alzheimer’s Disease Neuroimaging Feature：本研究调查了白质高信号（WMH）体积，脑脊液（CSF），阿尔茨海默病（AD）病理学标志物与脑和海马体积减少之间的关系 Result：白质增强与不成比例的进行性海马萎缩有关。在未痴呆的老年人中，血管损伤和AD病理学的共同作用是导致海马萎缩的主要原因。 160Functional Reserve: Experience Participating in Instrumental Activities of Daily Living is Associated with Gender and Functional Independence in Mild Cognitive Impairment. C. Berezuk, K. K. Zakzanis, J. Ramirez, A. C. Ruocco, J. D. Edwards, B. L. Callahan, S. E. Black and I. Alzheimer’s Disease Neuroimaging Feature：研究男性和女性在MCI中的功能障碍方面的差异，并确定性别差异是否与潜在的功能储备有关 Result：虽然效果很小，男性性别与大量MCI患者的功能能力差异显着相关。此外，这一男性缺点可部分解释为该队列中男性的IADL经历较低。具有更多IADL经验的个体可能会发展出更大的功能储备，这可能会延迟或减缓MCI的功能下降。 16 161Predicting progression from mild cognitive impairment to Alzheimer’s disease using longitudinalcallosal atrophy S. Minhas, A. Khanum, F. Riaz, S. A. Khan and A. AlviFeature：使用纵向胼a体萎缩预测从轻度认知障碍到阿尔茨海默病的进展 162Shape-Attributes of Brain Structures as Biomarkers for Alzheimer’s Disease T. Glozman, J. Solomon, F. Pestilli, L. Guibas and I. Alzheimer’s Disease NeuroimagingFeature：我们描述了一种基于大脑结构形状差异的两种类型痴呆分类的全自动框架。我们的框架对于确定阿尔茨海默病的发病敏感，在对MCIc与NC进行分类时达到高达88.13％的准确性，优于以前的方法。 163Longitudinal changes in microstructural white matter metrics in Alzheimer’s disease C. D. Mayo, E. L. Mazerolle, L. Ritchie, J. D. Fisk, J. R. Gawryluk and I. Alzheimer’s Disease NeuroimagingFeature：阿尔茨海默病微观结构白质指标的纵向变化，对白质显微结构的敏感性是研究AD生物标志物的一个很有前途的途径。有助于早期诊断AD的症状前生物标志物 164Groupwise Envelope Models for Imaging Genetic Analysis Y. Park, Z. Su and H. ZhuFeature：本文的目的是开发用于多元线性回归的分组包络模型，以建立多变量响应和协变量之间的关联。可以显着提高测试和估计的效率，该模型在有效估计中的有效性 165Predictive Utility of Marketed Volumetric Software Tools in Subjects at Risk for Alzheimer Disease: Do Regions Outside the Hippocampus Matter? T. P. Tanpitukpongse, M. A. Mazurowski, J. Ikhena and J. R. PetrellaFeature：我们的目的是评估在两个商业上可用的脑容量软件包中个体与联合区域容量的预后有效性，以预测轻度认知障碍患者转化为阿尔茨海默病。 Result：将这些工具与人口统计学和其他生物标志物测量相结合，将海马体积作为唯一的体积生物标志物是合理的 166Rey’s Auditory Verbal Learning Test scores can be predicted from whole brain MRI in Alzheimer’s disease Moradi E1, Hallikainen I2, Hänninen T3, Tohka J4; Alzheimer’s Disease Neuroimaging InitiativeFeature：目的是通过机器学习的方法，基于结构磁共振成像(MRI)数据来综合研究RAVLT评分可预测的程度，以及寻找评估RAVLT评分最重要的大脑区域。可以基于观察到的或估计的RAVLT评分来预测MCI受试者在3年内转化为AD，其准确性与基于MRI的生物标志物相当。 explanation：Rey的听觉言语学习测验（RAVLT）是一种强大的神经心理学工具，用于测试情景记忆，广泛用于痴呆症和痴呆前症状的认知评估。 167Association analysis of rare variants near the APOE region with CSF and neuroimaging biomarkers of Alzheimer’s disease K. Nho, S. Kim, E. Horgusluoglu, S. L. Risacher, L. Shen, D. Kim, S. Lee, T. Foroud, L. M. Shaw and J. Q. TrojanowskiFeature：APOE区域附近罕见变异和阿尔茨海默病神经影像生物标志物的关联分析,说明下一代测序和定量内表型在评估稀有变异体中的作用，这些变异体可能有助于解释AD和其他复杂疾病中缺失的遗传性。 Result：在调整APOE基因型后，跨越APOE区域的基因内的罕见变异与LOAD相关的CSFAβ1-42和神经成像生物标志物显着相关。 168Temporal association patterns and dynamics of amyloid-beta and tau in Alzheimer’s disease A. K. Ower, C. Hadjichrysanthou, L. Gras, J. Goudsmit, R. M. Anderson, F. de Wolf and I. Alzheimer’s Disease NeuroimagingFeature：阿尔茨海默病中淀粉样蛋白-b和tau的时间关联模式和动态.生物标志物轨迹有助于对疾病进展进行无偏见，客观的评估。定量轨迹可能在临床试验设计中有用，因为它们可以更详细地了解旨在延缓生物疾病发展的治疗方法的有效性。 169Freesurfer cortical normative data for adults using Desikan-Killiany-Tourville and ex vivo protocols O. Potvin, L. Dieumegarde, S. Duchesne and A. s. D. N. InitiativeFeature：我们根据年龄，性别，估计的颅内体积（eTIV）开发了FreeSurfer形态学估计皮质测量的规范数据，这些规范允许人们测量偏离个体正常性的程度，同时考虑影响这些估计的因素。我们的目的是为Desikan-Killianny-Tourville（DKT）和基于体外的标记方案制定类似的标准值，并检查这三种图册之间的差异 objective：1.为DKT和离体标记方案制定规范值 2.描述标记协议之间预测模型的差异 3.确定在病理人群中使用标准Z分数时，地图集的选择是否会产生实质性差异 170Validation of (18)F-FDG-PET Single-Subject Optimized SPM Procedure with Different PET Scanners L. Presotto, T. Ballarini, S. P. Caminiti, V. Bettinardi, L. Gianolli and D. Perani Feature：用不同的PET扫描仪验证18F-FDG-PET单一主题优化的SPM程序,使用基于SPM软件包的优化方法可大大提高诊断准确性。 17 171Comparison of Cortical and Subcortical Measurements in Normal Older Adults across Databases and Software Packages S. Rane, A. Plassard, B. A. Landman, D. O. Claassen and M. J. DonahueFeature：跨数据库和软件包的正常老年人的皮质和皮层下测量的比较，评估使用不同软件包获得的皮质下体积之间的协议.这项工作提供了一个结合ADNI和PPMI的成像数据，以提高统计能力，以及询问不同病理，如阿尔茨海默氏症和帕金森病的常见机制。 172Brain explorer for connectomic analysis H. Li, S. Fang, J. A. Contreras, J. D. West, S. L. Risacher, Y. Wang, O. Sporns, A. J. Saykin, J. Goni, L. Shen and I. Alzheimer’s Disease NeuroimagingFeature：我们通过在相同解剖结构的背景下结合科学和信息可视化技术，为脑成像数据开发了一种新的集成可视化解决方案.通过视觉探索，这种集成的解决方案可以帮助识别具有高度相关的功能激活及其激活模式的大脑区域。视觉检测分化特征还可能发现基于图像的脑疾病表型生物标志物。 173Dynamic predictions in Bayesian functional joint models for longitudinal and time-to-event data: An application to Alzheimer’s disease K. Li and S. Luo (重复181) Feature：贝叶斯功能关节模型中纵向和时间到事件数据的动态预测：阿尔茨海默病的应用.基于所收集的信息准确预测痴呆症的时间有助于医生监测患者的疾病进展并做出早期知情的医疗决策。 method：我们首先提出了一个功能性联合模型，以考虑联合建模框架中纵向和生存子模型中的功能预测器。然后，我们基于其标量和功能测量，开发用于参数估计的贝叶斯方法和用于预测受试者的未来结果轨迹和痴呆风险的动态预测框架。 174Frequency Specific Effects of ApoE epsilon4 Allele on Resting-State Networks in Nondemented Elders Y. Liang, Z. Li, J. Wei, C. Li, X. Zhang and A. s. D. N. Initiative （重复182） Feature：ApoEε4等位基因对非痴呆老年人休息状态网络的频率特异性影响，便于在早期寻找敏感且可靠的生物标志物 Method：我们应用静息状态功能磁共振成像（fMRI）来检查载脂蛋白E（ApoE）ε4等位基因对默认模式网络（DMN）和显着网络（SN）的功能连接性的影响。 Result：结果表明，在研究RSN功能连通性时，静息状态信号具有频率依赖性效应(越来越多的研究人员认为功能连接可能是频率特异性的)。 175Multifactorial causal model of brain (dis) organization and therapeutic intervention: Application to Alzheimer’s disease %20organization%20and%20therapeutic%20intervention-%20Application%20to%20Alzheimer%E2%80%99s%20disease.pdf)Y. Iturria-Medina, F. M. Carbonell, R. C. Sotero, F. Chouinard-Decorte, A. C. Evans and A. s. D. N. InitiativeFeature：在此，我们提出了一个大脑(dis)组织和治疗干预的时空多因素因果模型(MCM)，该模型解释了局部因果交互作用、通过物理大脑网络传播的效应、认知改变和最佳治疗干预的识别。可以解释进行性神经障碍的病理演变和实施多种介入策略的影响. 176Analysis of longitudinal diffusion-weighted images in healthy and pathological aging: An ADNI study F. Kruggel, F. Masaki, A. Solodkin and I. Alzheimer’s Disease NeuroimagingFeature：通过将线性模型与线性混合效应模型进行交换来扩展纵向成像数据的模型,我们的分类器为可获得的生物标志物提供了有前途的功能，可以预测转变为阿尔茨海默病的风险。 177Prediction and classification of Alzheimer disease based on quantification of MRI deformation X. Long, L. Chen, C. Jiang, L. Zhang and I. Alzheimer’s Disease NeuroimagingFeature：基于MRI变形量化的阿尔茨海默病预测与分类.我们提出了一种机器学习方法，用于区分健康老年人AD或轻度认知障碍（MCI）患者，并通过计算和分析组间大脑的区域形态差异来预测MCI患者的AD转换 178Automatic Alzheimer’s Disease Recognition from MRI Data Using Deep Learning Method S. Luo, X. Li and J. LiFeature：利用深度学习方法从MRI数据中自动识别阿尔茨海默病 Measure：它描述了一种基于3D脑MRI深度学习的自动AD识别算法。该算法使用卷积神经网络（CNN）来实现AD识别。它的独特之处在于，在AD识别中将大脑的三维拓扑视为一个整体，从而获得准确的识别。实验表明，该算法具有较高的AD识别准确度，灵敏度为1，特异度为0.93。 179DTI measurements for Alzheimer’s classification T. Maggipinto, R. Bellotti, N. Amoroso, D. Diacono, G. Donvito, E. Lella, A. Monaco, M. Antonella Scelsi and S. TangaroFeature：DTI可以深入了解白质微观结构的完整性，并在早期阶段识别出受阿尔茨海默病（AD）影响的白质区域。我们测量了特征选择偏差对分类性能的显着影响，对采用了有偏见的特征选择策略的DTI进行评估 180Learning non-linear patch embeddings with neural networks for label fusion G. Sanroma, O. M. Benkarim, G. Piella, O. Camara, G. Wu, D. Shen, J. D. Gispert, J. L. Molinuevo, M. A. Gonzalez Ballester and I. Alzheimer’s Disease Neuroimaging （重复185） Feature：我们提出了一个使用神经网络计算补丁嵌入的框架，以增加PBLF中基于相似性的加权投票的判别能力，能够适应在大脑结构分割中更广泛的解剖变异性 18 181Dynamic predictions in Bayesian functional joint models for longitudinal and time-to-event data: An application to Alzheimer’s disease K. Li and S. Luo （重复） 182Frequency Specific Effects of ApoE epsilon4 Allele on Resting-State Networks in Nondemented Elders Y. Liang, Z. Li, J. Wei, C. Li, X. Zhang and A. s. D. N. Initiative （重复） 183Detecting genetic association through shortest paths in a bidirected graph M. Ueki, Y. Kawasaki, G. Tamiya and I. for Alzheimer’s Disease NeuroimagingFeature：提出了一种用于在多元回归模型中检测具有显着但弱的边际关联的隐藏SNP的新方法。 Result：所提出的方法可以检测LD隐藏的敏感性SNP，这些SNP未通过边际关联检验或现有的多变量方法检测到。当应用于阿尔茨海默病神经影像学倡议（ADNI）的真实GWAS数据时，我们的方法检测到两组SNP：一组在含有载脂蛋白E（APOE）基因的区域，另一组在接近信号素5A的区域（SEMA5A）基因。 184Automatic labeling of MR brain images through extensible learning and atlas forests L. Xu, H. Liu, E. Song, M. Yan, R. Jin and C. C. HungFeature：通过可扩展学习和阿特拉斯森林自动标记MR脑图像，基于Multiatlas的方法因其简单性和鲁棒性而广泛应用于MR脑图像分割，该方法提供了极好的准确性。 185Learning non-linear patch embeddings with neural networks for label fusion G. Sanroma, O. M. Benkarim, G. Piella, O. Camara, G. Wu, D. Shen, J. D. Gispert, J. L. Molinuevo, M. A. Gonzalez Ballester and I. Alzheimer’s Disease Neuroimaging （重复） 186Risk factors for amyloid positivity in older people reporting significant memory concern J. Zhang, W. Zhou, R. M. Cassidy, H. Su, Y. Su, X. Zhang and I. Alzheimer’s Disease NeuroimagingFeature：本研究的目的是确定报告主观认知能力下降（SCD）患者脑内淀粉样蛋白积聚的风险因素。识别这些风险因素将有助于更好地识别应该接受神经影像学研究以确认斑块存在并开始干预的患者，以及加强对阿尔茨海默病发病机制的研究。 187Differential Regional Distribution of Juxtacortical White Matter Signal Abnormalities in Aging and Alzheimer’s Disease E. R. Lindemer, D. N. Greve, B. Fischl, J. C. Augustinack, D. H. Salat and I. Alzheimer’s Disease Neuroimaging（重复193）Feature：老年人和阿尔茨海默病患者皮质白质信号异常WMSA的微分区域分布. Objective：观察AD患者大脑中WMSA的空间分布模式是否与认知健康老化者不同 Result：结果表明WMSA是AD发展的重要病理组成部分 188Power analysis to detect treatment effects in longitudinal clinical trials for Alzheimer’s disease Z. Huang, G. Muniz-Terrera and B. D. M. Tom （重复194）Feature：功效分析，以检测阿尔茨海默病纵向临床试验中的治疗效果 Conclusion：在设计临床试验时考虑组分评分的多变量/联合分布而不是单个综合评分的分布可以导致功效的增加和样本量的减少，以便在早期AD的临床试验中检测治疗效果。 189Optimizing Neuropsychological Assessments for Cognitive, Behavioral, and Functional Impairment Classification: A Machine Learning StudyBattista P1, Salvatore C1, Castiglioni I1Feature：对认知、行为和功能障碍分类的神经心理学评估进行优化:机器学习研究，有助于用于分类和诊断AD的临床措施 Objective：评估机器学习在量化神经心理学评估过程中的潜力，并优化甚至减少用于对AD患者进行分类的神经心理学测试的数量，同样在损伤的早期阶段。 190Cortical Atrophy is Associated with Accelerated Cognitive Decline in Mild Cognitive Impairment with Subsyndromal Depression M. M. Gonzales, P. S. Insel, C. Nelson, D. Tosun, N. Mattsson, S. G. Mueller, Sacuiu, Bickford, W. Weiner and R. S. Mackin Feature：研究轻度认知障碍（MCI）和慢性亚综合征性抑郁症（SSD）患者认知功能下降与皮质萎缩之间的关系 Result：患有慢性SSD的个体可能代表MCI亚群，其非常容易受到加速认知衰退的影响，这种影响可能受额叶和前扣带萎缩的影响 19 191 A comparison of accurate automatic hippocampal segmentation methods URL：http://adni.loni.usc.edu/adni-publications/Zandifar)%202017_neuroimaging.pdf A. Zandifar, V. Fonov, P. Coupe, J. Pruessner, D. L. Collins and I. Alzheimer’s Disease Neuroimaging Feature：准确自动海马分割方法的比较 Method：比较了四种完全自动化的海马分割方法，它们与手动分割的一致性以及它们在临床环境中用作AD生物标志物的能力 Result：我们的研究表明，基于非线性补丁的纠错分割方法是最准确的自动分割方法，与手动分割最符合（= 0.894） 192Regional 18F-Fluorodeoxyglucose Hypometabolism is Associated with Higher Apathy Scores Over Time in Early Alzheimer DiseaseJ. R. Gatchel, J. Donovan, J. Locascio, J. A. Becker, M. Rentz, A. Sperling, A. Johnson, A. Marshall and A. s. D. N. InitiativeFeature：研究了冷漠与区域18F-氟脱氧葡萄糖（FDG）代谢在认知正常，轻度认知障碍和来自阿尔茨海默病神经影像学倡议数据库的AD痴呆受试者之间的关联。研究AD蛋白病在冷漠发病机制中的潜在作用。有助于制定预防和治疗AD的策略 193Differential Regional Distribution of Juxtacortical White Matter Signal Abnormalities in Aging and Alzheimer’s Disease E. R. Lindemer, D. N. Greve, B. Fischl, J. C. Augustinack, D. H. Salat and I. Alzheimer’s Disease NeuroimagingFeature：（重复187） 194Power analysis to detect treatment effects in longitudinal clinical trials for Alzheimer’s disease Z. Huang, G. Muniz-Terrera and B. D. M. Tom （重复188） 195Plasma neurofilament light chain levels in Alzheimer’s disease. W. Zhou, J. Zhang, F. Ye, G. Xu, H. Su, Y. Su, X. Zhang and I. Alzheimer’s Disease NeuroimagingFeature：阿尔茨海默病中的血浆神经丝轻链水平.检查了血浆NFL是否可能是AD的前驱和痴呆阶段的潜在生物标志物 Result：结果表明，血浆NFL水平可能不是诊断AD的前驱和痴呆阶段的有用生物标志物。 196Prediction of Conversion to Alzheimer’s Disease with Longitudinal Measures and Time-To-Event Data K. Li, W. Chan, R. S. Doody, J. Quinn, S. Luo and I. Alzheimer’s Disease NeuroimagingFeature：通过纵向测量和事件发生时间数据预测阿尔茨海默病的转变 Objective：比较各种临床和生物标志物轨迹，以跟踪进展和预测从遗忘性轻度认知障碍到可能的AD的转换 Result：最强的预测因子是ADAS-Cog 13 Conclusion：除基线特征外，还可以通过纳入纵向变化信息来改善AD转换的预测。认知测量一直是重要的，并且通常比成像测量更强的预测因子。 197MRI-based classification models in prediction of mild cognitive impairment and dementia in late-life depression A. K. Lebedeva, E. Westman, T. Borza, M. K. Beyer, K. Engedal, D. Aarsland, G. Selbaek and A. K. HabergFeature：基于MRI的分类模型预测晚期抑郁症中的轻度认知障碍和痴呆 198Efficient Groupwise Registration for Brain MRI by Fast Initialization P. Dong, X. Cao, J. Zhang, M. Kim, G. Wu and D. ShenFeature：通过快速初始化测试图像的分组注册MRI，我们最终可以使用现有的分组注册方法来快速细化分组注册结果。与最先进的分组登记方法相比，ADNI数据集上的实验结果显示出显着提高的计算效率和竞争性配准精度。 199The interactive effect of demographic and clinical factors on hippocampal volume: A multicohort study on 1958 cognitively normal individuals D. Ferreira, O. Hansson, J. Barroso, Y. Molina, A. Machado, J. A. Hernandez-Cabrera, J. S. Muehlboeck, E. Stomrud, K. Nagga, O. Lindberg, D. Ames, G. Kalpouzos, L. Fratiglioni, L. Backman, C. Graff, P. Mecocci, B. Vellas, M. Tsolaki, I. Kloszewska, H. Soininen, S. Lovestone, H. Ahlstrom, L. Lind, E. M. Larsson, L. O. Wahlund, A. Simmons, E. Westman, f. t. A. s. D. N. I. the AddNeuroMed consortium, B. Australian Imaging and g. Lifestyle Study of Ageing researchFeature：人口统计学和临床因素对海马体积的交互影响：1958年认知正常个体的多项研究 200Left frontal cortex connectivity underlies cognitive reserve in prodromal Alzheimer disease N. Franzmeier, M. Duering, M. Weiner, M. Dichgans, M. Ewers and I. Alzheimer’s Disease Neuroimaging Feature：检测阿尔茨海默病（AD）左侧额叶皮质（LFC）的更高全局功能连接是否与更多年的教育（代理认知储备[CR]）相关，并减轻AD相关氟脱氧葡萄糖之间的关联（ FDG）-PET hypome-formolism和情节记忆。 Conclusion：较高的gLFC连接性是CR的功能性基质，有助于在早期AD中出现FDG-PET代谢减退时相对良好地维持情景记忆。","categories":[{"name":"ADNI","slug":"ADNI","permalink":"http://yoursite.com/categories/ADNI/"}],"tags":[]},{"title":"系统集成项目管理知识点","slug":"系统集成项目管理软考","date":"2018-11-03T04:43:04.000Z","updated":"2018-11-10T10:45:49.816Z","comments":true,"path":"2018/11/03/系统集成项目管理软考/","link":"","permalink":"http://yoursite.com/2018/11/03/系统集成项目管理软考/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 项目管理类项目立项项目可行性研究 可行性研究的步骤 1. 机会研究2. 初步可行性研究3. 详细可行性研究 - 小型项目可以不进行详细可行性研究4. 项目论证5. 项目评估 - 项目评估由第三方进行 - 决策的主要依据6. 项目可行性研究报告的编写、提交和获得批准 甲方立项管理(解决项目的组织战略符合性问题)的四个阶段 1. 项目识别2. 项目论证 - 项目社会影响评价 - 项目环境影响评价 - 项目国民经济评价 - 项目财务评价3. 投标4. 签订合同 承建方(乙方)的项目管理步骤 1. 项目机会识别2. 项目论证 - 技术可行性分析 - 项目风险分析 - 人力资源配置分析 - 项目财务分析 - 对其他投标者的相关情况分析 - 有效防范风险3. 投标 项目识别--乙方项目评估 -- 主管 项目建议书(又称立项申请，由项目建设单位编写，非承建单位)包括的核心内容 1. 项目的必要性2. 项目的市场预测3. 产品方案或服务的市场预测4. 项目建设必须的条件 - 本期项目建设方案 供应商项目内部立项 内容：1. 项目资源估算2. 项目资源分配3. 准备项目任务书4. 任命项目经理作用:1. 分配资源2. 确定项目绩效目标3. 提升效率 项目生命周期 项目临时性：明确的开始和结束时间 项目整体管理项目范围管理 项目范围管理过程 1. 范围计划2. 范围定义3. 创建WBS4. 范围确认5. 范围控制 制定范围管理计划定义项目范围 范围说明书的内容 1. 项目合理性(资源需求)2. 项目目标3. 项目可交付成果清单 工作说明书SOW 工作说明书(任务书)是对项目所要提供交付的产品或服务的叙述性的描述 - 业务需求 - 产品范围描述 - 战略计划项目范围说明书则通过明确项目应该完成的工作而确定了项目的范围 创建WBS 工作包：WBS是最底层的工作单元 工作分解步骤 1. 识别2. 分解3. 确认4. 核实 WBS分解方法(逐层向下分解,渐进明细的) - 生命周期为第一层，可交付物为第二层 - 生命周期:&quot;需求分析、方案设计、实施准备、测试和验收&quot;(瀑布模型)- 重要的交付物为第一层- 子项目为第一层，再分解子项目的WBS WBS的制定需要所有项目干系人的参与，需要包括100％的工作内容 范围确认 范围确认(又称范围核实)：正式验收并接受已完成的项目可交付物的过程 一般步骤：1. 确定需要进行确认范围的时间2. 识别确认范围需要哪些投入3. 确定范围正式被接受的标准和要素4. 确定确认范围会议的组织步骤5. 组织确认范围会议 - 范围确认的对象不仅包括范围说明书，还包括项目管理计划和所有交付物- 范围确认的参加人员是客户和所有项目干系人，不仅限于项目组和质量管理员- 范围确认贯穿项目始终- 系统的终验报告作为范围确认证据- 用于范围确认的项目管理计划的组成部分包括如下范围基准 - 项目范围说明书 - WBS - WBS词典(WBS的详细说明) - 进行范围确认活动时应邀请客户参加范围确认与质量控制不同：前者是有关工作结果的接受问题；而后者是有关工作正确与否的问题。质量控制一般在范围确认之前完成. 范围确认采用的方法 - 检查(有时也称审查、产品审查、审计、巡查) - 测量、审查与确认- 群体决策技术 -一致同意 -大多数原则(超过50％) -相对多数原则 -独裁 何时关注 - 实际完成项目时间比计划提前 控制项目范围 范围控制是监控项目状态 (如项目的工作范围状态和产品范围状态)，也是控制变更的过程 变更影响分析由项目经理负责 变更管理中 变更初审的目的 - 确认变更的必要性- 格式校验、完整性校验，确保评估所需信息准备充分- 在干系人间 就提出供评估的变更信息达成共识 other 控制变更，推荐纠正措施 属于 监督和控制过程组 项目进度管理活动排序输出：- 项目进度网络图 活动资源估算方法：- 专家判断- 多方案分析：自制或者购买的决策- 出版的估算数据- 项目管理软件- 自下而上估算输出：- 资源分解结构- 请求的变更- 资源日历 项目历时估算- 主要方法和技术包括：专家判断、类比估算、参数估算、三点估算、后备估算 - 三点估算能评估时间与概率的关系，可以用于风险评估，属于定量分析 控制进度计划 制定进度计划时可采用的工具与技术 - 关键路径法- 资源平衡技术 - 资源平滑技术 - 活动只在其自由浮动时间和总浮动时间内延迟，不改变关键路径 常用的历时估算方法 - 类比估算(未掌握全部细节)- 参数估算- 三点估算- 后备估算 后备分析：以 应急时间、时间储备、缓冲时间为名称 增加时间 控制点：即里程碑. 关键路径不能包括所有项目进度控制点 项目进度表：横道图(gantt chart) 进度网络分析技术中的一种方法是关键链法（经常改变关键路径，结合了确定性与随机性，常考）、关键路线法(正向与反向分析) 控制项目进度 压缩工期 - 投入更多的资源以加速活动进程(赶工)- 指派经验更丰富的人去完成或帮助完成项目工作- 减小活动范围或降低活动要求- 通过改进方法或技术提高生产效率- 快速跟进(并行工作) 项目成本管理估算项目成本制定项目预算 挣值分析是成本控制的工具 成本预算的工具 - 参数估算- 资金限制平衡- 准备金分析 成本分类 - 直接成本：差旅费、工资- 间接成本：企业管理费、税金、福利、保卫费 控制项目成本绩效评估 项目绩效就是搜集项目所有基准数据并向项目干系人提供绩效信息 收集材料：- 被评价项目资料清单- 项目绩效预测- 调查问卷 项目评估包括 1. 盈利要求2. 客户满意度要求3. 后续项目指标要求4. 内部满意度要求 项目质量管理质量度量- 软件产品的使用质量的质量属性：有效性、生产率、安全性、满意度- 质量特性：功能性、可靠性、易用性、效率、维护性、可移植性- 质量属性：精确性、完整性、可靠性、及时性、经济性、可验证性、安全性 - 可靠性：故障次数 可用性：故障恢复时间- CMMI(软件能力成熟度模型)的过程改进目标：1.保证产品或服务质量 2.项目时间控制 3.用最低的成本 项目人力资源管理 编制人力资源计划 - 工具技术 - 组织结构图和职位描述 - 人力资源模板(可加快编制速度) - 非正式的人际网络 - 组织理论- 描述工具 - 工作分解结构 - 组织分解结构(OBS) - 资源分解结构(RBS)- 每个工具包只有一个明确的责任人 组建项目团队 - 生命周期：形成、震荡、正规、发挥(表现)、终止 - 项目团队加入新成员时，重新进入形成期- 方式：培训、扩展训练、认可和奖励 管理项目团队 - 马斯洛需要层次理论：生理、安全、社会(团建活动)、自尊- 赫兹伯格的双因素理论：保健因素和激励因素- X-Y理论：x假定人性本恶 y假定人性本善- 责任分配矩阵 直观- 团队建设内容：1.一般管理技能 2.培训 3.团队建设活动 4.基本原则 5.同地办公- 项目组织结构：项目经理权利从小到大依次是职能型、弱矩阵型、平衡矩阵型、强矩阵型、项目型 项目管理知识域 项目沟通管理 沟通管理计划 编制过程：1. 确定干系人的沟通信息需求2. 描述信息收集和文件归档的机构3. 发送信息和重要信息的格式- 冲突管理- 主要内容 - 项目干系人沟通要求 - 对要发布信息的描述 - 信息接收的个人或组织 - 信息传达所需的技术或方法 - 沟通频率 - 上报过程 - 随项目的进展对沟通管理计划更新与细化方法 - 通用词语表 发布信息 - 项目状态/评审会议的主要目的： - 介绍项目进展情况 - 项目是否偏离进度计划 - 说明造成进度偏离计划的原因和预防计划 - 汇报在项目执行中发现的问题及潜在的问题 - 应引起客户或项目负责人注意的事项 - 沟通(与上司沟通、与下属沟通、水平沟通) 管理干系人 - 主要目的：避免项目干系人在项目管理中的严重分歧，促进干系人对项目的理解和支持- 沟通过程管理的最终目标是保障干系人之间的有效沟通- 干系人分析贯穿项目的始终 冲突处理 - 解决冲突的范畴 - 强制 - 妥协 - 撤退 - 合作(得到大多数人都同意) - 问题解决(公开讨论，直至选择出一套最佳方案) - 求同存异 - 问题解决应该聚焦现在，而不是过去 沟通方式 - 引导技术 - 头脑风暴 - 冲突处理 - 问题解决 - 会议管理- 控制沟通的技术和方法 - 信息管理系统 - 专家判断 - 会议 项目风险管理 风险识别 - 输入：企业环境因素、组织过程资产、项目范围说明书、风险管理计划、项目管理计划- 输出：风险识别单- 方法：德尔菲技术(专家判断、大多数原则)、SWOT分析 定性风险分析 - 工具和技术(概率) - 风险概率及影响评估 - 概率及影响矩阵 - 风险数据质量评估 - 风险种类 - 风险紧急度评估 定量风险分析 工作成果：- 项目的概率分析- 实现成本和时间目标的概率- 量化风险优先级清单- 定量风险分析结果的趋势技术方法：- 表示技术：概率分布、专家判断、风险信息访谈- 建模技术：灵敏度分析、期望货币值分析、决策树分析、建模仿真 制定风险应对计划 - 检查措施- 缺陷补救措施：对 在质量审查和审核过程中发现的缺陷 制定的修复和消除影响的措施- 预防措施：消除潜在不良影响，降低风险发生的可能性而需要的措施 (常考)- 纠正措施：消除 已发现的不合格的情况 所采取的措施 监控项目风险 - 工具和技术 - 风险再评估 - 风险审计：检查并记录 - 偏差和趋势分析 - 技术绩效测量 - 储备分析 - 会议- 风险不能消除 项目采购和合同管理 询价 招标 - 信息邀请书RFI(Request For Information) 投标邀请书IFB(Invitation for bid) 报价邀请书RFQ(Request For Quotation) 建议邀请书RFP(Request For Proposal)：征求潜在供应商建议的文件- 集成商在招标阶段的工作顺序 1.研读招标公告 2.编制投标文件 3.提交投标文件 4.参与开标过程- 合同价款应为中标者的投标价 采购管理 合同管理 合同违约管理 - 索赔的性质属于经济补偿，并非惩罚- 在索赔事项发生后28天后，向监理工程师提出索赔意向通知- 承建单位严重违约的，可部分或全部终止合同，并采取善后措施 项目收尾 文档与配置管理需求管理项目管理高级知识大型及复杂项目管理信息系统工程监理- 信息系统工程监理遵循&quot;四控、三管、一协调&quot; - 四控 - 质量控制 - 进度控制 - 投资控制 - 变更控制 - 三管 - 合同管理 - 信息管理 - 安全管理 - 一协调 - 在信息系统工程实施过程中协调有关单位及人员间的工作关系 信息系统类IT信息化知识企业信息化 企业信息化结构 - 产品层- 作业层- 管理层- 决策层 - CRM(客户关系管理) - 自动化的销售、客户服务和市场营销 - 以客户为中心- ERP(企业资源计划) - 企业可以根据自身的情况灵活地选择和集成模块- SCM(供应链管理) - 把正确数量的商品在正确的时间配送到正确的地点的一套管理方法，有效控制各种信息流、资金流和物流 - 最重要的评价指标：客户满意度 客户数据 - 描述性数据：客户的基本信息- 促销性数据：企业为客户提供的产品和服务的历史数据(客服人员建议、广告数据等)- 交易性数据：客户对企业的回馈信息(历史购买记录、投诉数据、客户建议等) other - ssl通信协议用于保护电子商务交易中的敏感数据 系统运维管理 IT运维管理内容 1. 设备管理2. 应用服务3. 数据存储4. 业务5. 目录内容6. 资源资产7. 信息安全8. 日常活动- 对系统进行升级改造属于 开发 ，不属于运维系统运维分类：- 更正性维护- 适应性维护- 完善性维护- 预防性维护 IT服务 - 核心要素：人员(正确选人)、过程(正确做事)、技术(高效做事)和资源(保障做事)- 生命周期： - 规划设计 - 部署实施 - 服务运营 - 持续改进 - 监督管理 一般公认信息系统审计准则：职业准则、ISACA公告、职业道德规范 系统集成企业资质 系统集成主要包括设备系统集成和应用系统集成 我国信息系统服务管理的主要内容：计算机信息系统集成单位资质管理、信息系统项目经理资格管理、信息系统工程监理单位资质管理、信息系统工程监理人员资质管理 项目集成项目成功实施的保障：管理和商务 资质企业要求 人才要求:一级：具有计算机信息系统集成项目经理人数不少于25人，其中高级项目经理人数不少于8名二级：具有计算机信息系统集成项目经理人数不少于18人，其中高级项目经理人数不少于4名三级：具有计算机信息系统集成项目经理人数不少于6人，其中高级项目经理人数不少于1名四级：具有计算机信息系统集成项目经理人数不少于2人收入比例要求：一级：近三年的系统集成收入总额占营业收入总额的比例不低于70％二级：近三年的系统集成收入总额占营业收入总额的比例不低于60％三级：近三年的系统集成收入总额占营业收入总额的比例不低于50％注册资金:一级：不少于5000万元二级：不少于3000万元三级：不少于500万元四级：不少于30万元 云服务 云计算的服务形式 - Iaas(基础设施即服务)：向用户提供计算机(物理机和虚拟机)、存储空间等基本计算资源- Paas(平台即服务)：将软件研发的平台作为一种服务(如数据库管理系统、Web应用系统)- SaaS(软件即服务)：用户租用基于Web的软件来管理企业经营活动 混合云：将公有云(使用计算资源，解决访问量暴增的情况)与私有云(存放数据，安全)进行混合和匹配 移动互联网 物流信息技术主要包括条码技术、RFID技术、EDI技术、GPS技术和GIS技术 物联网 从架构上分为：- 感知层：负责信息采集和物物之间信息传输- 网络层：对采集的数据进行编码认证和传输- 应用层：结合行业信息化需求 商业智能BI- 基本体系结构：数据仓库、联机分析处理、数据挖掘- 主要功能 - 数据仓库(数据存储和访问) - 数据ETL(数据的抽取、转换和加载) - 数据统计输出(统计报表的设计和展示) - 分析功能- 实现层次 - 数据报表 - 多维数据分析 - 数据挖掘 other - 智能制造&quot;炼金术&quot;:信息物理系统CPS- 智能城市 - 数据及服务支撑层(关键技术：SOA、海量数据汇聚与存储、数据融合与处理、智能挖掘分析、协同分析)- 中国制造2025 新一代信息技术产业 - 集成电路及专用装备 - 信息通信设备 - 操作系统及工业软件 信息系统基础软件工程- 软件规模估算方法 - 德尔菲法- 软件设计方法 - V模型方法 - 开发：需求分析 → 概要设计 → 详细设计 → 编码测试 - 测试：验收测试 ← 系统测试 ← 集成测试 ← 单元测试 - 原型法(反复修改来实现用户的最终系统需求) - 用例设计- 软件开发模型 - 瀑布模型(适用于需求明确或很少变更单的项目,结构化分析与设计) - 螺旋模型(瀑布模型+快速原型模型) - 每次迭代的活动依次是 制定计划、风险分析、实施工程、客户评估- 软件文档的质量等级 - 最低限度文档 - 内部文档 - 工作文档 - 正式文档 ruan’jian专题类计算题 三点估算 - 平均时间：(best + 4*average + worst)/6- 正态分布，期望值两边+-1个标准差的范围内，曲线下面积约占总面积的68％ +-2个标准差的范围内，曲线下面积约占总面积的95％ +-3个标准差的范围内，曲线下面积约占总面积的99％ - 标准差 (最坏-最好)/6 挣值估算 - PV:一切按计划 该时间点所需花费- EV:以实际进度为准 该时间段所完成的工作量 计划中所需花费 (按实际进度 计划本该花费)- AC:实际情况 进行到该时间点 的已支出花费- CV：EV-AC (计算费用偏差)- SV: EV-PV (计算进度偏差)- CPI：EV/AC (成本绩效)- SPI：EV/PV (进度绩效)- 完成尚需估算ETC - (非典型) ETC = BAC-EV - (典型) ETC = (BAC-EV)/CPI- 完工估计EAC=AC+ETC 运筹统计 关键路径 历时最长的路径 期望货币值 - 预期收益EMV = 概率*期望- 常结合决策数分析 投资回收期 给你n年时间，你只需要pt年就收回成本了 配置管理 配置管理的目标：为了系统地控制配置变更，在系统的整个生命周期中维持配置的完整性和可综合性，而标识系统在不同时间点上配置的管理。 配置管理的主要活动： 1. 制定配置管理计划2. 配置标识3. 配置控制4. 配置状态报告5. 配置审计 -配置审计也称配置审核或配置评价，包括功能配置审计和物理配置审计，分别用以验证当前配置项的一致性和完整性。其 实施 主要是为了确保项目配置管理的有效性，体现项目配置的最根本要求---不允许出现任何混乱现象. 1. 变更申请 2. 变更评估 3. 通告评估结果 4. 变更实施 5. 变更验证与确认 6. 变更的发布 7. 基于配置库的变更控制6. 发布管理和交付 配置库的分类 - 开发库：也称动态库、程序员库或工作库- 受控库：也称主库，包含当前的基线加上对基线的变更- 产品库：也称静态库、发行库、软件仓库，包含已发布使用的各种基线的存档配置库应该区分开发库和受控库，否则处于已发布状态的项目是不可能被随意修改配置项的. 配置控制委员会(Configuration Control Board,CCB)配置管理员(Configuration Management Officer,CMO) 编写配置管理计划 建立和维护配置管理系统 建立和维护配置库 配置项识别 建立和管理基线 版本管理和配置控制 配置状态报告 配置审计 发布管理和交付 对项目成员进行配置管理培训 合同管理合同分类 按项目付款方式划分的合同分类 总价合同：又称固定价格合同，是指在合同中确定一个完成项目的总价，承包人据此完成项目全部合同内容的合同。承包人(集成商)承担了需求变更等方面带来的风险。—-对甲方有利 成本补偿合同：甲方承担项目实际发生的一切费用，因此也承担了项目的全部风险。—-对甲方不利 工料合同： 成本补偿合同+总价合同 合同支付条款，应该规定以下3方面的内容 - 支付货款的条件- 结算支付的方式- 拒付货款，发包方有权部分或全部拒付货款 采购管理 编制采购管理计划 按照自制/外购的判断因素进行评估 采购计划应提交公司高层领导审批 实施采购 不能以报价最低作为选择乙方的依据(进行充分调研，了解调研采购产品的市场价格，以及潜在供应商的资信情况) 选择标准 应对集成商的经验和业绩(资质和声誉) 做出要求 双方签订合同时应确定明确的需求(就产品的型号、质量进行约定，约定合同交付物必要的质量检验和付款条件的把控) 控制采购 有效管理合同的执行(遇到问题，不能推卸责任) 结束采购 质量管理 质量管理的主要活动 1. 规划质量管理2. 实施质量保证3. 质量控制- 质量管理计划制定和实施过程中需要注意： - 明确质量管理相关各方职责 - 质量管理计划应该由项目经理带领项目组一起完成，并应组织相关人员进行评审，最后需要有效执行 - 明确质量保证(QA)人员的职责 - 区分质量保证和质量控制 QA的主要工作 - 制定质量管理计划- 按照计划实施质量管理活动- 发现问题要记录和沟通直至问题解决- 定期提交质量报告- 为项目组人员提供质量方面的培训 设计评审会议 应该由项目经理组织，QA也可以组织- 项目经理对整个项目负责(包括质量)，设计评审是保证质量的常见方式- QA如果有丰富的技术背景，也可以组织设计评审","categories":[],"tags":[{"name":"软考","slug":"软考","permalink":"http://yoursite.com/tags/软考/"}]},{"title":"Note About NiftyNet_dev","slug":"Note-About-NiftyNet-dev","date":"2018-10-29T12:58:57.000Z","updated":"2018-11-05T06:33:30.347Z","comments":true,"path":"2018/10/29/Note-About-NiftyNet-dev/","link":"","permalink":"http://yoursite.com/2018/10/29/Note-About-NiftyNet-dev/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 项目介绍 NiftyNet是一款开源的卷积神经网络平台，专门针对医学图像处理分析以及医学影像指导治疗,由WEISS (Wellcome EPSRC Centre for Interventional and Surgical Sciences), CMIC ( Centre for Medical Image Computing),HIG(High-dimensional Imaging Group)三家研究机构共同推出。 NiftyNet基于Tensorflow的开源卷积神经网络平台，这一模块化设计的开源平台包含了可共享的网络和预训练模型，在这些模块的帮助下我们可以方便快速地搭建针对医学图像处理的神经网络模型： 利用内置工具使用预训练模型； 将现有网络应用于自有的数据上进行调整； 快速为特殊的图像分析问题生成解决方案原型； 目前的NiftyNet支持医学图像分割和生成对抗网络，这是一个研究型平台、并不保证临床使用的稳定性和准确率，它具有以下一些功能方便医学图像处理的研究： ·用户接口方便的修改个性化网络元件参数； ·可共享网络和预训练模型； ·支持2-D，2.5-D,3-D,4-D的数据输入； ·支持多GPU的高效训练； ·内置了当前前沿的神经网络包括HighRes3DNet, 3D U-net, V-net, DeepMedic等，可以方便的使用； ·对医学图像分割的综合评价度量； NiftyNet支持：图像分割 图像分类（回归） auto-encoder（图像模型表示） GANs（图像生成） 项目结构平台描述：NiftyNet: a deep-learning platform for medical imaging 架构设计 NiftyNet应用程序类通过连接四个组件封装了针对不同医学图像分析应用程序的标准分析管道: Reader： 从文件中加载数据 Sampler： 为之后的处理生成合适的样本 Network： 处理输入 output handler： 包括在培训期间的损失和优化器，以及在推理和评估期间的聚合器 ApplicationDriver： 定义了跨所有应用程序的公共结构，并负责实例化数据分析管道并将计算分布到可用的计算资源 实现框架 Installation由于整个项目是基于Tensorflow，所以需要实现基于对应的版本： pip install tensorflow-gpu==1.3 //安装GPU版本pip install tensorflow==1.3 //安装CPU版本pip install niftynet //安装NiftyNet库，所以依赖可以自动完成安装 代码在PyCharm中运行源码的话，在对应脚本的解释器的配置中添加参数即可: 下载模型 python net_download.py highres3dnet_brain_parcellation_model_zoo 分割 python net_segment.py inference -c ~/niftynet/extensions/highres3dnet_brain_parcellation/highres3dnet_config_eval.ini Run /usr/local/bin/python3.6 /Users/Captain/Desktop/NiftyNet-dev/net_segment.py inference -c ~/niftynet/extensions/highres3dnet_brain_parcellation/highres3dnet_config_eval.iniNiftyNet version f3378259018b927e8fc6b20b06c53ac3886ee3a9 (no suitable tags)[CUSTOM]-- num_classes: 160-- output_prob: False-- label_normalisation: False-- softmax: True-- min_sampling_ratio: 0-- compulsory_labels: (0, 1)-- rand_samples: 0-- min_numb_labels: 1-- proba_connect: True-- evaluation_units: foreground-- inferred: ()-- weight: ()-- sampler: ()-- label: ()-- image: (&apos;Modality0&apos;,)-- name: net_segment[CONFIG_FILE]-- path: /Users/Captain/niftynet/extensions/highres3dnet_brain_parcellation/highres3dnet_config_eval.ini[MODALITY0]-- csv_file: -- path_to_search: data/OASIS/-- filename_contains: (&apos;nii&apos;,)-- filename_not_contains: ()-- filename_removefromid: -- interp_order: 0-- loader: None-- pixdim: (1.0, 1.0, 1.0)-- axcodes: (&apos;R&apos;, &apos;A&apos;, &apos;S&apos;)-- spatial_window_size: (96, 96, 96)[SYSTEM]-- cuda_devices: &quot;&quot;-- num_threads: 2-- num_gpus: 1-- model_dir: /Users/Captain/niftynet/models/highres3dnet_brain_parcellation-- dataset_split_file: ./dataset_split.csv-- event_handler: (&apos;model_saver&apos;, &apos;model_restorer&apos;, &apos;sampler_threading&apos;, &apos;apply_gradients&apos;, &apos;output_interpreter&apos;, &apos;console_logger&apos;, &apos;tensorboard_logger&apos;)-- iteration_generator: iteration_generator-- action: inference[NETWORK]-- name: highres3dnet-- activation_function: relu-- batch_size: 1-- smaller_final_batch_mode: pad-- decay: 0.0-- reg_type: L2-- volume_padding_size: (10, 10, 10)-- volume_padding_mode: minimum-- window_sampling: uniform-- queue_length: 5-- multimod_foreground_type: and-- histogram_ref_file: databrain_std_hist_models_otsu.txt-- norm_type: percentile-- cutoff: (0.001, 0.999)-- foreground_type: mean_plus-- normalisation: True-- whitening: True-- normalise_foreground_only: True-- weight_initializer: he_normal-- bias_initializer: zeros-- keep_prob: 1.0-- weight_initializer_args: &#123;&#125;-- bias_initializer_args: &#123;&#125;[INFERENCE]-- spatial_window_size: (128, 128, 128)-- inference_iter: 33000-- dataset_to_infer: -- save_seg_dir: ./parcellation_output-- output_postfix: _niftynet_out-- output_interp_order: 0-- border: (2, 2, 2)INFO:niftynet: starting segmentation applicationINFO:niftynet: `csv_file = ` not found, writing to &quot;/Users/Captain/niftynet/models/highres3dnet_brain_parcellation/Modality0.csv&quot; instead.INFO:niftynet: Overwriting existing: &quot;/Users/Captain/niftynet/models/highres3dnet_brain_parcellation/Modality0.csv&quot;.INFO:niftynet: [Modality0] search file folders, writing csv file /Users/Captain/niftynet/models/highres3dnet_brain_parcellation/Modality0.csvINFO:niftynet: Number of subjects 1, input section names: [&apos;subject_id&apos;, &apos;Modality0&apos;]-- using all subjects (without data partitioning).INFO:niftynet: Image reader: loading 1 subjects from sections (&apos;Modality0&apos;,) as input [image]INFO:niftynet: normalisation histogram reference models ready for image:(&apos;Modality0&apos;,)2018-11-04 18:06:59.209921: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMAINFO:niftynet: initialised window instanceINFO:niftynet: initialised grid sampler &#123;&apos;image&apos;: (1, 128, 128, 128, 1, 1), &apos;image_location&apos;: (1, 7)&#125;INFO:niftynet: using HighRes3DNetINFO:niftynet: Initialising dataset from generator...INFO:niftynet: starting from iter 33000INFO:niftynet: Accessing /Users/Captain/niftynet/models/highres3dnet_brain_parcellation/models/model.ckpt-33000INFO:niftynet: Restoring parameters from /Users/Captain/niftynet/models/highres3dnet_brain_parcellation/models/model.ckpt-33000INFO:niftynet: grid sampling image sizes: &#123;&apos;image&apos;: (180, 276, 276, 1, 1)&#125;INFO:niftynet: grid sampling window sizes: &#123;&apos;image&apos;: (128, 128, 128, 1, 1)&#125;INFO:niftynet: yielding 27 locations from imageINFO:niftynet: inference iter 0, (398.539150s)INFO:niftynet: inference iter 1, (313.655015s)INFO:niftynet: inference iter 2, (297.785388s)INFO:niftynet: inference iter 3, (2059.323541s)INFO:niftynet: inference iter 4, (357.954505s)INFO:niftynet: inference iter 5, (315.965626s)INFO:niftynet: inference iter 6, (380.311502s)INFO:niftynet: inference iter 7, (312.727651s)INFO:niftynet: inference iter 8, (373.855815s)INFO:niftynet: inference iter 9, (386.695381s)INFO:niftynet: inference iter 10, (300.015597s)INFO:niftynet: inference iter 11, (268.670928s)INFO:niftynet: inference iter 12, (303.761694s)INFO:niftynet: inference iter 13, (308.727933s)INFO:niftynet: inference iter 14, (313.504975s)INFO:niftynet: inference iter 15, (283.154703s)INFO:niftynet: inference iter 16, (280.827804s)INFO:niftynet: inference iter 17, (286.342750s)INFO:niftynet: inference iter 18, (277.392094s)INFO:niftynet: inference iter 19, (289.777673s)INFO:niftynet: inference iter 20, (304.182126s)INFO:niftynet: inference iter 21, (304.781269s)INFO:niftynet: inference iter 22, (286.984278s)INFO:niftynet: inference iter 23, (299.828858s)INFO:niftynet: inference iter 24, (314.818588s)INFO:niftynet: inference iter 25, (322.952987s)INFO:niftynet: inference iter 26, (273.040366s)INFO:niftynet: inference iter 27, (280.480217s)Saved /Users/Captain/niftynet/models/highres3dnet_brain_parcellation/parcellation_output/OAS1_0145_MR2_mpr_n4_anon_sbj_111_niftynet_out.nii.gzINFO:niftynet: stopping -- event handler: OutputInterpreter.INFO:niftynet: cleaning up...INFO:niftynet: stopping sampling threadsINFO:niftynet: SegmentationApplication stopped (time in second 10501.22).Process finished with exit code 0注：Mac pro A1502 运行了近三个小时 Run Problem Install packages failed - cv2 - pip3 install opencv-python- yaml - pip3 install pyyaml Others - The NiftyNetExamples server is not running - 直接从他处拷贝至~/niftynet 执行结果：(运行完成会生成一个100__niftynet_out.nii文件，此文件可以用spm12和mriCron打开) 文档文档主要包括三个主要部分，分别是指引、资源和接口参考三个部分： Guide部分： 主要包括平台简介、安装指南和配置文件的设置；同时还有一个模型库可供用户选择合适的模型适配具体的问题；如果无法满足需求的情况下，它还提供了如何建立网络的教程，可以一步一步创建自己的新网络； Resource部分： 这里包含了一系列资源，除了项目的网络还包括源码和源码镜像、以及模型库。同时还提供了Stack Overflow提问区域供用户交流； API参考部分： 网络几大模块的说明，主要有application、contrib、engine、evaluation、io、layer、network和utilities等功能包供用户使用，每一个都有详尽的参数描述和使用指南； 相关链接： https://pypi.org/project/NiftyNet/","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[{"name":"NiftyNet","slug":"NiftyNet","permalink":"http://yoursite.com/tags/NiftyNet/"}]},{"title":"gitNode_js","slug":"gitNode-js","date":"2018-10-28T13:40:14.000Z","updated":"2018-12-28T13:41:39.687Z","comments":true,"path":"2018/10/28/gitNode-js/","link":"","permalink":"http://yoursite.com/2018/10/28/gitNode-js/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… Node.js v10.15.0 文档 let child_process = require('child_process');let path = require('path')console.log(path.resolve(__dirname, '../portal_build'))let iocmds = [ `cp -r dist ../portal_build`, `cp -r views ../portal_build`,]let cmds = [ `git add .`, `git commit -m 'update in $&#123;new Date()&#125;'`, `git push`]for (let iocmd of iocmds) &#123; child_process.execSync(iocmd, (err, stdout, stderr) =&gt; &#123; if (err) return console.log(err) console.log(stdout) &#125;);&#125;for (let cmd of cmds) &#123; child_process.execSync(cmd, &#123; cwd: '/Users/Captain/Downloads/Github/Learning_DenseNet' &#125;, (err, stdout, stderr) =&gt; &#123; if (err) return console.log(err) console.log(stdout) &#125;);&#125;","categories":[],"tags":[{"name":"Node.js","slug":"Node-js","permalink":"http://yoursite.com/tags/Node-js/"}]},{"title":"Note_About_ADNI","slug":"Note-About-ADNI","date":"2018-10-28T01:46:32.000Z","updated":"2018-11-30T11:55:00.702Z","comments":true,"path":"2018/10/28/Note-About-ADNI/","link":"","permalink":"http://yoursite.com/2018/10/28/Note-About-ADNI/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】ADNI 官网介绍笔记","text":"【阅读时间】XXX min XXX words【阅读内容】ADNI 官网介绍笔记 数据访问权限申请链接 Welcome阿尔茨海默病神经影像学倡议（ADNI）将研究人员与研究数据联系起来，因为它们致力于确定阿尔茨海默病（AD）的进展。ADNI研究人员收集，验证和利用数据，包括MRI和PET图像，遗传学，认知测试，脑脊液和血液生物标志物作为疾病的预测因子。来自北美ADNI研究的研究资源和数据可通过本网站获得，包括阿尔茨海默病患者，轻度认知障碍受试者和老年人控制。 欢迎来到网站，其中添加了ADNI3的内容。该网站建立在ADNI1，ADNI-GO和ADNI2研究的基础上，旨在确定整个阿尔茨海默病的临床，认知，成像，遗传和生物化学生物标志物之间的关系。ADNI3将继续努力发现，优化，标准化和验证AD临床研究中使用的临床试验措施和生物标志物。 ABOUTADNI研究的三个总体目标是： 在尽可能早的阶段（痴呆前）检测AD，并确定用生物标志物跟踪疾病进展的方法。 通过在尽可能早的阶段应用新的诊断方法（此时干预可能最有效），支持AD干预，预防和治疗的进展。 持续管理ADNI的创新数据访问政策，该政策提供所有数据(数据共享)，而不是对世界上所有科学家进行禁运。 STUDY DESIGN关于BIOMARKERS生物标志物是生物状态的物质、测量或指标。在临床症状出现之前可能存在生物标志物。ADNI使用各种生物标志物来帮助预测阿尔茨海默病的发病。 该图描绘了生物标志物作为AD的指标。曲线表明AD过程中五种生物标志物从正常到异常的变化（对痴呆的正常认知） 在脑脊液中或通过淀粉样蛋白PET成像测量的β-淀粉样蛋白（Aβ） 由脑脊液中测量的tau蛋白表示的神经变性，或通过FDG-PET测量的突触功能障碍 脑结构萎缩，主要在内侧颞叶，通过结构MRI测量 记忆丧失，通过认知测试来衡量 临床功能，通过认知测试测量的一般认知下降表示。 变化1-3表示可以在痴呆诊断之前观察到的生物标志物，而变化4-5是痴呆症诊断的经典指标。 CLINICIAL STUDY CN：正常衰老/认知正常. CN参与者是ADNI研究中的对照受试者。他们没有表现出抑郁，轻度认知障碍或痴呆的迹象。 SMC 重要记忆关注 – 解决健康老年人对照组与MCI之间的差距 MCI： 轻度认知障碍. MCI参与者日常活动基本上得到保留，其他认知领域没有显着的损伤水平，也没有痴呆症的迹象。使用Wechsler Memory Scale Logical Memory II确定MCI水平（早期或晚期）。 EMCI：早期认知障碍. LMCI：晚期认知障碍. AD：Alzheimer disease. 阿尔茨海默病 DATA TYPESADNI研究人员在参与研究期间从研究志愿者那里收集了几种类型的数据，使用一套标准的协议和程序来消除不一致性。此信息可通过LONI图像和数据存档（IDA）免费提供给授权的调查员 。 临床ADNI临床数据集包括关于每个受试者的临床信息，包括招募，人口统计学，身体检查和认知评估数据。可以将整套临床数据作为逗号分隔值（CSV）文件批量下载。 遗传遗传因素在阿尔茨海默病中起重要作用。全基因组关联研究（GWAS）采用标记之间关联的测试，称为单核苷酸多态性（SNP）和感兴趣的表型。来自病例对照GWAS和其他类型的遗传关联研究的发现可以提供用于检查源自ADNI成像和其他生物标志物数据集的定量表型的目标。 APOE的4等位基因是已知的AD最强大的遗传风险因素，如果拥有一个4等位基因的人患AD的风险增加了2- 3倍，那么如果有两个等位基因的人患AD的风险增加了12倍。 MRI图像MRI – 核磁共振成像 原始，预处理和后处理图像文件，FMRI和DTI 这些图像的收集对于满足ADNI开发生物标记物以追踪阿尔茨海默病的进展和潜在病理学变化的目标至关重要。 该项目将收集MRI（结构，扩散加权成像，灌注和静息状态序列）; 使用florbetapir F18（florbetapir）或florbetaben F18（florbetaben）的淀粉样蛋白PET; 18F-FDG-PET（FDG-PET）; CSF用于Aβ，tau，磷酸化tau（AKA磷酸化酶）和其他蛋白质; AV-1451 PET; 和遗传和尸检数据，以确定这些生物标志物与基线临床状态和认知下降的关系。 PET图像PET – 正电子发射型计算机断层显像 原始，预处理和后处理图像文件，PIB（ADNI1），FDG（ADNI1 / GO / 2），FLORBETAPIR（ADNI GO / 2/3），FLORBETABEN（ADNI3）和TAU IMAGING（ADNI3）这些图像的收集对于满足ADNI开发生物标记物以追踪阿尔茨海默病的进展和潜在病理学变化的目标至关重要。 an overview of the PET data collected throughout the ADNI study AVAILABLE IMAGE DATA 生物样本ADNI的目标之一是收集参与者的血液，尿液和脑脊液（CSF）等生物样本。鼓励有兴趣的调查员，无论是否与ADNI网站相关联，都可以申请使用这种有限的资源。但是，除非初步数据显示出明显优越的性能，否则不建议将ADNI样本用于技术开发或不同技术之间的比较。 METHODS AND TOOLS 生物标记分析 遗传数据方法 蛋白质组分析 MRI分析 PET分析 神经病学方法 RARC批准的研究","categories":[],"tags":[{"name":"ADNI","slug":"ADNI","permalink":"http://yoursite.com/tags/ADNI/"}]},{"title":"Lecture_Convolutional_Neural_Networks","slug":"Lecture-Convolutional-Neural-Networks","date":"2018-10-27T10:18:13.000Z","updated":"2018-11-19T16:58:27.908Z","comments":true,"path":"2018/10/27/Lecture-Convolutional-Neural-Networks/","link":"","permalink":"http://yoursite.com/2018/10/27/Lecture-Convolutional-Neural-Networks/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 卷积神经网络（CNNs/ConvNets）卷积神经网络和上一章讲的常规神经网络非常相似： 它们都是由神经元组成,神经元中有具有学习能力的权重和偏差.每个神经元都得到一些输入数据,进行内积运算后再进行激活函数运算. 整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出是不同类别的评分. 在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或Softmax），并且在神经网络中我们实现的各种技巧和要点依旧适用于卷积神经网络. 那么有哪些地方变化了呢？卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质.这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量. 结构概述 回顾：常规神经网络. ​ 在上一章中，神经网络的输入是一个向量，然后在一系列的隐层中对它做变换.每个隐层都是由若干的神经元组成，每个神经元都与前一层中的所有神经元连接.但是在一个隐层中，神经元相互独立不进行任何连接.最后的全连接层被称为“输出层”，在分类问题中，它输出的值被看做是不同类别的评分值. 常规神经网络对于大尺寸图像效果不尽人意. 在CIFAR-10中，图像的尺寸是32x32x3（宽高均为32像素，3个颜色通道），因此，对应的的常规神经网络的第一个隐层中，每一个单独的全连接神经元就有32x32x3=3072个权重.这个数量看起来还可以接受，但是很显然这个全连接的结构不适用于更大尺寸的图像.举例说来，一个尺寸为200x200x3的图像，会让神经元包含200x200x3=120,000个权重值.而网络中肯定不止一个神经元，那么参数的量就会快速增加！显而易见，这种全连接方式效率低下，大量的参数也很快会导致网络过拟合. 神经元的三维排列. 卷积神经网络针对输入全部是图像的情况，将结构调整得更加合理，获得了不小的优势.与常规神经网络不同，卷积神经网络的各层中的神经元是3维排列的：宽度、高度和深度（这里的深度指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数）.举个例子，CIFAR-10中的图像是作为卷积神经网络的输入，该数据体的维度是32x32x3（宽度，高度和深度）.我们将看到，层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式.对于用来分类CIFAR-10中的图像的卷积网络，其最后的输出层的维度是1x1x10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，向量是在深度方向排列的. 左边是一个3层的神经网络.右边是一个卷积神经网络，图例中网络将它的神经元都排列成3个维度（宽、高和深度）.卷积神经网络的每一层都将3D的输入数据变化为神经元3D的激活数据并输出.在这个例子中，红色的输入层装的是图像，所以它的宽度和高度就是图像的宽度和高度，它的深度是3（代表了红、绿、蓝3种颜色通道）. 卷积神经网络是由层组成的.每一层都有一个简单的API：用一些含或者不含参数的可导的函数，将输入的3D数据变换为3D的输出数据. 用来构建卷积神经网络的各种层一个简单的卷积神经网络是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层.卷积神经网络主要由三种类型的层构成：卷积(Convolutional)层，汇聚(Pooling)层和全连接(Full-Connected)层（全连接层和常规神经网络中的一样）.通过将这些层叠加起来，就可以构建一个完整的卷积神经网络. 网络结构例子：这仅仅是个概述，下面会更详解的介绍细节.一个用于CIFAR-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层].细节如下： 输入[32x32x3]存有图像的原始像素值，本例中图像宽高均为32，有3个颜色通道. 卷积层中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积.卷积层会计算所有神经元的输出.如果我们使用12个滤波器（也叫作核），得到的输出数据体的维度就是[32x32x12]. ReLU层将会逐个元素地进行激活函数操作,比如使用以0为阈值的$max(0,x)$作为激活函数.该层对数据尺寸没有改变,还是[32x32x12]. 汇聚层在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]. 全连接层将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值.正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接. 由此看来，卷积神经网络一层一层地将图像从原始像素值变换成最终的分类评分值.其中有的层含有参数，有的没有.具体说来，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）.而ReLU层和汇聚层则是进行一个固定不变的函数操作.卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了. 小结： 简单案例中卷积神经网络的结构，就是一系列的层将输入数据变换为输出数据（比如分类评分）. 卷积神经网络结构中有几种不同类型的层（目前最流行的有卷积层、全连接层、ReLU层和汇聚层）. 每个层的输入是3D数据，然后使用一个可导的函数将其变换为3D的输出数据. 有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）. 有的层有额外的超参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）. 卷积神经网络的激活输出例子 左边的输入层存有原始图像像素，右边的输出层存有类别分类评分.在处理流程中的每个激活数据体是铺成一列来展示的.因为对3D数据作图比较困难，我们就把每个数据体切成层，然后铺成一列显示.最后一层装的是针对不同类别的分类得分，这里只显示了得分最高的5个评分值和对应的类别.完整的网页演示在我们的课程主页.本例中的结构是一个小的VGG网络，VGG网络后面会有讨论. 现在讲解不同的层，层的超参数和连接情况的细节. 卷积层 Convolutional Layer卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量. 概述和直观介绍：首先讨论的是，在没有大脑和生物意义上的神经元之类的比喻下，卷积层到底在计算什么.卷积层的参数是有一些可学习的滤波器集合构成的.每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致.举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）.在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积.当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应.直观地来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状或者车轮状图案. 在每个卷积层上，我们会有一整个集合的滤波器（比如12个），每个都会生成一个不同的二维激活图.将这些激活映射在深度方向上层叠起来就生成了输出数据. 以大脑做比喻：如果你喜欢用大脑和生物神经元来做比喻，那么输出的3D数据中的每个数据项可以被看做是神经元的一个输出，而该神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）.现在开始讨论神经元的连接，它们在空间中的排列，以及它们参数共享的模式. 局部连接：在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的.相反，我们让每个神经元只与输入数据的一个局部区域连接.该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）.在深度方向上，这个连接的大小总是和输入量的深度相等.需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致. 例1：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）.注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致. 例2：假设输入数据体的尺寸是[16x16x20]，感受野尺寸是3x3，那么卷积层中每个神经元和输入数据体就有3x3x20=180个连接.再次提示：在空间上连接是局部的（3x3），但是在深度上是和输入数据体一致的（20）. 左边：红色的是输入数据体（比如CIFAR-10中的图像），蓝色的部分是第一个卷积层中的神经元.卷积层中的每个神经元都只是与输入数据体的一个局部在空间上相连，但是与输入数据体的所有深度维度全部相连（所有颜色通道）.在深度方向上有多个神经元（本例中5个），它们都接受输入数据的同一块区域（感受野相同）.至于深度列的讨论在下文中有. 右边：神经网络章节中介绍的神经元保持不变，它们还是计算权重和输入的内积，然后进行激活函数运算，只是它们的连接被限制在一个局部空间. 空间排列：上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式.3个超参数控制着输出数据体的尺寸：深度（depth），步长（stride）和零填充（zero-padding）.下面是对它们的讨论： 首先，输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西.举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活.我们将这些沿着深度方向排列、感受野相同的神经元集合称为深度列（depth column），也有人使用纤维（fibre）来称呼它们. 其次，在滑动滤波器的时候，必须指定步长.当步长为1，滤波器每次移动1个像素.当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素.这个操作会让输出数据体在空间上变小. 在下文可以看到，有时候将输入数据体用0在边缘处进行填充是很方便的.这个零填充（zero-padding）的尺寸是一个超参数.零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）. 输出数据体在空间上的尺寸可以通过输入数据体尺寸（$W$），卷积层中神经元的感受野尺寸（$F$），步长（$S$）和零填充的数量（$P$）的函数来计算.（这里假设输入数组的空间形状是正方形，即高度和宽度相等）输出数据体的空间尺寸为(W-F+2P)/S+1.比如输入是7x7，滤波器是3x3，步长为1，填充为0，那么就能得到一个5x5的输出(其中$\\frac{7-3+0}{1}+1=5$ ）.如果步长为2，输出就是3x3(其中 $\\frac{7-3+0}{2}+1=3$ ). 空间排列的图示 在本例中只有一个空间维度（x轴），神经元的感受野尺寸F=3，输入尺寸W=5，零填充P=1. 左边：神经元使用的步长S=1，所以输出尺寸是(5-3+2)/1+1=5. 右边：神经元的步长S=2，则输出尺寸是(5-3+2)/2+1=3. 注意当步长S=3时是无法使用的，因为它无法整齐地穿过数据体.从等式上来说，因为(5-3+2)=4是不能被3整除的. 本例中，神经元的权重是[1,0,-1]，显示在图的右上角，偏差值为0.这些权重是被所有黄色的神经元共享的. 使用零填充：在上面左边例子中，注意输入维度是5，输出维度也是5.之所以如此，是因为感受野是3并且使用了1的零填充.如果不使用零填充，则输出数据体的空间维度就只有3，因为这就是滤波器整齐滑过并覆盖原始数据需要的数目.一般说来，当步长$S=1$时，零填充的值是$P=(F-1)/2$，这样就能保证输入和输出数据体有相同的空间尺寸. 步长的限制：注意这些空间排列的超参数之间是相互限制的.举例说来，当输入尺寸$W=10$，不使用零填充则$P=0$，滤波器尺寸$F=3$，这样步长$S=2$就行不通，因为$(W-F+2P)/S+1=(10-3+0)/2+1=4.5$，结果不是整数，这就是说神经元不能整齐对称地滑过输入数据体.因此，这些超参数的设定就被认为是无效的，一个卷积神经网络库可能会报出一个错误，或者修改零填充值来让设置合理，或者修改输入数据体尺寸来让设置合理，或者其他什么措施.在后面的卷积神经网络结构小节中，读者可以看到合理地设置网络的尺寸让所有的维度都能正常工作，这件事可是相当让人头痛的.而使用零填充和遵守其他一些设计策略将会有效解决这个(空间尺寸计算结果不是整数，致使网络库报错)问题. 真实案例：Krizhevsky构架赢得了2012年的ImageNet挑战，其输入图像的尺寸是[227x227x3].在第一个卷积层，神经元使用的感受野尺寸$F=11$，步长$S=4$，不使用零填充$P=0$.因为(227-11)/4+1=55，卷积层的深度$K=96$，则卷积层的输出数据体尺寸为[55x55x96]. 55x55x96个神经元中，每个都和输入数据体中一个尺寸为[11x11x3]的区域全连接.在深度列上的96个神经元都是与输入数据体中同一个[11x11x3]区域连接，但是权重不同.有一个有趣的细节，在原论文中，说的输入图像尺寸是224x224，这是肯定错误的，因为(224-11)/4+1的结果不是整数.这件事在卷积神经网络的历史上让很多人迷惑，而这个错误到底是怎么发生的没人知道.我的猜测是Alex忘记在论文中指出自己使用了尺寸为3的额外的零填充. 参数共享：在卷积层中使用参数共享是用来控制参数的数量.就用上面的例子，在第一个卷积层就有55x55x96=290,400个神经元，每个有11x11x3=364个参数和1个偏差.将这些合起来就是290400x364=105,705,600个参数.单单第一层就有这么多参数，显然这个数目是非常大的. 作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用.基于这个假设，可以显著地减少参数数量.换言之，就是将深度维度上一个单独的2维切片看做深度切片（depth slice），比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55].在每个深度切片上的神经元都使用同样的权重和偏差.在这样的参数共享下，例子中的第一个卷积层就只有96个不同的权重集了，一个权重集对应一个深度切片，共有96x11x11x3=34,848个不同的权重，或34,944个参数（+96个偏差）.在每个深度切片中的55x55个权重使用的都是同样的参数.在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度.这样，每个切片只更新一个权重集. 注意，如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的卷积（这就是“卷积层”名字由来）.这也是为什么总是将这些权重集合称为滤波器（filter）（或卷积核（kernel）），因为它们和输入进行了卷积. Krizhevsky等学习到的滤波器例子 这96个滤波器(权重集合)的尺寸都是[11x11x3]，在一个深度切片中，每个滤波器都被55x55个神经元共享.注意参数共享的假设是有道理的：如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也会同样是有用的，这是因为图像结构具有平移不变性.所以在卷积层的输出数据体的55x55个不同位置中，就没有必要重新学习去探测一个水平边界了. 注意有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候.这时候我们就应该期望在图片的不同位置学习到完全不同的特征.一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心.你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习.在这个例子中，通常就放松参数共享的限制，将层称为局部连接层（Locally-Connected Layer）. Numpy例子：为了让讨论更加的具体，我们用代码来展示上述思路.假设输入数据体是numpy数组X.那么： 一个位于(x,y)的深度列（或纤维）将会是X[x,y,:]. 在深度为d处的深度切片，或激活图应该是X[:,:,d]. 卷积层例子：假设输入数据体X的尺寸X.shape:(11,11,4)，不使用零填充($P=0$)，滤波器的尺寸是$F=5$，步长$S=2$.那么输出数据体的空间尺寸就是(11-5)/2+1=4，即输出数据体的宽度和高度都是4.那么在输出数据体中的激活映射（称其为V）看起来就是下面这样（在这个例子中，只有部分元素被计算）： - V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0- V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0- V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0- V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0... 在numpy中，*操作是进行数组间的逐元素相乘.权重向量W0是该神经元的权重，b0是其偏差.在这里，W0被假设尺寸是W0.shape: (5,5,4)，因为滤波器的宽高是5，输入数据量的深度是4.注意在每一个点，计算点积的方式和之前的常规神经网络是一样的.同时，计算内积的时候使用的是同一个权重和偏差（因为参数共享），在宽度方向的数字每次上升2（因为步长为2）.要构建输出数据体中的第二张激活图，代码应该是： - V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1- V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1- V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1- V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1- V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1 （在y方向上）- V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1 （或两个方向上同时）.... 我们访问的是V的深度维度上的第二层（即index1），因为是在计算第二个激活图，所以这次试用的参数集就是W1了.在上面的例子中，为了简洁,略去了卷积层对于输出数组V中其他部分的操作.还有，要记得这些卷积操作通常后面接的是ReLU层，对激活图中的每个元素做激活函数运算，这里没有显示. 小结： 我们总结一下卷积层的性质： 输入数据体的尺寸为$W_1\\times H_1\\times D_1$ 4个超参数： 滤波器的数量$K$ 滤波器的空间尺寸$F$ 步长$S$ 零填充数量$P$ 输出数据体的尺寸为$W_2\\times H_2\\times D_2$ ，其中： $W_2=(W_1-F+2P)/S+1$ (W–width H–height) $H_2=(H_1-F+2P)/S+1$ （宽度和高度的计算方法相同） $D_2=K$ 由于参数共享，每个滤波器包含$F\\cdot F\\cdot D_1$个权重，卷积层一共有$F\\cdot F\\cdot D_1\\cdot K$个权重和$K$个偏置. (k为滤波器的数目) 在输出数据体中，第$d$个深度切片（空间尺寸是$W_2\\times H_2$），用第$d$个滤波器和输入数据进行有效卷积运算的结果（使用步长$S$），最后在加上第$d$个偏差. 对这些超参数，常见的设置是$F=3$，$S=1$，$P=1$.同时设置这些超参数也有一些约定俗成的惯例和经验，可以在下面的卷积神经网络结构章节中查看. 卷积层演示：下面是一个卷积层的运行演示.因为3D数据难以可视化，所以所有的数据（输入数据体是蓝色，权重数据体是红色，输出数据体是绿色）都采取将深度切片按照列的方式排列展现.输入数据体的尺寸是$W_1=5,H_1=5,D_1=3$，卷积层参数$K=2,F=3,S=2,P=1$.就是说，有2个滤波器，滤波器的尺寸是$3\\cdot 3$，它们的步长是2.因此，输出数据体的空间尺寸是(5-3+2)/2+1=3.注意输入数据体使用了零填充$P=1$，所以输入数据体外边缘一圈都是0.下面的例子在绿色的输出激活数据上循环演示，展示了其中每个元素都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后求其总和，最后加上偏差得来. Convolution demo.webarchive 用矩阵乘法实现：卷积运算本质上就是在滤波器和输入数据的局部区域间做点积.卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法： 输入图像的局部区域被im2col操作拉伸为列.比如，如果输入是[227x227x3]，要与尺寸为11x11x3的滤波器以步长为4进行卷积，就取输入中的[11x11x3]数据块，然后将其拉伸为长度为11x11x3=363的列向量.重复进行这一过程，因为步长为4，所以输出的宽高为(227-11)/4+1=55，所以得到im2col操作的输出矩阵X_col的尺寸是[363x3025]，其中每列是拉伸的感受野，共有55x55=3,025个.注意因为感受野之间有重叠，所以输入数据体中的数字在不同的列中可能有重复. 卷积层的权重也同样被拉伸成行.举例，如果有96个尺寸为[11x11x3]的滤波器，就生成一个矩阵W_row，尺寸为[96x363]. 现在卷积的结果和进行一个大矩阵乘np.dot(W_row, X_col)是等价的了，能得到每个滤波器和每个感受野间的点积.在我们的例子中，这个操作的输出是[96x3025]，给出了每个滤波器在每个位置的点积输出. 结果最后必须被重新变为合理的输出尺寸[55x55x96]. 这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在X_col中被复制了多次.但是，其优点是矩阵乘法有非常多的高效实现方式，我们都可以使用（比如常用的BLAS API）.还有，同样的im2col思路可以用在汇聚操作中. 反向传播：卷积操作的反向传播（同时对于数据和权重）还是一个卷积（但是是和空间上翻转的滤波器）.使用一个1维的例子比较容易演示. 1x1卷积：一些论文中使用了1x1的卷积，这个方法最早是在论文Network in Network中出现.人们刚开始看见这个1x1卷积的时候比较困惑，尤其是那些具有信号处理专业背景的人.因为信号是2维的，所以1x1卷积就没有意义.但是，在卷积神经网络中不是这样，因为这里是对3个维度进行操作，滤波器和输入数据体的深度是一样的.比如，如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）. 扩张卷积：最近一个研究（Fisher Yu和Vladlen Koltun的论文）给卷积层引入了一个新的叫扩张（dilation）的超参数.到目前为止，我们只讨论了卷积层滤波器是连续的情况.但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张.举例，在某个维度上滤波器w的尺寸是3，那么计算输入x的方式是：w[0]*x[0] + w[1]*x[1] + w[2]*x[2]，此时扩张为0.如果扩张为1，那么计算为： w[0]*x[0] + w[1]*x[2] + w[2]*x[4].换句话说，操作中存在1的间隙.在某些设置中，扩张卷积与正常卷积结合起来非常有用，因为在很少的层数内更快地汇集输入图片的大尺度特征.比如，如果上下重叠2个3x3的卷积层，那么第二个卷积层的神经元的感受野是输入数据体中5x5的区域（可以成这些神经元的有效感受野是5x5）.如果我们对卷积进行扩张，那么这个有效感受野就会迅速增长. 汇聚层 Pooling Layer通常，在连续的卷积层之间会周期性地插入一个汇聚层.它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合.汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸.最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉.每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）.深度保持不变.汇聚层的一些公式： 输入数据体尺寸$W_1\\cdot H_1\\cdot D_1$ 有两个超参数： 空间大小$F$ 步长$S$ 输出数据体尺寸$W_2\\cdot H_2\\cdot D_2$，其中 $ W_2=(W_1-F)/S+1$ $H_2=(H_1-F)/S+1$ $D_2=D_1$ 因为对输入进行的是固定函数计算，所以没有引入参数 在汇聚层中很少使用零填充 在实践中，最大汇聚层通常只有两种形式：一种是$F=3,S=2$，也叫重叠汇聚（overlapping pooling），另一个更常用的是$F=2,S=2$.对更大感受野进行汇聚需要的汇聚尺寸也更大，而且往往对网络有破坏性. 普通汇聚（General Pooling）：除了最大汇聚，汇聚单元还可以使用其他的函数，比如平均汇聚（average pooling）或L-2范式汇聚（L2-norm pooling）.平均汇聚历史上比较常用，但是现在已经很少使用了.因为实践证明，最大汇聚的效果比平均汇聚要好. 汇聚层在输入数据体的每个深度切片上，独立地对其进行空间上的降采样. 左边：本例中，输入数据体尺寸[224x224x64]被降采样到了[112x112x64]，采取的滤波器尺寸是2，步长为2，而深度不变((224-2+0)/2+1=112). 右边：最常用的降采样操作是取最大值，也就是最大汇聚，这里步长为2，每个取最大值操作是从4个数字中选取（即2x2的方块区域中）. 反向传播：回顾一下反向传播的内容，其中$max(x,y)$函数的反向传播可以简单理解为将梯度只沿最大的数回传.因此，在向前传播经过汇聚层的时候，通常会把池中最大元素的索引记录下来（有时这个也叫作道岔（switches）），这样在反向传播的时候梯度的路由就很高效. 不使用汇聚层：很多人不喜欢汇聚操作，认为可以不使用它.比如在Striving for Simplicity: The All Convolutional Net一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层.通过在卷积层中使用更大的步长来降低数据体的尺寸.有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的.比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）.现在看起来，未来的卷积网络结构中，可能会很少使用甚至不使用汇聚层. 归一化层 Normalization Layer在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制.但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的.对于不同类型的归一化层，可以看看Alex Krizhevsky的关于cuda-convnet library API的讨论. 全连接层 Full-Connected Layer在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样.它们的激活可以先用矩阵乘法，再加上偏差.更多细节请查看神经网络章节. 将全连接层转化成卷积层全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数.然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的.因此，将此两者相互转化是可能的： 对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层.权重矩阵是一个巨大的矩阵，除了某些特定块（这是因为有局部连接），其余部分都是零.而在其中大部分块中，元素都是相等的（因为参数共享）. 相反，任何全连接层都可以被转化为卷积层.比如，一个$K=4096$的全连接层，输入数据体的尺寸是$7\\times 7\\times 512$，这个全连接层可以被等效地看做一个$F=7,P=0,S=1,K=4096$的卷积层.换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了.因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成$1\\times 1\\times 4096$，这个结果就和使用初始的那个全连接层一样了. 全连接层转化为卷积层：在两种变换中，将全连接层转化为卷积层在实际运用中更加有用.假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）.从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分.我们可以将这3个全连接层中的任意一个转化为卷积层： 针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为$F=7$，这样输出数据体就为[1x1x4096]了. 针对第二个全连接层，令其滤波器尺寸为$F=1$，这样输出数据体为[1x1x4096]. 对最后一个全连接层也做类似的，令其$F=1$，最终输出为[1x1x1000] 实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器.那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动（即把一张更大的图片的不同区域都分别带入到卷积网络，得到每个区域的得分），得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作. 举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分.上述的把全连接层转换成卷积层的做法会更简便.如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组（因为途径5个汇聚层，尺寸变为224/2/2/2/2/2 = 7），那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384/2/2/2/2/2 = 12）.然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)/1 + 1 = 6）.这个结果正是浮窗在原图经停的6x6个位置的得分！ 面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的. 自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源.这一技巧在实践中经常使用，一次来获得更好的结果.比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值.(多加思考) 最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决.比如我们想用步长为16的浮窗.那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络. Net Surgery上一个使用Caffe演示如何在进行变换的IPython Note教程. 卷积神经网络的结构卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）.ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作.在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的. 层的排列规律卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成全连接层也较为常见.最后的全连接层得到输出，比如分类评分等.换句话说，最常见的卷积神经网络结构如下： INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC 其中*指的是重复次数，POOL?指的是一个可选的汇聚层.其中N &gt;=0,通常N&lt;=3,M&gt;=0,K&gt;=0,通常K&lt;3.例如，下面是一些常见的网络结构规律： - INPUT -&gt; FC,实现一个线性分类器，此处N = M = K = 0.- INPUT -&gt; CONV -&gt; RELU -&gt; FC- INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC.此处在每个汇聚层前有一个卷积层.- INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]*3 -&gt; [FC -&gt; RELU]*2 -&gt; FC.此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征. 几个小滤波器卷积层的组合比一个大滤波器卷积层好：假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）.在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野.第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野.同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野.假设不采用这3个3x3的卷积层，二是使用一个单独的有7x7的感受野的卷积层，那么所有神经元的感受野也是7x7，但是就有一些缺点.首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征.其次，假设所有的数据有$C$个通道，那么单独的7x7卷积层将会包含$C\\times (7\\times 7\\times C)=49C^2$个参数，而3个3x3的卷积层的组合仅有$3\\times (C\\times (3\\times 3\\times C))=27C^2$个参数.直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层.前者可以表达出输入数据中更多个强力特征，使用的参数也更少.唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存. 最新进展：传统的将层按照线性进行排列的方法已经受到了挑战，挑战来自谷歌的Inception结构和微软亚洲研究院的残差网络（Residual Net）结构.这两个网络（下文案例学习小节中有细节）的特征更加复杂，连接结构也不同. 层的尺寸设置规律到现在为止，我们都没有提及卷积神经网络中每层的超参数的使用.现在先介绍设置结构尺寸的一般性规则，然后根据这些规则进行讨论： 输入层（包含图像的）应该能被2整除很多次.常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512. 卷积层应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长$S=1$.还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸.比如，当$F=3$，那就使用$P=1$来保持输入尺寸.当$F=5,P=2$，一般对于任意$F$，当$P=(F-1)/2$的时候能保持输入尺寸.如果必须使用更大的滤波器尺寸（比如7x7之类），通常只用在第一个面对原始图像的卷积层上. 汇聚层负责对输入数据的空间维度进行降采样.最常用的设置是用用2x2感受野（即$F=2$）的最大值汇聚，步长为2（$S=2$）.注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）.另一个不那么常用的设置是使用3x3的感受野，步长为2.最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差. 减少尺寸设置的问题：上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样.如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起. 为什么在卷积层使用1的步长？在实际应用中，更小的步长效果更好.上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换. 为何使用零填充？使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能.如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉. 因为内存限制所做的妥协：在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升.例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64].这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）.因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的.在实践中，人们倾向于在网络的第一个卷积层做出妥协.例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）.在AlexNet中，滤波器的尺寸的11x11，步长为4. 案例学习（LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet）下面是卷积神经网络领域中比较有名的几种结构： LeNet： 第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的.当然，最著名还是被应用在识别数字和邮政编码等的LeNet结构. AlexNet：AlexNet卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现.AlexNet在2012年的ImageNet ILSVRC 竞赛中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）.这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）. ZF Net：Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为 ZFNet（Zeiler &amp; Fergus Net的简称）.它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小. GoogLeNet：ILSVRC 2014的胜利者是谷歌的Szeged等实现的卷积神经网络.它主要的贡献就是实现了一个奠基模块，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）.还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均汇聚，把大量不是很重要的参数都去除掉了.GooLeNet还有几种改进的版本，最新的一个是Inception-v4. VGGNet：ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为VGGNet.它主要的贡献是展示出网络的深度是算法优良性能的关键部分.他们最好的网络包含了16个卷积/全连接层.网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚.他们的预训练模型是可以在网络上获得并在Caffe中使用的.VGGNet不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）.其中绝大多数的参数都是来自于第一个全连接层.后来发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量. ResNet：残差网络（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现.它使用了特殊的跳跃链接，大量使用了批量归一化（batch normalization）.这个结构同样在最后没有使用全连接层.读者可以查看何恺明的的演讲（视频，PPT），以及一些使用Torch重现网络的实验.ResNet当前最好的卷积神经网络模型（2016年五月）.何开明等最近的工作是对原始结构做一些优化，可以看论文Identity Mappings in Deep Residual Networks，2016年3月发表. VGGNet的细节：我们进一步对VGGNet的细节进行分析学习.整个VGGNet中的卷积层都是以步长为1进行3x3的卷积，使用了1的零填充，汇聚层都是以步长为2进行了2x2的最大值汇聚.可以写出处理过程中每一步数据体尺寸的变化，然后对数据尺寸和整体权重的数量进行查看： INPUT: [224x224x3] memory: 224*224*3=150K weights: 0CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*3)*64 = 1,728CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*64)*64 = 36,864POOL2: [112x112x64] memory: 112*112*64=800K weights: 0CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456POOL2: [56x56x128] memory: 56*56*128=400K weights: 0CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*128)*256 = 294,912CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824POOL2: [28x28x256] memory: 28*28*256=200K weights: 0CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*256)*512 = 1,179,648CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296POOL2: [14x14x512] memory: 14*14*512=100K weights: 0CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296POOL2: [7x7x512] memory: 7*7*512=25K weights: 0FC: [1x1x4096] memory: 4096 weights: 7*7*512*4096 = 102,760,448FC: [1x1x4096] memory: 4096 weights: 4096*4096 = 16,777,216FC: [1x1x1000] memory: 1000 weights: 4096*1000 = 4,096,000TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)TOTAL params: 138M parameters 注意，大部分的内存和计算时间都被前面的卷积层占用，大部分的参数都用在后面的全连接层，这在卷积神经网络中是比较常见的.在这个例子中，全部参数有140M，但第一个全连接层就包含了100M的参数. 计算上的考量在构建卷积神经网络结构时，最大的瓶颈是内存瓶颈.大部分现代GPU的内存是3/4/6GB，最好的GPU大约有12GB的内存.要注意三种内存占用来源： 来自中间数据体尺寸：卷积神经网络中的每一层中都有激活数据体的原始数值，以及损失函数对它们的梯度（和激活数据体尺寸一致）.通常，大部分激活数据都是在网络中靠前的层中（比如第一个卷积层）.在训练时，这些数据需要放在内存中，因为反向传播的时候还会用到.但是在测试时可以聪明点：让网络在测试运行时候每层都只存储当前的激活数据，然后丢弃前面层的激活数据，这样就能减少巨大的激活数据量. 来自参数尺寸：即整个网络的参数的数量，在反向传播时它们的梯度值，以及使用momentum、Adagrad或RMSProp等方法进行最优化时的每一步计算缓存.因此，存储参数向量的内存通常需要在参数向量的容量基础上乘以3或者更多. 卷积神经网络实现还有各种零散的内存占用，比如成批的训练数据，扩充的数据等等. 一旦对于所有这些数值的数量有了一个大略估计（包含激活数据，梯度和各种杂项），数量应该转化为以GB为计量单位.把这个值乘以4，得到原始的字节数（因为每个浮点数占用4个字节，如果是双精度浮点数那就是占用8个字节），然后多次除以1024分别得到占用内存的KB，MB，最后是GB计量.如果你的网络工作得不好，一个常用的方法是降低批尺寸（batch size），因为绝大多数的内存都是被激活数据消耗掉了. 参考链接：ConvNet notes/卷积神经网络笔记","categories":[],"tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://yoursite.com/tags/cs231n/"}]},{"title":"Lecture_Neural Networks Part 3","slug":"Lecture-Neural-Networks-Part-3","date":"2018-10-24T07:07:12.000Z","updated":"2018-11-26T02:55:07.491Z","comments":true,"path":"2018/10/24/Lecture-Neural-Networks-Part-3/","link":"","permalink":"http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 在前面章节中，我们讨论了神经网络的静态部分：如何创建网络的连接、数据和损失函数。本节将致力于讲解神经网络的动态部分，即神经网络学习参数和搜索最优超参数的过程。 梯度检查理论上将进行梯度检查很简单，就是简单地把解析梯度和数值计算梯度进行比较. 使用中心化公式 在使用有限差值近似来计算数值梯度的时候，$\\frac{df(x)}{dx}=\\frac{f(x+h)-f(x-h)}{2h}$(use instead) 效果较好 使用相对误差来比较 - 相对误差&gt;1e-2：通常就意味着梯度可能出错。- 1e-2&gt;相对误差&gt;1e-4：要对这个值感到不舒服才行。- 1e-4&gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高。- 1e-7或者更小：好结果，可以高兴一把了。要知道的是网络的深度越深，相对误差就越高。所以如果你是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。相反，如果一个可微函数的相对误差值是1e-2，那么通常说明梯度实现不正确。 使用双精度 一个常见的错误是使用单精度浮点数来进行梯度检查。这样会导致即使梯度实现正确，相对误差值也会很高（比如1e-2）。在我的经验而言，出现过使用单精度浮点数时相对误差为1e-2，换成双精度浮点数时就降低为1e-8的情况。 目标函数的不可导点（kinks） 不可导点是指目标函数不可导的部分，由ReLU（$max(0,x)$）等函数，或SVM损失，Maxout神经元等引入。考虑当$x=-1e6$时，对ReLU函数进行梯度检查。因为$x&lt;0$，所以解析梯度在该点的梯度为0。然而，在这里数值梯度会突然计算出一个非零的梯度值，因为$f(x+h)$可能越过了不可导点(例如：如果$h&gt;1e-6$)，导致了一个非零的结果。实际上这种情况很常见。 注意，在计算损失的过程中是可以知道不可导点有没有被越过的。在具有max(x,y)形式的函数中持续跟踪所有“赢家”的身份，就可以实现这一点。其实就是看在前向传播时，到底x和y谁更大。如果在计算f(x+h)和f(x-h)的时候，至少有一个“赢家”的身份变了，那就说明不可导点被越过了，数值梯度会不准确。解决上面的不可导点问题的一个办法是使用更少的数据点。如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。 不要让正则化吞没数据。 推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。 记得关闭随机失活（dropout）和数据扩张（augmentation） 在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。 检查少量的维度。 在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度然后假设其他维度是正确的。注意：确认在所有不同的参数中都抽取一部分来梯度检查。 合理性（Sanity）检查 寻找特定情况的正确损失值 在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。 提高正则化强度时导致损失值变大 对小数据子集过拟合 在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。 检查学习过程在训练神经网络的时候，应该跟踪多个重要数值。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道如何修改超参数以获得更高效的学习过程。 在下面的图表中，x轴通常都是表示周期（epochs）单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。 损失函数训练期间第一个要跟踪的数值就是损失值，它在前向传播时对每个独立的批数据进行计算。 左图展示了不同的学习率的效果。过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。右图显示了一个典型的随时间变化的损失函数值，在CIFAR-10数据集上面训练了一个小的网络，这个损失函数值曲线看起来比较合理（虽然可能学习率有点小，但是很难说），而且指出了批数据的数量可能有点太小（因为损失值的噪音很大）。 训练集与验证集准确率在训练分类器的时候，需要跟踪的第二重要的数值是验证集和训练集的准确率。 在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度。在图中，蓝色的验证集曲线显示相较于训练集，验证集的准确率低了很多，这就说明模型有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。另一种可能就是验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。 权重：更新比例最后一个应该跟踪的量是权重中更新值的数量和全部值的数量之间的比例。需要对每个参数集的更新比例进行单独的计算和跟踪。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。 # 假设参数向量为W，其梯度向量为dWparam_scale = np.linalg.norm(W.ravel())update = -learning_rate*dW # 简单SGD更新update_scale = np.linalg.norm(update.ravel())W += update # 实际更新print update_scale / param_scale # 要得到1e-3左右 每层的激活数据与梯度分布第一层可视化如果数据是图像像素数据，那么把第一层特征可视化会有帮助： 将神经网络第一层的权重可视化的例子。左图中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低.右图的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好. 参数更新一旦能使用反向传播计算解析梯度，梯度就能被用来进行参数更新了。进行参数更新有好几种方法，接下来都会进行讨论。 一阶（随机梯度下降）方法，动量方法，Nesterov动量方法 普通更新. 最简单的更新形式是沿着负梯度方向改变参数（因为梯度指向的是上升方向，但是我们通常希望最小化损失函数） # 普通更新x += - learning_rate * dx 其中learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展。 动量（Momentum）更新 这样最优化过程可以看做是模拟参数向量（即质点）在地形上滚动的过程。在普通版本中，梯度直接影响位置。而在这个版本的更新中，物理观点建议梯度只是影响速度，然后速度再影响位置： # 动量更新v = mu * v - learning_rate * dx # 与速度融合x += v # 与位置融合 在这里引入了一个初始化为0的变量v和一个超参数mu。说得不恰当一点，这个变量（mu）在最优化的过程中被看做动量（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。通过交叉验证，这个参数通常设为[0.5,0.9,0.95,0.99]中的一个。和学习率随着时间退火（下文有讨论）类似，动量随时间变化的设置有时能略微改善最优化的效果，其中动量在学习过程的后阶段会上升。一个典型的设置是刚开始将动量设为0.5而在后面的多个周期（epoch）中慢慢提升到0.99。 通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。 Nesterov动量与普通动量有些许不同，最近变得比较流行。在理论上对于凸函数它能得到更好的收敛，在实践中也确实比标准动量表现更好一些。 Nesterov动量的核心思路是，当参数向量位于某个位置x时，观察上面的动量更新公式可以发现，动量部分（忽视带梯度的第二个部分）会通过mu * v稍微改变参数向量。因此，如果要计算梯度，那么可以将未来的近似位置x + mu * v看做是“向前看”，这个点在我们一会儿要停止的位置附近。因此，相比“旧”位置x的梯度，计算x + mu \\* v的梯度会更有意义。 既然我们知道动量将会把我们带到绿色箭头指向的点，我们就不要在原点（红色点）那里计算梯度了。使用Nesterov动量，我们就在这个“向前看”的地方计算梯度。 x_ahead = x + mu * v # 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)v = mu * v - learning_rate * dx_aheadx += v 然而在实践中，人们更喜欢和普通SGD或上面的动量方法一样简单的表达式。通过对x_ahead = x + mu * v使用变量变换进行改写是可以做到的，然后用x_ahead而不是x来表示上面的更新。也就是说，实际存储的参数向量总是向前一步的那个版本。x_ahead的公式（将其重新命名为x）就变成了： v_prev = v # 存储备份v = mu * v - learning_rate * dx # 速度更新保持不变x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式 学习率退火在训练深度网络的时候，让学习率随着时间退火通常是有帮助的。通常，实现学习率退火有3种方式： 随步数衰减：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。 指数衰减。数学公式是$\\alpha=\\alpha_0e^{-kt}$，其中$\\alpha_0,k$是超参数，t是迭代次数（也可以使用周期作为单位）。 1/t衰减的数学公式是$\\alpha=\\alpha_0/(1+kt)$，其中$\\alpha_0,k$是超参数，t是迭代次数。 在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。 二阶方法在深度网络背景下，第二类常用的最优化方法是基于牛顿法的，其迭代如下：$\\displaystyle x\\leftarrow x-[Hf(x)]^{-1}\\nabla f(x)$ 逐参数适应学习率方法（Adagrad，RMSProp）前面讨论的所有方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。学习率调参是很耗费计算资源的过程，所以很多工作投入到发明能够适应性地对学习率调参的方法，甚至是逐个参数适应学习率调参。 Adagrad # 假设有梯度和参数向量xcache += dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) RMSprop 用一种很简单的方式修改了Adagrad方法，让它不那么激进，单调地降低了学习率。具体说来，就是它使用了一个梯度平方的滑动平均： cache = decay_rate * cache + (1 - decay_rate) * dx**2x += - learning_rate * dx / (np.sqrt(cache) + eps) Adam 看起来像是RMSProp的动量版 m = beta1*m + (1-beta1)*dxv = beta2*v + (1-beta2)*(dx**2)x += - learning_rate * m / (np.sqrt(v) + eps) ​ 上面的动画可以帮助你理解学习的动态过程。 左边是一个损失函数的等高线图，上面跑的是不同的最优化算法。 基于动量的方法出现了射偏了的情况，使得最优化过程看起来像是一个球滚下山的样子。 右边展示了一个马鞍状的最优化地形，其中对于不同维度它的曲率不同（一个维度下降另一个维度上升）。 SGD很难突破对称性，一直卡在顶部。 RMSProp之类的方法能够看到马鞍方向有很低的梯度。因为在RMSProp更新方法中的分母项，算法提高了在该方向的有效学习率，使得RMSProp能够继续前进。 超参数调优训练一个神经网络会遇到很多超参数设置。神经网络最常用的设置有： 初始学习率 学习率衰减方式（例如一个衰减常量） 正则化强度（L2惩罚，随机失活强度） 调参要点和技巧： 实现 更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。记住这一点很重要，因为这会影响你设计代码的思路。 比起交叉验证最好使用一个验证集 在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。 超参数范围 在对数尺度上进行超参数搜索。例如，一个典型的学习率应该看起来是这样：learning_rate = 10 \\ uniform(-6, 1)**。也就是说，我们从标准分布中随机生成了一个数字，然后让它成为10的阶数。对于正则化强度，可以采用同样的策略。直观地说，这是因为学习率和正则化强度都对于训练的动态进程有乘的效果。 随机搜索优于网格搜索 通常，有些超参数比其余的更重要，通过随机搜索，而不是网格化的搜索，可以让你更精确地发现那些比较重要的超参数的好数值。 对于边界上的最优值要小心 这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候.一旦我们得到一个比较好的值，一定要确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。 从粗到细地分阶段搜索 先进行初略范围搜索，然后根据好的结果出现的地方，缩小范围进行搜索。 贝叶斯超参数最优化 主要是研究在超参数空间中更高效的导航算法。其核心的思路是在不同超参数设置下查看算法性能时，要在探索和使用中进行合理的权衡。 评价模型集成在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候平均它们预测结果。集成的模型数量增加，算法的结果也单调提升（但提升效果越来越少）。还有模型之间的差异度越大，提升效果可能越好。进行集成有以下几种方法： 同一个模型，不同的初始化。使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。这种方法的风险在于多样性只来自于不同的初始化条件。 在交叉验证中发现最好的模型。使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。 一个模型设置多个记录点。如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。 在训练的时候跑参数的平均值。和上面一点相关的，还有一个也能得到1-2个百分点的提升的小代价方法，这个方法就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。这样你就对前几次循环中的网络状态进行了平均。你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。 模型集成的一个劣势就是在测试数据的时候会花费更多时间。最近Geoff Hinton在“Dark Knowledge”上的工作很有启发：其思路是通过将集成似然估计纳入到修改的目标函数中，从一个好的集成中抽出一个单独模型。 总结训练一个神经网络需要： 利用小批量数据对实现进行梯度检查，还要注意各种错误. 进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率. 在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数量相对于总参数量的比例（一般在1e-3左右），然后如果是对于卷积神经网络，可以将第一层的权重可视化. 推荐的两个更新方法是SGD+Nesterov动量方法，或者Adam方法. 随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候. 使用随机搜索（不要用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索. 进行模型集成来获得额外的性能提高. 参考链接：Neural Nets notes 3、神经网络笔记3（上）（下）","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[{"name":"CS231n","slug":"CS231n","permalink":"http://yoursite.com/tags/CS231n/"}]},{"title":"Lecture_Neural Networks Part 2","slug":"Lecture-Neural-Networks-Part-2","date":"2018-10-22T08:27:27.000Z","updated":"2018-11-19T07:01:32.821Z","comments":true,"path":"2018/10/22/Lecture-Neural-Networks-Part-2/","link":"","permalink":"http://yoursite.com/2018/10/22/Lecture-Neural-Networks-Part-2/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 设置数据和模型具体来说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。 数据预处理 均值减法（Mean subtraction）是预处理最常用的形式。 归一化（Normalization）是指将数据的所有维度都归一化，使其数值范围都近似相等。 零中心化（zero-centered）+ 每个维度都除以其标准差 对每个维度都做归一化，使得每个维度的最大和最小值是1和-1 一般数据预处理流程： 左边：原始的2维输入数据。 中间：在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。 右边：每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。 PCA和白化（Whitening）是另一种预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构 PCA/白化。左边是二维的原始数据。中间：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。右边：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。 强调： 任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。即应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值 权重初始化错误：全零初始化. 如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。 小随机数初始化权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来打破对称性。W = 0.01 * np.random.randn(D,H) 使用1/sqrt(n)校准方差：w = np.random.randn(n) / sqrt(n)(其中n是输入数据的数量)这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification文中给出了一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是$2.0/n$。代码为w = np.random.randn(n) * sqrt(2.0/n)。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。 稀疏初始化（Sparse initialization）另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。 偏置（biases）的初始化。通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。 批量归一化（Batch Normalization）在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。最后一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。 正则化（L2/L1/Maxnorm/Dropout）正则化：防止过拟合 L2正则化可能是最常用的正则化(Regularization)方法.可以通过惩罚目标函数中所有参数的平方将其实现。即对于网络中的每个权重$w$，向目标函数中增加一个$\\frac{1}{2}\\lambda w^2$，其中$\\lambda$是正则化强度。L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。 L1正则化是另一个相对常用的正则化方法。对于每个$w$我们都向目标函数增加一个$\\lambda|w|$。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。 最大范式约束（Max norm constraints）是给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量$\\overrightarrow{w}$必须满足$||\\overrightarrow{w}||_2&lt;c$这一条件，一般$c$值为3或者4。即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，这是因为它的参数更新始终是被限制着的。 随机失活（Dropout）是一个简单又极其有效的正则化方法。与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数$p$的概率被激活或者被设置为0。 在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。 实际更倾向使用反向随机失活（inverted dropout），它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。 损失函数我们已经讨论过损失函数的正则化损失部分，它可以看做是对模型复杂程度的某种惩罚。损失函数的第二个部分是数据损失，它是一个有监督学习问题，用于衡量分类算法的预测结果（即分类评分）和真实标签结果之间的一致性。数据损失是对所有样本的数据损失求平均。也就是说，$L=\\frac{1}{N}\\sum_iL_i$中，$N$是训练集数据的样本数。 分类问题 在该问题中，假设有一个装满样本的数据集，每个样本都有一个唯一的正确标签（是固定分类标签之一）. 最常见的损失函数就是SVM.$\\displaystyle L_i=\\sum_{j\\not=y_i}max(0,f_j-f_{y_i}+1)$ 平方折叶损失（即使用$ max(0,f_j-f_{y_i}+1)^2$）算法的结果会更好 第二个常用的损失函数是Softmax分类器. 它使用交叉熵损失：$\\displaystyle L_i=-log(\\frac{e^{f_{y_i}}}{\\sum_je^{f_j}})$ 属性（Attribute）分类 若每个样本的标签$y_i$是一个二值向量，每个样本可能有，也可能没有某个属性，而且属性之间并不相互排斥，此时应为每个属性创建一个独立的二分类的分类器。$\\displaystyle L_i=\\sum_jmax(0,1-y_{ij}f_j)$ 回归问题是预测实数的值的问题. L2范式 $L_i=||f-y_i||^2_2$ L1范式则是要将每个维度上的绝对值加起来：$L_i=||f-y_i||_1=\\sum_j|f_j-(y_i)_j|$ 当面对一个回归任务，首先考虑是不是必须这样。一般而言，尽量把你的输出变成二分类，然后对它们进行分类，从而变成一个分类问题。 结构化预测（structured prediction）结构化损失是指标签可以是任意的结构，例如图表、树或者其他复杂物体的情况。 小结 推荐的预处理操作是对数据的每个特征都进行零中心化，然后将其数值范围都归一化到[-1,1]范围之内。 使用标准差为$\\sqrt{2/n}$的高斯分布来初始化权重，其中$n$是输入的神经元数。例如用numpy可以写作：w = np.random.randn(n) * sqrt(2.0/n)。 使用L2正则化和随机失活的倒置版本。 使用批量归一化。 讨论了在实践中可能要面对的不同任务，以及每个任务对应的常用损失函数。 参考链接：神经网络笔记 2","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[{"name":"CS231n","slug":"CS231n","permalink":"http://yoursite.com/tags/CS231n/"}]},{"title":"Lecture_Neural Networks Part 1","slug":"Lecture-Neural-Networks-Part-1","date":"2018-10-22T05:25:48.000Z","updated":"2018-11-19T06:36:21.399Z","comments":true,"path":"2018/10/22/Lecture-Neural-Networks-Part-1/","link":"","permalink":"http://yoursite.com/2018/10/22/Lecture-Neural-Networks-Part-1/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 单个神经元建模生物动机和连接 将神经元的激活率建模为激活函数（activation function）f，它表达了轴突上激活信号的频率.激活函数，非线性函数，”扭曲”得分函数. 由于历史原因，激活函数常常选择使用sigmoid函数$\\sigma$，该函数输入实数值（求和后的信号强度），然后将输入值压缩到0-1之间。 作为线性分类器的单个神经元一个单独的神经元可以用来实现一个二分类分类器，比如二分类的Softmax或者SVM分类器。 常用的激活函数 左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。 Sigmoid：$\\sigma(x)=\\frac{1}{1+e^{-x}}$ Sigmoid函数饱和使梯度消失 Sigmoid函数的输出不是零中心的 Tanh：$tanh(x)=2\\sigma(2x)-1$ （tanh神经元是一个简单放大的sigmoid神经元） Tanh也存在饱和问题 Tanh的输出是零中心的 ReLU(校正线性单元：Rectified Linear Unit)激活函数: $f(x)=max(0,x)$ ReLU对于随机梯度下降的收敛有巨大的加速作用 ReLU单元比较脆弱并且可能“死掉”.通过合理设置学习率，这种情况的发生概率会降低 Leaky ReLU是为解决“ReLU死亡”问题的尝试 Maxout：$max(w^T_1x+b_1,w^T_2x+b_2)$ Maxout是对ReLU和leaky ReLU的一般化归纳. 在同一个网络中混合使用不同类型的神经元是非常少见的. 神经网络结构层组织将神经网络算法以神经元的形式图形化 左边是一个2层神经网络，隐层由4个神经元（也可称为单元（unit））组成，输出层由2个神经元组成，输入层是3个神经元。该网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置，共26个可学习的参数。 右边是一个3层神经网络，两个含4个神经元的隐层。该网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。 注意：全连接层（fully-connected layer）。全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接.上面两个神经网络的图例，都使用的全连接层. 前向传播计算例子完整的3层神经网络的前向传播就是简单的3次矩阵乘法，其中交织着激活函数的应用。 # 一个3层神经网络的前向传播:f = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)x = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)h1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)h2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)out = np.dot(W3, h2) + b3 # 神经元输出(1x1) 全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。 表达能力神经网络可以近似任何连续函数。 虽然在理论上深层网络（使用了多个隐层）和单层网络的表达能力是一样的，但是就实践经验而言，深度网络效果比单层网络好(设置的网络深度也应适度而行)。 设置层的数量和尺寸注意：不应该因为害怕出现过拟合而使用小网络。相反，应该进尽可能使用大网络，然后使用正则化技巧来控制过拟合。 小节 介绍了生物神经元的粗略模型； 讨论了几种不同类型的激活函数，其中ReLU是最佳推荐； 介绍了神经网络，神经元通过全连接层连接，层间神经元两两相连，但是层内神经元不连接； 理解了分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算； 理解了神经网络是一个通用函数近似器，但是该性质与其广泛使用无太大关系。之所以使用神经网络，是因为它们对于实际问题中的函数的公式能够某种程度上做出“正确”假设。 讨论了更大网络总是更好的这一事实。然而更大容量的模型一定要和更强的正则化（比如更高的权重衰减）配合，否则它们就会过拟合。在后续章节中我们讲学习更多正则化的方法，尤其是dropout。 参考链接：神经网络笔记1 上、神经网络笔记1 下","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[{"name":"CS231n","slug":"CS231n","permalink":"http://yoursite.com/tags/CS231n/"}]},{"title":"Lecture-backpropagation","slug":"Lecture-backpropagation","date":"2018-10-22T02:54:04.000Z","updated":"2018-11-19T04:04:02.675Z","comments":true,"path":"2018/10/22/Lecture-backpropagation/","link":"","permalink":"http://yoursite.com/2018/10/22/Lecture-backpropagation/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 简介本节将帮助读者对反向传播形成直观而专业的理解。反向传播是利用链式法则递归计算表达式的梯度的方法。 简单表达式和理解梯度函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度. 复合表达式，链式法则，反向传播链式法则指出将这些梯度表达式链接起来的正确方式是相乘，比如$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}$。在实际操作中，这只是简单地将两个梯度数值相乘. # 设置输入值x = -2; y = 5; z = -4# 进行前向传播q = x + y # q becomes 3f = q * z # f becomes -12# 进行反向传播:# 首先回传到 f = q * zdfdz = q # df/dz = q, 所以关于z的梯度是3 (f = 3*z)dfdq = z # df/dq = z, 所以关于q的梯度是-4 (f = -4*q)# 现在回传到q = x + ydfdx = 1.0 * dfdq # dq/dx = 1. (q = x+5) 这里的乘法是因为链式法则 dfdy = 1.0 * dfdq # dq/dy = 1. (q = -2+y) 上图 $f(x,y,z)=(x+y)*z$ 的真实值计算线路展示了计算的视觉化过程。前向传播从输入计算到输出（绿色），反向传播从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。 直观理解反向传播这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等。 反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的输出值更高. 任何可微分的函数都可以看做门。可以根据需要将一个函数分拆成多个简单门(计算反向传播就简单了)；也可以将多个门组合成一个门从而可以进行简化(让代码量更少，效率更高). 模块：Sigmoid例子 $$ f(w,x) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}} $$ 使用sigmoid激活函数的2维神经元的例子。输入是[x0, x1]，可学习的权重是[w0, w1, w2]。一会儿会看见，这个神经元对输入数据做点积运算，然后其激活数据被sigmoid函数挤压到0到1之间。$$ \\sigma(x) = \\frac{1}{1+e^{-x}} \\\\\\\\ \\rightarrow \\hspace{0.3in} \\frac{d\\sigma(x)}{dx} = \\frac{e^{-x}}{(1+e^{-x})^2} = \\left( \\frac{1 + e^{-x} - 1}{1 + e^{-x}} \\right) \\left( \\frac{1}{1+e^{-x}} \\right) = \\left( 1 - \\sigma(x) \\right) \\sigma(x) $$ 和之前的计算流程比起来，现在的计算使用一个单独的简单表达式即可。因此，在实际的应用中将这些操作装进一个单独的门单元中将会非常有用。 反向传播实践：分段计算$$ f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2} $$ 构建前向传播的代码模式：(对前向传播变量进行缓存) x = 3 # 例子数值y = -4# 前向传播sigy = 1.0 / (1 + math.exp(-y)) # 分子中的sigmoid #(1)num = x + sigy # 分子 #(2)sigx = 1.0 / (1 + math.exp(-x)) # 分母中的sigmoid #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # 分母 #(6)invden = 1.0 / den #(7)f = num * invden # 搞定！ #(8) (1) $sigy=\\sigma(y)=\\frac{1}{1+e^{-y}}$ (2) $num = x + sigy =x+\\sigma(y)$ (3) $sigx=\\sigma(x)=\\frac{1}{1+e^{-x}}$ (4) $xpy =x+y$ (5) $xpysqr = xpy**2=(x+y)^2$ (6) $den = sigx + xpysqr=\\sigma(x)+(x+y)^2$ (7) $invden=1/{den}=\\frac{1}{\\sigma(x)+(x+y)^2}$ (8) $f=num/invden=\\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}$ 反向传播的代码模式：(在不同分支的梯度要相加) # 回传 f = num * invdendnum = invden # 分子的梯度:分母 #(8)dinvden = num # 分母的梯度:分子 #(8)# 回传 invden = 1.0 / den dden = (-1.0 / (den**2)) * dinvden #(7)# 回传 den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# 回传 xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# 回传 xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# 回传 sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# 回传 num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# 回传 sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1)# 完成! 嗷~~ 回传流中的模式 一个展示反向传播的例子. 加法操作将(反向传来的)梯度相等地分发给它的输入. 取最大操作将梯度路由给更大的输入. 乘法操作拿取输入激活数据，对它们进行交换，然后乘以梯度. 用户向量化操作的梯度矩阵相乘的梯度：可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作：(分析维度) # 前向传播W = np.random.randn(5, 10)X = np.random.randn(10, 3)D = W.dot(X)# 假设我们得到了D的梯度dD = np.random.randn(*D.shape) # 和D一样的尺寸dW = dD.dot(X.T) #.T就是对矩阵进行转置dX = W.T.dot(dD) 使用小而具体的例子：有些读者可能觉得向量化操作的梯度计算比较困难，建议是写出一个很小很明确的向量化例子，在纸上演算梯度，然后对其一般化，得到一个高效的向量化操作形式。 小结 对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。 讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。 在下节课中，将会开始定义神经网络，而反向传播使我们能高效计算神经网络各个节点关于损失函数的梯度。换句话说，我们现在已经准备好训练神经网络了，本课程最困难的部分已经过去了！ConvNets相比只是向前走了一小步。 参考链接：反向传播笔记","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[{"name":"CS231n","slug":"CS231n","permalink":"http://yoursite.com/tags/CS231n/"}]},{"title":"Lecture_Optimization","slug":"CS231n-Lecture-Optimization","date":"2018-10-20T09:18:06.000Z","updated":"2018-11-19T03:18:09.964Z","comments":true,"path":"2018/10/20/CS231n-Lecture-Optimization/","link":"","permalink":"http://yoursite.com/2018/10/20/CS231n-Lecture-Optimization/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 课程链接参考课程笔记：Optimization: Stochastic Gradient Descent 笔记翻译：最优化笔记（上）、最优化笔记（下） 损失函数可视化损失函数的分段线性结构 从一个维度方向上对数据损失值的展示。x轴方向就是一个权重，y轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与0阈值的比较。 最优化 Optimization 最优化的目标就是找到能够最小化损失函数值的W 策略#1：一个差劲的初始方案：随机搜索既然确认参数集W的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以随机尝试很多不同的权重，然后看其中哪个最好。 核心思路：迭代优化。我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。 策略#2：随机本地搜索第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机$W$开始，然后生成一个随机的扰动$\\delta W$ ，只有当$W+\\delta W$的损失值变低，我们才会更新。 评价： 较策略一准确率更高些，但依然不够高，且过于浪费计算资源 策略#3：跟随梯度前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。这个方向就是损失函数的梯度（gradient）.注：梯度就是在每个维度上偏导数所形成的向量。 梯度计算 缓慢的近似方法（数值梯度法numerical gradient），实现相对简单，但耗费计算资源太多 分析梯度法analytic gradient 计算迅速，结果精确，但是实现时容易出错，且需要使用微分 在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做梯度检查 利用有限差值计算梯度实践考量：实际中用中心差值公式（centered difference formula）$[f(x+h)-f(x-h)]/2h$效果较好 步长的影响：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为学习率）将会是我们在调参中最重要的超参数之一。 效率问题：这个策略不适合大规模数据，我们需要更好的策略。 微分分析计算梯度一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了. 梯度下降现在可以计算损失函数的梯度了，程序重复地计算梯度然后更新参数，这一过程称为梯度下降. 核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。 小批量数据梯度下降（Mini-batch gradient descent）小批量数据的梯度就是对整个数据集梯度的一个近似。因此，在实践中通过计算小批量数据的梯度可以实现更快地收敛，并以此来进行更频繁的参数更新。(提高计算效率) 小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。 随机梯度下降（Stochastic Gradient Descent 简称SGD）小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为随机梯度下降（Stochastic Gradient Descent 简称SGD）。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据 比100次计算1个数据要高效很多。 你有时会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。 提取图片特征（Image Features） 颜色直方图（Color Histogram） 定向梯度直方图（Histogram of Oriented Gradient） 词袋模型（Bag of Words） 小结 Summary of the information flow 最优化的目标就是找到能够最小化损失函数值的W.数据集中的(x,y)是给定的.权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量f中。损失函数包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分f和实际标签y之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，我们计算权重的梯度（如果愿意的话，也可以计算数据上的梯度），然后使用它们来实现参数的更新。 将损失函数比作了一个高维度的最优化地形，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。 提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。 函数的梯度给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是h，用来计算数值梯度）。 参数更新需要有技巧地设置步长。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。 讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用梯度检查来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。 介绍了梯度下降算法，它在循环中迭代地计算梯度并更新参数。 这节课的核心内容是：理解并能计算损失函数关于权重的梯度，是设计、训练和理解神经网络的核心能力。","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[{"name":"CS231n","slug":"CS231n","permalink":"http://yoursite.com/tags/CS231n/"}]},{"title":"Lecture_Linear_classification","slug":"Lecture_Linear_classification","date":"2018-10-18T12:01:32.000Z","updated":"2018-11-20T13:39:31.988Z","comments":true,"path":"2018/10/18/Lecture_Linear_classification/","link":"","permalink":"http://yoursite.com/2018/10/18/Lecture_Linear_classification/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 课程链接 Linear classification: Support Vector Machine, Softmax 线性分类笔记(上)/线性分类笔记(中)/线性分类笔记(下) 线性分类器 图像空间的示意图,其中每个图像是一个点,且有3个分类器.以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低. Interactive web demo 评分函数 Score function线性映射 $$ f(x_i,W,b)=Wx_i+b $$ W(Weights 权重) : [K*D] (W的每一行对应一个分类的模板(“原型”)) 如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转. x_i(输入数据) : [D*1] b(bias vector 偏差向量) : [K*1] 允许分类器对应的直线平移.如果没有偏差,无论权重如何,在x_i=0时分类分值始终为0.这样所有分类器的线都不得不穿过原点. 该函数的输出值为对应各类别的score 我们的目的就是找到最优化的参数W、b,即为每一个分类找到最好的模板. 评分函数在正确的分类的位置应当得到最高的评分（score） 偏差和权重的合并Bias trick $$ f(x_i,W,b)=Wx_i+b\\rightarrow f(x_i,W) = Wx_i $$ 其中，W[K*D]→W[K*(D+1)], x_i[D,1] → x_i[(D+1),1] 将线性分类器看作模板匹配 Interpretation of linear classifiers as template matching W的每一行为对应一个分类的模板(“原型”).注意，船的模板如期望的那样有很多蓝色像素。如果图像是一艘船行驶在大海上，那么这个模板利用内积计算图像将给出很高的分数。 图像数据预处理 Image data preprocessing 对于输入的特征做归一化（normalization）处理 对每个特征减去平均值来中心化数据 归一. 区间变为[-1,1] 损失函数 Loss function我们将使用损失函数（Loss Function）（有时也叫代价函数Cost Function或目标函数Objective）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。，对训练集中数据做出准确分类预测和让损失值最小化这两件事是等价的。 多类支持向量机损失 Multiclass Support Vector Machine Loss SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高,且至少高出一个边界值delta.如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。 Hinge Loss 针对第i个数据的多类SVM的损失函数定义：$$ L_i=\\sum_{j\\not=y_i}max(0,s_j-s_{y_i}+\\Delta) $$ Data Loss $$ L = { \\frac{1}{N} \\sum_i L_i } $$ 正则化 Regularization防止过拟合：向损失函数增加一个正则化惩罚（regularization penalty） R(W)部分，使其不能完全匹配训练集.最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重.对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响.$$R(W) = \\sum_k\\sum_l W_{k,l}^2$$ 需要注意的是，和权重不同，偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此通常只对权重$W$正则化，而不正则化偏差$b$。因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当$W=0$的特殊情况下，才能得到损失值为0 完整的多类SVM损失函数$$ L = \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\frac{\\lambda}{2} R(W) }_\\text{regularization loss} \\\\\\\\ $$ 将其展开完整公式是：$$ L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\frac{\\lambda}{2} \\sum_k\\sum_l W_{k,l}^2 $$ 对于每一个输入数据$x_i$都有一个L值 设置超参$\\Delta$和λ超参数$\\Delta$和λ失函数中的数据损失和正则化损失之间的权衡。理解这一点的关键是要知道，权重W的大小对于分类分值有直接影响（当然对他们的差异也有直接影响）：当我们将W中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如$\\Delta$=1或$\\Delta$=100).从某些角度来看是没意义的，因为权重自己就可以控制差异变大和缩小。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过正则化强度λ来控制） SVM 梯度计算$$ L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\right] $$ $$ \\left\\{\\begin{aligned} \\nabla_{w_{y_i}} L_i = & -\\left(\\sum_{j \\ne y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0)\\right)x_i & j = y_i \\\\ \\nabla_{w_j} L_i = & 1(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0) x_i & j \\ne y_i \\end{aligned}\\right. $$ 其中$\\mathbb{1}$是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。具体的理论推导得出的梯度计算是否正确，会有 gradient check 的数值计算方式来检查。(代码实现中对dW的解释) Softmax分类器与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释。在Softmax分类器中，函数映射$f(x_i;W)=Wx_i$保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将折叶损失（hinge loss）替换为交叉熵损失（cross-entropy loss） 公式如下：$\\displaystyle Li=-log(\\frac{e^{f_{y_i}}}{\\sum_je^{f_j}})$ 或等价于 $L_i=-f_{y_i}+log(\\sum_je^{f_j})$ softmax函数函数$f_j(z)=\\frac{e^{z_j}}{\\sum_ke^{z_k}}$被称作softmax 函数.函数对输入向量z(score值)进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1.$$P(y_i|x_i,W)=\\frac{e^{f_{y_i}}}{\\sum_je^{f_j}}$$可以解释为是给定图像数据$x_i$，以$W$为参数，分配给正确分类标签$y_i$的归一化概率。从概率论的角度来理解，我们就是在最小化正确分类的负对数概率(令正确分类的概率尽趋近于1，即令错误分类的概率均趋近于0)，这可以看做是在进行最大似然估计（MLE）。 交叉熵在“真实”分布(未知)$p$和估计分布(样本分布)$q$之间的交叉熵定义： $\\displaystyle H(p,q)=-\\sum_xp(x) logq(x)$ 交叉熵损失函数“想要”预测分布的所有概率密度都在正确分类上,让预测分布与真实分布保持一致。Softmax分类器所做的就是最小化在估计分类概率（就是上面的$e^{f_{y_i}}/\\sum_je^{f_j}$）和“真实”分布之间的交叉熵，在这个解释中，“真实”分布就是所有概率密度都分布在正确的类别上（比如：$p=[0,…1,…,0]$中在$y_i$的位置就有一个单独的1） SVM和Softmax的比较 针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量f（本节中是通过矩阵乘来实现）。不同之处在于对f中分值的解释： SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。SVM的最终的损失值是1.58。 Softmax分类器将这些数值看做是每个分类没有归一化的对数概率，鼓励正确分类的归一化的对数概率变高，其余的变低。Softmax的最终的损失值是0.452。但要注意SVM和Softmax的最终损失值(1.58和0.452)两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。 Softmax分类器为每个分类提供了“可能性”：可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的置信水平。 在实际使用中，SVM和Softmax经常是相似的：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。 SVM更加“局部目标化（local objective）”.SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM($\\Delta$=1)来说没什么不同，只要满足超过边界值等于1，那么损失值就等于0。SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。 softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。 小结 定义了从图像像素映射到不同类别的分类评分的评分函数。在本节中，评分函数是一个基于权重W和偏差b的线性函数。 与kNN分类器不同，参数方法的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重W进行一个矩阵乘法运算。 介绍了偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵。 定义了损失函数（介绍了SVM和Softmax线性分类器最常用的2个损失函数）。损失函数能够衡量给出的参数集与训练集数据真实类别情况之间的一致性。在损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的。 现在我们知道了如何基于参数，将数据集中的图像映射成为分类的评分，也知道了两种不同的损失函数，它们都能用来衡量算法分类预测的质量。但是，如何高效地得到能够使损失值最小的参数呢？这个求得最优参数的过程被称为最优化. 参考：cs231n assignment1 svm","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[{"name":"CS231n","slug":"CS231n","permalink":"http://yoursite.com/tags/CS231n/"}]},{"title":"Concept_Note","slug":"Concept-Note","date":"2018-10-18T08:48:07.000Z","updated":"2018-12-19T16:16:42.919Z","comments":true,"path":"2018/10/18/Concept-Note/","link":"","permalink":"http://yoursite.com/2018/10/18/Concept-Note/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… Convolutional NetworksConvolutional Networks Video Introduction score function: 得分函数的目的：我们要做的就是对于一个给定的输入，比如一张小猫的图片，通过一系列复杂的变换（中间的过程咱们暂且当做一个黑盒子）能得到这个输入对应于每个类别的得分数值. Loss Fuction: 量化我们对训练结果的满意程度，换句话说，是衡量分类器的错误程度 正则化 正则化项即惩罚函数，该项对模型向量进行“惩罚”，从而避免单纯最小二乘问题的过拟合问题。 激活函数（activation function）f：非线性函数，”扭曲”得分函数. 验证数据集和测试数据集 我们也可以将验证数据集看作考试中的模拟训练测试，将测试数据集看作考试中的最终测试，通过两个结果 看测试的整体能力，但是测试数据集最后会有绝对的主导作用 。","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[]},{"title":"CS231n_Assignment_note","slug":"CS231n-Assignment-note","date":"2018-10-17T12:57:13.000Z","updated":"2018-11-18T13:47:05.365Z","comments":true,"path":"2018/10/17/CS231n-Assignment-note/","link":"","permalink":"http://yoursite.com/2018/10/17/CS231n-Assignment-note/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… cs231n 课程作业 Assignment 1 cs231n 课程作业 Assignment 2 cs231n 课程作业 Assignment 3 可参考: https://www.jianshu.com/u/0ee5409b18bf standford-cs231n-assignment1 Assignment1Assignment2- layers.pyspatial_groupnorm_forwardspatial_groupnorm_backwardlayernorm_forwardlayernorm_backward Python Error: no module named ‘past’ 由于目前普遍使用的是python3版本，其中已经移除 xrange 这一操作（具体来说是与range合并了）。但是在作业的实例代码中考虑python2和python3版本的兼容，仍然使用了xrange操作。于是对于python3版本用户来说就需要重新引入： from past.builtins import xrange 但是在运行的时候就遇到了no module named ‘past’问题，原因是没有安装past库 但是这里比较坑的一点就是安装past库使用的代码竟然是 pip install future 竟然是future！！！","categories":[],"tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://yoursite.com/tags/cs231n/"}]},{"title":"Assignment_1_KNN","slug":"CS231n-Assignment-1-KNN","date":"2018-10-17T09:39:40.000Z","updated":"2019-01-01T14:06:48.229Z","comments":true,"path":"2018/10/17/CS231n-Assignment-1-KNN/","link":"","permalink":"http://yoursite.com/2018/10/17/CS231n-Assignment-1-KNN/","excerpt":"【阅读时间】8 min 2043 words【阅读内容】……","text":"【阅读时间】8 min 2043 words【阅读内容】…… 效果演示 代码实现部分 计算test样本与training样本的L2距离.L2距离的定义：$$ L_2(I_1,I_2) = \\sqrt{{\\sum_p{(I_1^p - I_2^p)^2}}} $$ @card{ Open cs231n/classifiers/k_nearest_neighbor.py and implement compute_distances_two_loops.def compute_distances_two_loops(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in range(num_test): for j in range(num_train): ##################################################################### # TODO: # # Compute the l2 distance between the ith test point and the jth # # training point, and store the result in dists[i, j]. You should # # not use a loop over dimension. # ##################################################################### pass ### X - X_test.shape == (500,3072) X_train.shape = (5000,3072) # dists[i][j] = np.sqrt(np.sum((X[i]-self.X_train[j])**2)) dists[i][j] = np.sqrt(np.sum(np.square(X[i,:] - self.X_train[j,:]))) ##################################################################### # END OF YOUR CODE # ##################################################################### return dists #dists.shape = (500, 5000) } @card{ Now lets speed up distance matrix computation by using partial vectorization with one loop. Implement the function compute_distances_one_loop def compute_distances_one_loop(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in range(num_test): ####################################################################### # TODO: # # Compute the l2 distance between the ith test point and all training # # points, and store the result in dists[i, :]. # ####################################################################### pass # dists[i] = np.sqrt(np.sum((self.X_train - X[i]) ** 2, 1)) dists[i] = np.sqrt(np.sum(np.square(self.X_train - X[i]), axis=1)) ####################################################################### # END OF YOUR CODE # ####################################################################### return dists # dists.shape = (500, 5000) } @card{ Now implement the fully vectorized version inside compute_distances_no_loops 数学说明 def compute_distances_no_loops(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) ######################################################################### # TODO: # # Compute the l2 distance between all test points and all training # # points without using any explicit loops, and store the result in # # dists. # # # # You should implement this function using only basic array operations; # # in particular you should not use functions from scipy. # # # # HINT: Try to formulate the l2 distance using matrix multiplication # # and two broadcast sums. # ######################################################################### pass dists += np.sum(self.X_train ** 2, axis=1).reshape(1, num_train) dists += np.sum(X ** 2, axis=1).reshape(num_test, 1) # reshape for broadcasting dists -= 2 * np.dot(X, self.X_train.T) dists = np.sqrt(dists) ######################################################################### # END OF YOUR CODE # ######################################################################### return dists # dists.shape = (500, 5000) } @card{ Now implement the function predict_labels def predict_labels(self, dists, k=1): num_test = dists.shape[0] y_pred = np.zeros(num_test) for i in range(num_test): # A list of length k storing the labels of the k nearest neighbors to # the ith test point. closest_y = [] ######################################################################### # TODO: # # Use the distance matrix to find the k nearest neighbors of the ith # # testing point, and use self.y_train to find the labels of these # # neighbors. Store these labels in closest_y. # # Hint: Look up the function numpy.argsort. # ######################################################################### pass # sorted_index = np.argsort(dists[i]) # closest_y = self.y_train[sorted_index[:k]] closest_y = self.y_train[np.argsort(dists[i])[:k]] ######################################################################### # TODO: # # Now that you have found the labels of the k nearest neighbors, you # # need to find the most common label in the list closest_y of labels. # # Store this label in y_pred[i]. Break ties by choosing the smaller # # label. # ######################################################################### pass y_pred[i] = np.bincount(closest_y).argmax() # timeLabel = sorted([(np.sum(np.array(closest_y) == y_), y_) for y_ in set(closest_y)])[-1] # y_pred[i] = timeLabel[1] # appear_times = &#123;&#125; # for label in closest_y: # if label in appear_times: # appear_times[label] += 1 # else: # appear_times[label] = 0 # # find most commen label # y_pred[i] = max(appear_times, key=lambda x: appear_times[x]) ######################################################################### # END OF YOUR CODE # ######################################################################### return y_pred } @card{ We will now determine the best value of this hyperparameter with cross-validation.使用cross validation的方法，来选择hyper-parameter超参数k的值.cross validation的原理是，将training样本集分成n份（如下图中的例子，是5份），每一份叫做一个fold，然后依次迭代这n个fold，将其作为validation集合，其余的n-1个fold一起作为training集合，然后进行训练并计算准确率。选择一组候选k值，依次迭代执行上面描述的过程，最终根据准确率，进行评估选择最合适的k值. num_folds = 5k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]X_train_folds = []y_train_folds = []################################################################################# TODO: ## Split up the training data into folds. After splitting, X_train_folds and ## y_train_folds should each be lists of length num_folds, where ## y_train_folds[i] is the label vector for the points in X_train_folds[i]. ## Hint: Look up the numpy array_split function. ################################################################################## Your codeX_train_folds = np.array_split(X_train, num_folds)y_train_folds = np.array_split(y_train, num_folds)################################################################################# END OF YOUR CODE ################################################################################## A dictionary holding the accuracies for different values of k that we find# when running cross-validation. After running cross-validation,# k_to_accuracies[k] should be a list of length num_folds giving the different# accuracy values that we found when using that value of k.k_to_accuracies = &#123;&#125;################################################################################# TODO: ## Perform k-fold cross validation to find the best value of k. For each ## possible value of k, run the k-nearest-neighbor algorithm num_folds times, ## where in each case you use all but one of the folds as training data and the ## last fold as a validation set. Store the accuracies for all fold and all ## values of k in the k_to_accuracies dictionary. ################################################################################## Your codefor k_candi in k_choices: k_to_accuracies[k_candi] = [] for i in range(num_folds): X_test_hy = X_train_folds[i] y_test_hy = y_train_folds[i] X_train_hy = np.vstack(X_train_folds[0:i]+X_train_folds[i+1:]) y_train_hy = np.hstack(y_train_folds[0:i]+y_train_folds[i+1:]) # x_trai = np.array(X_train_folds[:f] + X_train_folds[f+1:])# y_trai = np.array(Y_train_folds[:f] + Y_train_folds[f+1:]) # x_trai = x_trai.reshape(-1, x_trai.shape[2])# y_trai = y_trai.reshape(-1) classifier.train(X_train_hy, y_train_hy) dists_hy = classifier.compute_distances_no_loops(X_test_hy) y_test_pred_hy = classifier.predict_labels(dists_hy, k=k_candi) # Compute the fraction of correctly predicted examples num_correct_hy = np.sum(y_test_pred_hy == y_test_hy) accuracy_hy = float(num_correct_hy) / len(y_test_hy) k_to_accuracies[k_candi].append(accuracy_hy)################################################################################# END OF YOUR CODE #################################################################################print(k_to_accuracies)# Print out the computed accuraciesfor k in sorted(k_to_accuracies): for accuracy in k_to_accuracies[k]: print('k = %d, accuracy = %f' % (k, accuracy)) } @card{ numpy/matplotlib 部分函数说明1. numpy.flatnonzero(): 输入一个矩阵，返回了其中非零元素的位置2. numpy.random.choice(a, size=None, replace=True, p=None) - a: If an ndarray, a random sample is generated from its elements. If an int, the random sample is generated as if a was np.arange(n) - size : int or tuple of ints, optional - replace : boolean, optional If you want only unique samples then this should be false. - p : 1-D array-like, optional The probabilities associated with each entry in a. If not given the sample assumes a uniform distribution over all entries in a.3. matplotlib.pyplot.subplot(X,X,X)： - 前两个数表示子图组成的矩阵的行列数，比如有6个子图，排列成3行2列，那就是subplot(3,2,X)。最后一个数表示要画第X个图了。4. matplotlib.pyplot.imshow(X,interpolation='none',cmap=None) - X: 要绘制的图像或数组. - interpolation 插值方式 [None, 'none', 'nearest', 'bilinear', 'bicubic', 'spline16', 'spline36', 'hanning', 'hamming', 'hermite', 'kaiser', 'quadric', 'catrom', 'gaussian', 'bessel', 'mitchell', 'sinc', 'lanczos'] - cmap: 颜色图谱（colormap), 默认绘制为RGB(A)颜色空间.5. numpy.reshape(a, newshape, order='C') - 注：给出一个m*n的矩阵，如果newshape给的参数是（x, -1）,那么函数会自动判别newshape为（x, m*n/x）,这里的x一定要能被m*n整除！6. numpy.argsort()：输出排好序的元素下标, a[np.argsort(a)]的结果才是最终排好序的结果.7. numpy.binicount(x, weight = None, minlength = None) &gt;&gt;&gt; x = np.array([0, 1, 1, 3, 2, 1, 7]) &gt;&gt;&gt; np.bincount(x) array([1, 3, 1, 1, 0, 0, 0, 1])8. x_norm=np.linalg.norm(x, ord=None, axis=None, keepdims=False) - linalg=linear（线性）+algebra（代数），norm则表示范数 - x: 表示矩阵（也可以是一维） - ord：范数类型 - ord=1：列和的最大值 - ord=2：|λE-ATA|=0，求特征值，然后求最大特征值的算术平方根 - ord=np.inf：行和的最大值 - axis：处理类型 - axis=1表示按行向量处理，求多个行向量的范数 - axis=0表示按列向量处理，求多个列向量的范数 - axis=None表示矩阵范数 - keepding：是否保持矩阵的二维特性 - True表示保持矩阵的二维特性，False相反9. np.dot(A, B)：对于二维矩阵，计算真正意义上的矩阵乘积，同线性代数中矩阵乘法的定义.10. sum(a, axis=None, dtype=None, out=None, keepdims=&lt;class 'numpy._globals._NoValue'&gt;) - axis : - axis = None: 对所有元素求和 - axis = 0: 对所有在同一列的元素求和 - axis = 1: 对所有在同一行的元素求和11. np.vstack(tup): 沿着竖直方向将矩阵堆叠起来 - Note: the arrays must have the same shape along all but the first axis. 除开第一维外，被堆叠的矩阵各维度要一致.12. np.hstack(tup):沿着水平方向将数组堆叠起来13. plt.scatter(X, Y)： 散点图 (x,y)即坐标14. np.random.randn(d0,d1,…,dn) - randn函数返回一个或一组样本，具有标准正态分布。 - d表示维度 - 返回值为指定维度的array } @card{ Summary即使经过调优，knn算法的准确率也不足30%，可以知道knn算法并不适合用于图像分类学习任务. } @card{ 参考： 【实验小结】cs231n assignment1 knn 部分 cs231n 课程作业 Assignment 1、standford-cs231n-assignment1 }","categories":[{"name":"MachineLearning","slug":"MachineLearning","permalink":"http://yoursite.com/categories/MachineLearning/"}],"tags":[{"name":"CS231n","slug":"CS231n","permalink":"http://yoursite.com/tags/CS231n/"},{"name":"Assignment","slug":"Assignment","permalink":"http://yoursite.com/tags/Assignment/"}]},{"title":"Python_Note","slug":"Python-Note","date":"2018-10-04T05:28:48.000Z","updated":"2018-10-06T06:25:05.122Z","comments":true,"path":"2018/10/04/Python-Note/","link":"","permalink":"http://yoursite.com/2018/10/04/Python-Note/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 字符串 字符串是一个sequence word = 'hello'for letter in word: print(letter) 函数 python函数参数传递可以理解为就是变量传值操作 enumerate()使用 如果对一个列表，既要遍历索引又要遍历元素时，首先可以这样写： list1 = [\"这\", \"是\", \"一个\", \"测试\"]for i in range (len(list1)): print i ,list1[i]123 上述方法有些累赘，利用enumerate()会更加直接和优美： list1 = [\"这\", \"是\", \"一个\", \"测试\"]for index, item in enumerate(list1): print index, item&gt;&gt;&gt;0 这1 是2 一个3 测试12345678 enumerate还可以接收第二个参数，用于指定索引起始值，如： list1 = [\"这\", \"是\", \"一个\", \"测试\"]for index, item in enumerate(list1, 1): print index, item&gt;&gt;&gt;1 这2 是3 一个4 测试","categories":[],"tags":[]},{"title":"Morvan_Python","slug":"Morvan-Python","date":"2018-09-27T03:14:01.000Z","updated":"2018-12-01T14:15:57.080Z","comments":true,"path":"2018/09/27/Morvan-Python/","link":"","permalink":"http://yoursite.com/2018/09/27/Morvan-Python/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 基础语法自调用如果想要在执行脚本的时候执行一些代码，比如单元测试，可以在脚本最后加上单元测试 代码，但是该脚本作为一个模块对外提供功能的时候单元测试代码也会执行，这些往往我们不想要的，我们可以把这些代码放入脚本最后： if __name__ == '__main__': #code_here 如果执行该脚本的时候，该 if 判断语句将会是 True,那么内部的代码将会执行。 如果外部调用该脚本，if 判断语句则为 False,内部代码将不会执行。 可变参数顾名思义，函数的可变参数是传入的参数可以变化的，1个，2个到任意个。当然可以将这些 参数封装成一个 list 或者 tuple 传入，但不够 pythonic。使用可变参数可以很好解决该问题，注意可变参数在函数定义不能出现在特定参数和默认参数前面，因为可变参数会吞噬掉这些参数。 def report(name, *grades): total_grade = 0 for grade in grades: total_grade += grade print(name, 'total grade is ', total_grade) 定义了一个函数，传入一个参数为 name, 后面的参数 *grades 使用了 * 修饰，表明该参数是一个可变参数，这是一个可迭代的对象。该函数输入姓名和各科的成绩，输出姓名和总共成绩。所以可以这样调用函数 report(&#39;Mike&#39;, 8, 9)，输出的结果为 Mike total grade is 17, 也可以这样调用 report(&#39;Mike&#39;, 8, 9, 10)，输出的结果为 Mike total grade is 27 关键字参数关键字参数可以传入0个或者任意个含参数名的参数，这些参数名在函数定义中并没有出现，这些参数在函数内部自动封装成一个字典(dict). def portrait(name, **kw): print('name is', name) for k,v in kw.items(): print(k, v) 定义了一个函数，传入一个参数 name, 和关键字参数 kw，使用了 ** 修饰。表明该参数是关键字参数，通常来讲关键字参数是放在函数参数列表的最后。如果调用参数portrait(&#39;Mike&#39;, age=24, country=&#39;China&#39;, education=&#39;bachelor&#39;) 输出: name is Mikeage 24country Chinaeducation bachelor 通过可变参数和关键字参数，任何函数都可以用 universal_func(*args, **kw) 表达。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Morvan","slug":"Morvan","permalink":"http://yoursite.com/tags/Morvan/"}]},{"title":"LibraryManagementSystem_Notes","slug":"LibraryManagementSystem-Notes","date":"2018-09-26T08:35:09.000Z","updated":"2018-09-26T11:38:30.838Z","comments":true,"path":"2018/09/26/LibraryManagementSystem-Notes/","link":"","permalink":"http://yoursite.com/2018/09/26/LibraryManagementSystem-Notes/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… 基于bean层(ENTITY层 — 实体层 数据库在项目中的类 )中Table的设计,在DAO/Service/Controller层进行填充 DAO层(database access object 数据持久层 — 主要与数据库进行交互) public interface UserBkunitDAO extends JpaRepository&lt;UserBkunit, Integer&gt;&#123; Page&lt;UserBkunit&gt; findAllByUser(User reader, Pageable pageable); &#125;// 注：DAO层首先会创建DAO接口，然后会在配置文件中定义该接口的实现类.DAO设计的总体规划需要和设计的表，和实现类之间一一对应. Service层(业务逻辑层 — 负责业务模块的逻辑应用设计) @Servicepublic class ReaderFunctionService &#123; @Autowired private UserService userService; @Autowired private UserBkunitDAO userBkunitDAO; public Page&lt;UserBkunit&gt; queryborrowedBooks(int start, int size) &#123; User reader = userService.getUser(); start = start &lt; 0 ? 0 : start; Sort sort = new Sort(Sort.Direction.DESC, \"date\"); Pageable pageable = PageRequest.of(start, size, sort); Page&lt;UserBkunit&gt; page = userBkunitDAO.findAllByUser(reader, pageable); return page; &#125;&#125;// 注：Service层应该既调用DAO层的接口，又要提供接口给Controller层的类来进行调用. Controller层(action层/控制层 — 控制业务逻辑，与View层结合紧密) @Controllerpublic class ReaderFunctionController &#123; @Autowired private ReaderFunctionService readerfunctionservice; @RequestMapping(value = \"/reader/borrowedBooks\",method = RequestMethod.GET) @ResponseBody public String queryBorrowTools(Model model, @RequestParam(value = \"start\", defaultValue = \"0\") int start,@RequestParam(value = \"size\", defaultValue = \"10\") int size) &#123; Page&lt;UserBkunit&gt; page = readerfunctionservice.queryborrowedBooks(start, size); model.addAttribute(\"page\", page); return \"queryBorrowedBooks\"; &#125;&#125;// 注：不关心业务逻辑的具体实现,仅仅需要调用service层里的一个方法即可.","categories":[{"name":"XD","slug":"XD","permalink":"http://yoursite.com/categories/XD/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"【2】一维随机变量及其分布","slug":"二、一维随机变量及其分布","date":"2018-08-28T10:20:53.000Z","updated":"2018-08-28T10:35:58.314Z","comments":true,"path":"2018/08/28/二、一维随机变量及其分布/","link":"","permalink":"http://yoursite.com/2018/08/28/二、一维随机变量及其分布/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… @column-2{ @card{ 离散$$ F(x,y) = P \\{ X \\leqslant x,Y \\leqslant y\\}=\\int_{-\\infty}^{x} du \\int_{-\\infty}^{y} f(u,v)dv $$ $$F_{x}(x)$$ 就是这样 $$F(x,y) = P \\{ X \\leqslant x,Y \\leqslant y\\}$$$$ 就是这样 } @card{ 连续} }","categories":[{"name":"Math","slug":"Math","permalink":"http://yoursite.com/categories/Math/"}],"tags":[{"name":"概率论","slug":"概率论","permalink":"http://yoursite.com/tags/概率论/"}]},{"title":"分布函数与数字特征","slug":"概率论","date":"2018-08-24T15:28:24.000Z","updated":"2018-08-24T16:56:35.566Z","comments":true,"path":"2018/08/24/概率论/","link":"","permalink":"http://yoursite.com/2018/08/24/概率论/","excerpt":"【阅读时间】XXX min XXX words【阅读内容】……","text":"【阅读时间】XXX min XXX words【阅读内容】…… @column-2{ @card{ 分布函数一维离散型r.v.$$F(x)=\\sum_{x{i}\\leqslant x} P_{i}$$一维连续型r.v.$$ F(x)= P \\{ X \\leqslant x \\}=\\int_{-\\infty}^{x}f(t)dt $$ 二维连续型r.v.$$ F(x,y) = P \\{ X \\leqslant x,Y \\leqslant y\\}=\\int_{-\\infty}^{x} du \\int_{-\\infty}^{y} f(u,v)dv $$ } @card{ 数学期望一维离散型r.v.$$E(x)=\\sum x_{i}P_{i}$$一维连续型r.v.$$E(X) = \\int_{-\\infty}^{+ \\infty} xf(x)dx$$ } }","categories":[{"name":"Math","slug":"Math","permalink":"http://yoursite.com/categories/Math/"}],"tags":[{"name":"概率论","slug":"概率论","permalink":"http://yoursite.com/tags/概率论/"}]},{"title":"编译原理知识点","slug":"编译原理知识汇总","date":"2018-08-17T17:32:07.000Z","updated":"2018-08-18T03:22:02.614Z","comments":true,"path":"2018/08/18/编译原理知识汇总/","link":"","permalink":"http://yoursite.com/2018/08/18/编译原理知识汇总/","excerpt":"【阅读时间】20min 11001words【阅读内容】XD编译原理教材知识汇总","text":"【阅读时间】20min 11001words【阅读内容】XD编译原理教材知识汇总 第一章 引言1.从面向机器的语言到面向人类的语言汇编指令：用符号表示的指令被称为汇编指令汇编语言：汇编指令的集合称为汇编语言 2.语言之间的翻译转换(也被称为预处理)：高级语言之间的翻译，如FORTRAN到ADA的转换编译：高级语言可以直接翻译成机器语言，也可以翻译成汇编语言，这两个翻译过程称为编译汇编：从汇编语言到机器语言的翻译被称为汇编交叉汇编：将一个汇编语言程序汇编成为可在另一机器上运行的机器指令成为交叉汇编反汇编：把机器语言翻译成汇编语言反编译：把汇编语言翻译成高级语言 3. 编译器与解释器（1）语言翻译的两种基本形态 解释器与编译器的主要区别:运行目标程序时的控制权在解释器而不在目标程序. （2）各自特点 编译器:工作效率高,即时间快、空间省；交互性与动态性差,可移植性差. 解释器:工作效率低,,即时间慢、空间费；交互性与动态性好,可移植性好. 共同点:均完成对源程序的翻译.差异:编译器采用先翻译后执行,解释器采用边翻译边执行. 4. 编译器的工作原理与基本组成（0）通用程序设计语言的主要成份 声明＋操作＝完整定义 （1）以过程为基本结构的程序设计语言的组成 声明性语句：提供操作对象的性质，如数据类型、值、作用域等； 操作性语句：确定操作的计算次序，完成实际操作。 过程定义 = 过程头＋过程体 （2）以阶段划分编译器 注：符号表管理器和出错处理贯穿编译器工作的各个阶段. （3）编译器各阶段工作 1&gt; 词法分析：词法分析的输入是源程序,输出是识别出的记号流.目的是识别单词. 至少分以下几类：关键字(保留字)、标识符、字面量、特殊符号 2&gt; 语法分析： 输入是词法分析器返回的记号流,输出是语法树.目的是得到语言结构并以树的形式表示.对于声明性语句,进行符号表的查填,对于可执行语句,检查结构合理的表达式运算是否有意义. 3&gt; 语义分析：根据语义规则对语法树中的语法单元进行静态语义检查,如类型检查和转换等,目的在于保证语法正确的结构在语义分析上也是合法的. 4&gt; 中间代码生成(可选)：生成一种既接近目标语言,又与具体机器无关的表示,便于代码优化与代码生成. (到目前为止，编译器与解释器可以一致) 5&gt; 中间代码优化(可选)：局部优化、循环优化、全局优化等；优化实际上是一个等价变换，变换前后的指令序列完成同样的功能，但在占用的空间上和程序执行的时间上都更省、更有效 6&gt; 目标代码生成：不同形式的目标代码—汇编语言形式、可重定位二进制代码形式、内存形式(Load-and-Go) 7&gt; 符号表管理：合理组织符号,便于各阶段查找\\填写等. 8&gt; 出错处理： 动态错误：源程序中的逻辑错误，发生在程序运行的时候。也称为动态语义错误静态错误：静态错误分为语法错误和静态语义错误. &lt;1&gt; 语法错误：有关语言结构上的错误，如单词拼写错误、表达式缺少操作数、begin和end不匹配&lt;2&gt; 静态语义错误：分析源程序时可以发现的语言意义上的错误，如加法的两个操作数一个是整形变量，另一个是数组名 （4）编译器的分析\\综合模式 逻辑上把编译器分为分析(前端)部分和综合(后端)部分.1&gt; 分析(前端)：语言结构和意义的分析； 从词法分析到中间代码生成各阶段的工作2&gt; 综合(后端)：语言意义处理；从中间代码生成到目标代码生成的各阶段的工作3&gt; 编译器和解释器的区别往往是在形成中间代码之后开始的. 5. 编译器扫描的遍数每个阶段将程序完整分析一遍的工作模式称为一遍扫描。(将源程序或源程序的某种形式的中间表示完整分析一遍，亦称作一遍扫描) 第二章 词法分析1. 词法分析中的若干问题(1) 记号、模式与单词 单词的分类：关键字(保留字)、标识符、字面量、特殊符号模式（pattern）：产生/识别单词的规则记号（token）：按照某个模式(或规则)识别出的元素(一组)单词（lexeme）：被识别出的元素的值(字符串本身) ，也称为词值 (2) 词法分析器的作用与工作方式 词法分析器的作用： 1&gt; 识别记号并交给语法分析器(根据模式识别记号)2&gt; 滤掉源程序中的无用成分,如注释、空格和回车等3&gt; 处理与具体平台有关的输入(如文件结束符的不同表示等)4&gt; 调用符号表管理器和出错处理器，进行相关处理 工作方式：1.单独一遍扫描2.作为语法分析器的子程序3.并行方式 2. 模式的形式化描述(1) 字符串与语言 语言L是有限字母表∑上有限长度字符串的集合.定义中强调两个有限，因为计算机的表示能力有限 ：1&gt; 字母表是有限的，即字母表中元素是有限多个；2&gt; 字符串的长度是有限的，即字符串中字符个数是有限多个。 (字符串与字符串集合相关的概念与运算,如前缀、后缀、子串、子序列等，字符串的并、交、连接、差、闭包) (2) 正规式与正规集 令Σ是一个有限字母表，则Σ上的 正规式 及其表示的集合递归定义如下: 1. ε是正规式，它表示集合 L(ε) = &#123;ε&#125; 2. 若a是Σ上的字符，则a是正规式，它表示集合L(a)=&#123;a&#125; 3. 若正规式r和s分别表示集合L(r)和L(s)，则 （a） r|s是正规式，表示集合L(r)∪L(s)， （b） rs是正规式，表示集合L(r)L(s)， （c） r*是正规式，表示集合(L(r))*， （d）(r)是正规式，表示的集合仍然是L(r)。 括弧用来改变运算的先后次序！ 可用正规式描述(其结构)的语言称为 正规语言 或 正规集 。 若运算的优先级和结合性做下述约定: 1. 三种运算均具有左结合性质； 2. 优先级从高到低顺序排列为:闭包运算、连接运算、或运算。则正规式中不必要的括号可以被省略。 若正规式P和Q表示了同一个正规集，则称P和Q是等价的，记为P=Q (3) 简化正规式描述(主要是简化书写上的复杂) (a) 正闭包 若r是表示L(r)的正规式，则r+是表示(L(r))+的正规式，且下述等式成立:r+ = rr* = rr，r = r+|ε; +与*具有相同的运算结合性和优先级(b) 可缺省 若r是正规式，则r?是表示L(r)∪&#123;ε&#125;的正规式，且下述等式成立:r? = r|ε ? 与 * 具有相同的运算结合性和优先级(c) 串 若r是若干字符进行连接运算构成的正规式，则:串“r” = r ，且: ε= “”， a = “a”（a是Σ的任一字符）(d) 字符组 若r是若干字符进行|运算构成的正规式，则可改写为 [r’]，其中r’可以有如下两种书写形式： 枚举: 如 a|b|e|h，可写为 [abeh]： 分段: 如0|1|2|3|4|5|6|7|8|9|a|b|c|d|e , 可写为： [0-9a-e](e) 非字符组 若[r]是一个字符组形式的正规式，则[^r]是表示∑- L([r])的正规式。 3. 记号的识别——有限自动机(1) 不确定的有限自动机（NondeterministicFinite Automaton, NFA） NFA是一个五元组（5-tuple）：M =（S，∑，move，s0，F），其中（1） S是有限个状态（state）的集合；（2） ∑是有限个输入字符（包括ε）的集合；（3） move是一个状态转移函数，move(si，ch)=sj表示，当前状态si下若遇到输入字符ch，则转移到状态sj；（4） s0是唯一的初态（也称开始状态）；（5） F是终态集（也称接受状态集），它是S的子集，包含了所有的终态。 直观的表示方式 ① 状态转换图：用一个有向图来直观表示NFA② 状态转换矩阵：用一个矩阵来直观表示NFA (矩阵中，状态对应行，字符对应列) NFA(识别记号)的特点NFA识别记号的最大特点是它的不确定性，即在当前状态下对同一字符有多于一个的下一状态转移。 具体体现：定义： move函数是1对多的；状态转换图：从同一状态出发，可通过多于一条标记相同字符的边转移到不同的状态；状态转换矩阵： M[si,a]是一个状态的集合 NFA识别记号存在的问题 1.只有尝试了全部可能的路径,才能确定一个输入序列不被接受,而这些路径的条数随着路径长度的增长成指数增长2.识别过程中需要进行大量回朔，时间复杂度升高且算法复杂 (2) 确定的有限自动机（Deterministic Finite Automaton, DFA） 定义: DFA是NFA的一个特例，其中： （1）没有状态具有ε状态转移(ε-transition)，即状态转换图中没有标记ε的边； （2）对每个状态s和每个字符a，最多有一个下一状态。 特点：与NFA相比，DFA的特征：确定性 定义：move（si, a)函数都是 1对1 的； 转换图 从一个状态出发的任2条边上的标记均不同； 转换矩阵：M[si,a]是一个状态 且字母表不包括ε。提示：正规式和有限自动机从两个侧面表示正规式。正规式是描述，自动机是识别。 4. 从正规式到词法分析器构造词法分析器的一般方法和步骤：1. 用正规式描述模式（为记号设计正规式）；2. 为每个正规式构造一个NFA，它识别正规式所表示的正规集；3. 将构造的NFA转换成等价的DFA，这一过程也被称为确定化；4. 优化DFA，使其状态数最少，这一过程也被称为最小化；5. 根据优化后的DFA构造词法分析器。 (1) 从正规式到NFA Thompson 算法 (2) 从NFA到DFA - smove(S, a)：从状态集S出发，标记为a的下一状态全体。与move(s, a)的唯一区别：用状态集取代状态- ε-闭包(T)：从状态集T出发，不经任何字符达到的状态全体- “子集法”构造DFA (3) 最小化DFA ​ ① 对于任何两个状态t和s，若从一状态出发接受输入字符串ω，而从另一状态出发不接受ω. 或者，② 从t出发和从s出发到达不同的接受状态，则称ω对状态t和s是可区分的. ​ 不可区分的状态位于一个组内，可以合并成一个状态. 主要步骤：​ 1.初始划分：终态组 ， 非终态组；​ 2.利用可区分的概念，反复分裂划分中的组Gi，直到不可再分裂；​ 3.由最终划分构造D’，关键是选代表和修改状态转移；​ 4.消除可能的死状态和不可达状态。 5. 从DFA构造词法分析器分类：表驱动型的词法分析器；直接编码的词法分析器比较： 表驱动 直接编码 分析器的速度 慢 快 程序与模式的关系 无关 有关 适合的编写方法 工具生成 手工编写 分析器的规模 较大 较小 第三章 语法分析词法分析：记号的集合，字符串由字母组成，线性结构语法分析：句子的集合，句子由记号组成，非线性结构（树） 语法分析的双重含义： 语法规则：上下文无关文法（子集：LL文法或LR文法） 语法分析：下推自动机（LL或LR分析器）、自上而下分析、自下而上分析 1. 语法分析的若干问题许多编译器，特别是由自动生成工具构造的编译器，往往其前端的中心部件就是语法分析器 （1）语法分析器的作用 根据词法分析器提供的记号流，为语法正确的输入构造分析树（或语法树） 检查输入中的语法（可能包括词法）错误，并调用出错处理器进行适当处理 （2）语法错误的处理原则 源程序中可能出现的错误 语法(包括词法)错误和语义错误(静态语义错误和动态语义错误) 注：跟第一章的分类角度不同，第一章是从静态错误(语法错误，静态语义错误)和动态错误(动态语义错误)分类的，但是殊途同归。 词法错误：指非法字符或拼写错关键字、标识符等语法错误：指语法结构出错，如少分号、括号不匹配、begin/end不配对等静态语义错误：如类型不一致、参数不匹配等动态语义错误(逻辑错误)：如死循环、变量为零时作除数等 2. 上下文无关文法(CFG)（1）上下文无关文法(Context Free Grammar,CFG) CFG是一个四元组G =（N，T，P，S），其中（1） N是非终结符（Nonterminals）的有限集合；（2） T是终结符（Terminals）的有限集合，且N∩T=Φ；（3） P是产生式（Productions）的有限集合，A→α，其中A∈N(左部),α∈(N∪T)*(右部),若α=ε，则称A→ε为空产生式(也可以记为A →);（4） S是非终结符，称为文法的开始符号（Start symbol） 注： S ∈ N , N可以出现在产生式左边和右边，T绝不出现在产生式左边. （2）CFG产生语言的基本方法－推导 CFG（产生式）通过推导的方法产生语言，即（通俗地讲）从开始符号S开始，反复使用产生式：将产生式左部的非终结符替换为右部的文法符号序列(展开产生式，用=&gt;表示)，直到得到一个终结符序列。 1&gt; 直接推导：利用产生式产生句子的过程中，将用产生式A→γ的右部代替文法符号序列αAβ中的A得到αγβ的过程，称αAβ直接推导出αγβ，记作：αAβ=&gt;αγβ 2&gt; 零步或多步推导：若对于任意文法符号序列α1，α2，…αn，有α1=&gt;α2=&gt;…=&gt;αn，则称此过程为零步或多步推导，记为：α1 =*&gt; αn，其中α1=αn的情况为零步推导。 3&gt; 至少一次推导：若α1≠αn，即推导过程中至少使用一次产生式,则称此过程为至少一步推导，记为：α1 =+&gt; αn (推导具有自反性和传递性) 4&gt; 由 CFGG 所产生的语言L(G)被定义为: L(G) = { ω┃S ωand ω∈T }，​ L(G)称为上下文无关语言(Context Free Language, CFL)，ω称为句子。​ 若S = &gt; α，α∈(N∪T)*，则称α为G的一个句型。句子一定是句型，反之不是。 5&gt; 在推导过程中，若每次直接推导均替换句型中最左边的非终结符，则称为最左推导，由最左推导产生的句型被称为左句型。 类似的可以定义最右推导与右句型，最右推导也被称为规范推导。 （3）推导、分析树与语法树 1、分析树既反映语言结构的实质，也反映推导过程。 2、对CFGG的句型，分析树被定义为具有下述性质的一棵树。 （1） 根由开始符号所标记； （2） 每个叶子由一个终结符、非终结符、或ε标记； （3） 每个内部结点由一个非终结符标记； （4） 若A是某内部节点的标记，且X1，X2，…，Xn是该节点从左到右所有孩子的标记，则A→X1X2…Xn是一个产生式。若A→ε，则标记为A的结点可以仅有一个标记为ε的孩子。 注：分析树的叶子，从左到右构成G的一个句型。若叶子仅由终结符标记，则构成一个句子。 3、对CFG G的句型，表达式的语法树被定义为具有下述性质的一棵树: （1） 根与内部节点由表达式中的操作符标记； （2） 叶子由表达式中的操作数标记； （3）用于改变运算优先级和结合性的括号，被隐含在语法树的结构中。 语法树是表示表达式结构的最好形式 （4）二义性与二义性的消除 二义性：若文法G对 同 一句子产生不止一棵分析树，则称G是二义的. 结论：1&gt; 一个句子有多于一棵分析树，仅与文法和句子有关，与采用的推导方法无关；2&gt; 造成文法二义的根本原因：文法中缺少对文法符号优先级和结合性的规定 二义性消除的方法：① 改写二义文法为非二义文法；② 规定二义文法中符号的优先级和结合性，使仅产生一棵分析树。 3. 语法与文法简介（1）正规式与上下文无关文法 记号可以用正规式描述，正规式适合描述线性结构，如标识符、关键字、注释等. 句子可以用CFG描述，CFG适合描述具有嵌套(层次)性质的非线性结构，如不同结构的句子if-then-else\\while-do等 正规式所描述的语言结构均可以用CFG描述，反之不一定. （2）上下文有关文法CSG 典型的这类语言结构包含：计数问题的抽象、变量的声明与引用、过程调用时形参与实参的一致性检查等.描述它们的文法被称为上下文有关文法(Context Sensitive Grammar，CSG).这些语言结构无法用上下文无关文法CSG来描述. （3）形式语言与自动机简介 ​ 若文法G=(N，T，P，S)的每个产生式α→β中，均有α∈(N∪T)，且至少含有一个非终结符，β∈(N∪T)，则称G为0型文法. ​ 对0型文法施加以下第i条限制，即得到i型文法。 ​ 1&gt; G的任何产生式α→β（S→ε除外）满足|α|≤|β|；​ 2&gt; G的任何产生式形如A→β，其中A∈N，β∈(N∪T)*；​ 3&gt; G的任何产生式形如A→a或者A→aB(或者A→Ba)，其中A和B∈N，a∈T。 文法 语言 自动机 短语文法(0型) 短语结构语言 图灵机 CSG(1型) CSL 线性界线自动机 CFG(2型) CFL 下推自动机 正规文法(3型) 正规集 有限自动机 4. 自上而下语法分析分为：递归下降分析法、预测分析法 基本思想：对任何一个输入序列ω，从S开始进行最左推导，直到得到一个合法的句子或发现一个非法结构。整个自上而下分析是一个试探的过程，是反复使用不同产生式谋求与输入序列匹配的过程。 提前准备——重写文法：1.消除左递归，以避免陷入死循环； 2.提取左因子，以避免回溯. （1）消除左递归 定义：若文法G中的非终结符A，对某个文法符号序列α存在推导A =+&gt; Aα，则称G是左递归的。若G中有形如A→Aα的产生式，则称该产生式对A直接左递归。 消除文法的直接左递归 A→Aα|β 替换为 A →βA&apos; A&apos;→αA&apos;|ε 首先，整理A产生式为如下形式：A→ Aα1|Aα2|…|Aαm|β1|β2|…|βn然后用下述产生式代替A产生式：A→ β1 A’|β2 A’| …|βn A’​ A’→ α1 A’ | α2 A’ | … | αm A’ |ε 消除文法的左递归 核心思想：将无直接左递归的非终结符展开到其他产生式,然后消除其他产生式中的直接左递归(如果有的话) 若G产生句子的过程中出现A=+A的推导，则无法消除左递归(出现回路) （2）提取左因子 提取文法的左因子 左因子产生原因：公共前缀：A → αβ1|αβ2方法：将 A → αβ1|αβ2|γ​ 替换为 A→αA’|γ A’→β1|β2 （3）递归下降分析 直接以程序代码（的方式）模拟产生式产生语言的过程: 基本思想：每个非终结符对应一个子程序（函数），过程体中： 产生式右部的非终结符：对应子程序调用， 产生式右部的终结符： 与输入记号序列进行匹配。 特点：1&gt; 子程序是递归的（因为文法是递归的）；2&gt; 程序与文法相关；3&gt; 它对文法的限制是不能有公共左因子和左递归；4&gt; 它是一种非形式化的方法，只要能写出子程序，用什么样的方法和步骤均可。 （4）预测分析器 ☆ 预测分析器由一张预测分析表、一个符号栈和一个驱动器组成，数学模型是下推自动机。☆ 对文法的限制是不能有公共左因子和左递归 预测分析器的核心概念：1&gt; 分析方法：格局与格局变换2&gt; 分析表+驱动器（模拟算法）3&gt; 预测分析表的构造4&gt; LL（文法、语言、分析器） ☆ 开始格局的剩余输入是全部输入序列，而接收格局中剩余输入应该为空，任何其他格局或出错格局中的剩余输入应该是全部输入序列的一个后缀. ☆ 改变格局的动作： ① 匹配终结符： 若top^=ip^(但≠#)，则pop且next(ip)；② 展开非终结符：若top^= X且M[X,ip^]=α(X→α)，则pop且push(α)；③ 报告分析成功： 若top ^= ip^ = #，则分析成功并结束；④ 报告出错：其它情况，调用错误恢复例程. ☆ 驱动器算法 ☆ 构造预测分析表 步骤：1. 构造文法符号X的FIRST集合和非终结符的FOLLOW集合；2. 根据两个集合构造预测分析表. 通俗地讲，α的FIRST集合就是从α开始可以导出的文法符号序列中的开头终结符。而A的FOLLOW集合，就是从开始符号可以导出的所有含A的文法符号序列中紧跟A之后的终结符. 计算X的FIRST集合 —–自下而上计算 计算所有非终结符的FOLLOW集合 —— 自上而下计算 构造预测分析表 LL(1)文法 文法G被称为是LL(1)文法，当且仅当为它构造的预测分析表中不含多重定义的条目。由此分析表所组成的分析器被称为LL(1)分析器，它所分析的语言被称为LL(1)语言。 ☆ 第一个L代表从左到右扫描输入序列，第二个L表示产生最左推导，1表示在确定分析器的每一步动作时向前看一个终结符. 推论3.2 G是LL(1)的，当且仅当G的任何两个产生式A→α|β满足:1. 对任何终结符a，α和β不能同时推导出以a开始的串；即First(α) ∩ First(β) = ∅2. α和β最多有一个可以推导出ε；3. 若β =*&gt; ε,则α不能导出以FOLLOW(A)中终结符开始的任何串. 即First(α) ∩ Follow(A) = ∅ ☆ 无论是递归下降子程序法还是非递归的预测分析法，他们都只能处理LL(1)文法. 5. 自下而上语法分析☆ 自上而下分析采用的是推导;自下而上分析采用的是归约(规范归约—剪句柄—移进/归约分析—SLR(1)分析器). （1）自下而上分析的基本方法 ☆ 基本思想：最左归约. 对于每个输入序列ω：从左到右扫描ω; 从ω开始,反复用产生式的左部替换产生式的右部(即当前句型中的句柄)、谋求对ω的匹配,最终得到文法的开始符号，或者发现一个错误。 ☆ 基本概念： a) &gt; 设αβδ是文法G的一个句型，若存在S=*&gt;αAδ，A=+&gt;β， 则称β是句型αβδ相对于A的&quot;短语&quot;. &gt; 特别的，若 有A→β，则 称β是句型αβδ相对于产生式A→β的&quot;直接短语&quot;. &gt; 一个句型的最左直接短语被称为&quot;句柄&quot;. 特征： 1. 短语：以非终结符为根子树中所有从左到右的叶子； 2. 直接短语：只有父子关系的子树中所有从左到右排列的叶子（树高为2）； 3. 句柄：最左边父子关系树中所有从左到右排列的叶子（句柄是唯一的） b)最左归约：若 α是文法G的句子且满足下述条件，则称序列αn，αn-1，...，α0是α的一个最左归约。 1) αn = α 2) α0 = S（S是G 的开始符号） 3) 对任何i(0&lt;i&lt;=n)，αi-1是将αi中句柄替换为相应产生式左部非终结符得到的 ☆ 最左归约的逆过程是一个最右推导，分别称最右推导和最左归约为规范推导和规范归约. c）移进-归约分析器 1. 工作方式：格局与格局变换 2. 分析表 3. 驱动器（模拟算法） 4. SLR分析表的构造 5. LR（文法、语言、分析器）☆ 改变格局的动作：1. 移进(shift)：当前剩余输入的下一终结符进栈。2.归约(reduce)：将栈顶句柄替换为对应非终结符(最左归约)3.接受(accept)：宣告分析成功4. 报错(error)：发现语法错误，调用错误恢复例程 (2) LR分析 a) LR分析与LR文法LR分析：允许左递归，但不能有二义 定义3.15 若为文法G构造的移进-归约分析表中不含多重定义的条目，则称G为&quot;LR(k)文法&quot;，分析器被称为是&quot;LR(k)分析器&quot;，它所识别的语言被称为&quot;LR(k)语言&quot;。&quot;L&quot;表示从左到右扫描输入序列，&quot;R&quot;表示逆序的最右推导，&quot;k&quot;表示为确定下一动作向前看的终结符个数，一般情况下k&lt;=1。当k=1时，简称&quot;LR&quot;。 构造SLR(1)分析器 活前缀与LR(0)项目 第1步 第2~N步 状态 词法–DFA ε-closure(S) ε-closure(smove(S,a)) 状态集 语法–DFA closure(I) closure(goto(I,x)) 项目集 出现在移进-归约分析器栈中的右句型的前缀，被称为文法G的活前缀(viable prefix).LR(0)项目(简称项目)是这样一个产生式，在它右边的某个位置有一个点”.”。对于A→ε，它仅有一个项目A→.。项目A→α.β显示了分析过程中看到(移进)了产生式的多少。β不为空的项目称为可移进项目，β为空的项目称为可归约项目. 拓广文法与识别活前缀的DFA G’ = G ∪ {S’ → S}其中：S’ → S是识别S的初态，S’ → S. 是识别S的终态. 目的是使最终构造的DFA状态集中具有唯一的初态和终态. ① closure(I)：从项目集I不经任何文法符号到达的项目全体； ② goto(I，x)：所有从I经文法符号x能直接到达的项目全体。 项目[S’→.S]和所有“.”不在产生式右部最左边的项目称为核心项目(kernel items)，其它“.”在产生式右部最左边的项目(不包括[S’→.S])称为非核心项目(nonkernel items).核心项目：J=goto(I，X)，S&apos;→.S（作为项目集的代表）非核心项目：closure(J)-J（特点：可由J某中某项目算得） 识别活前缀 定义3.21 若存在最右推导S’=*&gt; αAω =&gt; αβ1β2ω，则称项目[A→β1.β2] 对活前缀αβ1有效。 当一个项目集中同时存在： 1. A→β1.β2和B→β.：既可移进又可归约，移进/归约冲突 2.A→α.和B→β.：均可指导下一步分析，归约/归约冲突解决方法：简单向前看一个终结符： 1. 移进/归约冲突：若FIRST(β2)∩FOLLOW(B)=Φ，冲突可解决 2. 归约/归约冲突：若FOLLOW(A)∩FOLLOW(B)=Φ，冲突可解决若冲突可以解决，则称文法为SLR(1)文法，构造的分析表为SLR(1)分析表。SLR(1)文法：简单向前看一个终结符即可解决冲突☆ 二义文法不是SLR(1)文法 第四章 静态语义分析采用语法制导翻译生成中间代码 1. 语法制导翻译简介（1）语法与语义的关系 语法是指语言的结构、即语言的“样子”；语义是指附着于语言结构上的实际含意，即语言的“意义”.一个语法上正确的句子，它所代表的意义并不一定正确. ☆ 语义分析的作用 • 检查结构正确的句子所表示的意思是否合法；• 执行规定的语义动作，如：表达式求值、符号表的查询/填写、中间代码生成等 ☆ 应用最广的语义分析方法是语法制导翻译，他的基本思想是将语言结构的语义以属性的形式赋予代表此结构的文法符号，而属性的计算以语义规则的形式赋予由文法符号组成的产生式. （2）属性/语义规则的定义 定义4.1 对于产生式A→α，其中α是由文法符号X1X2...Xn组成的序列，它的语义规则可以表示为(4.1)所示关于属性的函数f： b := f(c1, c2, ..., ck) (4.1)语义规则中的属性存在下述性质与关系： (1) 称(4.1)中属性b依赖于属性c1, c2, ..., ck。 (2) 若b是A的属性，c1, c2, ..., ck是α中文法符号的属性，或者A的其它属性，则称b是A的综合属性。 (3) 若b是α中某文法符号Xi的属性，c1, c2, ..., ck是A的属性，或者是α中其它文法符号的属性，则称b是Xi的继承属性。 (4) 若语义规则的形式如下述(4.2)，则可将其想像为产生式左部文法符号A的一个虚拟属性。属性之间的依赖关系，在虚拟属性上依然存在。 f(c1, c2, ..., ck) (4.2) ■ ☆ 继承属性从前辈和兄弟的属性计算得到,综合属性从子孙和自身的其他属性计算得到. 即,继承属性“自上而下,包括兄弟”,综合属性“自下而上,包括自身”. （3）语义规则的两种形式 ☆ 语义规则的两种形式（忽略实现细节，二者作用等价） 语法制导定义(Syntax Directed Definition) 用抽象的属性和运算表示的语义规则；(公式，做什么) 翻译方案(Translation Scheme) 用具体的属性和运算表示的语义规则。(程序段，如何做) ☆ 继承属性是自上而下计算的，综合属性是自下而上计算的. （4）LR分析翻译方案的设计 ☆ LR分析中的语法制导翻译实质上是对LR语法分析的扩充： 扩充LR分析器的功能 当执行归约产生式的动作时，也执行相应产生式对应的语义动作。由于是归约时执行语义动作， ​ 因此限制语义动作仅能放在产生式右部的最右边； 扩充分析栈 ​ 增加一个与分析栈并列的语义栈，用于存放分析栈中文法符号所对应的属性值。 ☆ 扩充后的LR分析最适合对综合属性的计算，而对于继承属性的计算还需要进行适当的处理. 2. 中间代码简介☆ 中间代码应具备的特性1）便于语法制导翻译2）既与机器指令的结构相近,又与具体机器无关. 使用中间代码的好处:一是便于编译器程序的开发和移植,二是代码进行优化处理. ☆ 中间代码的主要形式：后缀式、树、三地址码等.最基本的中间代码形式是树🌲；最常用的中间代码形式是三地址码，它的实现形式常采用四元式形式。 ☆ 符号表是帮助声明语句实现存储空间分配的重要数据结构。 （1）后缀式 操作数在前，操作符紧随其后，无需用括号限制运算的优先级和结合性；便于求值. （2）三地址码 ① 三元式 形式： (i) (op, arg1, arg2) ​ 三地址码：(i):= arg1 op arg2 序号的双重含义：既代表此三元式，又代表三元式存放的结果 存放方式：数组结构，三元式在数组中的位置由下标决定 弱点：给代码的优化带来困难 ② 四元式 形式： ( i ) (op，arg1，arg2，result) ​ 所表示的计算： result:= arg1 op arg2 四元式与三元式的唯一区别：将由序号所表示的运算结果改为：用(临时)变量来表示。 此改变使得四元式的运算结果与其在四元式序列中的位置无关.为代码的优化提供了极大方便，因为这样可以删除或移动四元式而不会影响运算结果. ③ 树形表示 1&gt; 语法树真实反映句子结构，对语法树稍加修改（加入语义信息），即可以作为中间代码的一种形式(注释语法树)2&gt; 树的优化表示－DAG3&gt; 树与其他中间代码的关系 ☆ 树表示的中间代码与后缀式和三地址码之间有内在联系 树 → 后缀式 ​ 方法：对树进行深度优先后序遍历，得到的线性序列就是后缀式，或者说后缀式是树的一个线性化序列； 树 → 三元式/四元式 特点：树的每个非叶子节点和它的儿子对应一个三元式或四元式； 方法：对树的非叶子节点进行深度优先后序遍历，即得到一个三元式或四元式序列。 3. 符号表简介 符号表的作用：连接声明与引用的桥梁，记住每个符号的相关信息，如作用域和类型等，帮助编译的各个阶段正确有效地工作。 符号表的基本目标：有效记录信息、快速准确查找。 符号表设计的基本要求： 正确存储各类信息； 适应不同阶段的需求； 便于有效地进行查找、插入、删除和修改等操作； 空间可以动态扩充. （1）构成名字的字符串 构成名字的字符串的存储方式：直接存储—定长数据(直接将构成名字的字符串放在符号表条目中)和间接存储—变长数据(将构成名字的字符串统一存放在一个大的连续空间内，字符串与字符串之间采用特殊的分隔符隔开，符号表条目中仅存放指向该字符串首字符的指针). （2）名字的作用域 ☆ 程序语言范围的划分可以有两种划分范围的方式：并列和嵌套 ☆ 名字的作用域规则：规定一个名字在什么样的范围内应该表示什么意义. &lt;1&gt; 静态作用域规则（static-scope rule）：编译时就可以确定名字的作用域,即仅从静态读程序就可确定名字的作用域&lt;2&gt; 最近嵌套规则（most closely nested）：名字的声明在离其最近的内层起作用 （3）线性表 符号表以栈(线性表)的方式组织. 线性表上的操作：查找、插入、删除、修改 查找：从表头(栈顶)开始，遇到的第一个符合条件的名字；插入：先查找，再加入在表头（栈顶）； 关键字 = 名字＋作用域； （4）散列表 名字挂在两个链上(便于删除操作)： 散列链(hash link)： 链接所有具有相同hash值的元素，表头在表头数组中； 作用域链(scope link)：链接所有在同一作用域中的元素，表头在作用域表中. ☆ 操作：查找、插入、删除 4. 声明语句的翻译（1）变量的声明 ☆ 一个变量的声明应该由两部分来完成：类型的定义和变量的声明 类型定义：为编译器提供存储空间大小的信息 变量声明：为变量分配存储空间 组合数据的类型定义和变量声明：定义与声明在一起，定义与声明分离. 1&gt; 简单数据类型的存储空间是预先确定的，如int可以占4个字节，double可以占8个字节，char可以占1个字节等 2&gt; 组合数据类型变量的存储空间，需要编译器根据程序员提供的信息计算而定. （2） 过程 1．过程（procedure）：过程头(做什么) ＋ 过程体(怎么做)； - 函数: 有返回值的过程 - 主程序: 被操作系统调用的过程/函数2．过程的三种形式：过程定义、过程声明和过程调用。 过程定义：过程头+过程体； 过程声明：过程头；3. 左值与右值 1&gt; 直观上，出现在赋值号左边和右边的量分别称为左值和右值； 2&gt; 实质上，左值必须具有存储空间，右值可以仅是一个值，而没有存储空间. 3&gt; 形象地讲，左值是容器，右值是内容. 4. 参数传递 1&gt; 形参与实参 - 声明时的参数称为形参(parameter或formal parameter) - 引用时的参数称为实参(argument或actual parameter) 2&gt; 常见的参数传递形式：（不同的语言提供不同的形式） - 值调用（call by value）---过程内部对参数的修改，不影响作为实参的变量原来的值. - 引用调用（call by reference）--- 过程内部对形参的修改，实质上是对实参的修改. - 复写－恢复（copy-in/copy-out）--- ① 过程内对参数的修改不直接影响实参，避免了副作用; ② 返回时将形参内容恢复给实参，实现参数值的返回. - 换名调用（call by name）--- 宏调换 3&gt; 参数传递方法的本质区别： 实参是代表左值、右值、还是实参本身的正文. 5. 作用域信息的保存☆ 能够画出嵌套过程的嵌套关系树(P191 4.33),根据语法制导翻译(P193 4.35)画出分析树,写出推导步骤,构造的符号表 5. 简单算术表达式与赋值句P197 例4.36 主要是变量类型的转换 6. 数组元素的引用（1）数组元素的地址计算 注意是行主存储还是列主存储 （2）☆数组元素引用的语法制导翻译(考试热点之一) P201 例4.37 7. 布尔表达式布尔表达式的计算有两种方法：数值表示的直接计算和逻辑表示的短路计算 ☆ 布尔表达式短路计算的翻译：短路计算的控制流，真出口与假出口，真出口链与假出口链，拉链回填技术(P207 例4.41)（考试热点之一） 8. 控制语句控制语句的分类：①无条件转移、②条件转移、③循环语句、④分支语句 无条件转移(goto)\\条件转移(if、while) 条件转移的语法制导翻译：P213 例4.42 多看课件PPT，多做题练手","categories":[{"name":"XD","slug":"XD","permalink":"http://yoursite.com/categories/XD/"}],"tags":[{"name":"Compiler","slug":"Compiler","permalink":"http://yoursite.com/tags/Compiler/"}]},{"title":"高等数学18讲","slug":"高数18讲","date":"2018-08-16T05:01:38.481Z","updated":"2018-08-18T03:21:04.382Z","comments":true,"path":"2018/08/16/高数18讲/","link":"","permalink":"http://yoursite.com/2018/08/16/高数18讲/","excerpt":"【阅读时间】【阅读内容】张宇高数18讲各部分知识图谱","text":"【阅读时间】【阅读内容】张宇高数18讲各部分知识图谱 第1讲 高等数学常用基础知识点.png 第2讲 极限与连续 第5讲 中值定理 积分学汇总 第8讲 一元函数积分学的应用 第10讲 多元函数微分学 第17讲 三重积分、第一型曲线曲面积分.png","categories":[{"name":"XD","slug":"XD","permalink":"http://yoursite.com/categories/XD/"}],"tags":[{"name":"Math","slug":"Math","permalink":"http://yoursite.com/tags/Math/"},{"name":"Knowledge Map","slug":"Knowledge-Map","permalink":"http://yoursite.com/tags/Knowledge-Map/"}]},{"title":"Hello World","slug":"Hexo_Grammar","date":"2018-08-15T17:41:32.591Z","updated":"2019-01-01T14:19:41.451Z","comments":true,"path":"2018/08/16/Hexo_Grammar/","link":"","permalink":"http://yoursite.com/2018/08/16/Hexo_Grammar/","excerpt":"【阅读时间】2min 122words【阅读内容】Hexo 的基础操作——new、clean、generate、server、deploy","text":"【阅读时间】2min 122words【阅读内容】Hexo 的基础操作——new、clean、generate、server、deploy Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new [layout] \"postName\" #新建文章 More info: Writing Run server$ hexo server More info: Server Clean public files$ hexo clean Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment Trick Hexo 上传PDF 安装插件 $ npm install --save hexo-pdf 移动PDF至 source/_posts/postName (同名文件夹) $ hexo new \"postName\" markdown 使用 # 外部链接：&#123;% pdf http://7xov2f.com1.z0.glb.clouddn.com/bash_freshman.pdf %&#125;# 本地连接：&#123;% pdf ./pdf名字.pdf %&#125; HTML语法 给图片加标题 &lt;figure&gt; &lt;img src='image.jpg' alt='missing' /&gt; &lt;figcaption&gt;Caption goes here&lt;/figcaption&gt;&lt;/figure&gt; 放缩图片尺寸 &lt;img src=\"xxx.png\" style=\"zoom:80%\" /&gt; 固定图片宽度/高度 &lt;img src=\"http://xxx.png\" title=\"Logo\" width=\"100\" /&gt; 两张图片并排显示 &lt;!----左对齐-----&gt;&lt;figure class=\"half\"&gt; &lt;img src=\"http://xxx.jpg\"&gt; &lt;img src=\"http://yyy.jpg\"&gt;&lt;/figure&gt;&lt;!----居中-----&gt;&lt;center class=\"half\"&gt; &lt;img src=\"https://captainzj.github.io/2018/10/24/Lecture-Neural-Networks-Part-3/opt0.gif\" width=\"300\"/&gt; &lt;img src=\"https://captainzj.github.io/2018/10/24/Lecture-Neural-Networks-Part-3/opt.gif\" width=\"300\"/&gt;&lt;/center&gt; 三张并排显示 &lt;!----左对齐-----&gt;&lt;figure class=\"third\"&gt; &lt;img src=\"http://xxx.jpg\"&gt; &lt;img src=\"http://yyy.jpg\"&gt; &lt;img src=\"http://zzz.jpg\"&gt;&lt;/figure&gt; 注意事项 Template render error 原因：_post中某MD文件数学公式的格式有问题 解决：\\$\\$ \\$\\$ \\$\\$ \\$\\$ 并排显示图片 现象：使用相对路径(如./Lecture-Neural-Networks-Part-3/opt0.gif)部署后 无法显示 解决：使用部署后的绝对路径(如https://captainzj.github.io/2018/10/24/Lecture-Neural-Networks-Part-3/opt0.gif) 页面标题显示错序 原因：标题级别间隔使用 The { { } } is nunjucks syntax, therefore please avoid using them in posts.- [√] \\sigma = \\sqrt&#123;\\sigma^2&#125; - [x] \\sigma = \\sqrt&#123;&#123;\\sigma&#125;^2&#125; # 若不写在code block中依旧无法渲染","categories":[{"name":"Annotation","slug":"Annotation","permalink":"http://yoursite.com/categories/Annotation/"}],"tags":[]},{"title":"Indigo Grammar","slug":"Indigo Grammar","date":"2018-04-03T17:17:28.000Z","updated":"2018-08-24T15:50:53.836Z","comments":true,"path":"2018/04/04/Indigo Grammar/","link":"","permalink":"http://yoursite.com/2018/04/04/Indigo Grammar/","excerpt":"【阅读时间】10min 1032words【阅读内容】Indigo主题在page布局下的页面编辑语法","text":"【阅读时间】10min 1032words【阅读内容】Indigo主题在page布局下的页面编辑语法 Image Blockquote 当blockquote、img、pre、figure为第一级内容时，在page布局中拥有card阴影，所有标题居中展示。 Content@card{ 目前的想法是预定义一系列内容模块，通过像输入 Markdown 标记一样来简单调用。好在 Markdown 没有把所有便于输入的符号占用，最终我定义了@moduleName{ ... }这种标记格式。如果你使用过Asp.Net MVC，一定会很熟悉这种用法，没错，就是razor。 page布局中的title和subtitle对应 Markdown 中的title和description。 基本的内容容器还是card，你可以这样使用card： @card&#123;在`page`页中，建议把内容都放到`card`中。&#125; 需要注意的是：标记与内容之间必须空一行隔开。至于为何要这样，看到最后就明白了。 } Column@column-2{ @card{ 左与card标记类似，分栏的标记是这样的： @column-2&#123;@card&#123;# 左&#125;@card&#123;# 右&#125;&#125; 为了移动端观感，当屏幕宽度小于 480 时，column将换行显示。 } @card{ 右column中的每一列具有等宽、等高的特点，最多支持三栏： @column-3&#123;@card&#123;左&#125;@card&#123;中&#125;@card&#123;右&#125;&#125; } } Three columns@column-3{ @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } @card{ 话式片平九业影查类办细开被支，置军争里老5备才才目板。 且数置百容机，规的空界往，十陕志入。料解格清收权厂值动且习，识生能化路速年边，类儿2带杏性热求已。 } } Timeline@card{ 在timeline模块中，你的 5 号标题#####和六号标题######将被“征用”，用作时间线上的标记点： @timeline&#123;##### 2016@item&#123;###### 11月6日为 Card theme 添加 page layout。&#125;&#125; @item中多行内容可以换行输入，目前不允许隔行： @timeline&#123;##### 2016@item&#123;###### 11月6日第一行 第二行 /* ok */&#125;@item&#123;###### 11月6日第一行第二行 /* error */&#125;&#125; } @timeline{ 2016@item{ 11月6日为 Card theme 添加 page layout。加快绿化空间好看 } @item{ 10月31日本地化多说。 } @item{ 10月24日为 Indigo 主题创建 Card 分支。 } 2015@item{ 2月24日发布 Indigo 主题到 hexo.io。 } @item{ 1月22日创建 Indigo 主题。 } } CodeBlock// 自定义内容块实现page.content.replace(/&lt;p&gt;&#125;&lt;\\/p&gt;/g, '&lt;/div&gt;') .replace(/&lt;p&gt;@([\\w-]+)&#123;&lt;\\/p&gt;/g, function(match, $1)&#123; return '&lt;div class=\"'+ $1 +'\"&gt;' &#125;) @card{ 这里可以解释，为什么标记之间必须要隔一行了。 当你在 Markdown 中隔行输入时，会形成新的段落，而如果一个段落中的内容仅仅是我们约定的标记，就可以用很容易的用正则匹配到替换为对应的模块容器。 } End@card{ 为了解决 Hexo 自定义页面slug为空不能很好的使用多说评论这个问题，现在已经给每个自定义页面自动生成了hexo-page-path这种格式的slug。本来准备用date做格式的最后一节，测试中发现 page 中的date值为修改时间，是动态的。综合考虑使用了路径path。 以后可以根据需要添加更多模块支持。 打赏和评论默认开启，可根据需要在 Markdown 头部定义是否关闭。 }","categories":[{"name":"Annotation","slug":"Annotation","permalink":"http://yoursite.com/categories/Annotation/"}],"tags":[]}]}