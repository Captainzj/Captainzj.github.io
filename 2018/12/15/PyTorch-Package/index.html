<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>PyTorch_Docs | Go Further | Stay Hungry, Stay Foolish</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#142421">
    
    
    <meta name="keywords" content="">
    <meta name="description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch_Docs">
<meta property="og:url" content="http://yoursite.com/2018/12/15/PyTorch-Package/index.html">
<meta property="og:site_name" content="Go Further">
<meta property="og:description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/12/15/PyTorch-Package/channel.png">
<meta property="og:image" content="http://yoursite.com/2018/12/15/PyTorch-Package/SimpleCNN.png">
<meta property="og:image" content="http://yoursite.com/2018/12/15/PyTorch-Package/LinearSize.png">
<meta property="og:image" content="https://pytorch.org/docs/stable/_images/ReLU.png">
<meta property="og:updated_time" content="2018-12-24T05:01:31.838Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PyTorch_Docs">
<meta name="twitter:description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta name="twitter:image" content="http://yoursite.com/2018/12/15/PyTorch-Package/channel.png">
    
        <link rel="alternate" type="application/atom+xml" title="Go Further" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">CaptainSE</h5>
          <a href="mailto:841145636@qq.com" title="841145636@qq.com" class="mail">841145636@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/Captainzj" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">PyTorch_Docs</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">PyTorch_Docs</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-12-15T12:22:24.000Z" itemprop="datePublished" class="page-time">
  2018-12-15
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#torch"><span class="post-toc-number">1.</span> <span class="post-toc-text">torch</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Tensor"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">Tensor</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-Tensor-view"><span class="post-toc-number">1.1.1.</span> <span class="post-toc-text">torch.Tensor.view</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-cat"><span class="post-toc-number">1.1.2.</span> <span class="post-toc-text">torch.cat</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#torch-nn"><span class="post-toc-number">2.</span> <span class="post-toc-text">torch.nn</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Containers"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">Containers</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-nn-Module"><span class="post-toc-number">2.1.1.</span> <span class="post-toc-text">torch.nn.Module</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#forward-input"><span class="post-toc-number">2.1.1.1.</span> <span class="post-toc-text">forward(*input)</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-nn-Sequential"><span class="post-toc-number">2.1.2.</span> <span class="post-toc-text">torch.nn.Sequential</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Convolution-Layers"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">Convolution Layers</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-nn-Conv1d"><span class="post-toc-number">2.2.1.</span> <span class="post-toc-text">torch.nn.Conv1d</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-nn-Conv2d"><span class="post-toc-number">2.2.2.</span> <span class="post-toc-text">torch.nn.Conv2d</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-nn-Conv3d"><span class="post-toc-number">2.2.3.</span> <span class="post-toc-text">torch.nn.Conv3d</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Pooling-Layers"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">Pooling Layers</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-nn-MaxPool2d"><span class="post-toc-number">2.3.1.</span> <span class="post-toc-text">torch.nn.MaxPool2d</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-nn-AvgPool2d"><span class="post-toc-number">2.3.2.</span> <span class="post-toc-text">torch.nn.AvgPool2d</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Non-Linear-Activations"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">Non-Linear Activations</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ReLU"><span class="post-toc-number">2.4.1.</span> <span class="post-toc-text">ReLU</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Linear"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">Linear</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-nn-Linear"><span class="post-toc-number">2.5.1.</span> <span class="post-toc-text">torch.nn.Linear</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#NormalizationLayers"><span class="post-toc-number">2.6.</span> <span class="post-toc-text">NormalizationLayers</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#BatchNorm2d"><span class="post-toc-number">2.6.1.</span> <span class="post-toc-text">BatchNorm2d</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Dropout-Layers"><span class="post-toc-number">2.7.</span> <span class="post-toc-text">Dropout Layers</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-nn-Dropout"><span class="post-toc-number">2.7.1.</span> <span class="post-toc-text">torch.nn.Dropout</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#torch-nn-init"><span class="post-toc-number">2.8.</span> <span class="post-toc-text">torch.nn.init</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torch-nn-init-kaiming-uniform"><span class="post-toc-number">2.8.1.</span> <span class="post-toc-text">torch.nn.init.kaiming_uniform_</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#torch-nn-functional"><span class="post-toc-number">3.</span> <span class="post-toc-text">torch.nn.functional</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Pooling-functions"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">Pooling functions</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#adaptive-avg-pool2d"><span class="post-toc-number">3.1.1.</span> <span class="post-toc-text">adaptive_avg_pool2d</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#torchvision-transforms"><span class="post-toc-number">3.1.2.</span> <span class="post-toc-text">torchvision.transforms</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#torch-utils-model-zoo"><span class="post-toc-number">4.</span> <span class="post-toc-text">torch.utils.model_zoo</span></a></li></ol>
        </nav>
    </aside>


<article id="post-PyTorch-Package"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">PyTorch_Docs</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-12-15 20:22:24" datetime="2018-12-15T12:22:24.000Z"  itemprop="datePublished">2018-12-15</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p>
<a id="more"></a>
<h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><h4 id="torch-Tensor-view"><a href="#torch-Tensor-view" class="headerlink" title="torch.Tensor.view"></a>torch.Tensor.view</h4><p><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view" target="_blank" rel="noopener"><code>view</code>(*<em>shape</em>) → Tensor</a> Returns a new tensor with the same data as the <code>self</code> tensor but of a different <code>shape</code>.</p>
<p><strong>Example:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x.view(<span class="number">16</span>)   </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">torch.Size([<span class="number">16</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = x.view(<span class="number">-1</span>, <span class="number">8</span>)  <span class="comment"># the size -1 is inferred(推导) from other dimensions</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">8</span>])  <span class="comment"># 2 = (4*4)/8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=x.view(<span class="number">2</span>,<span class="number">-1</span>)  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">8</span>]) <span class="comment"># 8 = (4*4)/2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b=x.view(<span class="number">4</span>,<span class="number">-1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.size()</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>]) <span class="comment"># 4 = (4*4)/4</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Parameters:</strong> <strong>shape</strong> (<em>torch.Size</em> <em>or</em> <em>int…</em>) – the desired size</li>
</ul>
<h4 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h4><p><a href="https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops" target="_blank" rel="noopener">Docs</a></p>
<p><strong>Example:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)  <span class="comment"># dim:0 按列级联(排成一列)</span></span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">1</span>)  <span class="comment"># dim:1 按行级联(排成一行)</span></span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>,  <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>,  <span class="number">0.6580</span>,</span><br><span class="line">         <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>, <span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>, <span class="number">-0.1034</span>,</span><br><span class="line">         <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><h3 id="Containers"><a href="#Containers" class="headerlink" title="Containers"></a>Containers</h3><h4 id="torch-nn-Module"><a href="#torch-nn-Module" class="headerlink" title="torch.nn.Module"></a>torch.nn.Module</h4><h5 id="forward-input"><a href="#forward-input" class="headerlink" title="forward(*input)"></a>forward(*<em>input</em>)</h5><p><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward" target="_blank" rel="noopener"><code>forward</code>(*<em>input</em>)</a>  Defines the computation performed at every call.</p>
<h4 id="torch-nn-Sequential"><a href="#torch-nn-Sequential" class="headerlink" title="torch.nn.Sequential"></a>torch.nn.Sequential</h4><p><a href="https://pytorch.org/docs/stable/nn.html#sequential" target="_blank" rel="noopener">torch.nn.Sequential</a> 类是 torch.nn 中的一种序列容器，通过在容器中嵌套各种实现神经网络中具体功能相关的类，来完成对神 经网络模型 的搭建，最主要的是，<code>参数会按照我们定义好的序列自动传递下去</code>。我们可以将嵌套在容器中的各个部分看作各种不同的模块，这些模块可以自由组合。模块的加入 一般有两种方式 ， 一种是<strong>直接嵌套</strong>，另 一 种是以 <strong>orderdict</strong> 有序字典的方式进行传入，这两种方式的唯一区别是，使用后者搭建的模型的每个模块都有我们自定义的名字 ， 而前者默认使用从零开始的数字序列作为每个模块的名字。</p>
<h3 id="Convolution-Layers"><a href="#Convolution-Layers" class="headerlink" title="Convolution Layers"></a>Convolution Layers</h3><center><br>    <img src="/2018/12/15/PyTorch-Package/channel.png" width="600"><br></center>

<center><br><img src="/2018/12/15/PyTorch-Package/SimpleCNN.png" width="600"><br></center>

<center><br><img src="/2018/12/15/PyTorch-Package/LinearSize.png" width="600"><br></center>

<p><a href="https://www.youtube.com/watch?v=LgFNRIFxuUo" target="_blank" rel="noopener">PyTorch Lecture 10: Basic CNN</a></p>
<h4 id="torch-nn-Conv1d"><a href="#torch-nn-Conv1d" class="headerlink" title="torch.nn.Conv1d"></a>torch.nn.Conv1d</h4><p><a href="https://pytorch.org/docs/stable/nn.html#conv1d" target="_blank" rel="noopener">Docs</a></p>
<p><strong>Examples:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Conv1d(<span class="number">16</span>,  , <span class="number">3</span>, stride=<span class="number">2</span>) <span class="comment"># (in_channels, out_channels, kernel_size, stride)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br></pre></td></tr></table></figure>
<p><strong>Output：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">33</span>, <span class="number">24</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.weight.size()</span><br><span class="line">torch.Size([<span class="number">33</span>, <span class="number">16</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.bias.size()</span><br><span class="line">torch.Size([<span class="number">33</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>$ (N, C_{\text{in}}, L)$  —  $\text{input.size :} (20, 16, 50)$</li>
<li><p>$ (N, C_{\text{out}}, L_{\text{out}})$ — $\text{output.size :} (20,  33, 24)$ </p>
<ul>
<li>$C_{\text{out}}  \text{ : out_channels}$</li>
<li>$L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}<br>\times (\text{kernel_size} - 1) - 1}{\text{stride}} + 1\right\rfloor \Rightarrow  24 = \left\lfloor\frac{50 + 2 \times 0 - 1<br>\times (3 - 1) - 1}{2} + 1\right\rfloor$</li>
</ul>
</li>
<li><p><strong>weight</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) – the learnable weights of the module of shape $\text{(out_channels, in_channels, kernel_size)}$.</p>
</li>
<li><strong>bias</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) –the learnable bias of the module of shape $\text{(out_channels)}$.</li>
</ul>
<h4 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d"></a>torch.nn.Conv2d</h4><p><a href="https://pytorch.org/docs/stable/nn.html#conv2d" target="_blank" rel="noopener">Docs</a>  <a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#class-torchnnconv2din95channels-out95channels-kernel95size-stride1-padding0-dilation1-groups1-biastrue" target="_blank" rel="noopener">文档说明</a></p>
<p><strong>Examples:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># With square kernels and equal stride</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">2</span>) <span class="comment"># (in_channels, out_channels, kernel_size, stride)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># non-square kernels and unequal stride and with padding</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># non-square kernels and unequal stride and with padding and dilation</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Conv2d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>), stride=(<span class="number">2</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>), dilation=(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br></pre></td></tr></table></figure>
<p><strong>Output:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">100</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">33</span>, <span class="number">26</span>, <span class="number">100</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.weight.size()</span><br><span class="line">torch.Size([<span class="number">33</span>, <span class="number">16</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.bias.size()</span><br><span class="line">torch.Size([<span class="number">33</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>$(N, C_{in}, H_{in}, W_{in})$  — $\text{input.size :} (20, 16, 50, 100)$</li>
<li>$(N, C_{out}, H_{out}, W_{out})$ — $\text{output.size :} (20, 33, 26, 100)$<ul>
<li>$C_{\text{out}}  \text{ : out_channels}$</li>
<li>$H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]<br>\times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor  \Rightarrow 26  = \left\lfloor\frac{50  + 2 \times 4- 3<br>\times (3 - 1) - 1}{2} + 1\right\rfloor$</li>
<li>$W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]<br>\times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor \Rightarrow 100 =  \left\lfloor\frac{100  + 2 \times 2 - 1<br>\times (5- 1) - 1}{1} + 1\right\rfloor$</li>
</ul>
</li>
<li><strong>weight</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) – the learnable weights of the module of shape $\text{(out_channels, in_channels, kernel_size[0], kernel_size[1])}$.</li>
<li><strong>bias</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) – the learnable bias of the module of shape $\text{(out_channels)}$. </li>
</ul>
<h4 id="torch-nn-Conv3d"><a href="#torch-nn-Conv3d" class="headerlink" title="torch.nn.Conv3d"></a>torch.nn.Conv3d</h4><p><a href="https://pytorch.org/docs/stable/nn.html#conv3d" target="_blank" rel="noopener">Docs</a></p>
<p><strong>Examples:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># With square kernels and equal stride</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Conv3d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># non-square kernels and unequal stride and with padding</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Conv3d(<span class="number">16</span>, <span class="number">33</span>, (<span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">4</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br></pre></td></tr></table></figure>
<p><strong>Output:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">33</span>, <span class="number">8</span>, <span class="number">50</span>, <span class="number">99</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.weight.size()</span><br><span class="line">torch.Size([<span class="number">33</span>, <span class="number">16</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.bias.size()</span><br><span class="line">torch.Size([<span class="number">33</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>$(N, C_{in}, D_{in}, H_{in}, W_{in})$  — $\text{input.size :}20, 16, 10, 50, 100)$</li>
<li><p>$(N, C_{out}, D_{out}, H_{out}, W_{out})$  — $\text{output.size :} (20, 33, 8, 50, 99)$ </p>
<ul>
<li>$H_{out} = \left\lfloor\frac{H{in} + 2 \times \text{padding}[1] - \text{dilation}[1]<br>\times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor \<br>\Rightarrow 50 =  \left\lfloor\frac{50 + 2 \times 2 - 1<br>\times (5- 1) - 1}{1} + 1\right\rfloor$</li>
<li>$D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]<br>\times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor \\Rightarrow 8 =  \left\lfloor\frac{10 + 2 \times 4 - 1<br>\times (3 - 1) - 1}{2} + 1\right\rfloor$</li>
<li>$W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]<br>\times (\text{kernel_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor \\Rightarrow 99 = \left\lfloor\frac{100 + 2 \times 0 - 1<br>\times (2 - 1) - 1}{1} + 1\right\rfloor$</li>
</ul>
</li>
<li><p><strong>weight</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) – the learnable weights of the module of shape$\text{ (out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2])}$ </p>
</li>
<li><strong>bias</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener"><em>Tensor</em></a>) – the learnable bias of the module of shape $\text{(out_channels)}$. </li>
</ul>
<h3 id="Pooling-Layers"><a href="#Pooling-Layers" class="headerlink" title="Pooling Layers"></a>Pooling Layers</h3><h4 id="torch-nn-MaxPool2d"><a href="#torch-nn-MaxPool2d" class="headerlink" title="torch.nn.MaxPool2d"></a>torch.nn.MaxPool2d</h4><p><a href="https://pytorch.org/docs/stable/nn.html#maxpool2d" target="_blank" rel="noopener">Docs</a>  <a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#class-torchnnmaxpool2dkernel95size-stridenone-padding0-dilation1-return95indicesfalse-ceil95modefalse" target="_blank" rel="noopener">文档说明</a></p>
<p>Applies a 2D max pooling over an input signal composed of several input planes.</p>
<p>In the simplest case, the output value of the layer with input size$(N, C, H, W)​$, output $(N, C, H_{out}, W_{out})​$ and <code>kernel_size</code> $(kH, kW)​$can be precisely described as:</p>
<p>$\begin{aligned}<br>out(N_i, C_j, h, w) ={} &amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \text{input}(N_i, C_j, \text{stride[0]} \times h + m,<br>\text{stride[1]} \times w + n)<br>\end{aligned}$</p>
<p><strong>Examples:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># pool of square window of size=3, stride=2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># pool of non-square window</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.MaxPool2d((<span class="number">3</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">32</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br></pre></td></tr></table></figure>
<p><strong>Output:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">32</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">16</span>, <span class="number">24</span>, <span class="number">31</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li><p>$(N, C, H_{in}, W_{in})$ — $\text{input.size:}(20, 16, 50, 32) $</p>
</li>
<li><p>$(N, C, H_{out}, W_{out})$ — $\text{output.size: } (20, 16, 24, 31)$</p>
</li>
<li><p>$H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]}<br>\times (\text{kernel_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor \\Rightarrow 24 = \left\lfloor\frac{50 + 2 \times 0 - 1<br>\times (3 - 1) - 1}{2} + 1\right\rfloor $</p>
</li>
<li>$W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]}<br>\times (\text{kernel_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor \ \Rightarrow 31 = \left\lfloor\frac{32 + 2 \times 0 - 1<br>\times (2 - 1) - 1}{1} + 1\right\rfloor$</li>
</ul>
<h4 id="torch-nn-AvgPool2d"><a href="#torch-nn-AvgPool2d" class="headerlink" title="torch.nn.AvgPool2d"></a>torch.nn.AvgPool2d</h4><p><a href="https://pytorch.org/docs/stable/nn.html#avgpool2d" target="_blank" rel="noopener">Docs</a></p>
<p><strong>Examples:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># pool of square window of size=3, stride=2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.AvgPool2d(<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># pool of non-square window</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.AvgPool2d((<span class="number">3</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">32</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br></pre></td></tr></table></figure>
<p><strong>Output:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>, <span class="number">32</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">16</span>, <span class="number">24</span>, <span class="number">31</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>$(N, C, H_{in}, W_{in})$ — $\text{input.size: } (20, 16, 50, 32)$</li>
<li>$(N, C, H_{out}, W_{out})$ — $\text{output.size: } (20, 16, 24, 31)$</li>
<li>$H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] -<br>\text{kernel_size}[0]}{\text{stride}[0]} + 1\right\rfloor \Rightarrow 24 = \left\lfloor\frac{50  + 2 \times 0 -<br>3}{2} + 1\right\rfloor$</li>
<li><p>$W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] -<br>\text{kernel_size}[1]}{\text{stride}[1]} + 1\right\rfloor \Rightarrow 31 = \left\lfloor\frac{32  + 2 \times 0 -<br>2}{1} + 1\right\rfloor$</p>
</li>
<li><p><strong><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#dropout-layers" target="_blank" rel="noopener">torch.nn.Dropout</a></strong> torch.nn.Dropout 类用于防止卷积神经网络在训练的过程中发生过拟合 ， 其工作原理简单来说就是在模型训练的过程中，以一定的随机概率将卷积神经网络模型的部分参数归零(“丢弃”)， 以达到减少相邻两层神经连接的目的。</p>
</li>
</ul>
<h3 id="Non-Linear-Activations"><a href="#Non-Linear-Activations" class="headerlink" title="Non-Linear Activations"></a>Non-Linear Activations</h3><h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p><a href="https://pytorch.org/docs/stable/nn.html#relu" target="_blank" rel="noopener">Docs</a></p>
<center><br>    <img src="https://pytorch.org/docs/stable/_images/ReLU.png" width="400"><br></center>

<p><strong>Example:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.ReLU()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br></pre></td></tr></table></figure>
<p><strong>Output:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input</span><br><span class="line">tensor([<span class="number">0.7526</span>, <span class="number">0.9017</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">0.7526</span>, <span class="number">0.9017</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input</span><br><span class="line">tensor([<span class="number">-0.5070</span>, <span class="number">0.4540</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">0.0000</span>, <span class="number">0.4540</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>Input:$(N, <em>)$ where </em> means, any number of additional dimensions</li>
<li>Output: $(N, *)$, same shape as the input</li>
</ul>
<h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><h4 id="torch-nn-Linear"><a href="#torch-nn-Linear" class="headerlink" title="torch.nn.Linear"></a>torch.nn.Linear</h4><p><a href="https://pytorch.org/docs/stable/nn.html#linear" target="_blank" rel="noopener">Docs</a></p>
<p>Applies a linear transformation to the incoming data: $y = xA^T + b$</p>
<p><strong>Examples:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">128</span>, <span class="number">20</span>)  <span class="comment"># (N,∗,in_features)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(output.size())</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">30</span>])   <span class="comment"># (N,∗,out_features)</span></span><br></pre></td></tr></table></figure>
<p><strong>Output:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input</span><br><span class="line">tensor([[<span class="number">-0.8833</span>,  <span class="number">0.1130</span>,  <span class="number">0.0446</span>,  ..., <span class="number">-0.2786</span>,  <span class="number">2.0762</span>,  <span class="number">0.6139</span>],</span><br><span class="line">        [<span class="number">-0.9236</span>,  <span class="number">3.1720</span>, <span class="number">-0.9857</span>,  ...,  <span class="number">0.1017</span>, <span class="number">-0.4042</span>, <span class="number">-0.1072</span>],</span><br><span class="line">        [ <span class="number">0.0153</span>,  <span class="number">0.8800</span>, <span class="number">-0.6031</span>,  ..., <span class="number">-0.2836</span>,  <span class="number">0.7584</span>, <span class="number">-2.3324</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [ <span class="number">0.9628</span>, <span class="number">-0.3177</span>, <span class="number">-0.7577</span>,  ...,  <span class="number">0.2819</span>,  <span class="number">0.9684</span>, <span class="number">-1.8474</span>],</span><br><span class="line">        [ <span class="number">0.3380</span>,  <span class="number">1.0946</span>, <span class="number">-1.3399</span>,  ..., <span class="number">-0.0043</span>, <span class="number">-0.9811</span>,  <span class="number">0.3067</span>],</span><br><span class="line">        [ <span class="number">0.6834</span>,  <span class="number">0.5804</span>, <span class="number">-0.8192</span>,  ...,  <span class="number">1.2167</span>,  <span class="number">1.3583</span>, <span class="number">-1.5123</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([[ <span class="number">1.0747</span>,  <span class="number">1.2864</span>,  <span class="number">0.6512</span>,  ...,  <span class="number">0.1053</span>, <span class="number">-0.0487</span>, <span class="number">-0.1705</span>],</span><br><span class="line">        [ <span class="number">0.2707</span>, <span class="number">-0.7018</span>, <span class="number">-0.4553</span>,  ..., <span class="number">-0.2100</span>, <span class="number">-0.3003</span>,  <span class="number">1.0038</span>],</span><br><span class="line">        [ <span class="number">0.1943</span>, <span class="number">-0.3070</span>, <span class="number">-0.3651</span>,  ...,  <span class="number">1.1940</span>, <span class="number">-0.2991</span>,  <span class="number">0.0455</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [ <span class="number">0.4118</span>, <span class="number">-0.3984</span>,  <span class="number">0.2089</span>,  ...,  <span class="number">0.7984</span>, <span class="number">-0.6598</span>,  <span class="number">0.2150</span>],</span><br><span class="line">        [<span class="number">-0.1639</span>, <span class="number">-1.5081</span>, <span class="number">-0.4011</span>,  ...,  <span class="number">0.9673</span>,  <span class="number">0.3524</span>,  <span class="number">0.0993</span>],</span><br><span class="line">        [ <span class="number">0.5961</span>, <span class="number">-0.4150</span>, <span class="number">-0.1207</span>,  ...,  <span class="number">0.3189</span>, <span class="number">-0.0829</span>,  <span class="number">0.5195</span>]],</span><br><span class="line">       grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.weight.size()</span><br><span class="line">torch.Size([<span class="number">30</span>, <span class="number">20</span>])  <span class="comment"># (out_features,in_features)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m.bias.size()</span><br><span class="line">torch.Size([<span class="number">30</span>])   <span class="comment">#  (out_features)</span></span><br></pre></td></tr></table></figure>
<h3 id="NormalizationLayers"><a href="#NormalizationLayers" class="headerlink" title="NormalizationLayers"></a>NormalizationLayers</h3><p><a href="https://www.youtube.com/watch?v=NGO0oxdz-zs" target="_blank" rel="noopener">Batch Normalization 批标准化 (PyTorch tutorial 神经网络 教学)</a> </p>
<p><a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/5-04-A-batch-normalization/" target="_blank" rel="noopener">什么是批标准化 (Batch Normalization)_Morvan</a>让数值保持在激活函数的有效区间，可避免梯度消失。Batch Normalization (BN) 就被添加在每一个全连接和激励函数之间.</p>
<p><a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">深度学习中 Batch Normalization为什么效果好？</a> </p>
<h4 id="BatchNorm2d"><a href="#BatchNorm2d" class="headerlink" title="BatchNorm2d"></a>BatchNorm2d</h4><p><a href="https://pytorch.org/docs/stable/nn.html#batchnorm2d" target="_blank" rel="noopener">Docs</a></p>
<p><strong>Examples:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># With Learnable Parameters</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.BatchNorm2d(<span class="number">100</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Without Learnable Parameters</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.BatchNorm2d(<span class="number">100</span>, affine=<span class="keyword">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">20</span>, <span class="number">100</span>, <span class="number">35</span>, <span class="number">45</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br></pre></td></tr></table></figure>
<p><strong>Output:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">100</span>, <span class="number">35</span>, <span class="number">45</span>])  <span class="comment">#  (same shape as input）</span></span><br></pre></td></tr></table></figure>
<h3 id="Dropout-Layers"><a href="#Dropout-Layers" class="headerlink" title="Dropout Layers"></a>Dropout Layers</h3><h4 id="torch-nn-Dropout"><a href="#torch-nn-Dropout" class="headerlink" title="torch.nn.Dropout"></a>torch.nn.Dropout</h4><p><a href="https://pytorch.org/docs/stable/nn.html#dropout" target="_blank" rel="noopener">Docs</a></p>
<p><strong>Examples:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">20</span>, <span class="number">16</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br></pre></td></tr></table></figure>
<p><strong>Output:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">16</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">16</span>])  <span class="comment">#  Output is of the same shape as input</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input</span><br><span class="line">tensor([[ <span class="number">1.7224</span>,  <span class="number">1.3201</span>, <span class="number">-0.8480</span>],</span><br><span class="line">        [ <span class="number">1.8960</span>, <span class="number">-0.1245</span>,  <span class="number">0.6991</span>],</span><br><span class="line">        [ <span class="number">1.1756</span>,  <span class="number">0.2378</span>,  <span class="number">1.4059</span>],</span><br><span class="line">        [ <span class="number">0.2427</span>,  <span class="number">0.2278</span>, <span class="number">-0.7612</span>],</span><br><span class="line">        [<span class="number">-0.8882</span>,  <span class="number">0.2088</span>,  <span class="number">1.5004</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([[ <span class="number">2.1530</span>,  <span class="number">1.6501</span>, <span class="number">-1.0601</span>],</span><br><span class="line">        [ <span class="number">2.3700</span>, <span class="number">-0.1556</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">1.4694</span>,  <span class="number">0.2972</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.3034</span>,  <span class="number">0.0000</span>, <span class="number">-0.9515</span>],</span><br><span class="line">        [<span class="number">-0.0000</span>,  <span class="number">0.2610</span>,  <span class="number">1.8755</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="torch-nn-init"><a href="#torch-nn-init" class="headerlink" title="torch.nn.init"></a>torch.nn.init</h3><h4 id="torch-nn-init-kaiming-uniform"><a href="#torch-nn-init-kaiming-uniform" class="headerlink" title="torch.nn.init.kaiming_uniform_"></a>torch.nn.init.kaiming_uniform_</h4><p><a href="https://pytorch.org/docs/stable/nn.html?highlight=nn.init#torch.nn.init.kaiming_normal_" target="_blank" rel="noopener">Docs</a></p>
<h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><h3 id="Pooling-functions"><a href="#Pooling-functions" class="headerlink" title="Pooling functions"></a>Pooling functions</h3><h4 id="adaptive-avg-pool2d"><a href="#adaptive-avg-pool2d" class="headerlink" title="adaptive_avg_pool2d"></a>adaptive_avg_pool2d</h4><p><a href="https://pytorch.org/docs/stable/nn.html#adaptive-avg-pool2d" target="_blank" rel="noopener">Docs</a></p>
<p><strong>Examples:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># target output size of 5x7</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.AdaptiveAvgPool2d((<span class="number">5</span>,<span class="number">7</span>))  <span class="comment"># output_size – (H, W) </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># target output size of 7x7 (square)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.AdaptiveAvgPool2d(<span class="number">7</span>)  <span class="comment"># a single H for a square image H x H</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">10</span>, <span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># target output size of 10x7</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.AdaptiveMaxPool2d((<span class="keyword">None</span>, <span class="number">7</span>))  <span class="comment"># None, which means the size will be the same as that of the input.</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">10</span>, <span class="number">9</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">10</span>, <span class="number">7</span>])</span><br></pre></td></tr></table></figure>
<h4 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h4><ul>
<li><p><strong>torchvision.transforms.Resize:</strong>用于对载入的图片数据按我们需求的大小进行缩放。 传递给这个类的参数可以是一个整型数据，也可以是一个类似于$ (h,w)$的序列， 其中 ， h 代表高度， w 代表宽度，但是如果使用的是 一 个整型数据，那么表示缩放的宽度和高度都是这个整型数据的值 。 </p>
</li>
<li><p><strong>torchvision.transforms.Scale</strong>: 用于对载入的图片数据按我 们 需求的大小进 行缩 放，用法和 torchvision.transforms.Resize类似。 </p>
</li>
<li><p><strong>torchvision.transforms.CenterCrop:</strong>用 于对载入的图片以图片中心为参考点 ， 按我们需要的大小进行裁剪 。传递给这个类 的参数可以是一个整型数据 ，也可以 是一个类似 于( h,w)的序列 。 </p>
</li>
<li><p><strong>torchvision.transforms.RandomCrop:</strong> 用于对载入的图片按我们需要的大小进行随机裁剪。传递给这个类的参数可以是一个整型数据，也可以是一个类似于$ (h,w)$的序列。 </p>
</li>
<li><p><strong>torchvision.transforms.RandomHorizontaIFlip:</strong> 用于对载入的图片按随机概率进行水平翻转。我们可以通过传递给这个类的参数自定义随机概率 ，如果没有定义 ，则使用 默认的概率值 0.5。</p>
</li>
<li><p><strong>torchvision.transforms.RandomVerticalFlip:</strong> 用于对载入的图片按随机概率进行垂直翻转。 我们可以通过传递给这个类的参数自定义随机概率 ，如果没有定义 ，则 使用默 认的概率值 0.5。 </p>
</li>
<li><p><strong>torchvision.transforms.ToTensor:</strong> 用于对载入的图片数据进行类型转换 ， 将之前构成 PIL 图片的数据转换成 Tensor数据类型的变量 ，让 PyTorch 能够对其进行计算和处理。 </p>
</li>
<li><p><strong>torchvision.transforms.ToPILlmage:</strong> 用于将 Tensor变量的数据转换成 PIL 图片数据， 主要是为了方便图片内容的显示。</p>
</li>
</ul>
<h2 id="torch-utils-model-zoo"><a href="#torch-utils-model-zoo" class="headerlink" title="torch.utils.model_zoo"></a>torch.utils.model_zoo</h2><p><a href="https://pytorch.org/docs/stable/model_zoo.html#module-torch.utils.model_zoo" target="_blank" rel="noopener">torch.utils.model_zoo.load_url</a></p>
<p><strong>Example:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>state_dict = torch.utils.model_zoo.load_url(<span class="string">'https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth'</span>)</span><br></pre></td></tr></table></figure>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2018-12-24T05:01:31.838Z" itemprop="dateUpdated">2018-12-24 13:01:31</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="CaptainSE">
            CaptainSE
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            

            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2018/12/15/PyTorch-Package/&title=《PyTorch_Docs》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2018/12/15/PyTorch-Package/&title=《PyTorch_Docs》 — Go Further&source=【阅读时间】XXX min XXX words【阅读内容】……" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2018/12/15/PyTorch-Package/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《PyTorch_Docs》 — Go Further&url=http://yoursite.com/2018/12/15/PyTorch-Package/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2018/12/15/PyTorch-Package/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2018/12/17/深度学习之PyTorch实战计算机视觉/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">深度学习之PyTorch实战计算机视觉</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/12/13/GraduationDesign/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">GraduationDesign</h4>
      </a>
    </div>
  
</nav>



    

















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢老板~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>CaptainSE &copy; 2015 - 2018</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2018/12/15/PyTorch-Package/&title=《PyTorch_Docs》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2018/12/15/PyTorch-Package/&title=《PyTorch_Docs》 — Go Further&source=【阅读时间】XXX min XXX words【阅读内容】……" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2018/12/15/PyTorch-Package/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《PyTorch_Docs》 — Go Further&url=http://yoursite.com/2018/12/15/PyTorch-Package/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2018/12/15/PyTorch-Package/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACM0lEQVR42u3aS07DMBQF0O5/00ViBIKk9z4HpDrHoyhNWh8jmffx4xGP5+c4uv/105938vtHz1w2MDAw3pbxPB0/GcnPzJ4/X5QXVAwMjBsw2qnk22KyQO2SYWBgYBx93TmmvU7uYGBgYKxsuElCuxL8YWBgYMyS2GSis2AxKd5dlotjYGC8IaNtDPzn9R/2NzAwMN6E8RyNPAU9T4ZnzYBf3sLAwNiakSSfecraspMni40YAwNja0Y+rZV25lUtAQwMjLsxkuOkOTX/zpWmKQYGxt0YycvJndlUZkHk4Z8BAwNja8ZKENkGlLNWaHsUAwMDYz/G+cezlcinO9vco8YABgbGFow2HW03wSQRzQ97HX4zBgbGDRh5sWy90rWCfJGFY2BgbM1o08j8SEQbdOat0MP/GxgYGNsxkv0qb0m2DYNZoFn0YzEwMDZlzJqOl22as9AQAwPjZoxZ2JdvzXn6OuysYmBgbMdIDmPlae3KQbE29f12jYGBsTWjPpdR7nht+Lj0DAYGxi0ZyYbYluraiK4IKDEwMG7MaMtn+buzsLKeBAYGxhaM9ofbRHTleGvSOsXAwNib8SzHrAHZFunqgxcYGBhbM9bbnG25f+WtC5qgGBgYb8vIN9m23HZtjvkiScbAwLgBIy91rYeJs8DuBQwDAwNjoYjfluHa0h4GBgZGfvQhL8ytNAzqngYGBsZGjLYZMHs+X6bhAQsMDIxNGW1jYLaNtmFfvogYGBhbMz4Ant8sugHmIgsAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            clearTimeout(titleTime);
        } else {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
