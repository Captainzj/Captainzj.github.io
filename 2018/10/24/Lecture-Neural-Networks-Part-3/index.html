<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>Lecture_Neural Networks Part 3 | Go Further | Stay Hungry, Stay Foolish</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#142421">
    
    
    <meta name="keywords" content="CS231n">
    <meta name="description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta name="keywords" content="CS231n">
<meta property="og:type" content="article">
<meta property="og:title" content="Lecture_Neural Networks Part 3">
<meta property="og:url" content="http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/index.html">
<meta property="og:site_name" content="Go Further">
<meta property="og:description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/check_learning_loss.jpg">
<meta property="og:image" content="http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/check_learning_accuracy.jpg">
<meta property="og:image" content="http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/第一层可视化.png">
<meta property="og:image" content="http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/Nesterov动量.jpg">
<meta property="og:image" content="http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/随机搜索优于网格搜索.jpg">
<meta property="og:updated_time" content="2018-11-21T05:48:52.638Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lecture_Neural Networks Part 3">
<meta name="twitter:description" content="【阅读时间】XXX min XXX words【阅读内容】……">
<meta name="twitter:image" content="http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/check_learning_loss.jpg">
    
        <link rel="alternate" type="application/atom+xml" title="Go Further" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">CaptainSE</h5>
          <a href="mailto:841145636@qq.com" title="841145636@qq.com" class="mail">841145636@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/Captainzj" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Lecture_Neural Networks Part 3</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">Lecture_Neural Networks Part 3</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-10-24T07:07:12.000Z" itemprop="datePublished" class="page-time">
  2018-10-24
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/MachineLearning/">MachineLearning</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#梯度检查"><span class="post-toc-number">1.</span> <span class="post-toc-text">梯度检查</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#合理性（Sanity）检查"><span class="post-toc-number">2.</span> <span class="post-toc-text">合理性（Sanity）检查</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#检查学习过程"><span class="post-toc-number">3.</span> <span class="post-toc-text">检查学习过程</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#损失函数"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">损失函数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#训练集与验证集准确率"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">训练集与验证集准确率</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#权重：更新比例"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">权重：更新比例</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#每层的激活数据与梯度分布"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">每层的激活数据与梯度分布</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#第一层可视化"><span class="post-toc-number">3.4.1.</span> <span class="post-toc-text">第一层可视化</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#参数更新"><span class="post-toc-number">4.</span> <span class="post-toc-text">参数更新</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#一阶（随机梯度下降）方法，动量方法，Nesterov动量方法"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">一阶（随机梯度下降）方法，动量方法，Nesterov动量方法</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#学习率退火"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">学习率退火</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#二阶方法"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">二阶方法</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#逐参数适应学习率方法（Adagrad，RMSProp）"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">逐参数适应学习率方法（Adagrad，RMSProp）</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#超参数调优"><span class="post-toc-number">5.</span> <span class="post-toc-text">超参数调优</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#评价"><span class="post-toc-number">6.</span> <span class="post-toc-text">评价</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#模型集成"><span class="post-toc-number">6.1.</span> <span class="post-toc-text">模型集成</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#总结"><span class="post-toc-number">7.</span> <span class="post-toc-text">总结</span></a></li></ol>
        </nav>
    </aside>


<article id="post-Lecture-Neural-Networks-Part-3"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">Lecture_Neural Networks Part 3</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-10-24 15:07:12" datetime="2018-10-24T07:07:12.000Z"  itemprop="datePublished">2018-10-24</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/MachineLearning/">MachineLearning</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p>
<a id="more"></a>
<p>在前面章节中，我们讨论了神经网络的静态部分：如何创建网络的连接、数据和损失函数。本节将致力于讲解神经网络的动态部分，即神经网络<code>学习参数</code>和搜索<code>最优超参数</code>的过程。</p>
<h2 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h2><p>理论上将进行梯度检查很简单，就是简单地把解析梯度和数值计算梯度进行比较.</p>
<p><strong>使用中心化公式</strong>  在使用有限差值近似来计算数值梯度的时候，$\frac{df(x)}{dx}=\frac{f(x+h)-f(x-h)}{2h}$(use instead) 效果较好</p>
<ul>
<li><p><strong>使用相对误差来比较</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- 相对误差&gt;1e-2：通常就意味着梯度可能出错。</span><br><span class="line">- 1e-2&gt;相对误差&gt;1e-4：要对这个值感到不舒服才行。</span><br><span class="line">- 1e-4&gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高。</span><br><span class="line">- 1e-7或者更小：好结果，可以高兴一把了。</span><br><span class="line"></span><br><span class="line">要知道的是网络的深度越深，相对误差就越高。所以如果你是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。相反，如果一个可微函数的相对误差值是1e-2，那么通常说明梯度实现不正确。</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>使用双精度</strong></p>
<p>一个常见的错误是使用单精度浮点数来进行梯度检查。这样会导致即使梯度实现正确，相对误差值也会很高（比如1e-2）。在我的经验而言，出现过使用单精度浮点数时相对误差为1e-2，换成双精度浮点数时就降低为1e-8的情况。</p>
</li>
<li><p><strong>目标函数的不可导点（kinks）</strong></p>
<p>不可导点是指目标函数不可导的部分，由ReLU（$max(0,x)$）等函数，或SVM损失，Maxout神经元等引入。考虑当$x=-1e6$时，对ReLU函数进行梯度检查。因为$x&lt;0$，所以解析梯度在该点的梯度为0。然而，在这里数值梯度会突然计算出一个非零的梯度值，因为$f(x+h)$可能越过了不可导点(例如：如果$h&gt;1e-6$)，导致了一个非零的结果。实际上这种情况很常见。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意，在计算损失的过程中是可以知道不可导点有没有被越过的。在具有max(x,y)形式的函数中持续跟踪所有“赢家”的身份，就可以实现这一点。其实就是看在前向传播时，到底x和y谁更大。如果在计算f(x+h)和f(x-h)的时候，至少有一个“赢家”的身份变了，那就说明不可导点被越过了，数值梯度会不准确。</span><br><span class="line"></span><br><span class="line">解决上面的不可导点问题的一个办法是使用更少的数据点。如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>不要让正则化吞没数据。</strong></p>
<p>推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。</p>
</li>
<li><p><strong>记得关闭随机失活（dropout）和数据扩张（augmentation）</strong></p>
<p>在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。</p>
</li>
<li><p><strong>检查少量的维度。</strong></p>
<p>在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度然后假设其他维度是正确的。<strong>注意</strong>：确认在所有不同的参数中都抽取一部分来梯度检查。</p>
</li>
</ul>
<h2 id="合理性（Sanity）检查"><a href="#合理性（Sanity）检查" class="headerlink" title="合理性（Sanity）检查"></a>合理性（Sanity）检查</h2><ul>
<li><p><strong>寻找特定情况的正确损失值</strong></p>
<p>在使用小参数进行初始化时，<code>确保得到的损失值与期望一致</code>。最好先单独检查数据损失（让正则化强度为0）。例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。</p>
</li>
<li><p><strong>提高正则化强度时导致损失值变大</strong></p>
</li>
<li><p><strong>对小数据子集过拟合</strong></p>
<p>在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后<code>确保能到达0的损失值</code>。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。</p>
</li>
</ul>
<h2 id="检查学习过程"><a href="#检查学习过程" class="headerlink" title="检查学习过程"></a>检查学习过程</h2><p>在训练神经网络的时候，应该跟踪多个重要数值。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道<code>如何修改超参数以获得更高效的学习过程</code>。</p>
<p>在下面的图表中，x轴通常都是表示<strong>周期（epochs）</strong>单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般<code>更倾向跟踪周期</code>，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>训练期间第一个要跟踪的数值就是损失值，它在前向传播时对每个独立的批数据进行计算。</p>
<hr>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2018/10/24/Lecture-Neural-Networks-Part-3/check_learning_loss.jpg" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p><strong>左图</strong>展示了不同的学习率的效果。过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。<strong>右图</strong>显示了一个典型的随时间变化的损失函数值，在CIFAR-10数据集上面训练了一个小的网络，这个损失函数值曲线看起来比较合理（虽然可能学习率有点小，但是很难说），而且指出了批数据的数量可能有点太小（因为损失值的噪音很大）。</p>
<hr>
<h3 id="训练集与验证集准确率"><a href="#训练集与验证集准确率" class="headerlink" title="训练集与验证集准确率"></a>训练集与验证集准确率</h3><p>在训练分类器的时候，需要跟踪的第二重要的数值是验证集和训练集的准确率。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2018/10/24/Lecture-Neural-Networks-Part-3/check_learning_accuracy.jpg" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在训练集准确率和验证集准确率中间的空隙指明了<code>模型过拟合</code>的程度。在图中，蓝色的验证集曲线显示相较于训练集，验证集的准确率低了很多，这就说明模型有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。另一种可能就是验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。</p>
<h3 id="权重：更新比例"><a href="#权重：更新比例" class="headerlink" title="权重：更新比例"></a>权重：更新比例</h3><p>最后一个应该跟踪的量是权重中<code>更新</code>值的数量和全部值的数量之间的比例。需要对每个参数集的更新比例进行单独的计算和跟踪。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设参数向量为W，其梯度向量为dW</span></span><br><span class="line">param_scale = np.linalg.norm(W.ravel())</span><br><span class="line">update = -learning_rate*dW <span class="comment"># 简单SGD更新</span></span><br><span class="line">update_scale = np.linalg.norm(update.ravel())</span><br><span class="line">W += update <span class="comment"># 实际更新</span></span><br><span class="line"><span class="keyword">print</span> update_scale / param_scale <span class="comment"># 要得到1e-3左右</span></span><br></pre></td></tr></table></figure>
<h3 id="每层的激活数据与梯度分布"><a href="#每层的激活数据与梯度分布" class="headerlink" title="每层的激活数据与梯度分布"></a>每层的激活数据与梯度分布</h3><h4 id="第一层可视化"><a href="#第一层可视化" class="headerlink" title="第一层可视化"></a>第一层可视化</h4><p>如果数据是图像像素数据，那么把第一层特征可视化会有帮助：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2018/10/24/Lecture-Neural-Networks-Part-3/第一层可视化.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>将神经网络第一层的权重可视化的例子。<br>左图中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低.<br>右图的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好.</p>
<h2 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h2><p>一旦能使用反向传播计算解析梯度，梯度就能被用来进行参数更新了。进行参数更新有好几种方法，接下来都会进行讨论。</p>
<h3 id="一阶（随机梯度下降）方法，动量方法，Nesterov动量方法"><a href="#一阶（随机梯度下降）方法，动量方法，Nesterov动量方法" class="headerlink" title="一阶（随机梯度下降）方法，动量方法，Nesterov动量方法"></a>一阶（随机梯度下降）方法，动量方法，Nesterov动量方法</h3><ul>
<li><p><strong>普通更新</strong>. 最简单的更新形式是沿着负梯度方向改变参数（因为梯度指向的是上升方向，但是我们通常希望最小化损失函数）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通更新</span></span><br><span class="line">x += - learning_rate * dx</span><br></pre></td></tr></table></figure>
<p>其中learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展。</p>
</li>
<li><p><strong>动量（Momentum）更新</strong></p>
<p>这样最优化过程可以看做是模拟参数向量（即质点）在地形上滚动的过程。在普通版本中，梯度直接影响位置。而在这个版本的更新中，物理观点建议梯度只是影响速度，然后速度再影响位置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动量更新</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># 与速度融合</span></span><br><span class="line">x += v <span class="comment"># 与位置融合</span></span><br></pre></td></tr></table></figure>
<p>在这里引入了一个初始化为0的变量<strong>v</strong>和一个超参数<strong>mu</strong>。说得不恰当一点，这个变量（mu）在最优化的过程中被看做<em>动量</em>（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。通过交叉验证，这个参数通常设为[0.5,0.9,0.95,0.99]中的一个。和学习率随着时间退火（下文有讨论）类似，动量随时间变化的设置有时能略微改善最优化的效果，其中动量在学习过程的后阶段会上升。一个<code>典型的设置</code>是刚开始将动量设为0.5而在后面的多个周期（epoch）中慢慢提升到0.99。</p>
<blockquote>
<p>通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。</p>
</blockquote>
</li>
<li><p><strong>Nesterov动量</strong>与普通动量有些许不同，最近变得比较流行。在理论上对于凸函数它能得到更好的收敛，在实践中也确实比标准动量表现更好一些。</p>
<p>Nesterov动量的核心思路是，当参数向量位于某个位置<strong>x</strong>时，观察上面的动量更新公式可以发现，动量部分（忽视带梯度的第二个部分）会通过<strong>mu * v</strong>稍微改变参数向量。因此，如果要计算梯度，那么可以将未来的近似位置<strong>x + mu * v</strong>看做是“向前看”，这个点在我们一会儿要停止的位置附近。因此，相比“旧”位置<strong>x</strong>的梯度，<code>计算x + mu \* v的梯度</code>会更有意义。</p>
<p><img src="/2018/10/24/Lecture-Neural-Networks-Part-3/Nesterov动量.jpg" alt="Nesterov动量"></p>
<p>既然我们知道动量将会把我们带到绿色箭头指向的点，我们就不要在原点（红色点）那里计算梯度了。使用Nesterov动量，我们就在这个“向前看”的地方计算梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_ahead = x + mu * v	<span class="comment"># 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)</span></span><br><span class="line">v = mu * v - learning_rate * dx_ahead</span><br><span class="line">x += v</span><br></pre></td></tr></table></figure>
<p>然而在实践中，人们更喜欢和普通SGD或上面的动量方法一样简单的表达式。通过对<strong>x_ahead = x + mu * v</strong>使用变量变换进行改写是可以做到的，然后用<strong>x_ahead</strong>而不是<strong>x</strong>来表示上面的更新。也就是说，<code>实际存储的参数向量总是向前一步的那个版本</code>。<strong>x_ahead</strong>的公式（将其重新命名为<strong>x</strong>）就变成了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_prev = v <span class="comment"># 存储备份</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># 速度更新保持不变</span></span><br><span class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v <span class="comment"># 位置更新变了形式</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="学习率退火"><a href="#学习率退火" class="headerlink" title="学习率退火"></a>学习率退火</h3><p>在训练深度网络的时候，让学习率随着时间退火通常是有帮助的。通常，实现学习率退火有3种方式：</p>
<ul>
<li><strong>随步数衰减</strong>：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。</li>
<li><strong>指数衰减</strong>。数学公式是$\alpha=\alpha_0e^{-kt}$，其中$\alpha_0,k$是超参数，t是迭代次数（也可以使用周期作为单位）。</li>
<li><strong>1/t衰减</strong>的数学公式是$\alpha=\alpha_0/(1+kt)$，其中$\alpha_0,k$是超参数，t是迭代次数。</li>
</ul>
<p>在实践中，我们发现<code>随步数衰减的随机失活（dropout）更受欢迎</code>，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。</p>
<h3 id="二阶方法"><a href="#二阶方法" class="headerlink" title="二阶方法"></a>二阶方法</h3><p>在深度网络背景下，第二类常用的最优化方法是基于牛顿法的，其迭代如下：$\displaystyle x\leftarrow x-[Hf(x)]^{-1}\nabla f(x)$</p>
<h3 id="逐参数适应学习率方法（Adagrad，RMSProp）"><a href="#逐参数适应学习率方法（Adagrad，RMSProp）" class="headerlink" title="逐参数适应学习率方法（Adagrad，RMSProp）"></a>逐参数适应学习率方法（Adagrad，RMSProp）</h3><p>前面讨论的所有方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。学习率调参是很耗费计算资源的过程，所以很多工作投入到发明能够<code>适应性地对学习率调参</code>的方法，<code>甚至</code>是逐个参数适应学习率调参。</p>
<ul>
<li><p><strong>Adagrad</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设有梯度和参数向量x</span></span><br><span class="line">cache += dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>RMSprop</strong></p>
<p>用一种很简单的方式修改了Adagrad方法，让它不那么激进，单调地降低了学习率。具体说来，就是它使用了一个梯度平方的滑动平均：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cache =  decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Adam</strong></p>
<p>看起来像是RMSProp的动量版</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = beta1*m + (<span class="number">1</span>-beta1)*dx</span><br><span class="line">v = beta2*v + (<span class="number">1</span>-beta2)*(dx**<span class="number">2</span>)</span><br><span class="line">x += - learning_rate * m / (np.sqrt(v) + eps)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<center class="half"><br>    &lt;img src=./Lecture-Neural-Networks-Part-3/opt0.gif width=”300”/&gt;<br>    &lt;img src=/Users/Captain/Documents/Captainzj.github.io/source/_posts/Lecture-Neural-Networks-Part-3/opt.gif width=”300”/&gt;<br></center>

<p>​    上面的动画可以帮助你理解学习的动态过程。</p>
<ul>
<li><p><strong>左边</strong>是一个损失函数的等高线图，上面跑的是不同的最优化算法。</p>
<ul>
<li>基于动量的方法出现了射偏了的情况，使得最优化过程看起来像是一个球滚下山的样子。</li>
</ul>
</li>
<li><p><strong>右边</strong>展示了一个马鞍状的最优化地形，其中对于不同维度它的曲率不同（一个维度下降另一个维度上升）。</p>
<ul>
<li>SGD很难突破对称性，一直卡在顶部。</li>
<li>RMSProp之类的方法能够看到马鞍方向有很低的梯度。因为在RMSProp更新方法中的分母项，算法提高了在该方向的有效学习率，使得RMSProp能够继续前进。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="超参数调优"><a href="#超参数调优" class="headerlink" title="超参数调优"></a>超参数调优</h2><p>训练一个神经网络会遇到很多超参数设置。神经网络最常用的设置有：</p>
<ul>
<li>初始学习率</li>
<li>学习率衰减方式（例如一个衰减常量）</li>
<li>正则化强度（L2惩罚，随机失活强度）</li>
</ul>
<p>调参要点和技巧：</p>
<ul>
<li><p><strong>实现</strong>  </p>
<p>更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。记住这一点很重要，因为这会影响你设计代码的思路。</p>
</li>
<li><p><strong>比起交叉验证最好使用一个验证集</strong></p>
<p>在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。</p>
</li>
<li><p><strong>超参数范围</strong></p>
<p>在对数尺度上进行超参数搜索。例如，一个典型的学习率应该看起来是这样：<strong>learning_rate = 10 \</strong> uniform(-6, 1)**。也就是说，我们从标准分布中随机生成了一个数字，然后让它成为10的阶数。对于正则化强度，可以采用同样的策略。直观地说，这是因为学习率和正则化强度都对于训练的动态进程有乘的效果。</p>
</li>
<li><p><strong>随机搜索优于网格搜索</strong></p>
<p><img src="/2018/10/24/Lecture-Neural-Networks-Part-3/随机搜索优于网格搜索.jpg" alt=""></p>
<p>通常，有些超参数比其余的更重要，通过随机搜索，而不是网格化的搜索，可以让你更精确地发现那些比较重要的超参数的好数值。</p>
</li>
<li><p><strong>对于边界上的最优值要小心</strong></p>
<p>这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候.一旦我们得到一个比较好的值，一定要确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。</p>
</li>
<li><p><strong>从粗到细地分阶段搜索</strong></p>
<p>先进行初略范围搜索，然后根据好的结果出现的地方，缩小范围进行搜索。</p>
</li>
<li><p><strong>贝叶斯超参数最优化</strong></p>
<p>主要是研究在超参数空间中更高效的导航算法。其核心的思路是在不同超参数设置下查看算法性能时，要在探索和使用中进行合理的权衡。</p>
</li>
</ul>
<h2 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h2><h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候平均它们预测结果。集成的模型数量增加，算法的结果也单调提升（但提升效果越来越少）。还有模型之间的差异度越大，提升效果可能越好。进行集成有以下几种方法：</p>
<ul>
<li><strong>同一个模型，不同的初始化</strong>。使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。这种方法的风险在于多样性只来自于不同的初始化条件。</li>
<li><strong>在交叉验证中发现最好的模型</strong>。使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。</li>
<li><strong>一个模型设置多个记录点</strong>。如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。</li>
<li><strong>在训练的时候跑参数的平均值</strong>。和上面一点相关的，还有一个也能得到1-2个百分点的提升的小代价方法，这个方法就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。这样你就对前几次循环中的网络状态进行了平均。你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。</li>
</ul>
<p>模型集成的一个劣势就是在测试数据的时候会花费更多时间。最近Geoff Hinton在“<a href="http://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DEK61htlw8hY" target="_blank" rel="noopener">Dark Knowledge</a>”上的工作很有启发：其思路是通过将集成似然估计纳入到修改的目标函数中，从一个好的集成中抽出一个单独模型。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>训练一个神经网络需要：</p>
<ul>
<li>利用小批量数据对实现进行<code>梯度检查</code>，还要注意各种错误.</li>
<li>进行<code>合理性检查</code>，确认初始损失值是合理的，在小数据集上能得到100%的准确率.</li>
<li>在训练时，<code>跟踪</code>损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数量相对于总参数量的比例（一般在1e-3左右），然后如果是对于卷积神经网络，可以将第一层的权重可视化.</li>
<li>推荐的两个<code>更新</code>方法是SGD+Nesterov动量方法，或者Adam方法.</li>
<li>随着训练进行<code>学习率衰减</code>。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候.</li>
<li>使用<code>随机搜索</code>（不要用网格搜索）来搜索最优的超参数。分阶段<code>从粗</code>（比较宽的超参数范围训练1-5个周期）<code>到细</code>（窄范围训练很多个周期）地来搜索.</li>
<li>进行<code>模型集成</code>来获得额外的性能提高.</li>
</ul>
<p>参考链接：<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">Neural Nets notes 3</a>、神经网络笔记3<a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" target="_blank" rel="noopener">（上）</a><a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" target="_blank" rel="noopener">（下）</a></p>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2018-11-21T05:48:52.638Z" itemprop="dateUpdated">2018-11-21 13:48:52</time>
</span><br>


        
    </div>
    
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/avatar.jpg" alt="CaptainSE">
            CaptainSE
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CS231n/">CS231n</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/&title=《Lecture_Neural Networks Part 3》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/&title=《Lecture_Neural Networks Part 3》 — Go Further&source=【阅读时间】XXX min XXX words【阅读内容】……" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Lecture_Neural Networks Part 3》 — Go Further&url=http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2018/10/27/Lecture-Convolutional-Neural-Networks/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">Lecture_Convolutional_Neural_Networks</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/10/22/Lecture-Neural-Networks-Part-2/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Lecture_Neural Networks Part 2</h4>
      </a>
    </div>
  
</nav>



    

















</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢老板~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.jpg" data-alipay="/img/alipay.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>CaptainSE &copy; 2015 - 2018</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/&title=《Lecture_Neural Networks Part 3》 — Go Further&pic=http://yoursite.com/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/&title=《Lecture_Neural Networks Part 3》 — Go Further&source=【阅读时间】XXX min XXX words【阅读内容】……" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Lecture_Neural Networks Part 3》 — Go Further&url=http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2018/10/24/Lecture-Neural-Networks-Part-3/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACIUlEQVR42u3aQY7DMAgF0N7/0pkDVE0+kFYT+3k1mkaJnxcIDK9XvI4P6/2Z9+eTX8//f9vCwMB4LOM4XefbSjDJRpPvnh8iBgbGDoxPEez8mQRZDb7JdzEwMDCqjPxtvRQTAwMDo8dI0ruLZK4YfDEwMDDyZG7yyX9Ri2NgYDyQUU34fvn3V/obGBgYj2IcxTUvSvN0sLArDAyMpRl5gEs+lgxM3BVwMTAwdmYkG51cuiVHkA9nFE4IAwNjCcYNIe+mBmezOYGBgbEoIw+LeXjNmwHzY8LAwNiBUQ18vQu4XiAeDathYGAsxMhTwF54rQbN3hUeBgbGDoz5BX1+WdZrGFzsEAMDY2lGUkDmxW2SMvaK2OY7MTAwFmXkyV+14Oy1OZsNCQwMjIUY+dho78q+OsaRj1ZgYGDsyUgSsgm4V44Whi0wMDAWZVSL0mp7MqdWe60fE0QMDIxFGfkl/mRb+YVdrzDGwMDYh3HXDMP3eBdfx8DAWJSRX9PnxW31IKoDZBdFLAYGxqKMScg7gtUbqig0TTEwMLZhJOEvTxB7wxzJG5q5LQYGxgMZR3HNt1s9xCiVxMDAWJpx17bykYi88VketsDAwFiakV+39Tb9o/QRAwNjA0a1WJ2UvpNiuFCLY2BgbMzolabV1cxeMTAwMAahc3KRFwV3DAyMDRj5WFh+9d8bvEgCLgYGxm6MXt5VGIK/qbU5KX0xMDAey/gDnVN+TDuEmSwAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            clearTimeout(titleTime);
        } else {
            document.title = 'Go Further | Stay Hungry, Stay Foolish';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
