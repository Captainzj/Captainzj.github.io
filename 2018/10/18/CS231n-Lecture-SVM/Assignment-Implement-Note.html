<hr>
<p>title: Assignment_Implement_Note<br>layout:<br>date: 2018-11-20 19:10:14<br>categories:</p>
<h2 id="tags-cs231n"><a href="#tags-cs231n" class="headerlink" title="tags: cs231n"></a>tags: cs231n</h2><p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p>
<!--more-->
<h5 id="SVM-梯度计算"><a href="#SVM-梯度计算" class="headerlink" title="SVM 梯度计算"></a>SVM 梯度计算<div id="SVM 梯度计算"></div></h5><p>$$<br>L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]<br>$$</p>
<p>$$ {%raw%}<br>\left{\begin{aligned}<br>\nabla_{w_{y_i}} L_i = &amp; -\left(\sum_{j \ne y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0)\right)x_i &amp; j = y_i \<br>\nabla_{w_j} L_i = &amp; 1(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i &amp; j \ne y_i<br>\end{aligned}\right.<br>$$ {%endraw%}<br>其中$\mathbb{1}$是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。</p>
<pre><code class="python"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span>
  dW = np.zeros(W.shape)  <span class="comment">#(3073, 10)</span>

  num_classes = W.shape[<span class="number">1</span>] <span class="comment">#10</span>
  num_train = X.shape[<span class="number">0</span>]  <span class="comment">#500</span>
  loss = <span class="number">0.0</span>
  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):  <span class="comment">#[0,500)</span>
    scores = X[i].dot(W)   <span class="comment">#矩阵乘法  (1,3073)*(3073,10)</span>
    correct_class_score = scores[y[i]] <span class="comment">#S_yi  该图像在正确标签上的得分</span>
    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):
      <span class="keyword">if</span> j == y[i]:
        <span class="keyword">continue</span>
      margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note: delta = 1  </span>
      <span class="keyword">if</span> margin &gt; <span class="number">0</span>: 
        loss += margin
        dW[:, y[i]] -= X[i, :].T <span class="comment"># this is really a sum over j != y_i</span>
        dW[:, j] += X[i, :].T <span class="comment"># sums each contribution of the x_i's</span>

  loss /= num_train
  dW /= num_train
  loss += <span class="number">0.5</span> *reg * np.sum(W * W) 
  dW += reg*W

  <span class="keyword">return</span> loss, dW

<span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span>
  loss = <span class="number">0.0</span>
  dW = np.zeros(W.shape) <span class="comment"># dW.shape==(3073,500)</span>
  num_classes=W.shape[<span class="number">1</span>]
  num_train=X.shape[<span class="number">0</span>]

  scores=X.dot(W) <span class="comment">#(500,10) = (500,3073)*(3073,10)</span>
  scores_correct = scores[np.arange(num_train), y] <span class="comment">#(500,) scores_correct[i]=scores[i,y[i]]</span>
  scores_correct=np.reshape(scores_correct,(num_train,<span class="number">-1</span>))  <span class="comment">#(500,1) = (500,500*1/500)</span>
  margins=scores-scores_correct+<span class="number">1</span> <span class="comment">#delta=1  #scores.shape=(500,10)</span>
  margins=np.maximum(<span class="number">0</span>,margins)
  margins[np.arange(num_train),y]=<span class="number">0</span>
  loss=np.sum(margins)/num_train
  loss += <span class="number">0.5</span> * reg * np.sum(W * W)


  <span class="comment"># compute the gradient</span>
  margins[margins &gt; <span class="number">0</span>] = <span class="number">1</span>  <span class="comment">#margins中大于0的元素，数值赋为1;其余数值不变   shape==(500,10)</span>
  row_sum = np.sum(margins, axis=<span class="number">1</span>)                  <span class="comment"># 1 by N  (1行N列)</span>
  margins[np.arange(num_train), y] = -row_sum        <span class="comment">#margins[np.arange(num_train), y] 赋-row_sum前，值为0   shape==(500,)</span>
  <span class="comment">#print(margins)  ##necessary to understand</span>
  dW += np.dot(X.T, margins)/num_train + reg * W     <span class="comment"># D by C dW.shape==(3073，10) X.T.shape==(3073,500)  margins.shape==(500,10) </span>

  <span class="keyword">return</span> loss, dW
</code></pre>
<ul>
<li>此处解释仅关注dW(即对权重的梯度计算) <code>np.dot(X.T, margins)</code>  </li>
</ul>
<p>$$ {%raw%}<br>\underbrace{\underbrace{\begin{bmatrix}\overrightarrow{X_C}&amp;\overrightarrow{X_D} &amp;\overrightarrow{X_A}&amp; \overrightarrow{X_B}\end{bmatrix}}_\text{可看作dW (D,C)}<br>=\underbrace{\begin{bmatrix}\overrightarrow{X_c}&amp;\overrightarrow{X_d}&amp;\overrightarrow{X_a}&amp;\overrightarrow{X_b}\end{bmatrix}}<em>\text{可看作X.T (D,N)}<br>\underbrace{\begin{bmatrix}  1&amp;1  &amp;-3  &amp;1 \ 1 &amp; 1 &amp;  1&amp;-3\-3&amp;  1&amp;  1&amp; 1\  1&amp;  -3&amp;1  &amp;1  \end{bmatrix}}</em>\text{可看作margins (N,C)}}_\text{svm_loss_vectorized dW计算过程}<br>=\underbrace{\begin{bmatrix}   \overrightarrow{X_a}+\overrightarrow{X_b}-3\overrightarrow{X_c}+\overrightarrow{X_d}&amp;\overrightarrow{X_a}+\overrightarrow{X_b}+\overrightarrow{X_c}-3\overrightarrow{X_d} &amp;-3\overrightarrow{X_a}+\overrightarrow{X_b}+\overrightarrow{X_c}+\overrightarrow{X_d}&amp;\overrightarrow{X_a}-3\overrightarrow{X_b}+\overrightarrow{X_c}+\overrightarrow{X_d}   \end{bmatrix}}_\text{svm_loss_naive dW计算过程}<br>$$ {%endraw%}</p>
<p>$margins.shape==scores.shape$    $-row_sum=-3(代码中为-9) $</p>
<p>矩阵基本知识：<br>$$ {%raw%}<br>(1)\begin{bmatrix}\overrightarrow{A}&amp;\overrightarrow{B}  &amp;\overrightarrow{C}  &amp; \overrightarrow{D}\end{bmatrix}<br>=\begin{bmatrix}\overrightarrow{a}&amp;\overrightarrow{b}  &amp;\overrightarrow{c}&amp;\overrightarrow{d}\end{bmatrix}\begin{bmatrix} -3&amp;  1&amp;  1&amp; 1\  1&amp;  -3&amp;1  &amp;1 \  1&amp;1  &amp;-3  &amp;1 \ 1 &amp; 1 &amp;  1&amp;-3 \end{bmatrix}<br>=\begin{bmatrix} -3\overrightarrow{a}+\overrightarrow{b}+\overrightarrow{c}+\overrightarrow{d}&amp;\overrightarrow{a}-3\overrightarrow{b}+\overrightarrow{c}+\overrightarrow{d}  &amp; \overrightarrow{a}+\overrightarrow{b}-3\overrightarrow{c}+\overrightarrow{d}&amp;\overrightarrow{a}+\overrightarrow{b}+\overrightarrow{c}-3\overrightarrow{d}  \end{bmatrix}<br>$$ {%endraw%}<br>$$ {%raw%}<br>(2)\begin{bmatrix}\overrightarrow{C}&amp;\overrightarrow{D} &amp;\overrightarrow{A}&amp; \overrightarrow{B}\end{bmatrix}<br>=\begin{bmatrix}\overrightarrow{c}&amp;\overrightarrow{d}&amp;\overrightarrow{a}&amp;\overrightarrow{b}\end{bmatrix}<br>\begin{bmatrix}  1&amp;1  &amp;-3  &amp;1 \ 1 &amp; 1 &amp;  1&amp;-3\-3&amp;  1&amp;  1&amp; 1\  1&amp;  -3&amp;1  &amp;1  \end{bmatrix}<br>=\begin{bmatrix}   \overrightarrow{a}+\overrightarrow{b}-3\overrightarrow{c}+\overrightarrow{d}&amp;\overrightarrow{a}+\overrightarrow{b}+\overrightarrow{c}-3\overrightarrow{d} &amp;-3\overrightarrow{a}+\overrightarrow{b}+\overrightarrow{c}+\overrightarrow{d}&amp;\overrightarrow{a}-3\overrightarrow{b}+\overrightarrow{c}+\overrightarrow{d}   \end{bmatrix}<br>$$ {%endraw%}</p>
<p>$$ {%raw%}<br>(3) \begin{bmatrix}\overrightarrow{A}\\overrightarrow{B}\\overrightarrow{C}\\overrightarrow{D}\end{bmatrix}<br>=\begin{bmatrix} -3&amp;  1&amp;  1&amp; 1\  1&amp;  -3&amp;1  &amp;1 \  1&amp;1  &amp;-3  &amp;1 \ 1 &amp; 1 &amp;  1&amp;-3 \end{bmatrix}\begin{bmatrix}\overrightarrow{a}\\overrightarrow{b}\\overrightarrow{c}\\overrightarrow{d}\end{bmatrix}<br>=\begin{bmatrix}-3\overrightarrow{a}+\overrightarrow{b}+\overrightarrow{c}+\overrightarrow{d}\\overrightarrow{a}-3\overrightarrow{b}+\overrightarrow{c}+\overrightarrow{d}\ \overrightarrow{a}+\overrightarrow{b}-3\overrightarrow{c}+\overrightarrow{d}\\overrightarrow{a}+\overrightarrow{b}+\overrightarrow{c}-3\overrightarrow{d}  \end{bmatrix}<br>$$ {%endraw%}</p>
<p>$$ {%raw%}<br>(4) \begin{bmatrix}\overrightarrow{C}\\overrightarrow{D}\\overrightarrow{A}\\overrightarrow{B}\end{bmatrix}<br>=\begin{bmatrix}   1&amp;1  &amp;-3  &amp;1 \ 1 &amp; 1 &amp;  1&amp;-3 \-3&amp;  1&amp;  1&amp; 1\  1&amp;  -3&amp;1  &amp;1\end{bmatrix}<br>\begin{bmatrix}\overrightarrow{c}\\overrightarrow{d}\\overrightarrow{a}\\overrightarrow{b}\end{bmatrix}<br>=\begin{bmatrix} \overrightarrow{a}+\overrightarrow{b}-3\overrightarrow{c}+\overrightarrow{d}\\overrightarrow{a}+\overrightarrow{b}+\overrightarrow{c}-3\overrightarrow{d}\-3\overrightarrow{a}+\overrightarrow{b}+\overrightarrow{c}+\overrightarrow{d}\\overrightarrow{a}-3\overrightarrow{b}+\overrightarrow{c}+\overrightarrow{d}  \end{bmatrix}<br>$$ {%endraw%}</p>
<ul>
<li>$C = A B $ 等价于 $C^T = B^T    A^T$</li>
</ul>
