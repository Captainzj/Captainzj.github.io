<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Go Further</title>
  
  <subtitle>Stay Hungry, Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-24T03:45:34.070Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>CaptainSE</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>云平台功能</title>
    <link href="http://yoursite.com/2019/07/24/%E4%BA%91%E5%B9%B3%E5%8F%B0%E5%8A%9F%E8%83%BD/"/>
    <id>http://yoursite.com/2019/07/24/云平台功能/</id>
    <published>2019-07-24T02:59:41.000Z</published>
    <updated>2019-07-24T03:45:34.070Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="热力图叠加"><a href="#热力图叠加" class="headerlink" title="热力图叠加"></a>热力图叠加</h3><h4 id="npy-2-png-without-padding"><a href="#npy-2-png-without-padding" class="headerlink" title=".npy 2 .png without padding"></a>.npy 2 .png without padding</h4><ul><li><p><strong>Recommended:</strong> </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imsave(savepath, img_array. cmap=<span class="string">'rainbow'</span>)  <span class="comment"># cmap -- colormap</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Others without padding:</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line">scipy.misc.imsave(savepath, img_array) <span class="comment"># Use ``imageio.imwrite`` instead</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line">imageio.imwirte(savepath, img_array)  <span class="comment"># Lossy conversion from float64 to uint8</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line">img = Image.fromarray(img_array.astype(<span class="string">'uint8'</span>)).convert(<span class="string">"RGB"</span>)</span><br><span class="line">img.save(savepath, cmap=<span class="string">'rainbow'</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Others with padding</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure()</span><br><span class="line">plt.axis(<span class="string">'off'</span>)  <span class="comment"># 仅不可见，但依旧存在</span></span><br><span class="line">plt.imshow(img_array, cmap=<span class="string">'bone'</span>)</span><br><span class="line">plt.savefig(savepath, bbox_inched=<span class="string">'tight'</span>)</span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/jifaley/article/details/79687000" target="_blank" rel="noopener">plt.savefig() 输出图片去除旁边的空白区域</a> 未测试</p></li></ul><h4 id="背景透明化"><a href="#背景透明化" class="headerlink" title="背景透明化"></a>背景透明化</h4><p>参考：<a href="https://blog.csdn.net/qq_40878431/article/details/82941982" target="_blank" rel="noopener"> Python PIL.Image之修改图片背景为透明</a></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以第一个像素为准，相同色改为透明</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transparent_back</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img.convert(<span class="string">'RGBA'</span>)</span><br><span class="line">    L, H = img.size</span><br><span class="line">    color_0 = img.getpixel((<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(H):</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">            dot = (l,h)</span><br><span class="line">            color_1 = img.getpixel(dot)</span><br><span class="line">            <span class="keyword">if</span> color_1 == color_0:</span><br><span class="line">                color_1 = color_1[:<span class="number">-1</span>] + (<span class="number">0</span>,)</span><br><span class="line">                img.putpixel(dot,color_1)</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    img=Image.open(<span class="string">'round.png'</span>)</span><br><span class="line">    img=transparent_back(img)</span><br><span class="line">    img.save(<span class="string">'round2.png'</span>)</span><br></pre></td></tr></table></figure><h5 id="扩展链接"><a href="#扩展链接" class="headerlink" title="扩展链接"></a>扩展链接</h5><ul><li><p><a href="https://blog.csdn.net/JNingWei/article/details/78241973" target="_blank" rel="noopener">opencv: 图片 设置 透明度 并 叠加(cv2.addWeighted)</a></p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">cv2.addWeighted(src1, alpha, src2, beta, gamma[, dst[, dtype]]) → dst.</span><br></pre></td></tr></table></figure><p>其中，<code>alpha</code> 为 <code>src1</code> 透明度，<code>beta</code> 为 <code>src2</code> 透明度.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 底板图案</span></span><br><span class="line">bottom_pic = <span class="string">'elegent.jpg'</span></span><br><span class="line"><span class="comment"># 上层图案</span></span><br><span class="line">top_pic = <span class="string">'lena.jpg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">bottom = cv2.imread(bottom_pic)</span><br><span class="line">top = cv2.imread(top_pic)</span><br><span class="line"><span class="comment"># 权重越大，透明度越低</span></span><br><span class="line">overlapping = cv2.addWeighted(bottom, <span class="number">0.8</span>, top, <span class="number">0.2</span>, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 保存叠加后的图片</span></span><br><span class="line">cv2.imwrite(<span class="string">'overlap(8:2).jpg'</span>, overlapping)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://blog.csdn.net/m0_37606112/article/details/78632896" target="_blank" rel="noopener">opencv3.3.1+python3.6.3图像上添加背景透明logo</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line">cv2.namedWindow("logo")  #定义一个窗口</span><br><span class="line">frame=cv2.imread('girl.png') #捕获图像1</span><br><span class="line">logo=cv2.imread('logo.jpg')</span><br><span class="line">logo_gray=cv2.cvtColor(logo,cv2.COLOR_BGR2GRAY)</span><br><span class="line">rows,cols,channels = logo.shape</span><br><span class="line"></span><br><span class="line">dx,dy=120,150  # roi初始坐标</span><br><span class="line">roi=frame[dx:dx+cols,dy:dy+rows]</span><br><span class="line">for i in range(cols):</span><br><span class="line">    for j in range(rows):</span><br><span class="line">        if (logo[i,j][0]+logo[i,j][1]+logo[i,j][2])&lt;=20:roi[i,j]=roi[i,j]</span><br><span class="line">        else:roi[i,j]=logo[i,j]</span><br><span class="line"><span class="meta">#</span><span class="bash">roi=cv2.addWeighted(logo,0.5,roi,0.5,1)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">roi=cv2.add(logo,roi)</span></span><br><span class="line">frame[dx:dx+cols,dy:dy+rows]=roi</span><br><span class="line"></span><br><span class="line">cv2.imshow("logo",frame)#显示轮廓</span><br><span class="line"></span><br><span class="line">cv2.waitKey(0)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Multimodal-Neuroimaging-Feature-Learning-for-Multiclass-Diagnosis-of-Alzheimer’s-Disease-翻译</title>
    <link href="http://yoursite.com/2019/07/19/Multimodal-Neuroimaging-Feature-Learning-for-Multiclass-Diagnosis-of-Alzheimer%E2%80%99s-Disease-%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/07/19/Multimodal-Neuroimaging-Feature-Learning-for-Multiclass-Diagnosis-of-Alzheimer’s-Disease-翻译/</id>
    <published>2019-07-19T05:15:52.000Z</published>
    <updated>2019-07-19T05:27:40.406Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p><a href="https://scholar.google.com/scholar?q=Multimodal+Neuroimaging+Feature+Learning+for+Multiclass+Diagnosis+of+Alzheimer%E2%80%99s+Disease&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart" target="_blank" rel="noopener">《Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer’s Disease》</a>  多模式神经影像学特征学习用于阿尔茨海默病的多类诊断</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><strong><em>Abstract</em></strong></h2><p>阿尔茨海默病（AD）的准确诊断对于患者护理是必不可少的，并且随着疾病调节剂在疾病早期可用而变得越来越重要。虽然研究已将机器学习方法应用于AD的计算机辅助诊断，但由于缺乏表示神经影像生物标记物的有效策略，因此在先前的方法中显示出诊断性能的瓶颈。在这项研究中，我们设计了一个新的诊断框架与深度学习架构，以帮助诊断AD。该框架使用零掩蔽策略进行数据融合，以从多种数据模式中提取补充信息。与之前最先进的工作流程相比，我们的方法能够在一个设置中融合多模态神经成像功能，并且可能需要较少标记的数据。 AD的二元分类和多类分类均实现了性能提升。讨论了拟议框架的优点和局限性。</p><p>Index Terms—Alzheimer’s disease (AD), classification, deep Learning, MRI, neuroimaging, positron emission tomography (PET).</p><h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><p>ALZHEIMER’S疾病（AD）是一种退行性脑紊乱，其特征是进行性痴呆，其特征在于特定神经细胞的退化，神经炎斑块的存在和神经原纤维缠结[1]。记忆和其他认知功能的下降是通常的早期综合症。由于社会年龄的增长，AD将成为未来几十年的全球负担。据报道，2006年全世界共有2660万例AD病例，其中约56％的病例处于早期阶段。预计到2050年，AD患者人数将增长四倍，达到1.068亿[2]。 AD的精确诊断被认为是一项困难的临床任务，其特异性不足，因为当意识受损时，不能对精神状态进行评估。另一个困难是由于其他非AD痴呆综合征的混乱引起的。轻度认知障碍（MCI）是AD的一个前驱阶段，最近引起了研究人员的注意，因为它对临床试验很有用。尽管MCI并未显着干扰日常活动，但已经不断证明MCI患者存在AD进展的高风险[3]。为了预测MCI的转变风险，可以将MCI受试者进一步分类为MCI转换器（cMCI）和MCI非转换器（ncMCI）。检测早期阶段以及整个AD进展范围是至关重要的;因此，在发生不可逆的脑损伤之前，允许患者控制危险因素，例如单纯收缩期高血压[4]，[5]。神经影像学技术，如磁共振成像（MRI）[6]  -  [11]和正电子发射断层扫描（PET）[12]  -  [18]，已被广泛用于AD的评估，以及许多其他非成像生物标志物[ 6]，[19]，[20]。<br>已经提出机器学习方法来帮助AD的诊断。预先计算的医学描述符被广泛用于表示生物医学图像。近似测量，例如体积[21]和葡萄糖的脑代谢率（CMRGlc）[22]，通常是根据分割的3-D脑区域（ROI）计算出来的，并用于AD分类。支持向量机（SVM）[23]，贝叶斯方法[24]，或其他方法[25]，[26]。但是，此类工作流程存在一些限制。基于这些传统机器学习者的方法通常在二元分类中很好地工作，例如从正常对照（NC）受试者中分类AD受试者，但是很难将它们扩展到多类[27]。因此，虽然AD的诊断应该自然地建模为多类分类问题，但它通常被简化为一组二元分类任务[23,28]，将AD或MCI受试者与NC受试者区分开来。另一个限制是嵌入临床先验知识。 Liu等人最近提出了一种基于图切割算法的方法。 [10]。该工作流程调整了图形切割算法，其参数对应于AD的不同阶段之间的关系。虽然这种定制往往会产生有希望的分类结果，但工作流程可能对数据集的变化很敏感，并且很难扩展到大规模。 AD诊断的另一个挑战是以无监督的方法表示原始生物标志物。一些框架以监督的方式降低每种生物标记物的维度，然后融合特征形态以形成新的特征空间[29]  -  [31]。这种工作流程在很大程度上取决于难以实现的标记样品的数量。分离维数减少和数据融合也可能导致丢失补充信息。</p><p>我们相信，通过设计新的框架来有效地表示多种生物标记并有效地表征AD的多个阶段，可以优化先前的工作流程。具有浅结构和仿射数据变换的传统特征工程工作流通常简单地导致特征重复或维度选择。正如许多最近的研究所示，通过解开输入中的复杂模式，深度数据表示可以比多类分类中的浅层架构更有效[32]  -  [36]。深度学习架构通过多层特征表示逐步提取高级特征[37]。由于特征空间的连续变换，高级特征在分类问题中往往更加可分。<br>使用MR的Brosch和Tam报告说，多层学习结构能够有效地捕获与人口统计学和疾病信息相关的大脑区域的形状变化，例如心室大小[38]。在Suk等人提出的框架中。 [39]，对每种图像模态训练一个堆叠自动编码器（SAE）设置;然后，学习的高级特征进一步与多核支持向量机（MKSVM）融合。在这样的工作流程中，无论其他模态如何，都可以学习单模态高级特征，这可能会忽略特征学习中不同模态之间的协同作用。<br>在这项研究中，我们提出了一种新的多层AD诊断框架，其中嵌入了深度学习架构，其受益于多模态神经影像学特征之间的协同作用。该框架由SAE和soft-max逻辑回归器构成。自动编码器以无人监督的方式表示数据，可以扩展为在实践中使用未标记的数据。当提供多模态神经影像图像数据时，所提出的框架能够进行数据融合。遵循去噪自动编码器的概念[40]，我们将双掩模策略应用于双峰深度学习任务，以提取不同图像模态之间的协同作用。通过随机隐藏训练集的一种形态，神经网络的隐藏层倾向于能够通过推断多模态特征之间的相关性来重构丢失的模态与损坏的输入。通过深度学习架构中嵌入的soft-max回归，我们的框架能够将AD患者分为四个AD阶段。<br>本文的其余部分安排如下。我们在第二部分介绍了拟议的学习框架和培训策略。本研究的实验和结果见第III节。我们在第四节和第五节讨论了该文件的拟议框架和结论。</p><h2 id="II-METHODOLOGY"><a href="#II-METHODOLOGY" class="headerlink" title="II. METHODOLOGY"></a>II. METHODOLOGY</h2><p>所提出的框架的流程如图1所示。在该研究中，MR和PET数据被用作两种输入神经成像模态。 首先对所有收集的脑图像进行预处理并将其分割成83个功能ROI，并从每个ROI计算一组描述符。 数据集分为训练集和测试集。 我们仅对训练样本执行弹性网[9]，[41]，[42]，以选择特征参数的判别子集。 然后使用训练数据集中的所选特征子集训练由若干自动编码器组成的多层神经网络。 网络的每一层都通过非线性变换获得了前一层的更高层次的抽象[43]  -  [45]。 softmax层添加在SAE的顶部以进行分类。 然后用标记的测试样品评估训练的网络。</p><h3 id="A-Data-Acquisition-and-Feature-Extraction"><a href="#A-Data-Acquisition-and-Feature-Extraction" class="headerlink" title="A. Data Acquisition and Feature Extraction"></a><em>A. Data Acquisition and Feature Extraction</em></h3><p>本研究中使用的神经影像学数据来自阿尔茨海默病神经影像学倡议（ADNI）数据库1 [46]。该数据库于2003年由国家老龄化研究所，国家生物医学成像和生物工程研究所，食品和药物管理局，私营制药公司和非营利组织发起，作为一个为期五年的公共合作伙伴关系。 ADNI项目的主要目的是研究结合多种生物标记物（如MRI，PET和CSF数据以及神经心理学评估）预测MCI和早期AD的进展的效果。大约200个正常实例和400个MCI实例被跟踪了三年; 200名AD患者在两年内随访。确定敏感的生物标志物对AD的进展也可能有助于临床医生发现新的治疗方法，以及其他可能的生物医学探索。<br>我们从ADNI获得了两个数据集。对于仅具有MR图像的数据集，从ADNI库中回收了816个年龄和性别匹配的受试者，并从每个受试者获得T1加权的MR图像。我们排除了20名具有多次转换或逆转的受试者以及数据不完整的21名MCI受试者。我们将从第一次扫描起0.5至3年转换为AD的MCI受试者标记为cMCI，否则将MCI受试者标记为ncMCI。正常受试者和AD患者被标记为NC和AD [10]。所有原始MR图像均按照ADNI MR图像协议进行校正，并使用图像配准工具包[48]非线性地注册到ICBM_152模板[47]。由于无法容忍的失真，仅排除了17张图像。最后，758名MR受试者被保留用于在该研究中进行的实验，包括180名AD受试者，160名cMCI受试者，214名ncMCI受试者和204名正常老化对照受试者。<br>对于具有多模式数据融合的数据集，从基线群组中选择331个年龄和性别匹配的受试者，包括77个NC-，102个ncMCI-，67个cMCI-，85个AD受试者，其具有MR和PET数据。每个实例都与T1加权体积和FDG-PET图像相关联。使用前面描述的MR图像的类似工作流程对所有3-D图像进行预处理。使用FSL FLIRT将PET图像与相应的MR图像对齐[49]。<br>对于每个注册的三维图像，使用增强配准方法的多特征传播在模板空间中映射了83个脑区[50]。从MR图像中提取灰质体积，与[9]和[10]相同。对于PET图像，我们提取了与[22]和[51]相同的区域平均CMRGlc特征。然后，我们将特征归一化为介于0和1之间，以通过移位负值和重新缩放来支持S形解码器。</p><h3 id="B-Learning-Framework"><a href="#B-Learning-Framework" class="headerlink" title="B. Learning Framework"></a><em>B. Learning Framework</em></h3><p><strong>1) Pretraining SAEs</strong></p><p><strong>2) Multimodal Data Fusion:</strong>：当多个图像时，模态用于模型训练，需要模态融合方法来发现不同模态之间的协同作用。共享表示可以通过联合训练自动编码器和连接的MR和PET输入来获得。第一个共享隐藏层用于模拟不同数据模态之间的相关性。然而，简单的特征连接策略通常导致隐藏的神经元仅由单一模态激活，因为MR和PET的相关性是高度非线性的。灵感来自Ngiam等人。 [54]，我们将预训练方法应用于一定比例的损坏输入，这些输入仅提供一种模态，遵循深度建筑训练的去噪概念。通过用0替换这些输入，随机隐藏其中一种形式;其余的训练样本都有两种形式。训练第一自动编码器的隐藏层以重建来自与隐藏模态混合的输入的所有原始输入。原始输入和破坏的输入独立地传播到神经网络的较高层，以使用相同的神经网络获得清洁表示和噪声表示。然后逐步训练每个较高层以从传播的噪声表示重建清洁的高级表示。因此，一些隐藏的神经元有望推断出不同神经影像学模式之间的相关性。</p><p><strong>3) Fine-Tuning for AD Classification</strong></p><p>对于AD诊断，我们将任务建模为包含四个预定义标签的四级分类问题：NC，cMCI，ncMCI和AD。 虽然无监督网络学到的特征也可以转移到传统分类器，但是，软性逻辑回归使我们能够通过微调联合优化整个网络。<br>由无监督网络提取的特征通过softmax回归[55]输入到输出层。 softmax层使用不同的激活函数，其可能具有与先前层中应用的非线性不同的非线性。</p><p>…</p><h3 id="C-Feature-Examination"><a href="#C-Feature-Examination" class="headerlink" title="C. Feature Examination"></a><em>C. Feature Examination</em></h3><h2 id="III-EXPERIMENTS-AND-RESULTS"><a href="#III-EXPERIMENTS-AND-RESULTS" class="headerlink" title="III. EXPERIMENTS AND RESULTS"></a>III. EXPERIMENTS AND RESULTS</h2><h3 id="A-Visualization-of-High-Level-Biomarkers"><a href="#A-Visualization-of-High-Level-Biomarkers" class="headerlink" title="A. Visualization of High-Level Biomarkers"></a><em>A. Visualization of High-Level Biomarkers</em></h3><p>利用第II-C节中描述的特征检查方法，我们计算了每个脑ROI的稳定性得分，并将稳定性得分映射到NC对象的掩蔽的3-D MR图像（83个ROI），如图3所示。 各种投资回报率之间的区别清晰可见。 较暗的区域往往比较轻的ROI对AD和MCI的进展更敏感，因为从这些ROI中提取的特征往往同样有益于所有隐藏的神经元。 光区域未被表示为完全无关紧要，但携带较少的预测信息。</p><h3 id="B-Performance-Evaluation"><a href="#B-Performance-Evaluation" class="headerlink" title="B. Performance Evaluation"></a><em>B. Performance Evaluation</em></h3><p>我们将提出的框架与使用单核SVM和MKSVM的广泛应用的方法进行了比较[23]，[28]。为了评估所提出的数据融合方法，我们将零掩模方法与[39]中提出的架构进行了比较，该架构独立地训练两个SAE，然后在每个SAE经过微调后将高级特征与MKSVM融合。如第II-A部分所述，使用从MR图像和PET图像中提取的相同特征评估所有实验。<br>拟议的框架在MATLAB 2013a上实施。基于SVM的实验使用LIBSVM [58]进行。 MKSVM是通过使用预先计算的内核并使用相对权重融合多个内核来实现的。<br>通过使用十倍交叉验证进行评估。在包括多种模态的实验中，我们将性能与仅单模态数据，MR或PET以及具有两种模态的数据融合方法进行了比较。为避免“幸运试验”，我们从每个班级中随机抽取训练和测试实例，以确保它们与原始数据集具有相似的分布。对整个网络进行了培训，并使用90％的数据进行了微调，然后在每个验证试验中对其余样本进行了测试。在每个验证试验中使用对数域中的近似搜索选择所有比较方法的超参数以获得最佳执行模型[59]。在所有基于神经网络的实验中使用了两个隐藏层，因为添加额外的隐藏层未显示AD分类的进一步改进。假设两个非线性变换可以理想地表示AD分类的神经影像学特征是合理的。根据每个折叠中的分类性能，在30和200之间选择隐藏层中的神经元数量。在每个神经网络中，隐藏层共享相同数量的隐藏神经元[60]。使用训练样本训练MKSVM。按照[23]中的工作流程，通过步长为0.1的粗网格搜索选择MKSVM中每个内核的相对权重。在使用MKSVM融合两个SAE网络的实验中，每个SAE首先进行预训练并用训练数据进行微调，然后，从每个网络获得的高级特征与MKSVM融合，并采用前面所述的程序。<br>1）MR实验（758名受试者）：我们首先用758个3-D MR图像评估了所提出的框架。由于仅呈现了一种模态，因此在SVM和所提出的方法中都没有使用模态融合策略。<br>表I中显示了二元分类（NC与AD和NC与MCI）的性能。前两列是各个类的精确度，以下三列是整体性能，包括准确度，灵敏度和特异性。所提出的方法（SAE）在通过引导总体准确性（82.59％）和总体灵敏度（86.83％）对来自NC受试者的AD受试者进行分类方面优于SVM。这两种方法的总体特征非常接近（78.89％和77.78％）。在从MCI分类NC的所有整体性能测量中，所提出的方法优于SVM。所提出的方法在对NC受试者进行分类时实现了高出5％的精确度。</p><p>表II中显示了多类分类的性能。前四列是各个班级的精确度，以下三列是整体表现。所提出的方法在三个类别中比SVM执行更好的精确度（NC为52.40％，cMCI为38.71％，AD为46.89％）。所提出的方法导致总体准确度（46.30％）和总体特异性（77.78％）。 SVM实现了更高的灵敏度（75.00％）。总之，当仅呈现MR数据时，我们提出的方法在二元和多类AD分类问题中的大多数性能测量中优于最先进的基于SVM的方法。<br>2）MR和PET实验（331名受试者）：共有331名受试者同时获得MR和PET数据。我们首先仅使用MR图像（SVM-MR，SAE-MR）或PET图像（SVM-PET，SAE-PET）评估SVM的性能和所提出的基于SAE的方法。多核SVM的融合模式的性能显示为MKSVM。对于深度学习方法，我们将提出的零屏蔽训练策略（SAE-ZEROMASK）与简单特征连接（SAE-CONCAT）进行了比较。<br>二元分类性能显示在表III中。可以观察到，两种模态的实验（MKSVM，SAE-CONCAT和SAE-ZEROMASK）比在二元分类任务中仅具有单一模态的实验产生更好的性能。 SAE-CONCAT的总体准确度略高于MKSVM（90.15％ -  90.11％和77.65％ -  76.88％）。可以观察到，当使用所提出的SAE-ZEROMASK方法时，与SAE-CONCAT相比，所有测量中的性能都得到了提高。与ZERO-MASK相比，MKSVM在分类NC和AD方面的特异性略高。尽管SVM-MR在MCI上的精度略高（83.92％），但可以认为这种性能可能是由于决策失衡（NC上只有67％）。在所有方法中，SAE-ZEROMASK在NC和MCI之间的分类中取得了最均衡的表现（NC为81.95％，AD为83.88％），当MCI占据数据集的很大一部分时，这是相对困难的（246中有169个） ）。所提出的只有一个神经网络的数据融合方法SAE-ZEROMASK与2SAE-MKSVM实现了相当的性能，2SAE-MKSVM融合了来自两个独立训练网络的两个高级特征矩阵。 2SAE-MKSVM的准确率并不明显高于简单特征串联（77.90％至77.65％），因为在实验中观察到MKSVM为特征融合添加的仅保留了单模网络实现的更高精度。一些验证试验。<br>多类分类的性能如表IV所示。提议的框架与输入损坏（SAE-ZEROMASK）导致整体准确性和特异性（53.79％和86.98％）。基于深度学习的方法（SAE-CONCAT和SAE-ZEROMASK）引领NC，cMCI和AD的精确度。 cMCI的精确度受到cMCI实例数量的限制（331个中的67个），并受其兄弟类ncMCI（102个实例）的影响。对于ncMCI，SAE-ZEROMASK和MKSVM实现的精确度非常接近。与简单的特征串联（SAE-CONCAT）相比，SAE-ZEROMASK将整体精度提高了约5％。 SAE-ZEROMASK在整体准确性和特异性方面也优于其他数据融合选项2SAE-MKSVM。基于SVM的方法往往具有更好的灵敏度。</p><h2 id="IV-DISCUSSION"><a href="#IV-DISCUSSION" class="headerlink" title="IV. DISCUSSION"></a>IV. DISCUSSION</h2><h3 id="A-Model-Designing-and-Training"><a href="#A-Model-Designing-and-Training" class="headerlink" title="A. Model Designing and Training"></a><em>A. Model Designing and Training</em></h3><p>研究表明，学习具有原始数据的多重非线性表示的体系结构将产生有意义的分类特征[56]，[61]  -  [63]。为了在AD受试者中进行准确诊断，我们研究了神经影像生物标志物的多层表征在AD分类中的应用。我们的研究结果表明，多层结构可用于区分MR和PET受试者沿着AD进展的频谱，其准确度高于传统的浅层结构。分类的性能主要受益于学习架构的深度（来自复杂性理论的概念），其可以被示为特征空间的非线性变换序列。在微调期间，神经成像特征空间被扭曲和折叠以最小化训练数据上的分类损失。因此，在几层变换之后，不可分割的样本将在学习的高级特征空间中变得可分离。与传统方法相比，所提出的框架在提取基于神经影像学的ROI生物标记物之间的复杂相关性以及不同的特征形态方面更为强大。使用多层结构进行AD诊断的另一个动机是重复使用高级特征进行半监督学习[64]。除了监督数据融合或降维[29]之外，所提出的工作流程可以很容易地扩展到使用未标记的神经影像数据。<br>我们将不同的数据模态与所提出的零掩模融合策略结合起来，通过随机隐藏的一种模态传播噪声信号。训练自动编码器以利用损坏的输入信号重建原始输入信号。我们还试图避免在不同的数据模式上训练单独的神经网络，因为这可能会在特征学习期间忽略补充信息。具有一种隐藏模态的训练对象倾向于迫使一些神经元对MR和PET输入敏感，这使得零掩模融合网络不同，因为它具有两个独立的特征学习网络。值得注意的是，2SAE + MKSVM在NC和AD的二元分类中也实现了91.4％的总体分类准确度和91.67％的更高特异性。这可能表明，当不同特征聚类之间存在相对较大的边界时，两个特征融合方法之间的二元决策边界可能相似。观察所涉及的不可转换和可转换MCI主题的实验结果，我们假设当在嘈杂训练集中包括更微小的差异和更多异常值时，所提出的零掩模方法可能具有更多优点。</p><p>我们应用特征工程管道来提取MR和PET图像的初始ROI测量结果作为输入，而不是使用原始图像补丁进行医学特征学习.AD相关患者的三维医学图像之间的差异往往是微妙的，方差往往很大。从这个角度来看，网络决策系统的隐藏神经元也可以被解释为诊断规则的自动编码推断[65]。我们的实验表明，在使用ROI预计算特征时，无监督网络在预训练中实现了两个隐藏层的最佳性能。这意味着与使用原始图像作为输入的学习任务相比，当使用近似测量的成像特征时，实际上需要相对较浅的架构[38]。在我们的实验中，所有隐藏层中具有相同数量神经元的网络通常表现得更好。我们发现过度完成的歧管或低维流形都产生了AD分类的有效特征。根据不同的训练集选择隐藏神经元的数量。<br>使用弹性网的特征选择增强了所有检查方法的性能。它有助于控制由噪声和冗余特征参数引起的过度拟合。值得注意的是，大多数选定的特征参数都是由弹性网一致选择的。具有较少选择的特征参数的验证试验倾向于具有较高的泛化误差，这可能是由于训练集中包含的偏差异常值。<br>虽然提取的特征可以被其他一些传统的分类器使用，例如SVM，但我们将输出层与softmax回归连接到无监督网络。由于与其他层使用的非线性不同，softmax回归对应于多项式对数输出变量。结果，它能够在几个AD阶段之间对样本进行分类;它还简化了训练的微调阶段，因为softmax层可以与隐藏层共同优化。我们还研究了将微调功能转移到除嵌入式softmax回归器之外的流行分类器的框架设计。有趣的是，我们将深度学习网络学到的相同高级功能作为输入，所有被调查的分类器都倾向于做出高度一致的决策。</p><h3 id="B-Limitations-and-Future-Work"><a href="#B-Limitations-and-Future-Work" class="headerlink" title="B. Limitations and Future Work"></a><em>B. Limitations and Future Work</em></h3><p>考虑到可用神经成像数据的数量有限，我们假设可以使用可能具有较小方差的更多训练样本进一步提取不同生物标记之间的协同作用。所提出的数据融合策略遵循训练自动编码器的去噪方式，理论上增加了特征学习的难度，但控制了过度拟合。尽管四类AD分类的预测概率分布在决策系统中可能更具实际用途，但是在多类分类框架应用于临床应用之前，我们应该改进用可用数据集实现的性能。 。我们比较我们的方法的所有方法都倾向于过度拟合，但在训练集上具有高精度并且在测试集上具有低精度。由于具有神经网络（2SAE-MKSVM和SAE-ZEROMASK）的多模式学习架构是参数模型，我们假设当有更大的数据集时，它们可能有可能在多类AD诊断上获得更好的诊断准确性。这将允许以较低的方差更好地提取与主题无关的特征。<br>五，结论<br>我们提出了一种新的嵌入式深度学习AD诊断框架。该框架可以区分AD进展的四个阶段，并且需要较少的临床先验知识。由于无监督的特征表示嵌入在此工作流程中，因此在实践中有可能扩展到更多未标记的特征工程数据。在无人监督的预训练阶段，我们使用SAE来获得高级特征。当使用多种神经影像学模型时，我们应用零掩蔽策略来提取去噪方式之后不同模态之间的协同作用。在无监督的特征工程之后，使用softmax回归。我们使用了一种可视化高级脑生物标志物的新方法来分析提取的高级特征。<br>在第二阶段和第四阶段之间对AD分类进行了评估。基于MR和PET ADNI数据存储库，我们的框架优于最先进的基于SVM的方法和其他深度学习框架。因此，我们认为，所提出的方法可以成为代表多模态神经成像生物标志物的有力手段。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease-翻译</title>
    <link href="http://yoursite.com/2019/07/19/Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer%E2%80%99s-Disease-%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/07/19/Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease-翻译/</id>
    <published>2019-07-19T05:02:49.000Z</published>
    <updated>2019-07-19T05:14:51.293Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p><a href="https://scholar.google.com/scholar?q=Multimodal+Neuroimaging+Feature+Learning+With+Multimodal+Stacked+Deep+Polynomial+Networks+for+Diagnosis+of+Alzheimer&#39;s+Disease&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart" target="_blank" rel="noopener">Multimodal Neuroimaging Feature Learning With Multimodal Stacked Deep Polynomial Networks for Diagnosis of Alzheimer’s Disease</a>  多模态神经影像学特征学习多模态叠加深度多项式网络诊断阿尔茨海默病</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>阿尔茨海默病（AD）及其早期阶段（即轻度认知障碍）的准确诊断对于及时治疗和可能的AD延迟至关重要。多模态神经影像数据的融合，如磁共振成像（MRI）和正电子发射断层扫描（PET），已显示其对AD诊断的有效性。深度多项式网络（DPN）是最近提出的深度学习算法，其在大规模和小尺寸数据集上都表现良好。在这项研究中，提出了一种多模式堆叠DPN（MM-SDPN）算法，MM-SDPN由两级SDPN组成，用于融合和学习用于AD诊断的多模态神经成像数据的特征表示。具体而言，两个SDPN首先用于分别学习MRI和PET的高级特征，然后将其输入另一个SDPN以融合多模态神经影像信息。建议的MM-SDPN算法应用于ADNI数据集，以进行二进制分类和多类分类任务。实验结果表明，MM-SDPN优于最先进的基于多模态特征学习的算法用于AD诊断。</p><p>Index Terms—Alzheimer’s disease, deep learning, deep polynomial networks, multimodal stacked deep polynomial networks, multimodal neuroimaging.</p><h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><p>ALZHEIMER’S DISEASE（AD）是最常见的进行性神经退行性脑疾病之一，导致全世界老年人逐渐，不可逆转的记忆丧失和其他认知功能[1]。预计到2050年世界范围内AD患者人数将增加到1亿[2]。由于AD患病率的急剧增加，AD的准确诊断及其早期即轻度认知障碍（MCI）对于及时治疗和可能的AD延迟至关重要。<br>在过去的几十年中，神经成像技术，如磁共振成像（MRI），功能磁共振成像（fMRI）和正电子发射断层扫描（PET），已经深入推进了神经科学研究和临床应用[3]，[4]。可识别的成像生物标记物已被有效地用于AD的诊断或预后，这是由于它们通过神经成像可视化和定量测量脑结构和功能信息的优点[5]  -  [7]。<br>近年来，基于神经影像学的AD计算机辅助诊断（CAD）引起了人们的广泛关注[4]，[8]  -  [10]。此外，由于不同的神经影像学模型，如MRI和PET，可以提供不同的和互补的信息，以提高AD的诊断性能[4]，[11]，[12]，基于多模态神经影像学的AD分类具有引起了相当多的关注[4]，[13]  -  [18]。<br>CAD中机器学习算法的性能通常取决于数据表示（或特征），因此特征提取成为分类框架中的关键步骤[10]，[19]。用于神经成像数据的最常用的特征提取方法可以大致分为四类[8]，[20]：1）基于体素的方法，简单直接从体素强度中提取特征; 2）基于顶点的方法，其特征在皮质表面的顶点水平处定义; 3）基于感兴趣区域（ROI）的方法，从预定义的大脑区域提取特征; 4）基于补丁的方法，从本地补丁中学习新的特征表示。基于体素和顶点的特征通常具有非常高的维度，因此降低维数对于实现更紧凑和有效的特征是重要的。基于ROI的功能被广泛使用，因为它们不仅具有相对较低的特征维度，而且还覆盖整个大脑。然而，从ROI中提取的特征有些粗糙，并且不能反映脑疾病中涉及的微小或微妙变化。基于补丁的特征是从整个大脑中学习的，并且可以有效地捕获与疾病相关的病理。结果，这些特征通常可以获得更好的分类结果[20]  -  [24]。基于补丁的特征的方式本质上是一种特征重新表示的过程，用于学习，然后利用学习算法生成新的特征空间，其中深度学习（DL）已经实现了最先进的性能 [20]，[24]。</p><p>DL在各种应用中取得了巨大的成功，因为它是由Hinton等人首次引入的。在2006年[25]，[26]。与传统的浅层结构学习架构相比，DL开发了一种分层的分层架构，以产生高水平和更有效的数据表示[19]，[27]，[28]。近年来，DL在医学成像领域获得了良好的声誉，例如医学图像分类，检测和分割[29]  -  [35]。<br>在神经成像数据的情况下，DL可以有效地发现潜在或隐藏的表示，因此它已成功应用于AD和其他脑部疾病的诊断。 Suk等人。使用多模式深度限制玻尔兹曼机（RBM）来学习多模神经影像数据中的巨大3D斑块的特征，用于AD / MCI诊断[20]。古普塔等人。使用堆叠自动编码器（SAE）从自然图像中学习一组基础，然后应用卷积网络以获得更有效的AD分类特征表示[23]。 Brosch等人。通过深度置信网络（DBN）算法学习了一个低维数量的脑容量，以检测与AD的人口统计学和疾病参数相关的变异模式[36]。谢尔盖等人。还使用RBM和DBN来学习MRI和fMRI用于精神分裂症诊断的特征[34]。 Ithaput等。开发了一种随机去噪自动编码器算法，用于学习AD临床试验的高维神经影像学特征[37]。刘等人。还提出了一种基于SAE的多模态神经成像特征学习算法，用于学习基于ROI的AD诊断特征的特征表示[38]。 Payan和Montana结合稀疏自动编码器和卷积神经网络来学习AD预测局部斑块的特征表示[24]。 Suk和Shen将SAE与多任务特征选择和多核学习（MKL）算法结合起来，以学习用于AD分类的多模态神经影像数据和脑脊液（CSF）特征的ROI特征的潜在特征表示[39]。他们还提出了一种深层体系结构，以分层方式选择具有稀疏多任务学习功能的AD诊断功能[40]。李等人。开发了一种基于丢失技术的稳健多任务深度学习框架，以改善ADI / MCI诊断的ROI特征的表示[41]。陈等人。提出从纵向T1 MRI图像中提取特征，然后应用堆叠去噪稀疏自动编码器（SDSAE）融合这些特征以进行AD分期分析[42]。<br>另一方面，深度多项式网络（DPN）是一种新开发的有监督DL算法，其中每个节点计算其输入的线性或二次函数，因此学习的预测变量是输入空间上的多项式函数[43]。它不依赖于复杂的启发式，并且易于实现。与DBN算法相比，DPN在一些常用的大规模图像数据集上也取得了竞争性能[43]。值得注意的是，DPN还可以紧凑地表示有限样本数据集上的任何函数，因为其算法结构最初是针对小数据集开发的[43]。</p><p>由于神经影像数据集通常具有较小的标记样本[10]，作为小数据集的适当特征表示算法，受监督的DPN将潜在地从用于AD诊断的小神经影像数据中学习优越的特征表示。另一方面，特征提取的逐层堆叠通常在DL [19]中产生更好的表示，例如DBN和SAE [25]，[44]，这促使开发堆叠DPN（SDPN）算法以学习更高级别的特征表示。此外，已经证明，可以同时学习和融合多模态神经影像数据的多模式DL算法优于用于AD分类的单模态DL算法[20]，[38]，[39]，[41]。因此，值得研究多模堆叠DPN算法。<br>在这项工作中，提出了由上述因素驱动的多模式堆叠DPN（MM-SDPN）算法，然后将其应用于基于多模态神经成像的AD诊断。 MM-SDPN可以有效地融合和学习多模态神经影像学中的特征表示。<br>本文的其余部分安排如下。第二节提供了有关原始DPN和提议的MM-SDPN算法的介绍。所进行的实验和结果在第III节中给出，以评估所提出的MM-SDPN算法的性能。讨论和结论分别在第IV和V节中介绍。</p><h2 id="II-METHODS"><a href="#II-METHODS" class="headerlink" title="II. METHODS"></a>II. METHODS</h2><h3 id="A-Deep-Polynomial-Networks-Algorithm"><a href="#A-Deep-Polynomial-Networks-Algorithm" class="headerlink" title="A. Deep Polynomial Networks Algorithm"></a>A. Deep Polynomial Networks Algorithm</h3><h3 id="B-Stacked-Deep-Polynomial-Networks"><a href="#B-Stacked-Deep-Polynomial-Networks" class="headerlink" title="B. Stacked Deep Polynomial Networks"></a>B. Stacked Deep Polynomial Networks</h3><h3 id="C-Multimodal-Stacked-DPN"><a href="#C-Multimodal-Stacked-DPN" class="headerlink" title="C. Multimodal Stacked DPN"></a>C. Multimodal Stacked DPN</h3><p>为了通过SDPN融合和学习来自多媒体数据的特征表示，一个简单的解决方案是连接不同模态的所有向量。然而，这种简单的连接策略在一定程度上忽略了多种模态的多样性，并且不能很好地探索互补性质并且代表多种模态之间的高度非线性相关性。因此，我们提出了一种基于两级SDPN的MM-SDPN算法，如图4所示。<br>在第一阶段，每个神经影像数据将被馈送到其相应的SDPN模块，以学习高级特征表示。每种特定模态的高级特征反映了它自己的属性，但不同模态之间没有相关信息。然后，所有学习的特征在第二阶段被馈送到新的SDPN模块，以便与所有模态相关联。因此，最终学习的高级特征既包含每种模态的内在属性，也包含所有模态之间的相关性。因此，SDPN学到的特征更具有辨别力和鲁棒性。<br>在这项工作中，我们采用MRI和PET作为两种神经影像模式，这些模式常用于基于多模态神经影像学的AD分类[13]  -  [18]，[38]。值得注意的是，MM-SDPN中的融合策略与[38]中基于多模SAE的算法中的融合策略不同，以发现MRI和PET之间的协同作用。在[38]中，刘等人。采用预训练方法和一定比例的损坏输入，只有一种模态。特别是，他们通过将这些输入设置为0来随机隐藏一种模态，然后使用两种模态呈现其余训练样本。第一级AE中的隐藏层经过训练，可以重新组合输入中隐藏模态的所有原始输入。原始输入和损坏的输入都独立地传播到更高的网络层以实现清洁表示。在我们的MM-SDPN算法中，由于DPN在每个网络层中执行前馈监督学习而没有精细转向，因此难以执行与[38]中相同的学习策略来推断MRI和PET之间的相关性。因此，通过联合训练第二阶段SDPN与在第一阶段中学习的级联MRI和PET特征来学习共享表示。它类似于[42]中使用的简单融合方法。</p><h2 id="III-EXPERIMENTS-AND-RESULTS"><a href="#III-EXPERIMENTS-AND-RESULTS" class="headerlink" title="III. EXPERIMENTS AND RESULTS"></a>III. EXPERIMENTS AND RESULTS</h2><h3 id="A-Neuroimaging-Data-Preprocessing"><a href="#A-Neuroimaging-Data-Preprocessing" class="headerlink" title="A. Neuroimaging Data Preprocessing"></a>A. Neuroimaging Data Preprocessing</h3><p>为了评估提出的MM-SDPN算法的性能，这里使用来自阿尔茨海默病神经影像学倡议（ADNI）数据库的多模态神经影像学数据（<a href="http://www.loni.ucla.edu/ADNI）[45]。根据参考文献[14]，我们使用了相同的MRI和PET图像来自51名AD患者，99名MCI患者（43名MCI转换器（MCI-C），进展至AD，以及56名MCI非转换器（MCI-NC），谁没有在18个月内进展到AD，和52个正常对照（NC）具有相同的预处理和特征生成方法。" target="_blank" rel="noopener">www.loni.ucla.edu/ADNI）[45]。根据参考文献[14]，我们使用了相同的MRI和PET图像来自51名AD患者，99名MCI患者（43名MCI转换器（MCI-C），进展至AD，以及56名MCI非转换器（MCI-NC），谁没有在18个月内进展到AD，和52个正常对照（NC）具有相同的预处理和特征生成方法。</a><br>具体而言，预处理首先在MRI图像上进行，包括前连合（AC） - 后连合（PC）校正，N3算法强度不均匀[46]，以及由小脑提取的颅骨剥离和去除小脑。王等人。 [47]，[48]。然后通过FSL包中的FAST算法将MR图像分割成三种不同的组织，即灰质，白质和脑脊液[49]。在通过HAMMER算法[50]注册后，每个MR图像被分成93个ROI，基于模板，Kabani等人使用93个手动标记的ROI。 [51]。然后计算灰质组织的体积作为每个ROI的特征，产生93个特征。然后通过刚性配准将每个PET图像与其对应的MRI图像对准。将相同ROI的平均强度计算为PET图像的特征。因此，分别从MRI和PET图像中提取93个特征。</p><h3 id="B-Performance-Evaluation"><a href="#B-Performance-Evaluation" class="headerlink" title="B. Performance Evaluation"></a>B. Performance Evaluation</h3><p>执行四个分类任务，即AD与NC，MCI与NC，MCI-C与MCI-NC，以及AD与MCI-C对比MCI-NC对NC。<br>首先将提出的MM-SDPN算法与以下原始DPN和SDPN算法进行比较：（1）原始DPN分别从原始MRI和PET特征（称为DPN-3 MRI和DPN）的3层网络学习的特征-3-PET）; （2）原始DPN分别从原始MRI和PET特征（称为DPN-6-MRI和DPN-6-PET）获得的6层网络学习的特征; （3）SDPN分别从原始MRI和PET特征中学习的特征（称为SDPN-MRI和SDPN-PET）; （4）SDPN从连锁的MRI和PET特征（称为SDPN-MRI-PET）中学到的特征; （5）MM-SDPN从原始MRI和PET特征中学到的特征。<br>我们还将提出的MM-SDPN算法与用于AD分类的九种最先进的基于多模态学习的算法进行了比较，如表I所示。值得注意的是ADNI的MRI和PET子集以及提取的ROI特征。在我们的工作中，与参考文献[14]，[18]，[39]，[40]和[41]中使用的数据和特征相同。<br>在这项工作中，SDPN由2级基本DPN堆叠，每个基本DPN由3层网络组成。值得注意的是，上述SDPN也用于MM-SDPN。原始DPN算法主要有两个参数，即网络层数和每层隐藏节点数。在设置3层和6层DPN之后，我们通过对训练数据集进行贪婪搜索得到DPN-3算法中的数字隐藏节点，然后将它们作为所有其他基于DPN的算法的初始参数。进一步微调。<br>在这项工作中，SDPN由2级基本DPN堆叠，每个基本DPN由3层网络组成。值得注意的是，上述SDPN也用于MM-SDPN。原始DPN算法主要有两个参数，即网络层数和每层隐藏节点数。在设置3层和6层DPN之后，我们通过对训练数据集进行贪婪搜索得到DPN-3算法中的数字隐藏节点，然后将它们作为所有其他基于DPN的算法的初始参数。进一步微调。<br>两种分类器，即嵌入式线性分类器（LC）和线性支持向量机（SVM）[53]，被用来更全面地评估基于DPN的特征的性能。 SVM使用LIBSVM工具箱[54]执行。从单模式MRI或PET提取的原始ROI特征仅由SVM分类器处理。<br>对所有算法执行10倍交叉验证策略，并且该过程独立重复5次，以避免在交叉验证中随机分区数据集引入的采样偏差。选择分类准确度（ACC），灵敏度（SEN）和特异性（SPE）作为评估指标。分类结果由所有50个结果的平均值±SD（标准偏差）的格式给出。此外，接收器操作特性（ROC）曲线和ROC曲线下面积值（AUC）也用于SVM分类器。</p><h3 id="C-Results-on-AD-vs-NC"><a href="#C-Results-on-AD-vs-NC" class="headerlink" title="C. Results on AD vs. NC"></a>C. Results on AD vs. NC</h3><p>在AD和NC的分类中，表II显示了使用SVM分类器的不同特征学习算法的分类结果。可以发现MM-SDPN算法达到最佳性能，平均分类精度为97.13±4.44％，灵敏度为95.93±7.84％，特异性为98.53±5.05％，因为它成功地融合了MRI和PET信息。另一方面，尽管DPN-6（具有6层网络的DPN）在基于单一模态成像的AD分类的MRI和PET数据上优于DPN-3，但SDPN仍然比DPN-6略好，这表明其有效性由于堆叠技术的SDPN。<br>图5显示了不同算法的ROC曲线及其相应的AUC值。 MM-SDPN（0.972）的AUC值优于其他值。<br>如表III所示，当应用线性分类器时，不同的算法具有与表II中相似的趋势，这表明它不是分类器，而SPDN在最终分类性能中起关键作用。所提出的MM-SDPN算法再次获得最佳分类精度96.93±4.53％，灵敏度95.02±8.56％，特异性98.37±5.66％。 SDPN算法也优于原始DPN。在这里，DPN-6对于MRI和PET数据仍然优于DPN-3，但是SDPN比DPN-6获得了更好的结果。<br>表IV给出了所提出的MM-SDPN和其他最先进的基于多模态学习的算法的比较结果。 MM-SDPN在所有评估指标上都优于所有其他具有SVM和线性分类器的算法。与其他非SDPN多模态学习算法相比，具有SVM的MM-SDPN在准确度，灵敏度和特异性方面分别提高了至少1.23％，1.22％和2.20％，这表明了所提出的MM-SDPN算法的有效性。融合并学习MRI和PET数据的特征表示，用于AD分类。值得注意的是，提出的MM-SDPN算法优于[14]，[18]，[39]，[40]和[41]中的这些算法，在具有相同ROI的ADNI的相同MRI和PET子集上。特征。</p><h3 id="D-Results-on-MCI-vs-NC"><a href="#D-Results-on-MCI-vs-NC" class="headerlink" title="D. Results on MCI vs. NC"></a>D. Results on MCI vs. NC</h3><p>表V和VI分别显示了使用SVM和线性分类器对MCI与NC分类的不同基于DPN的算法的结果。可以看出，MM-SDPN仍然优于SVM和线性分类器的所有其他方法。最佳分类准确度，灵敏度和特异度分别为87.24±4.52％，97.91±4.17％和67.04±9.29％，SVM分别为86.99±4.82％，94.24±6.16％和71.32±9.93％。线性分类器。尽管DPN-6优于DPN-3，但SDPN在基于单模态成像的MRI和PET数据上的MCI分类方面优于DPN。<br>图6显示了不同算法的ROC曲线及其相应的AUC值，MM-SDPN达到了第二次性能，ROC值为0.901。<br>值得注意的是，在表V和VI中，由于样本不平衡问题，MCI患者的样本是NC受试者的两倍，因此灵敏度远高于MCI对NC的特异性。另一方面，对于MCI与NC，PET的结果优于MRI，这可以通过相对生物标志物的时间排序的理论模型来解释[55]。具体而言，FDG-PET代谢测量的变化先于MCI患者MRI脑结构的变化[55]。<br>如表VII所示，MM-SDPN再次优于比较的基于多模态学习的算法。与其他非SDPN算法相比，具有SVM的MM-SDPN分别在准确度和灵敏度方面提高了1.57％和2.54％。只有[38]中SAE的特异性非常高，但准确度和灵敏度都很低。</p><h3 id="E-Results-on-MCI-C-vs-MCI-NC"><a href="#E-Results-on-MCI-C-vs-MCI-NC" class="headerlink" title="E. Results on MCI-C vs. MCI-NC"></a>E. Results on MCI-C vs. MCI-NC</h3><p>众所周知，MCI-NC的MCI-C分类通常很困难，如先前的工作[18]，[20]，[41]所示。表VIII和IX给出了具有不同分类器的MCI-C对MCI-NC的不同基于DPN的算法的结果。可以观察到MM-SDPN再次获得最佳结果，其SVM分类器的平均分类准确度，灵敏度和特异性分别为78.88±4.38％，68.04±9.99％和86.81±9.12％，76.52±5.99线性分类器分别为％，62.50±10.65％和86.27±7.49％。对于这项困难的任务，SDPN对于单一模态神经成像数据的性能要比DPN-6和DPN-3好得多。<br>在图7中示出了不同算法的ROC曲线及其相应的AUC值，并且MM-SDPN再次实现了AUC值为0.801的最佳性能。<br>从表X可以看出，对于如此困难的分类任务，所提出的MM-SDPN仍然优于所有其他非SDPN多模态特征学习算法，提高了准确度和灵敏度至少2.96％和1.99％，分别用SVM分类器。基于深度RBM的算法实现了最佳特异性，但灵敏度较低。</p><h3 id="F-Results-on-AD-vs-MCI-C-vs-MCI-NC-vs-NC"><a href="#F-Results-on-AD-vs-MCI-C-vs-MCI-NC-vs-NC" class="headerlink" title="F. Results on AD vs. MCI-C vs. MCI-NC vs. NC"></a>F. Results on AD vs. MCI-C vs. MCI-NC vs. NC</h3><p>AD与MCI-C与MCI-NC与NC的多类分类是一项非常艰巨的任务。 表XI和XII的结果与其他二元分类的结果相似，表明MM-SDPN的分类精度为57.00±3.65％，灵敏度为53.65±4.04％，SVM的特异性为85.05±1.39％。 线性分类器的相应结果为55.34±4.57％，52.49±6.12％和84.18±1.99％。 同样，SDPN仍然优于DPN。<br>此外，如表XIII所示，与最先进的基于SAE的算法[38]相比，我们提出的具有SVM分类器的MM-SDPN算法实现了分类精度提高3.21％和灵敏度提高1.51％。</p><h2 id="IV-DISCUSSION"><a href="#IV-DISCUSSION" class="headerlink" title="IV. DISCUSSION"></a>IV. DISCUSSION</h2><p>在这项工作中，我们提出了一种MM-SDPN算法，可以有效地学习基于多模态神经影像学的AD诊断的特征。 ADNI数据集上的四组实验结果表明，与最先进的基于多模态学习的算法相比，所提出的MM-SDPN算法实现了最佳性能。特别，<br>在我们的研究中，原始ROI特征是低级特征，不能以良好的差异化方式表示AD的属性。当应用DPN来学习ROI特征的特征时，提供了更复杂的表示，因此DPN已经实现了显着的改进。在多次堆叠基本DPN块之后，获得更高级别的表示。因此，对于基于单模态神经成像的AD分类，SDPN比原始DPN具有更好的性能。值得注意的是，尽管随着隐层的增加，6层DPN优于3层DPN，但是根据神经网络中的通用逼近定理，太深的网络将增加计算复杂度，而近似的精度没有明显增加。工作。另一方面，结果还表明，具有两个3层DPN的SDPN优于6层DPN，因为第二层基本DPN中第一层的基础建立在更高级别的特征上，即串联特征第一级基本DPN，这个基础将在学习二级DPN后生成更有效和更高级别的功能。此外，与具有更深网络的DPN相比，SDPN更容易调整参数以实现相同的性能。<br>由于多模态神经影像数据的融合可以有效地有利于AD诊断的分类性能，因此提出了一种MM-SDPN算法，该算法由两阶段SDPN组成。两个SDPN分别应用于MRI和PET的ROI特征，以获得第一阶段中每种模态的摘要。然后将两个学习的特征集中并馈送到新的SDPN以学习融合特征，其包括每种模态的固有特性以及MRI和PET之间的相关性。与直接将单个SDPN应用于MRI和PET的集中ROI特征的方式相比，第一阶段DPN学习的高级特征将有益于并提高MM-SDPN中第二阶段SDPN的学习性能，因此， MM-SDPN具有更有效的学习和融合多模态神经影像数据的能力。<br>在这项研究中，两个分类器，即SVM和线性分类器，用于评估SDPN和MM-SDPN的性能。两个分类器都给出了类似的结果，这表明AD分类的良好性能更多地取决于学习的特征而不是分类器。因此，MM-SDPN真正有效地学习了一个好的特征表示。<br>DPN的三个主要特性有助于MM-SDPN的优异性能如下。 （1）神经影像数据集通常具有小的标记样本。当将DPN应用于如此小的数据集时，其构建的网络具有小节点，这保证了训练有素的深度网络和小样本[42]。 （2）在DPN网络中，第一个k层的节点构成了由k次多项式获得的所有值的基础。因此，DPN网络可能具有较大的偏差，但往往不会过度拟合（即低方差），即使对于较深的网络，偏差也会随着方差的增加而逐渐减小。因此，原则上，可以通过控制偏差 - 方差权衡来抑制过度拟合[43]。此外，DPN中的中间层连接非常稀疏，即中间层中的每个节点仅限于连接到少数其他节点，而不是前一层中的所有节点，这可以防止过度拟合[43]。因此，DPN的算法结构使其适用于小型数据集。 （3）由于神经影像数据通常仅提供有限的标记地面实况样本，并且先验标签信息有利于小数据的分类任务，因此监督DPN比不受控制的DL算法更适合于小神经成像数据集。此外，如工作中所示，所提出的MM-SDPN优于[38]和[39]中基于精细转向的监督SAE算法。</p><p>DPN是一种新的DL算法，其理论和算法的改进仍然很少。因此，在未来的工作中，我们不仅要进一步改进DPN算法，还要更加注重分析DPN的框架，特别是DPN与其他DL算法的区别。另一方面，本工作中提出的MM-SDPN算法显示了其对小数据集的有效性。事实上，DPN已经在[43]中从大规模数据中学习特征方面获得了良好的声誉。由于在连续的基本DPN之间没有前向和后向反馈，因此SDPN和MM-SDPN相对简单且快速。因此，它们也有望用于大规模数据。在未来的工作中，我们计划应用MM-SDPN直接从局部MRI和PET片段学习特征表示，这与[20]中的方法类似。此外，未来还将研究半监督MM-SDPN，因为获取有助于提高表征学习性能的未标记医学图像相对容易。由于MKL成功应用于多模态神经影像学数据，我们将尝试将MKL与MM-SDPN结合用于AD分类。来自第二阶段SDPN的学习特征和来自第一阶段SDPN中的MRI和PET的各个学习特征可以被视为MKL的多视图数据。因此，CSF功能也可以嵌入到MKL中，这有可能通过基于MM-SDPN和MKL的框架提高AD分类的性能。</p><h2 id="V-CONCLUSION"><a href="#V-CONCLUSION" class="headerlink" title="V. CONCLUSION"></a>V. CONCLUSION</h2><p>在这项工作中，提出了一种MM-SDPN算法。 它由两阶段SDPN组成，可以有效地学习和融合多模态数据，用于诊断阿尔茨海默病。 MM-SDPN实现了最先进的表现，用于对AD进展的两个阶段和四个阶段进行分类。 因此，所提出的MM-SDPN不仅可以作为多模神经成像数据的强大表示算法，还可以用于其他医学数据。</p><h3 id="ACKNOWLEDGMENT"><a href="#ACKNOWLEDGMENT" class="headerlink" title="ACKNOWLEDGMENT"></a>ACKNOWLEDGMENT</h3><p>作者要感谢阿尔茨海默氏病神经影像倡议为本文提供数据。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译</title>
    <link href="http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/</id>
    <published>2019-07-19T03:39:20.000Z</published>
    <updated>2019-07-19T05:01:41.104Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p><a href="https://arxiv.org/abs/1808.06452" target="_blank" rel="noopener">《Reproducible evaluation of classification methods in Alzheimer’s disease: Framework and application to MRI and PET data》</a>  阿尔茨海默病分类方法的可行性评估：MRI和PET数据的框架和应用</p><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>大量论文引入了新颖的机器学习和自动特征提取方法<br>阿尔茨海默病（AD）的分类。然而，虽然绝大多数这些作品使用公共数据集ADNI进行评估，但它们很难再现，因为验证的不同关键组件通常不易获得。这些组件包括选定的参与者和输入数据，图像预处理和交叉验证程序。不同方法的表现也难以客观地比较。特别地，通常难以评估方法的哪个部分（例如，预处理，特征提取或分类算法）提供真正的改进（如果有的话）。在本文中，我们使用三个公开可用的数据集（ADNI，AIBL和OASIS）提出了AD中可重复和客观分类实验的框架。该框架包括：i）将三个数据集自动转换为标准格式（BIDS）; ii）模块化的预处理流水线，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准。我们使用T1 MRI和FDG PET数据证明该框架用于对1960名参与者进行大规模评估。在此评估中，我们评估不同模态，预处理，要素类型（基于区域或体素的特征），分类器，训练集大小和数据集的影响。表演符合最新技术水平。对于所有分类任务，FDG PET优于T1 MRI。使用不同的图册，图像平滑，FDG PET图像的部分体积校正或特征类型没有发现性能差异。线性SVM和L2逻辑回归导致相似的性能，并且都优于随机森林。分类性能随着用于训练的受试者数量而增加。在ADNI上训练的分类器很好地适用于AIBL和OASIS。框架和实验的所有代码都是公开的：通用工具已集成到Clinica软件（<a href="http://www.clinica.run）中，特定于纸张的代码可从以下网址获得：https：//gitlab.icm-institute" target="_blank" rel="noopener">www.clinica.run）中，特定于纸张的代码可从以下网址获得：https：//gitlab.icm-institute</a> .ORG / aramislab / AD-ML。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>阿尔茨海默病（AD）影响全世界超过2000万人。早期识别AD对于充分护理患者和测试新疗法非常重要。神经影像学提供了识别AD的有用信息（Ewers等，2011）：由于灰质丢失引起的萎缩，解剖学磁共振成像（MRI），18F-氟脱氧葡萄糖正电子发射断层扫描（FDG PET）的低代谢，淀粉样蛋白的积累 - β蛋白与淀粉样蛋白PET成像。然后，主要关注的是分析这些标志物以在早期识别AD。特别是，机器学习方法有助于通过学习神经影像数据的判别模式来帮助识别AD患者。<br>已经提出了大量的机器学习方法来分类和预测AD阶段（参见（Falahati等人，2014; Haller等人，2011; Rathore等人，2017）以进行评论）。他们中的一些使用单一成像模式（通常是解剖学MRI）（Cuingnet等人，2011; Fan等人，2008; Klo€ppel等人，2008; Liu等人，2012; Tong等人。 （2014）和其他人提出结合多种方式（MRI和PET图像，流体生物标记物）（Gray等，2013; Jie等，2015; Teipel等，2015; Young等，2013; Yun et al。，2015; Zhang et al。，2011）。验证和比较这些方法需要随着时间的推移跟踪大量患者。大量已发表的作品使用了公开的阿尔茨海默病神经影像学倡议（ADNI）数据集。然而，他们的结果之间的客观比较几乎是不可能的，因为它们在以下方面不同：i）患者的子集（选择标准的规范不清楚）; ii）图像预处理管道（因此不清楚优越性能是来自分类还是预处理）; iii）特征提取和选择; iv）机器学习算法; v）交叉验证程序和vi）报告的评估指标。由于这些差异，很难得出哪些方法表现最佳，甚至给定模态是否提供有用的附加信息。结果，这些作品的实际影响仍然非常有限。此外，绝大多数这些作品使用ADNI数据集（ADNI1用于早期论文，最常见的是ADNI1，ADNI-GO和ADNI2的组合用于更近期的工作）。因此，即使存在其他公开可用的数据集，例如澳大利亚成像生物标记物和生活方式研究（AIBL）以及开放获取系列成像研究（OASIS），也很少对其他数据集的泛化进行评估。<br>比较论文（Cuingnet et al。，2011; Sabuncu et al。，2015）和挑战（Allen et al。，2016; Bron et al。，2015）通过允许基准测试，成为客观评估机器学习方法的重要一步在同一数据集上使用相同的预处理方法。然而，这些研究提供了方法的“静态”评估。评估数据集在研究时以其当前状态使用，而新患者不断包括在ADNI等研究中。同样，它们仅限于研究时使用的分类和预处理方法。因此很难用新方法补充它们。<br>在本文中，我们提出了一个可重复评估AD中机器学习算法的框架，并展示了它在从三个公开可用的数据集中获得的PET和MRI数据的分类中的用途：ADNI，AIBL和OASIS。具体来说，我们的贡献是三方面的：</p><p>i）管理公开数据集的框架及其与新主题的持续更新，特别是全脑自动转换为脑成像数据结构4（BIDS）格式的工具（Gorgolewski等，2016）;<br>ii）一组模块化的预处理管道，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准;<br>iii）对来自三个公开可用的神经成像数据集（ADNI，AIBL和OASIS）的T1 MRI和PET数据进行大规模评估。</p><p>我们使用这个框架来证明从三个数据集（ADNI，AIBL和OASIS）获得的T1 MRI和PET数据进行自动分类。 我们评估各种成分对分类性能的影响：模态（T1 MRI或PET），特征类型（体素或区域特征），预处理，诊断标准（标准NINCDS / ADRDA标准或淀粉样蛋白精制标准），分类算法。 首先在ADNI，AIBL和OASIS数据集上独立进行实验，并通过将对ADNI训练的分类器应用于AIBL和OASIS数据来评估结果的推广。<br>框架和实验的所有代码都是公开的：通用工具已经集成到Clinica5（Routier等，2018），这是一个开源软件平台，我们开发用于处理来自神经影像学研究的数据，以及 特定于纸张的代码可从以下网址获得：https：//gitlab.icm-institute.org/aramislab/AD-ML。</p><h2 id="2-Materials"><a href="#2-Materials" class="headerlink" title="2. Materials"></a>2. Materials</h2><h3 id="2-1-Datasets"><a href="#2-1-Datasets" class="headerlink" title="2.1. Datasets"></a>2.1. Datasets</h3><p>用于制备本文的部分数据来自阿尔茨海默病神经影像学倡议数据库（adni.loni.usc.edu）。 ADNI于2003年作为公私合作伙伴关系启动，由首席研究员Michael W. Weiner博士领导。 ADNI的主要目标是测试是否可以将连续MRI，PET，其他生物标志物以及临床和神经心理学评估结合起来，以测量轻度认知障碍（MCI）和早期AD的进展。在研究的三个阶段（ADNI1，ADNI GO和ADNI2），北美地区招募了超过1650名参与者。大约400名参与者被诊断患有AD，900名患有MCI，350名被诊断为对照受试者。使用三个主要标准对受试者进行分类（Petersen等，2010）。正常受试者没有记忆投诉，而患有MCI和AD的受试者都必须进行投诉。 CN和MCI受试者的迷你精神状态检查（MMSE）评分在24到30之间（包括在内），AD受试者在20到26之间（包括在内）。 CN受试者的临床痴呆评分（CDR）评分为0，MCI受试者为0.5，强制要求记忆盒评分为0.5或更高，AD受试者为0.5或1.其他标准可见于（Petersen等，2010）。<br>我们还使用了AIBL研究组收集的数据。与ADNI类似，澳大利亚影像，生物标志物和生活方式老龄化旗舰研究旨在发现哪些生物标志物，认知特征以及健康和生活方式因素决定了AD的发展。 AIBL已招募1100名参与者并收集了超过4。5年的纵向数据：211名AD患者，133名MCI患者和768名可比较的健康对照。之前已经报道了AIBL研究方法（Ellis等，2010,2009）。简而言之，MCI诊断是根据基于（Winblad等人，2004）标准的方案和NINCDS-ADRDA标准的AD诊断（McKhann等人，1984）进行的。请注意，大约一半被诊断为健康对照的受试者报告存在记忆（Ellis等，2010,2009）。<br>最后，我们使用了开放获取系列成像研究项目的数据，该项目的目的是使科学界免费获得大脑的MRI数据集。我们专注于“年轻，中年，非痴呆和痴呆老年人的横断面MRI数据”（Marcus等，2007），其中包括416名年龄在18到96.100之间的受试者的横断面收集。 60岁以上的受试者已经临床诊断为非常轻度至中度的AD。用于评估诊断的标准是CDR评分。 CDR大于0的所有参与者被诊断为可能的AD。请注意，OASIS中没有MCI主题。</p><h3 id="2-2-Participants"><a href="#2-2-Participants" class="headerlink" title="2.2. Participants"></a>2.2. Participants</h3><h4 id="2-2-1-ADNI"><a href="#2-2-1-ADNI" class="headerlink" title="2.2.1. ADNI"></a>2.2.1. ADNI</h4><p>从ADNI数据集创建了三个子集：ADNIT1w，ADNICLASS和ADNICLASS，Aß。 ADNIT1w包括所有参与者（N = 1628），在基线时T1加权（T1w）MR图像可用。 ADNICLASS包括1159名参与者，他们在基线时可获得具有已知有效分辨率的T1w MR图像和FDG PET扫描。 ADNICLASS，Aß是ADNICLASS的一个子集，包括已知的淀粉样蛋白状态的918名参与者，这些参与者分别使用1.47和1.10作为截止值从PiB或AV45 PET扫描确定（Landau等，2013）。 对于每个ADNI子集，考虑了五个诊断组：</p><ul><li>CN：在基线时被诊断为CN的受试者;</li><li>AD：在基线时被诊断为AD的受试者;</li><li>MCI：在基线时被诊断为MCI，EMCI或LMCI的受试者;</li><li>pMCI：在基线时被诊断为MCI，EMCI或LMCI的受试者在至少36个月内进行随访，并在第一次就诊和36个月就诊之间进展至AD;</li><li>sMCI：在基线时被诊断为MCI，EMCI或LMCI的受试者在至少36个月内被跟踪并且未进展至AD<br>他们的第一次访问和36个月的访问之间。</li></ul><p>当然，pMCI和sMCI小组的所有参与者也都在<br>MCI集团。 请注意，反过来是错误的，因为一些MCI受试者没有转换为AD，但没有足够长时间地说明他们是sMCI还是pMCI。 我们没有考虑具有显着记忆问题（SMC）的受试者，因为此类别仅存在于ADNI 2中。<br>表1-3总结了参与者组成的人口统计学，MMSE和全球CDR分数 $ADNI_{T1w}$, $ADNI_{CLASS}$ and $ADNI_{CLASS, Aß}$.</p><h4 id="2-2-2-AIBL"><a href="#2-2-2-AIBL" class="headerlink" title="2.2.2. AIBL"></a>2.2.2. AIBL</h4><p>在这项工作中考虑的AIBL数据集由608名参与者组成，他们在基线时可获得T1加权MR图像。 用于创建诊断组的标准与用于ADNI的标准相同。 表4总结了AIBL参与者的人口统计学，MMSE和全球CDR得分。</p><h4 id="2-2-3-OASIS"><a href="#2-2-3-OASIS" class="headerlink" title="2.2.3. OASIS"></a>2.2.3. OASIS</h4><p>在这项工作中考虑的OASIS数据集由193名年龄在61岁或以上的参与者组成（参与者被诊断为AD的最低年龄）。 表5总结了OASIS参与者的人口统计学，MMSE和全球CDR分数。</p><h3 id="2-3-Imaging-data"><a href="#2-3-Imaging-data" class="headerlink" title="2.3. Imaging data"></a>2.3. Imaging data</h3><h4 id="2-3-1-ADNI"><a href="#2-3-1-ADNI" class="headerlink" title="2.3.1. ADNI"></a>2.3.1. ADNI</h4><p><strong>2.3.1.1. T1加权MRI.</strong> 3D T1w图像的采集协议可以在ADNI 1的（Jack等人，2008）和ADNI GO / 2的（Jack等人，2010a）中找到。 图像可以在获取时或在经历多个预处理校正步骤之后下载，其中包括校正由于梯度非线性（gradwarp）引起的图像几何失真，校正在执行RF传输时发生的图像强度不均匀性 具有更均匀的体线圈，同时接收使用不均匀的头部线圈（B1不均匀性），并且由于3T处的波或介电效应或1.5的残余强度不均匀性导致的强度不均匀性降低 T扫描（N3）（Jack等，2010a，2008）。</p><p><strong>2.3.1.2. PET.</strong> ADNI FDG PET方案包括动态采集6个5分钟帧（ADNI 1）或4个5分钟帧（ADNI GO / 2），注射后30-60分钟（Jagust等，2015,2010）。 可以下载预处理的不同阶段的图像（帧平均，空间对齐，插值到标准体素大小，以及平滑到8 mm全宽半高的公共分辨率）。 即使没有在实验中使用，也可以获得用于ADNI 1的11C-Pittsburgh化合物B（PIB）和用于ADNI 1 / GO / 2的18F-Florbetapir，也称为AV45，以成像脑中淀粉样蛋白的沉积。。 该方案包括在注射后50至70分钟动态采集4个5分钟的帧（Jagust等，2015,2010）。 至于FDG PET，可以下载预处理的不同阶段的图像。</p><h4 id="2-3-2-AIBL"><a href="#2-3-2-AIBL" class="headerlink" title="2.3.2. AIBL"></a>2.3.2. AIBL</h4><p>用于AIBL受试者的T1w MR图像使用ADNI 3D T1w序列采集，with 1 ✕ 1 mm in-plane resolution，切片厚度为1.2mm，TR/TE/TI=2300/2.98/900，翻转角度为9 和视野240鉁 256和160片（Ellis等，2010）。 尽管他们没有在实验中使用，但也获得了Florbetapir，PiB和Flutemeta-mol PET数据。</p><h4 id="2-3-3-OASIS"><a href="#2-3-3-OASIS" class="headerlink" title="2.3.3. OASIS"></a>2.3.3. OASIS</h4><p>对于每个OASIS主题，三个或四个T1w图像，with 1 ✕ 1 mm in-plane resolution ，切片厚度为1.25 mm，TR/TE/TI = 9.7/4.0/20，翻转角10，视野256 在单次成像会话中，在1.5T扫描仪上获得了鉁256和128个切片（Marcus等，2007）。 对于每个受试者，还可以下载重新采样到1mm各向同性体素的运动校正的共同配准图像的平均值，以及空间标准化图像。</p><h2 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3. Methods"></a>3. Methods</h2><p>我们开发了一套统一的工具，用于数据管理，图像预处理，特征提取，分类和评估。 这些工具已集成到我们开发的Clinica6（Routier等，2018），这是一个开源软件平台。 转换工具允许在新主题可用时轻松更新数据集。 不同的组件以模块化方式设计：使用Nipype处理管道（Gorgolewski等，2011），以及使用scikit-learn7库的分类和评估工具（Pedregosa等，2011）。 这允许开发和测试其他方法作为给定步骤的替代，并且客观测量每个组分对结果的影响。 提供了一个简单的命令行界面，代码也可以用作Python库。</p><h3 id="3-1-Converting-datasets-to-a-standardized-data-structure"><a href="#3-1-Converting-datasets-to-a-standardized-data-structure" class="headerlink" title="3.1. Converting datasets to a standardized data structure"></a>3.1. Converting datasets to a standardized data structure</h3><p>尽管公共数据集非常有价值，但这些研究的一个重要困难在于临床和成像数据的组织。例如，ADNI和AIBL成像数据在下载状态下不依赖于社区标准来进行数据组织和缺乏明确的结构。对于参与者的给定访问存在多个图像采集，并且补充图像信息包含在许多csv文件中，使得对数据库和主题选择的探索非常复杂。为了组织数据，我们选择了BIDS格式（Gorgolewski等，2016），这是一种能够存储多种神经影像模型的社区标准。 BIDS基于文件层次结构而不是数据库管理系统，可以在任何环境中轻松部署。非常重要的是，我们提供的代码可以在下载到BIDS组织版本时自动执行数据转换，用于所有使用的数据集：ADNI，AIBL和OASIS。这允许其他组直接再现，而不必重新分配数据集，这在ADNI和AIBL的情况下是不允许的。我们还根据所需的成像模式，随访持续时间和诊断提供了用于选择受试者的工具，这使得可以使用具有尽可能多的研究对象的相同组。最后，我们提出了一个BIDS启发的标准化结构，用于实验的所有输出。</p><h4 id="3-1-1-Conversion-of-the-ADNI-dataset-to-BIDS"><a href="#3-1-1-Conversion-of-the-ADNI-dataset-to-BIDS" class="headerlink" title="3.1.1. Conversion of the ADNI dataset to BIDS"></a>3.1.1. Conversion of the ADNI dataset to BIDS</h4><p>ADNI到BIDS转换器要求用户下载所有ADNI研究数据（csv格式的表格数据）和感兴趣的成像数据。请注意，下载的文件必须与下载文件完全一致。以下步骤由自动转换器执行（无需用户干预）。要将成像数据转换为BIDS，首先从ADNIMERGE电子表格中获取其会话的主题列表。该列表针对每种感兴趣的模态与可用的扫描列表进行比较，如模态特定的csv文件（例如MRILIST.csv）所提供的。如果针对特定的一对主题会话获取模态，并且几个扫描和/或预处理的图像可用，则仅转换一个。关于T1扫描，当几个可用于单个会话时，选择首选扫描（如MAYOADIRL_MRI_IMAGEQC_12_08_15.csv中所标识）。如果未指定首选扫描，则选择更高质量的扫描（如MRIQUALI-TY.csv中所定义）。如果未找到质量控制，则我们选择第一次扫描。当可用时选择Gradwarp和B1不均匀校正图像，因为这些校正可以在临床环境中进行，否则选择原始图像。对于ADNI 1，1.5T图像是优选的，因为它们可用于更多患者。关于FDG PET扫描，选择在时间帧上共同登记和平均的图像。扫描失败的质量控制（如果在PETQC.csv中指定）将被丢弃。请注意，AV45 PET扫描也会被转换，但不会在实验中使用。一旦选择了感兴趣的图像并识别出图像文件的路径，就可以将成像数据转换为BIDS。当采用dicom格式时，首先使用dcm2niix工具将图像转换为nifti，或者在dcm2nii工具出现错误的情况下（Li et al。，2016）。通过为每个主题创建子文件夹来生成BIDS文件夹结构。在每个主题子文件夹中创建会话文件夹，并在每个会话子文件夹内创建一个模态文件夹。最后，将nifti中的每个图像复制到相应的文件夹并重命名以遵循BIDS规范。临床数据也转换为BIDS。不随时间变化的数据，例如受试者的性别，教育水平或基线诊断，可从ADNIMERGE电子表格中获取，并收集在位于BIDS文件夹层次结构顶部的participant.tsv文件中。依赖于会话的数据，例如临床分数，从特定的csv文件（例如MMSE.csv）获得，并收集在每个参与者子文件夹中的<subjectid> _session.tsv文件中。转换的临床数据在电子表格（clin-ical_specifications_adni.xlsx）中定义，该电子表格可与转换器的代码一起使用。如果用户想要转换其他临床数据，则可以轻松修改该文件。</subjectid></p><h4 id="3-1-2-Conversion-of-the-AIBL-dataset-to-BIDS"><a href="#3-1-2-Conversion-of-the-AIBL-dataset-to-BIDS" class="headerlink" title="3.1.2. Conversion of the AIBL dataset to BIDS"></a>3.1.2. Conversion of the AIBL dataset to BIDS</h4><p>AIBL到BIDS转换器要求用户下载AIBL非成像数据（csv格式的表格数据）和感兴趣的成像数据。将成像数据转换为BIDS依赖于特定于模态的csv文件，这些文件提供可用的扫描列表。对于每个AIBL参与者，每个会话可用的唯一T1w MR图像被转换。请注意，即使它们未用于此项工作，我们也会转换Florbetapir，PiB和Flutemetamol PET图像（每个会话只有一个图像可用于每个会话）。一旦选择了感兴趣的图像并识别出图像文件的路径，就按照上一节中描述的相同步骤将成像数据转换为BIDS。临床数据的转换依赖于在成像数据转换之后和包含非成像数据的csv文件之后获得的受试者和会话的列表。不随时间变化的数据收集在位于BIDS文件夹层次结构顶部的participant.tsv文件中，而会话相关数据收集在每个参与者子文件夹中的<subjectid> _session.tsv文件中。对于ADNI转换器，转换的临床数据在电子表格（clinical_specifications.xlsx）中定义，该电子表格可与转换器的代码一起使用，用户可以修改该代码。</subjectid></p><h4 id="3-1-3-Conversion-of-the-OASIS-dataset-to-BIDS"><a href="#3-1-3-Conversion-of-the-OASIS-dataset-to-BIDS" class="headerlink" title="3.1.3. Conversion of the OASIS dataset to BIDS"></a>3.1.3. Conversion of the OASIS dataset to BIDS</h4><p>OASIS到BIDS转换器要求用户下载OASIS-1成像数据和相关的csv文件。 要将成像数据转换为BIDS，可从下载的文件夹中获取主题列表。 对于每个受试者，在可用的多个T1w MR图像中，我们选择重新采样为1mm各向同性体素的运动校正的共同配准的单个图像的平均值，位于SUBJ_111子文件夹中。 确定图像文件的路径后，使用FreeSurfer的mri_convert工具（Fischl，2012）将Analyze格式的图像转换为nifti，创建BIDS文件夹层次结构，并将图像复制到相应的文件夹 并重命名。 使用成像数据转换后获得的受试者列表和包含非成像数据的csv文件转换临床数据，如上一节所述。</p><h3 id="3-2-Preprocessing-pipelines"><a href="#3-2-Preprocessing-pipelines" class="headerlink" title="3.2. Preprocessing pipelines"></a>3.2. Preprocessing pipelines</h3><p>开发了两条管道来预处理解剖学T1w MRI和PET图像。 这些管道具有基于Nipype的模块化结构，允许用户容易地连接和/或替换组件，并且依赖于使用公开可用的标准图像处理工具的完善的程序。 这些管道在Clinica中以名称t1-volume- *和pet-volume提供。</p><h4 id="3-2-1-Preprocessing-of-T1-weighted-MR-images"><a href="#3-2-1-Preprocessing-of-T1-weighted-MR-images" class="headerlink" title="3.2.1. Preprocessing of T1-weighted MR images"></a>3.2.1. Preprocessing of T1-weighted MR images</h4><p>对于解剖学T1w MRI，预处理管道基于SPM128。首先，统一分割程序（Ashburner和Friston，2005）用于同时执行输入图像的组织分割，偏差校正和空间归一化。接下来，使用DARTEL创建一个组模板，DARTEL是一种用于微分图像配准的算法（Ashburner，2007），来自受试者在原生空间的组织概率图，通常是GM，WM和CSF组织，在之前获得步。这里，不仅获得了组模板，还获得了每个主体的原生空间到DARTEL模板空间的变形字段。最后，应用DARTEL到MNI方法（Ashburner，2007），提供将原生空间图像注册到MNI空间：对于给定主题，其进入DARTEL模板的流场与DARTEL模板到MNI的转换相结合。空间，并将得到的变换应用于对象的不同组织图。结果，所有图像都在共同的空间中，提供跨主体的体素方式对应。</p><h4 id="3-2-2-Preprocessing-of-PET-images"><a href="#3-2-2-Preprocessing-of-PET-images" class="headerlink" title="3.2.2. Preprocessing of PET images"></a>3.2.2. Preprocessing of PET images</h4><p>PET预处理管道依赖于SPM12和PETPVC9工具进行部分体积校正（PVC）（Thomas等，2016）。我们假设每个PET图像具有使用上述管道预处理的相应T1w图像。第一步是使用SPM的Co寄存器方法将PET图像配准到原生空间中的相应T1w图像（Friston等，1995）。可以使用来自原生空间中的T1w的不同组织图作为输入区域来执行具有基于区域体素的（RBV）方法（Thomas等人，2011）的可选PVC步骤。然后，使用与对应的T1w相同的变换将PET图像登记到MNI空间中（使用DARTEL到MNI方法）。然后根据参考区域（FDG PET的侵蚀脑桥）对MNI空间中的PET图像进行强度归一化，并且我们获得标准化的摄取值比（SUVR）图。最后，我们使用二进制掩码来掩蔽非脑区域，所述二进制掩模是通过对MNI空间中的受试者的GM，WM和CSF组织概率图的总和进行阈值处理而得到的。由此产生的蒙版SUVR图像也位于一个公共空间中，并提供跨主体的体素相关性。</p><h3 id="3-3-Feature-extraction"><a href="#3-3-Feature-extraction" class="headerlink" title="3.3. Feature extraction"></a>3.3. Feature extraction</h3><p>从成像数据中提取了两种类型的特征：体素和区域特征。 在预处理之后，T1w MRI和FDG PET图像都在MNI空间中。 对于每个图像，第一类特征简单地对应于大脑中的所有体素。 从T1w MR图像获得的信号是灰质密度，从FDG PET图像获得的信号是SUVR。<br>区域特征对应于在不同地图集中获得的一组感兴趣区域（ROI）中计算的平均信号（灰质密度或SUVR），也在MNI空间中。 选择的五个地图集包含皮质和皮质下区域，并覆盖受AD影响的大脑区域。 它们描述如下：</p><ul><li>AAL2（Tzourio-Mazoyer等，2002）是基于单个受试者的解剖图谱。它是AAL的更新版本，可能是神经影像学文献中使用最广泛的分割图。它是在MNI空间中对空间标准化的单一主体高分辨率T1体积使用手动追踪建立的（Holmes等，1998）。它由覆盖整个皮层的120个区域以及主要的皮质下结构组成。</li><li>AICHA（Joliot等，2015）是基于多个主题的功能性地图集。它是使用由281名健康受试者的静息状态fMRI数据计算的组级功能连接概况的分组构建的。它由345个区域组成，覆盖整体皮质以及主要的皮质下结构。</li><li>Hammers（Gousias等，2008; Hammers等，2003）是一个基于多个科目的解剖图谱。它是使用30名健康受试者的解剖MRI进行手动追踪而建立的。然后将个体受试者分组登记到MNI空间以生成概率图谱以及最大概率图。后者用于目前的工作。它由覆盖整个皮层的69个区域以及主要的皮质下结构组成。</li><li>LPBA40（Shattuck等，2008）是基于多个受试者的解剖图谱。它是使用40名健康受试者的解剖MRI进行手动追踪而建立的。然后将各个主题分组登记到MNI空间以生成最大概率图。它由覆盖整个皮层的56个区域以及主要的皮质下结构组成。</li><li>Neuromorphometrics10是基于多个科目的解剖图谱。它是使用30名健康受试者的解剖MRI进行手动追踪而建立的。然后将各个主题分组登记到MNI空间以生成最大概率图。它由覆盖整个皮层的140个区域以及主要的皮质下结构组成。数据可用于“MIC-CAI 2012大挑战和多图谱标签研讨会”。</li></ul><p>LBPA40，Hammers和Neuro-morphometrics地图集之间的主要区别在于解剖学分割的细节程度（即区域的数量）。</p><h3 id="3-4-Classification-models"><a href="#3-4-Classification-models" class="headerlink" title="3.4. Classification models"></a>3.4. Classification models</h3><p>我们考虑了三种不同的分类器：线性SVM，具有L2正则化的逻辑回归和随机森林，所有这些都可以在Clinica中获得。线性SVM与体素和区域特征一起使用，因为其计算复杂性仅取决于使用其双重形式时的主体数量。另一方面，使用L2正则化和随机森林模型的逻辑回归仅用于基于区域的分析，因为它们的复杂性取决于特征的数量，这对于包含大约100万个体素的图像变得不可行。我们使用了scikit-learn库的实现（Pedregosa等，2011）。<br>对于执行的每个任务，我们获得描述给定特征对当前分类任务的重要性的特征权重。这些权重存储为分类输出的一部分，重建分类器的信息也是如此，如找到的最佳参数。对于每个分类，我们可以获得具有跨脑体素或区域的权重表示的图像。</p><h4 id="3-4-1-Linear-SVM"><a href="#3-4-1-Linear-SVM" class="headerlink" title="3.4.1. Linear SVM"></a>3.4.1. Linear SVM</h4><p>第一种方法是线性SVM。 为了减少计算负荷，使用线性核k对每对图像（xi，xj）（使用区域或体素特征）预先计算Gram矩阵K =（k（xi，xj））i，j 提供科目。 该Gram矩阵用作通用SVM的输入。 我们选择优化误差项的惩罚参数C. SVM的一个优点是，当使用预先计算的Gram矩阵（双SVM）时，计算时间取决于主题的数量，而不取决于特征的数量。 鉴于其简单性，线性SVM可用作比较不同方法的性能的基线。</p><h4 id="3-4-2-Logistic-regression-with-L2-regularization"><a href="#3-4-2-Logistic-regression-with-L2-regularization" class="headerlink" title="3.4.2. Logistic regression with L2 regularization"></a>3.4.2. Logistic regression with L2 regularization</h4><p>第二种方法是使用L2正则化的逻辑回归（通常用于减少过度拟合）。 对于线性SVM，我们优化了误差项的惩罚参数C. 具有L2正则化的逻辑回归直接优化每个特征的权重，并且特征的数量影响训练时间。 这就是我们仅将其用于区域特征的原因。</p><h4 id="3-4-3-Random-forest"><a href="#3-4-3-Random-forest" class="headerlink" title="3.4.3. Random forest"></a>3.4.3. Random forest</h4><p>使用的第三个分类器是随机森林。 与线性SVM和逻辑回归不同，随机森林是一种集合方法，它适合数据集的各个子样本上的许多决策树。 组合估计器可防止过度拟合并提高预测精度。 基于scikit-learn库（Pedregosa等，2011）提供的实现，可以优化大量参数。 在评估哪个影响较大的初步实验后，我们选择了以下两个超参数来优化：i）森林中的树木数量; ii）寻找最佳分割时要考虑的特征数量。 由于计算成本高，随机森林仅用于区域特征而不是体素特征。</p><h3 id="3-5-Evaluation-strategy"><a href="#3-5-Evaluation-strategy" class="headerlink" title="3.5. Evaluation strategy"></a>3.5. Evaluation strategy</h3><h4 id="3-5-1-Cross-validation"><a href="#3-5-1-Cross-validation" class="headerlink" title="3.5.1. Cross-validation"></a>3.5.1. Cross-validation</h4><p>分类性能的评估主要遵循最近的指南（Varoquaux等，2017）。进行交叉验证（CV），维持列车组（用于拟合模型）和测试集（用于评估性能）的独立性的经典策略。 CV过程包括两个嵌套循环：评估分类性能的外循环和用于优化模型超参数的内循环（C用于SVM和L2逻辑回归，树的数量和随机森林的分割特征） 。应当注意，在优化超参数时，使用CV的内环对于避免向上偏置性能是重要的。这一步并不总是在文献中得到适当的执行（Querbes等，2009; Wolz等，2011），导致过度乐观的结果，如（Eskildsen等，2013; Maggi- pinto等。 ，2017）。<br>我们在Clinica实施了三种不同的外部CV方法：k-fold，重复k-fold和重复随机分裂（所有这些分层），使用基于scikit-learn的工具（Pedregosa等，2011）。方法的选择取决于手头的计算资源。但是，只要有可能，建议使用具有大量重复次数的重复随机分组，以获得更稳定的性能估计。因此，我们用于每个实验250次迭代的随机分裂。我们报告评估指标的完整分布以及平均值和经验标准偏差，如（Raamana和Strother，2017）使用神经预测（Raamana，2017）。然而应该指出的是，交叉验证没有无偏差的方差估计（Bengio和Grandvalet，2004; Nadeau和Bengio，2003），经验方差在很大程度上低估了真实的方差。在解释经验方差值时应牢记这一点。此外，我们选择不对不同分类器的性能进行统计测试。这是一个复杂的问题，没有通用的解决方案。在许多出版物中，使用了交叉验证结果的标准t检验。然而，如Nadeau和Bengio（2003）所示，这种方法过于宽松，不应该应用。已经提出了更好的表现方法，例如保守的Z或校正的重采样t检验（Nadeau和Bengio，2003）。但是，必须谨慎使用此类方法，因为它们的行为取决于数据和交叉验证设置。因此，我们选择避免在本文中使用统计检验，以免误导读者。相反，我们报告了指标的完整分布。<br>对于超参数优化，我们实现了内部k折叠。对于每个分割，选择具有最高平衡精度的模型，然后将这些选定的模型在分割中平均以获得模型平均的利润，这应该具有稳定效果。在本文中，对于内环，用k 1/4 10进行实验。</p><h4 id="3-5-2-Metrics"><a href="#3-5-2-Metrics" class="headerlink" title="3.5.2. Metrics"></a>3.5.2. Metrics</h4><p>作为分类的输出，我们报告平衡准确度，ROC曲线下面积（AUC），准确度，灵敏度，特异性，以及每个受试者的预测类别，因此用户可以使用此信息计算其他所需指标。、</p><h3 id="3-6-Classification-experiments"><a href="#3-6-Classification-experiments" class="headerlink" title="3.6. Classification experiments"></a>3.6. Classification experiments</h3><p>在表6中详细列出了由数据可用性驱动的每个数据集的分析中考虑的不同分类任务。有关组成分的详细信息可在表2-5中找到。一般而言，我们执行临床诊断分类任务，或MCI受试者进化的“预测”任务。请注意，由于sMCI和pMCI类别中的参与者数量较少，因此未对AIBL执行涉及从MCI进展到AD的任务。然而，当更多进化的MCI受试者在AIBL中公开可用时，该框架将允许非常容易地进行这些实验。<br>根据特征的类型，测试了几个具有不同参数的分类器的性能。对于体素特征，唯一的分类器是线性SVM。使用高斯核对图像应用四种不同的平滑水平，从无平滑到半高全宽12mm（FWHM）。对于基于区域的分类实验，测试了三种分类器：线性SVM，逻辑回归和随机森林。使用五个地图集提取特征：AAL2，AICHA，Hammers，LPBA40和Neuromorphometrics。表7总结了这些信息。<br>对于所研究的数据集，可获得不同的成像模式：虽然T1W MRI和FDG PET图像均可用于ADNI参与者，但只有T1w MRI可用于AIBL和OASIS参与者。对于所考虑的每种模态，使用表7中详述的不同参数提取体素和区域特征。在该工作中测试的所有分类实验总结在表8中。如果没有另外说明，则从图像中提取FDG PET特征。没有经过PVC的。</p><h2 id="4-Results"><a href="#4-Results" class="headerlink" title="4. Results"></a>4. Results</h2><p>在这里，我们提供了一些我们认为最有价值的结果。 所有实验（包括其他任务，预处理参数，特征或分类器）的完整结果可在补充材料以及包含所有代码和实验的存储库中找到（<a href="https://gitlab.icm-institute.org/aramislab" target="_blank" rel="noopener">https://gitlab.icm-institute.org/aramislab</a> / AD- ML）。 在以下小节中，我们使用平衡准确度作为性能指标来呈现结果，但结果中提供了所有其他指标。</p><h3 id="4-1-Influence-of-the-atlas"><a href="#4-1-Influence-of-the-atlas" class="headerlink" title="4.1. Influence of the atlas"></a>4.1. Influence of the atlas</h3><p>为了评估选择地图集对分类精度的影响并潜在地确定首选地图集，选择了使用区域特征的线性SVM分类器。使用五种不同的图谱提取来自ADNI参与者的T1w MRI和FDG PET图像的特征：AAL2，AICHA，Hammers，LPBA40和Neuro-morphometrics。研究了三个分类任务：CN与AD，CN与pMCI和sMCI与pMCI。<br>如图1所示，没有特定的图集为所有任务提供最高的分类准确度。例如，Neuromorphometrics和AICHA在T1w和FDG PET图像上为CN与AD提供了更好的结果，对于T1w提供了LBPA40，而AAL2在两种成像模式下为CN与pMCI和sMCI与pMCI提供了最高的平衡准确度。对AIBL受试者进行了相同的分析（仅限T1w MR图像），同样地，没有任何图集在任务中始终比其他图谱更好地执行。对于以下基于区域的实验，选择AAL2图谱作为参考图谱，因为它具有良好的分类准确度，并广泛用于神经影像学界。同样，存储库中提供了所有其他结果。</p><h3 id="4-2-Influence-of-the-smoothing"><a href="#4-2-Influence-of-the-smoothing" class="headerlink" title="4.2. Influence of the smoothing"></a>4.2. Influence of the smoothing</h3><p>使用具有4mm，8mm和12mm的FWHM的高斯核不对T1w MRI和FDG PET图像进行平滑或平滑。为了确定不同平滑度对分类精度的影响，选择了使用体素特征的线性SVM分类器。研究了三个分类任务：CN与AD，CN与pMCI和sMCI与pMCI。图2中的结果表明，对于大多数分类任务，平衡精度在很大程度上不随光滑内核大小而变化。当从T1w MR图像中提取特征时，观察到CN与pMCI和sMCI与pMCI任务的唯一变化：平衡精度随着核尺寸略微增加。使用来自AIBL数据集的T1w MR图像进行相同的分析。平均精度也随着核尺寸略有增加，但平衡精度的标准偏差大于ADNI。由于平滑程度对分类性能没有明显影响，我们选择提供与基于体素的分类相关的后续结果，参考平滑度为4 mm。</p><h3 id="4-3-Influence-of-the-type-of-features"><a href="#4-3-Influence-of-the-type-of-features" class="headerlink" title="4.3. Influence of the type of features"></a>4.3. Influence of the type of features</h3><p>我们将使用线性SVM分类器将体素特征的平衡精度与参考平滑（4 mm FWHM的高斯核）与参考图谱（AAL2）的区域特征所获得的平衡精度进行比较。 这些特征是从ADNI参与者的T1w MRI和FDG PET图像中提取的。 评估了与以前相同的三个分类任务。<br>表9中显示的结果未显示使用体素或区域特征获得的平均平衡精度之间的显着差异。 在AIBL数据集的情况下，基于区域的分类的平衡准确度更高（对于AD与CN：基于体素的0.79 [0.059]，基于区域的0.86 [0.042]），但我们可以观察到相应的标准 偏差很高。</p><h3 id="4-4-Influence-of-the-classification-method"><a href="#4-4-Influence-of-the-classification-method" class="headerlink" title="4.4. Influence of the classification method"></a>4.4. Influence of the classification method</h3><p>使用三种不同的分类器进行基于区域的实验，以评估取决于所选分类器的平衡精度是否存在变化。 使用来自T1w MRI的参考AAL2图谱和ADNI参与者的FDG PET图像提取区域特征。 执行了三个先前定义的分类任务。<br>图3中显示的结果显示，具有L2正则化模型的线性SVM和逻辑回归导致类似的平衡精度，始终高于用随机森林获得的所有任务和测试的成像模态。</p><h3 id="4-5-Influence-of-the-partial-volume-correction-of-PET-images"><a href="#4-5-Influence-of-the-partial-volume-correction-of-PET-images" class="headerlink" title="4.5. Influence of the partial volume correction of PET images"></a>4.5. Influence of the partial volume correction of PET images</h3><p>使用线性SVM分类器进行基于区域和体素的分析，以评估用于部分体积效应的校正PET图像是否对分类准确性有影响。 具有和不具有PVC的ADNI参与者的FDG PET图像用于这些实验。<br>图4中显示的结果表明，使用和不使用PVC获得的平衡精度之间几乎没有差异。 使用体素功能时，无论是否存在PVC，平均平衡精度几乎相同。 使用区域特征时，当FDG PET图像未针对部分体积效应进行校正时，平均平衡精度的增加非常小。</p><h3 id="4-6-Influence-of-the-magnetic-field-strength"><a href="#4-6-Influence-of-the-magnetic-field-strength" class="headerlink" title="4.6. Influence of the magnetic field strength"></a>4.6. Influence of the magnetic field strength</h3><p>ADNI1参与者的大多数T1w扫描是在1.5T扫描仪上获得的，而3T扫描仪用于获取ADNIGO / 2参与者的MR图像。 为了评估场强的差异是否对分类性能产生影响，我们分别计算了1.5T和3T扫描的受试者的平衡准确度。 结果显示在表10中。我们观察到，无论实验如何，与1.5T扫描子集相比，3T扫描子集的平衡精度总是更高，这并不奇怪，因为3T图像应该具有更好的信号 噪声比。</p><h3 id="4-7-Influence-of-class-imbalance"><a href="#4-7-Influence-of-class-imbalance" class="headerlink" title="4.7. Influence of class imbalance"></a>4.7. Influence of class imbalance</h3><p>我们执行的任务是使用不平衡的类完成的。这种类别不平衡的范围从非常温和（对于ADNI的CN是CN的1.2倍）到中等（比pMCI多1.7倍的CN和比ADNI的pMCI多2倍的sMCI）到非常强（在AIBL中比AD多6.1倍的CN）。<br>我们的目的是评估这种阶级不平衡是否会影响绩效。为此目的，我们随机抽样亚组并进行实验，其中237 CN对237 AD，167 pMCI vs 167 CN和167 pMCI vs 167 pMCI用于ADNI和72 CN和72 AD用于AIBL。我们确保平衡子集的人口统计学和临床特征与原始子集没有差异。结果如图5所示。对于ADNI，其表现与完整人群的表现相似。对于AIBL，基于体素的功能的平衡组的性能要高得多。因此，似乎非常强的阶级不平衡（如AIBL中比例为6比1的情况）导致较低的性能，但适度的阶级不平衡（ADNI中高达2比1）得到充分处理。</p><h3 id="4-8-Influence-of-the-dataset"><a href="#4-8-Influence-of-the-dataset" class="headerlink" title="4.8. Influence of the dataset"></a>4.8. Influence of the dataset</h3><p>我们还想知道数据集的结果是否一致，因此我们比较了从ADNI，AIBL和OASIS获得的分类性能，用于区分控制对象与阿尔茨海默病患者的任务。从T1w MR图像中提取体素（4mm平滑）和区域（AAL2图谱）特征并与线性SVM分类器一起使用。我们测试了两种配置：在同一数据集上训练和测试分类器，在ADNI上训练分类器并在AIBL和OASIS上进行测试。<br>结果显示在表11中。在ADNI和AIBL上获得的性能是可比的并且远高于在OASIS上获得的性能。在ADNI培训和AIBL或OASIS测试时，平衡准确度至少与AIBL或OASIS上的训练和测试时一样高，这表明在ADNI上训练的分类器能够很好地推广到其他数据集。特别是，ADNI培训大大提高了OASIS的分类性能。我们的目的是评估这是否是由于ADNI中的大量受试者。为此目的，我们进行了相同的实验，但每个数据集的参与者子集大小相同。我们从每个数据集中随机抽取70名AD患者和70名CN参与者的人群，确保子群体的人口统计学和临床特征与原始人群没有差异。从表11中可以看出，使用该子集，基于体素的改进消失了，但仍然保留了区域特征。</p><h3 id="4-9-Influence-of-the-training-dataset-size"><a href="#4-9-Influence-of-the-training-dataset-size" class="headerlink" title="4.9. Influence of the training dataset size"></a>4.9. Influence of the training dataset size</h3><p>计算学习曲线以评估线性SVM分类器的性能如何根据训练数据集的大小而变化。仅使用ADNI参与者，我们测试了四种情景：从T1w MRI和FDG PET图像中提取的体素和区域特征。作为交叉验证，运行250次迭代，其中数据集被随机分成测试数据集（30％的样本）和训练数据集（70％的样本）。对于CN和AD，用于训练和测试每项不同任务的最大受试者数量为362，CN与pMCI为313，sMCI与pMCI为355。对于每次运行，使用10％到所有训练集（从7％到高达70％的样本）在相同的测试集上训练和评估10个分类器，每个样本使用的样本数量增加10％步。因此，用于培训的参与者数量从CN到20到197，sMCI为24到239，pMCI为12到117，AD为17到166。我们可以从图6中的学习曲线中观察到，正如预期的那样，平衡精度随着训练样本的数量而增加。<br>当使用通过组合来自ADNI和AIBL（由72个CN受试者和72个AD受试者组成的平衡子集）和来自ADNI，AIBL和OASIS的参与者获得的较大数据集时，还计算CN与AD任务的学习曲线。结果显示在图7中。我们观察到，对于相同数量的受试者，组合ADNI和AIBL或仅使用ADNI导致类似的平衡准确度。对于区域特征，与仅使用ADNI相比，ADNI和AIBL组合时的性能略高，但差异主要在标准偏差范围内。当结合ADNI和AIBL时，更多的受试者被用于训练时，平衡的准确性会略微增加。然而，当结合ADNI，AIBL和OASIS时，无论是多少科目，性能都比仅使用ADNI或组合ADNI和AIBL时更差。这可能是由于ADNI和AIBL遵循相同的诊断和采集协议，这与OASIS不同。</p><h3 id="4-10-Influence-of-the-diagnostic-criteria"><a href="#4-10-Influence-of-the-diagnostic-criteria" class="headerlink" title="4.10. Influence of the diagnostic criteria"></a>4.10. Influence of the diagnostic criteria</h3><p>我们通过使用关于每个受试者的淀粉样蛋白状态的信息（如果可用）来改进先前使用的诊断标准来定义新的分类任务。 从图8中可以看出，当将这些任务的性能与不使用淀粉样蛋白状态的相关任务进行比较时，对于所有新定义的任务，平均平衡准确度更高或至少相同。 我们必须注意，尽管所有受试者都不知道淀粉样蛋白状态，但尽管计数较少，但仍达到了这一性能。</p><h3 id="4-11-Computation-time"><a href="#4-11-Computation-time" class="headerlink" title="4.11. Computation time"></a>4.11. Computation time</h3><p>总的来说，我们使用SVM分类器进行了279次实验，使用逻辑回归分类器进行了155次实验，使用随机森林分类器进行了26次实验（有关任务和参数的详细信息，请参阅表6-8）。 使用具有72个核心（Xeon E5-2699 @ 2.30 GHz）和256 GB RAM的机器，运行434个SVMþ逻辑回归实验需要6天，运行26个随机森林实验需要8天。</p><h2 id="5-Discussion"><a href="#5-Discussion" class="headerlink" title="5. Discussion"></a>5. Discussion</h2><p>我们提出了一个开源框架，用于可重复评估AD分类方法，其中包含以下组件：i）转换器将三个公开可用的数据集标准化为BIDS; ii）用于T1w MRI和PET的标准化预处理和特征提取管道; iii）标准分类算法; iv）遵循最近的最佳实践的交叉验证程序。我们展示了它用于评估三个公共数据集上的不同成像模态，预处理选项，特征和分类器。<br>在这项工作中，我们首先致力于在AD中评估机器学习方法：i）更可重复; ii）更客观。再现性是基于相同数据和实验程序再现结果的能力。已经在不同领域提出了提高再现性的呼吁，包括神经成像（Poldrack等，2017）和机器学习（Ke等，2017）。再现性与复制不同，复制是在独立数据上确认结果的能力。可重复研究的关键要素包括：数据共享，使用社区标准存储数据，全自动数据处理，代码共享。我们的工作有助于通过不同方面提高AD ML研究的可重复性。第一个组件是将三个公共数据集全自动转换为社区标准BIDS。实际上，ADNI和AIBL无法重新分配。通过这些工具，我们希望能够轻松地基于这些数据集重现实验，而无需重新分发它们。特别是，与简单地公开使用的主题列表相比，我们为用户节省了大量时间。对于复杂的多模态数据集（例如ADNI（具有大量不完整数据，给定模态的多个实例和复杂元数据）尤其如此。第二个关键组件是公开可用的代码，用于预处理，特征提取和分类。这些贡献收集在Clinica11中，这是一个免费提供的临床神经科学研究软件平台。除了提高可重复性之外，我们希望这些工具还能使研究人员的工作变得更加容易。<br>我们也希望为更客观的评估做出贡献。对新方法（分类算法，特征提取方法或其他方法）的客观评估需要在不改变其他组件的情况下测试该特定组件。我们的框架包括用于T1加权MRI和FDG PET数据的预处理和特征提取的标准方法，以及标准分类工具。这些构成了一套基线方法，可以很容易地比较新方法。然后，研究新方法的研究人员可以直接用自己的解决方案替换管道的给定部分（例如特征提取，分类），并评估该特定新组件相对于所提供的基线方法的附加值。我们还提出了严格验证的工具，主要基于最近的指南（Varoquaux等，2017），并基于标准软件scikit-learn（Pedregosa等，2011）实施。其中包括：i）大量重复随机分割，以广泛评估性能的变化; ii）报告准确度和标准偏差的完整分布，而不仅仅是平均准确度; iii）用于超参数调整的足够嵌套CV。</p><p>然后，我们基于T1 MRI和FDG PET数据证明了该框架在不同分类任务中的应用。通过这一点，我们的目标是提供一个基准性能，可以比较先进的机器学习和特征提取方法。这些基线性能符合最先进的结果，这些结果已在（Arbabshirani等，2017; Falahati等，2014; Rathore）中进行了总结。<br>et al。，2017），其中CN与AD的分类准确度通常为80％至95％，sMCI与pMCI的分类准确度为60％至80％。例如，使用线性SVM，区域特征（AAL2）和FDG PET数据，我们报告CN与AD相比为88％，CN与pMCI为80％，sMCI与pMCI为73％。<br>ADNI中使用的诊断标准来自NINCDS-ADRDA（McKhann等，1984），其仅依赖于患者的症状和认知状态。然而，AD的明确诊断只能在尸检时进行，并且在很大比例的病例中发现临床诊断是错误的（Knopman等，2001）。在过去的十年中，AD的诊断取得了实质性进展。特别是，有人建议不仅要依靠临床和认知评估，还要依赖成像和脑脊液（CSF）生物标志物。这导致了新的诊断标准。尽管黄金标准仍然是尸检，但这导致在患者生命期间更准确地诊断AD。特别是，IWG（Dubois等，2007），IWG-2（Dubois等，2014）和NIA-AA（Albert等，2011）提出了β-淀粉样蛋白和/或tau蛋白的存在。 ; Jack等人，2011; McKhann等人，2011; Sperling等人，2011）。在这项工作中，我们评估了使用淀粉样蛋白精制诊断组是否改善了表现。从每个参与者的淀粉样蛋白PET扫描（PiB或AV45）确定淀粉样蛋白状态。我们发现了<br>使用淀粉样蛋白精制诊断的分类总是比使用NINCDS-ADRDA诊断的相关任务更好或至少类似，即使训练集包含更少的个体。<br>与T1w MRI相比，FDG PET的分类在任务，特征和分类方法上的表现始终更好。一些研究支持我们的发现（Dukart等，2011,2011; Gray等，2013; Ota等，2015; Young等，2013），而其他研究没有发现性能上的差异（Hinrichs等。 ，2009; Zhang等，2011; Zhu等，2014）。鉴于我们的研究样本量较大且严格的评估设计，我们认为FDG PET与MRI相比具有优越的性能，这是一个很好的发现。这可能是由于在萎缩之前可以在疾病过程中更早地检测到代谢减退（Jack等，2010b）。</p><p>多种参数和选项用于AD机器学习研究中的预处理和特征提取。它们对分类性能的影响尚不清楚，并且构成了分类方法可比性的问题。我们评估了图谱选择，平滑程度，PET图像校正部分体积效应以及特征类型（区域或体素）的影响。我们没有发现这些不同组分中的每一种对性能的系统影响。一些研究发现地图集对分类性能的影响（Ota等，2015; 2014）。然而，该研究中的受试者数量很少。在（Chu等人，2012）中，与使用所有体素相比，使用少量ROI的组合时发现了3％的改善。在我们的研究中，使用了更多的受试者和严格的验证过程。<br>我们比较了三种广泛使用的分类方法：SVM，Logistic回归与L2正则化和随机森林。我们的主要发现是后者的表现不佳。这可能是由包含相对同质值的大脑成像数据的性质引起的，并且应该显示体素或大脑区域的依赖性。数据的这些特征可以解释为什么试图找到平滑特征组合的技术（例如使用L2正则化的特征）更适合于单一模态分类问题。另一方面，当组合来自不同模态的特征（例如图像，临床数据和认知评分）时，随机森林或其他集合方法可能是有用的，如（Moradi等，2015;Sørensen等，2018）所述。在比较几种标准分类算法（如SVM，LDA或Naive Bayes）的其他论文中（Aguilar等，2013; Cabral等，2015; Sabuncu等，2015），结果未显示方法之间的差异。</p><p>我们还评估了类别不平衡的影响，在我们的数据集中，这种影响范围从非常温和（AD比AD对ADNI多1.2倍）到中度（比pMCI多1.7倍的CN和比ADNI的pMCI多2倍的sMCI）到非常强（在AIBL中，CN比AD多6.1倍。在基于体素的特征的情况下，我们发现非常强的类不平衡（如AIBL中比例为6比1的情况）导致性能降低但是适度的类不平衡（ADNI中高达2比1）得到妥善处理。另一方面，阶级不平衡对区域特征没有影响。这突出表明，当存在非常强的类不平衡和使用非常高维度的特征时，使用平衡组进行训练可能是有益的。<br>我们评估了各种成分对分类性能的影响：模态（T1w MRI与PET），特征类型，寰椎的选择，PVC，平滑，分类器。其他研究评估了其他成分的影响：不同类型的解剖学特征，包括体积，皮质厚度和其他表面特征（Go？mez-Sancho等，2018; Schwarz等，2016; Westman等， 2013），特征选择技术（Tohka等，2016），颅内体积归一化（Voevodskaya等，2014; Westman等，2013）。此外（Tohka等，2016），将LASSO和弹性网与SVM进行了比较，发现前一种方法提供了更高的性能。使用我们的框架也可以评估这些不同组件的影响。在本文中，我们将框架的应用限制为由于以下原因而选择的一组组件。基于体素和区域特征都包括在内，因为它们被广泛使用。另一方面，由于其计算成本，不包括基于Freesurfer的皮质测量。 PVC是一种非常常见的PET数据预处理方法。平滑被广泛用于神经影像学社区中基于体素的分析，并且评估其影响似乎是有用的。然而，在这样的选择中总是存在一些任意性，用框架研究其他组件会很有趣。<br>在这项工作中，我们使用了预定义的功能（在区域或体素级别）。应该提到的另一类方法是直接从数据中学习特征的方法。基于补丁的方法旨在自动学习主体和训练集之间的非局部相似性（Coup？et al。，2015,2012）。此外，深度学习方法可以自动学习多个尺度的相关特征，并且最近在AD的自动分类中变得流行（B€ackstro€et al。，2018; Liu et al。，2018; Lu et al。，2018; Suk等，2017）。两种类型的方法都产生了有希望的结果（例如pMCI与sMCI的比例为73％至83％）。此外，各种工作已经提出使用不同类型的数据驱动特征选择（例如单变量统计检验，多变量方法）（Chu等人，2012; Tohka等人，2016; Vemuri等人，2008）和维数降低（例如主成分分析，流形学习）（Beheshti等，2015; Guerrero等，2014; Liu等，2015; Salvatore等，2015）。这些方法具有改善性能的潜力，但需要使用严格的交叉验证程序进行验证（Eskildsen等，2013; Maggipinto等，2017）。可以使用我们的框架评估所有这些方法的附加值。这超出了本文的范围，留待将来工作。</p><p>使用多个数据集对于评估在不同条件下获得的不同群体的表现是否稳健非常重要。第一个组件包括在不同的数据集上执行相同的实验。我们发现ADNI和AIBL数据集的分类结果相似，但OASIS的分类结果要低得多。 OASIS的较低性能可能是由于诊断标准较不严格（在<br>OASIS，CDR&gt; 0的所有参与者被认为是AD）。了解分类器在一个数据集上进行训练并在另一个数据集上进行测试时，它也是有价值的。对ADNI数据进行过培训的分类器很好地适用于AIBL和OASIS。有趣的是，对于OASIS而言，与在OASIS培训时相比，ADNI培训时的表现大大增加。这种改善可能源于几个因素：更大的训练集大小，更高的图像质量或更严格的诊断标准。当使用相同大小的子集时，基于体素的特征所获得的改进消失，这表明增加的训练集大小很重要，特别是在使用非常高的维度特征时。另一方面，对于区域特征，与OASIS子集的训练相比，ADNI子集的训练改善了性能，表明其他因素（图像质量，更严格的诊断标准）有助于改善。一般来说，我们可以说分类器能够在不同的数据集中进行推广，如Dukart等，2013; Sabuncu等，2015中所述，特别是如果它们是使用具有严格诊断标准的大型多中心数据集获得的，就像ADNI的情况一样。<br>不出所料，增加训练集的规模可以提高分类表现。在其他研究中也发现了取决于训练集大小的结果的改进（Abdulkadir等，2011; Chu等，2012; Franke等，2010）。可以注意到，当组合多个数据集时，性能也随着训练集大小而增加。但是，当将OASIS与ADNI和AIBL结合使用时，性能低于仅使用AIBL和ADNI时的性能。这与OASIS的性能系统性低于ADNI和AIBL的性能一致。同样，这可能是由于诊断标准在OASIS中不太严格。有趣的是，根据目前可用的样本数量，尚未达到结果停止改善的程度。分类器的性能取决于为训练提供的图像数量所施加的限制，这意味着需要更多数据才能找到分类器的最佳性能。这些结果强调了对更多可公开获得的数据集的需求，该领域目前的大部分研究都依赖于这些数据集。</p><h2 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6. Conclusions"></a>6. Conclusions</h2><p>我们的可重复分类实验框架旨在解决基于机器学习的AD分类领域当前面临的问题，例如结果的可比性和可重复性。 它在T1w MRI和FDG PET数据中的应用允许广泛评估成像模态，预处理选项，特征和算法对性能的影响。 这些结果提供了基准性能，可以与其他方法进行比较。 我们希望框架和实验结果对AD领域的研究人员都有用。</p><h3 id="Acknowledgments"><a href="#Acknowledgments" class="headerlink" title="Acknowledgments"></a>Acknowledgments</h3><p>导致这些结果的研究得到了“Investissements d’avenir”ANR-10-IAIHU-06（Agence Natio- nale de la Recherche-10-IA Agence Institut Hospitalo-Universitaire-6）计划的资助，ANR-11- IDEX-004（Agence Nationale de la Recherche-11- Initiative d’Excellence-004，项目LearnPETMR编号SU-16-R-EMR-16），来自欧盟H2020计划（项目EuroPOND，拨款号666992，项目HBP SGA1授权号720270），来自ICM大脑理论计划（项目DYNAMO），来自欧洲研究理事会（Durrleman博士项目LEASP，授权号678304），来自NSF / NIH / ANR联合计划“计算机协作研究”神经科学“（项目HIPLAY7，授权号ANR-16-NEUC- 0001-01）和来自巴塞罗那协会公共部门的”Contrat d’Interface Local“计划（致Col-liot博士）（AP-HP） 。注：根据REA拨款协议no，从欧洲联盟第七框架计划（FP7 / 2007-2013）的人民计划（Marie Curie Actions）获得资金。 PCOFUND-GA-2013-609102，通过由法国校园协调的PRESTIGE计划。<br>该项目的数据收集和共享由Alz-资助 Heimer’s疾病神经影像学倡议（ADNI）（美国国立卫生研究院资助U01 AG024904）和DOD ADNI（国防部奖项编号W81XWH-12-2-0012）。 ADNI由国家老龄化研究所，国家生物医学成像和生物工程研究所资助，并通过以下方面的慷慨捐助：AbbVie，Alzheimer’s Association;阿尔茨海默氏症的药物发现基金会; Araclon Biotech; BioClinica，Inc。;生物遗传; Bristol-Myers Squibb Company; CereSpir，Inc。; Cogstate; Eisai Inc。; Elan Pharmaceuticals，Inc。;礼来公司;欧蒙; F. Hoffmann-La Roche Ltd及其附属公司Genentech，Inc。; Fujirebio公司; GE Healthcare; IXICO有限公司; Janssen Alzheimer Immunotherapy Research＆Development，LLC。;强生药业研发有限责任公司; Lumosity; Lundbeck公司; Merck＆Co.，Inc。; Meso Scale Diagnostics，LLC。; NeuroRx研究; Neurotrack技术;诺华制药公司;辉瑞公司; Piramal成像;施维雅;武田制药公司;和过渡治疗学。加拿大卫生研究院正在提供资金支持加拿大的ADNI临床站点。国家卫生研究院基金会（<a href="http://www.fnih.org）为私营部门的捐助提供了便利。受助组织是北加州研究和教育研究所，该研究由南加州大学阿尔茨海默氏症治疗研究所协调。" target="_blank" rel="noopener">www.fnih.org）为私营部门的捐助提供了便利。受助组织是北加州研究和教育研究所，该研究由南加州大学阿尔茨海默氏症治疗研究所协调。</a> ADNI数据由南加州大学的神经成像实验室传播。<br>OASIS项目得到以下资助：P50 AG05681，P01 AG03991，R01 AG021910，P20 MH071616和U24 RR021382。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer-Disease-翻译</title>
    <link href="http://yoursite.com/2019/07/18/Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer-Disease-%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/07/18/Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer-Disease-翻译/</id>
    <published>2019-07-18T05:49:56.000Z</published>
    <updated>2019-07-19T03:36:40.960Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p><a href="https://pubs.rsna.org/doi/full/10.1148/radiol.2016152703" target="_blank" rel="noopener">《Application of Machine Learning to Arterial Spin Labeling in Mild Cognitive Impairment and Alzheimer Disease》</a></p><p>机器学习在动脉自旋标记治疗轻度认知障碍和阿尔茨海默病中的应用</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h3 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h3><p>调查动脉自旋标记（ASL）灌注图的多变量模式识别分析是否可用于阿尔茨海默病（AD）和轻度认知障碍（MCI）患者的分类和单一主题预测以及主观认知能力下降（SCD）的受试者 使用W分数法去除性别和年龄的混杂影响。</p><h3 id="Materials-and-Methods"><a href="#Materials-and-Methods" class="headerlink" title="Materials and Methods"></a>Materials and Methods</h3><p>在具有可能AD的100名患者中获得了伪连续3.0-T ASL图像; 60例MCI患者，其中12例保持稳定，12例转为AD诊断，36例无随访; 100名SCD患者; 和26名健康对照受试者。 AD，MCI和SCD组分为性别和年龄匹配训练集（n = 130）和独立预测集（n = 130）。 对于每个参与者，针对每个体素计算针对年龄和性别（W分数）调整的标准化灌注分数。 使用诊断状态和灌注图执行支持向量机分类器的训练。 提取识别图并将其用于预测集中的单主题分类。 通过接受者操作特征（ROC）分析评估预测性能，以产生ROC曲线下面积（AUC）和灵敏度和特异性分布。</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>通过使用鉴别图在预测集中的单一主题诊断产生AD与SCD的优异表现（AUC，0.96; P &lt;.01），AD与MCI（AUC，0.89; P &lt;.01）的良好表现，以及差 MCI与SCD的表现（AUC，0.63; P = .06）。 应用AD与SCD鉴别图预测MCI亚组导致MCI诊断转为AD的患者与SCD患者相比具有良好的表现（AUC，0.84; P &lt;.01），MCI诊断患者的公平表现转为AD 与那些稳定的MCI（AUC，0.71; P&gt; .05）。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>使用自动化方法，年龄和性别调整的ASL灌注图可用于分类和预测AD的诊断，MCI向AD的转换，稳定的MCI和SCD，具有良好至极好的准确度和AUC值。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>阿尔茨海默病（AD）是最常见的痴呆类型（1），是65岁及以上人群的第五大死因（2）。结构磁共振（MR）成像在诊断患者与对照受试者的AD时具有高准确度，特别是在疾病的晚期阶段（3,4）。然而，通过使用结构MR成像识别AD的早期诊断准确性和早期AD诊断患者的预后价值仍存在问题（5）。</p><p>在Jack等人的动态2010生物标志物模型中，功能性AD相关的脑变化发生在结构变化之前（6）。与对照组（7,8）相比，AD患者使用专用磁共振成像测量了全脑低灌注，最突出的是顶叶结构，如后扣带皮层，前躯和下顶叶（7 -9）。与AD患者相比，轻度认知障碍（MCI）患者表现出相似但不太明显的低灌注模式（10），AD患者和对照受试者之间的定量脑血流值介于中间（11）。与对照组相比，MCI患者的枕叶和颞叶也有低灌注（11）。</p><p>动脉自旋标记（ASL）MR成像是一种非侵入性，快速且日益广泛的量化脑血流量的方法;与正电子发射断层扫描（PET）（12-14）相比，ASL代表了测量脑灌注的潜在替代方式，其可以促进痴呆治疗中的常规临床应用。用ASL测量的AD相关灌注变化与用PET测量的葡萄糖代谢改变强烈相关（9,15,16）。这一系列研究结果表明，ASL是早期诊断AD的有前景的替代功能生物标志物。</p><p>除了本文提到的与灌注相关的诊断参数之外，将多变量模式识别软件应用于结构MR成像数据已经在AD中产生了高诊断准确度（17）。与海马体积的视觉评估相比（18,19），自动分类导致MCI向AD转换的预测具有高准确性（20），这表明自动分类包括神经退行性病理过程的更多特征（18）。</p><p>支持向量机（SVM）代表二元机器学习多变量方法，可以训练这些方法在留一法交叉验证框架中对单个图像进行分类（17）。与单变量方法相比，这种多变量方法的优点包括增加统计功效和单一主题检查适用性，能够处理大量依赖的体素数据，更准确地类似于全局脑功能（21）。具体来说，在SVM的设置中，W得分方法是一种统计工具，可以减少二元测试中混杂因素的影响（22,23）。由于自动化的基于图像的分类器尚未在ASL的设置中进行诊断使用，因此开发用于单一主题分类的这种工具在临床上是相关的并且有助于筛选目的。</p><p>在目前的研究中，我们研究了ASL灌注图的多变量模式识别分析是否可用于AD和MCI患者的分类和单一主题预测以及使用W分数法去除混杂后主观认知能力下降（SCD）的受试者性别和年龄的影响。</p><h2 id="Materials-and-Methods-1"><a href="#Materials-and-Methods-1" class="headerlink" title="Materials and Methods"></a>Materials and Methods</h2><p>主要作者对该研究负全部责任，并对数据和所有分析以及患者同意书具有完全的访问权和权利。 所有作者都同意所有条件。</p><h3 id="Participants"><a href="#Participants" class="headerlink" title="Participants"></a>Participants</h3><p>当地机构审查委员会批准了这项研究。所有受试者均提供书面知情同意该研究回顾性地包括来自VU大学医学中心痴呆队列的阿尔茨海默病中心的311名参与者，他们在2010年10月至2012年11月期间接受了ASL MR成像（24）。排除标准是占位过程（n = 7），创伤后偏差（n = 6），大血管出血或梗塞（n = 4），癫痫（n = 4）或精神疾病（n = 5）的适应症， AD的非典型临床表现（n = 3）（PS，具有27年的经验），或脑提取或ASL获取失败（n = 22）。临床诊断是通过多学科团队在标准痴呆筛查的基础上达成的共识建立的，包括病史检查，身体和神经系统检查，筛查实验室检查，神经心理学测试和脑MR成像。尽可能获得脑脊液。 AD患者符合国家老龄化 - 阿尔茨海默氏症协会可能AD的标准（25）。在2012年之前，MCI诊断基于Petersen及其同事定义的标准（26）; 2012年之后，MCI诊断基于国家老龄化协会 - 阿尔茨海默氏症协会标准（25）。如果参与者不符合AD或MCI标准，则被视为SCD受试者。健康对照受试者的临床结果正常，没有认知能力下降（25）。这些标准导致包括100名患有可能的AD的患者，60名患有MCI的患者，100名患有SCD的患者和26名健康对照受试者。教育水平按七分制评定（27）。</p><p>诊断组内的参与者被随机分配到训练组或预测组（每组，n = 130; 50名AD患者，30名MCI患者和50名SCD患者），年龄和性别均衡分布（图1;表1,2）。</p><h3 id="Data-Acquisition"><a href="#Data-Acquisition" class="headerlink" title="Data Acquisition"></a>Data Acquisition</h3><p>使用3.0通道全身MR系统（Signa HDxt; GE Medical Systems，Milwaukee，Wis）通过使用八通道头部线圈收集成像数据。结构成像涉及使用矢状三维T1加权序列（反转恢复快速损坏梯度回波;重复时间[毫秒] /回波时间[毫秒]，7.8 / 3.0;反转时间，450毫秒;翻转角， 12°;和体素尺寸，1×0.9×0.9 mm）。伪连续ASL灌注图像（具有背景抑制的三维快速自旋回波采集;标记时间，1.5秒;标记后延迟，2.0秒;重复时间，4.8秒;回波时间，9毫秒;八臂的螺旋读数×512个样本;在减去标记后，使用单室模型（28）计算36×5.0mm的轴向截面; 3.2×3.2mm的面内分辨率;重建的像素尺寸为1.7×1.7mm;以及获取时间，4分钟）来自控制图像的图像。获得近似中等加权的图像，以通过使用具有相同参数的饱和度恢复采集来缩放每个参与者的灌注图像。</p><h3 id="Preprocessing-of-MR-Imaging-Data"><a href="#Preprocessing-of-MR-Imaging-Data" class="headerlink" title="Preprocessing of MR Imaging Data"></a>Preprocessing of MR Imaging Data</h3><p>针对三个方向上的梯度非线性校正T1加权和伪连续ASL图像。 使用脑软件库的功能磁共振成像或FSL软件（牛津大学，牛津，英国）进行进一步分析（29）。 T1加权图像的预处理包括去除非脑组织，对蒙特利尔神经病学研究所（MNI）空间进行归一化，以及使用部分体积估计进行组织分割。 ASL图像线性地记录到灰质密度图并映射到MNI标准空间，然后是高斯平滑，具有6mm全宽半最大值，并且以3mm各向同性分辨率重新采样（A.M.W.，具有15年的经验）。</p><h3 id="W-Score-Maps"><a href="#W-Score-Maps" class="headerlink" title="W Score Maps"></a><em>W</em> Score Maps</h3><p>因为女性的脑血流量高于男性（7），并且随着年龄的增长逐渐减少（30），这些混杂因素在二元分类之前通过使用W分数法（22,23）和脚本github.com/amwink被删除。 /bias/blob/master/scripts/bash/compute_w.sh。它在参考ASL图像的一般线性模型分析中计算每个混淆器对脑灌注的体素效应（图2a）。体素截距（β0），性别和年龄相关的回归系数（分别为β1和β2）和残差（ε）用于计算混淆校正的归一化统计量如下：[（测量灌注） - （预测）灌注）] /残差标准差。测量的灌注是预处理的ASL强度，预测的灌注是分别由个体的年龄和性别参数加权的体素效应的总和（图2b）。与Z分数一样，阴性W分数表示灌注较低，阳性分数表明灌注高于参考预期，考虑到个体的年龄和性别。</p><h3 id="SVM-Multivariate-Pattern-Recognition-in-the-Training-Set"><a href="#SVM-Multivariate-Pattern-Recognition-in-the-Training-Set" class="headerlink" title="SVM: Multivariate Pattern Recognition in the Training Set"></a>SVM: Multivariate Pattern Recognition in the Training Set</h3><p>在Matlab（Mathworks，Natick，Mass）中实现的神经成像工具箱（21）的模式识别为神经学图像提供了多变量模式分析。线性SVM生成多维超平面，以最佳方式分离标记组中的数据（监督学习）。对于二维矢量，这将是直线（31），但在我们的情况下，超平面具有所包括的体素的数量的维度。表示该超平面（21）的法向矢量的辨别图将每个体素的相对权重存储到分类中。</p><p>SVM接受了一次性交叉验证框架的培训，以区分AD患者，MCI患者和SCD患者。我们通过比较两个患者组的W得分图与具有SCD的受试者的W得分图来评估分类器的诊断价值。通过区分AD患者和患有MCI的患者的W评分图来评估分类器对疾病进展的敏感性。最后，使用转化为AD的MCI诊断患者和具有稳定MCI的患者的W评分图进行探索性分类训练，以研究分类器是否显示预后值。</p><p>分类准确性反映了算法的预测能力，因此具有直接的诊断相关性。因此，通过计算准确度，灵敏度，特异性和接受者操作特征（ROC）曲线来评估分类器性能，从该曲线计算ROC曲线下面积（AUC）。置换测试用于导出精度的P值（100个排列）（21）。</p><p>首先根据整个大脑计算训练精度。随后，使用阿尔茨海默氏病特异性感兴趣区域（ROI）的面罩来最大化训练精度。 ROI基于文献（1,7,8,10,11,16）和阈值组平均灌注图。它们包括顶叶，海马，枕叶及其组合。通过使用MNI结构和Harvard-Oxford皮质下结构图谱（29）（L.E.C.，F.H。和A.M.W.，分别具有1,1和15年的经验）创建掩模。</p><h3 id="SVM-Prediction-in-New-Participants"><a href="#SVM-Prediction-in-New-Participants" class="headerlink" title="SVM: Prediction in New Participants"></a>SVM: Prediction in New Participants</h3><p>在独立样本中复制结果既支持内部有效性又支持分类器的普遍性（32）。辨别图用于预测预测集灌注图的标签。辨别图显示预定义ROI中体素的辨别力，但不应解释为统计测试。相反，它们提供决策边界法向量的空间表示 - 即，区分组之间的每个体素的权重。正值（红色和黄色）表示对于更严重的条件具有预测值的体素，而负值（深蓝色和浅蓝色）表示对于不太严重的条件具有预测值的体素。单个受试者灌注图乘以辨别图，由训练偏差调整。使用个体积分乘积得分来定义类别，其可以通过使用ROC分析（L.E.C.，F.H。和A.M.W.）以简单的阈值来预测。</p><h3 id="Statistical-Analysis"><a href="#Statistical-Analysis" class="headerlink" title="Statistical Analysis"></a>Statistical Analysis</h3><p>在SPSS（版本20; SPSS，Chicago，Ill）中进行预测结果的统计分析。 通过ROC分析计算预测的特异性，灵敏度和AUC。 通过使用单因素方差分析评估组间连续测量的差异，使用事后Bonferroni校正进行多重比较。 进行χ2检验以评估参与者性别的频率分布（L.E.C.）。</p><h2 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h2><h3 id="Participant-Characteristics"><a href="#Participant-Characteristics" class="headerlink" title="Participant Characteristics"></a>Participant Characteristics</h3><p>在六个训练组和预测组之间未观察到显着性别（P = .74）或年龄（P = .68）差异。 在诊断组中，在训练组和预测组之间未观察到MMSE评分的显着差异（P&gt; 0.99，表1,2）。</p><p>在MCI亚组和12名匹配的SCD受试者之间未发现显着的患者性别（P&gt; .99）或年龄（P = .96）差异。 具有SCD的受试者具有比两个MCI亚组中的患者更高的MMSE得分（P &lt;.01），而MCI亚组在MMSE得分中没有差异（P = .47，表3）。</p><h3 id="Training-of-the-Classifiers"><a href="#Training-of-the-Classifiers" class="headerlink" title="Training of the Classifiers"></a>Training of the Classifiers</h3><p>训练结果概述见表4.对于AD与SCD，使用全脑ASLW评分图的训练准确率为87.0％（敏感性为84.0％，特异性为90.0％），AUC为0.94（P = .01）。 当训练限于顶叶和海马ROI时，获得进一步改善至89.0％（84.0％敏感性; 94.0％特异性; AUC，0.93; P = .01）。 AD与MCI全脑分析相比，训练准确率为78.8％（敏感性为84.0％，特异性为70.0％），AUC为0.84（P = .01）。 限制对顶叶和枕叶ROI的分析将准确性提高至83.8％（灵敏度83.3％;特异性84.0％; AUC，0.88; P = .01）。 最后，MCI与SCD全脑分析的准确度为57.5％（灵敏度为40.0％，特异性为68.0％），AUC为0.49（P = .42），在基于ROI的分析中未得到改善。 由此产生的具有最高精度的训练的鉴别图如图3所示。</p><h3 id="Predictions-Assessment-of-Generalizability"><a href="#Predictions-Assessment-of-Generalizability" class="headerlink" title="Predictions: Assessment of Generalizability"></a>Predictions: Assessment of Generalizability</h3><p>结果总结在表5中。在患有AD的患者中使用辨别权重与患有SCD的受试者使得能够在90.0％的个体中正确预测（94.0％敏感性和86.0％特异性）。 ROC曲线表现出优异的性能（AUC，0.96; 95％置信区间：0.92,1; P &lt;.001）。 AD患者与MCI患者使用鉴别权重可以在82.0％的个体中进行正确预测（84.0％的敏感性和80.0％的特异性）。 ROC曲线显示出良好的性能（AUC，0.89; 95％置信区间：0.81,0.97; P &lt;.001）。 在MCI患者中使用歧视权重与患有SCD的患者在仅60.0％的个体中能够正确预测（60.0％敏感性和60.0％特异性）。 ROC曲线表现出差的表现（AUC，0.63; 95％置信区间：0.50,0.76; P = .06;图4）。</p><h3 id="Exploratory-Analyses-Classifying-MCI-Subgroups"><a href="#Exploratory-Analyses-Classifying-MCI-Subgroups" class="headerlink" title="Exploratory Analyses: Classifying MCI Subgroups"></a>Exploratory Analyses: Classifying MCI Subgroups</h3><p>对于MCI诊断转为AD的患者与SCD患者相比，全脑训练的准确率为83.8％（灵敏度为66.7％，特异性为100％），AUC为0.90（P = .01）。 当训练限于后扣带皮层和海马ROI时，获得了进一步改善至87.5％（75.0％灵敏度; 100％特异性; AUC，0.92; P = .01）。 对于MCI诊断转为AD的患者与稳定MCI患者相比，全脑训练的准确率为70.8％（敏感性为66.7％，特异性为75.0％），AUC为0.77（P = .05）。 当训练局限于海马ROI时，获得了进一步改善至83.3％（83.3％的敏感性; 83.3％的特异性; AUC，0.77; P = .01）（表4）。 得到的鉴别图如图5所示。</p><p>由于小群组，没有匹配数据可用于基于转换为AD或稳定MCI的MCI诊断创建独立预测集。 然而，对于那些MCI诊断转化为AD的患者与使用SCD的患者相比，使用AD与SCD训练鉴别权重导致79.0％的个体的正确预测（灵敏度为83.0％，特异性为75.0％）。 ROC曲线表现出良好的性能（AUC，0.84; 95％置信区间：0.68,1; P &lt;.01）。 在MCI诊断中使用相同的鉴别权重转换为AD与稳定的MCI相比，在71.0％的个体中得到了正确的预测（灵敏度为67.0％，特异性为75.0％）。 ROC曲线显示出良好的性能（AUC，0.71; 95％置信区间：0.49,0.93; P = .08;表5;图6）。</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>在目前的研究中，灌注图的自动分类使我们能够以高精度区分具有不同AD阶段的患者。另外，鉴别权重可以用于独立数据集中的单主题诊断预测，具有良好至极好的准确度和AUC值。与放射科医师使用的传统评估策略（18,19）以及先前的研究相比，我们的训练和预测准确度相似，其中模式识别软件应用于全脑结构MR成像数据（17,18,20,33）。此外，本文提出的分类器具有良好的敏感性和特异性。这表明灌注差异与AD的诊断相关。</p><p>与先前的ASL研究（7-11,16）一致，通过使用AD特异性ROI而不是全脑ROI观察SVM训练的最高准确度。对于AD与SCD，组合的顶叶和海马ROI产生最高的准确度。然而，我们没有发现通过仅使用前躯体或后扣带皮层作为ROI来增加准确性，表明尽管这些区域显示最明显的低灌注（7-9,16），但是当不服用时，最佳分化的重要信息会丢失。整个顶叶考虑在内。可能的解释是默认模式网络的参与，已经证明在AD患者中活动较少并且包括几个顶叶区域（4）。</p><p>当应用顶叶和枕叶ROI时，我们实现了AD与MCI的最大训练准确度。尽管顶叶的使用与先前的ASL结果一致（10,11），但枕叶的应用是矛盾的，因为在MCI患者中观察到枕骨低灌注与对照组相比，但未与AD患者进行比较（ 11,13）。大多数AD样本由早发性AD患者组成，与晚发AD患者相比，总体和枕部低灌注和代谢减退更为明显（34,35），可以解释这种差异。</p><p>MCI患者与患有SCD的患者的自动分类没有产生高准确度，这在以前的工作基础上是出乎意料的（8-10）。 MCI组内缺乏同质性可能导致这种低精度，因为它禁止SVM找到模式。实际上，MCI亚组的训练改善了分类，与之前使用结构MR成像的研究相比，准确度略高（20,33）。该观察结果证明了分类器患者之间的异质性问题（33）。但是，通过使用大型训练数据集可以避免这个问题。由于MCI亚组的训练由小样本组成（n = 12），因此需要使用更大的样本复制我们的结果以支持MCI转换器的自动化方法的预后价值。</p><p>通过SVM分类培训，使用了一次性交叉验证框架;因此，相同的数据被重新用于学习和分类，这可能产生有偏见的结果。与该领域的大多数先前研究不同，在我们的研究中，我们使用独立的数据集进行训练和预测以评估普遍性。此外，我们的研究通过使用更大的样本量，减少疾病异质性问题和提高分类准确性，扩展了结构MR成像的先前结果。然而，在日常实践中真正临床应用SVM所需的数据量明显更高（33）。因此，一项大型多中心研究具有重要的临床意义。</p><p>我们的结果受到以下事实的限制：我们的AD样本中相对较高比例的患者包括早发性AD患者。进一步的研究涉及使用老年受试者的样本将使我们能够研究晚发性AD患者的适用性。其次，我们使用SCD受试者代替健康对照受试者进行分类。然而，患有SCD的受试者在记忆诊所和医院中遇到，因此用这种方法很好地代表了操作环境。</p><p>我们的结果支持自动分类可以促进并可能改善诊断的方式，特别是在没有经验的（神经）放射科医师的中心。此外，考虑到AD的高患病率，自动分类可能适用于筛查目的（1）。</p><p>总之，使用自动化方法，年龄和性别调整的ASL灌注图可用于分类和预测AD的诊断，MCI诊断转换为AD，稳定MCI和SCD，具有良好至极好的准确度和AUC值。</p><h2 id="Advances-in-Knowledge"><a href="#Advances-in-Knowledge" class="headerlink" title="Advances in Knowledge"></a>Advances in Knowledge</h2><ul><li><p>可以训练自动化机器学习方法，根据动脉自旋标记（ASL）图像区分主观认知能力下降的受试者，轻度认知障碍患者和阿尔茨海默病（AD）患者，具有较高的分类训练准确度（范围） ，83.8％-89.0％; P &lt;.01）。</p></li><li><p>基于这些训练的分类器可用于预测具有高诊断准确度的单个受试者的诊断（接收器操作特征曲线范围下的面积，0.89-0.96; P &lt;.001）。</p></li></ul><h2 id="Implications-for-Patient-Care"><a href="#Implications-for-Patient-Care" class="headerlink" title="Implications for Patient Care"></a>Implications for Patient Care</h2><ul><li><p>用于高精度（&gt; 82％）检测AD患者的三维假连续ASL图像的自动分类可支持基于图像的诊断，尤其是在没有经验（神经）放射科医师的中心。</p></li><li><p>三维假连续ASL图像的自动分类可用于AD筛查目的，而不会影响诊断准确性。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="ADNI" scheme="http://yoursite.com/tags/ADNI/"/>
    
  </entry>
  
  <entry>
    <title>Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease-翻译</title>
    <link href="http://yoursite.com/2019/07/16/Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer%E2%80%99s-disease%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/07/16/Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease翻译/</id>
    <published>2019-07-16T09:15:25.000Z</published>
    <updated>2019-07-16T10:53:36.250Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h2><p>阿尔茨海默病（AD）是最常见的神经变性疾病之一，具有常见的前驱性轻度认知障碍（MCI）阶段，其中记忆丧失是主要抱怨，随着行为问题和自我保健不足逐渐恶化。然而，并非所有临床诊断患有MCI的个体都进展为AD。一部分患有MCI的受试者要么进展到非AD痴呆症，要么在MCI阶段保持稳定而不进展为痴呆症。尽管目前还没有治愈性AD的治疗方法，但正确识别MCI期患者继续发展AD的个体是非常重要的，以便在不久的将来可以获得治愈性治疗。同时，也非常希望能够正确识别那些没有AD病理的MCI阶段的患者，这样他们就可以免于不必要的药理学干预，这些干预最多可能不会给他们带来好处，更糟糕的是，不利的副作用会进一步伤害他们。此外，在这些非AD病例中识别认知障碍的原因可能更容易和更简单，因此正确识别前驱AD也会对这些个体有益。氟脱氧葡萄糖正电子发射断层扫描（FDG-PET）捕获大脑的代谢活动，并且这种成像模式已经被报道在发生结构变化之前识别与AD有关的变化。使用FDG-PET成像设计分类器的先前工作一直很有前景。由于深度学习最近已成为一种强大的工具来挖掘特征并将其用于准确标记给定图像的群体成员，我们提出了一种新型深度学习框架，使用FDG-PET代谢成像来识别MCI阶段的受试者症状前AD并将其与其他MCI患者（非AD /非进行性）区分开来。我们的多尺度深度神经网络获得82.51％的分类准确度，仅使用来自单一模态（FDG-PET代谢数据）的测量值超过了最近文献中发表的其他类似FDG-PET分类器。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>阿尔茨海默病（AD）占所有类型痴呆症的约50-75％，影响了65岁以上的9人中的1人（Kawas，2003; Alzheimer’s and Association，2011）。其特征是进行性认知衰退，如记忆缺失，注意力和执行功能。虽然目前还没有治疗AD的治疗方法，但有几种有希望的药物化合物处于发展的后期阶段，预计很快就会有一个突破性的治疗方法。迄今为止被推测为导致疾病改善治疗缺乏成功的原因之一是无法正确识别轻度认知障碍（MCI）阶段的个体，这些个体将从那些发展AD的人发展为AD。由于其他原因导致MCI症状。当对那些MCI症状不是由于AD导致明显有希望的治疗被认为是失败的那些人施用时，AD的潜在有希望的治疗可能不会显示益处。在MCI阶段识别前驱AD患者的另一个原因是疾病早期的干预可能有助于延缓发病和/或降低面临完全AD的风险，而疾病过程中的干预措施可能减缓疾病，但在已经发生之后不能逆转病理引起的神经元损失。因此，在MCI阶段诊断为症状前AD是一项极其重要的任务，当治愈性治疗可用时，这将变得更加重要和紧迫。对于那些MCI是由AD以外的原因引起的人来说，这种诊断也很有价值，因为这些其他原因可能更容易和更容易识别和管理。<br>氟脱氧葡萄糖正电子发射断层扫描（FDG-PET）提供了大脑代谢活动的定量测量（Mosconi等，2010）。 AD中的脑区域代谢异常被认为发生在结构性脑变化发生之前（Jack等人，2010; Jagust等人，2006）。此外，区域代谢异常被认为是AD患者功能和认知能力下降的基础（Landau等，2011; Kawachi等，2006）。因此，FDG-PET被认为是AD症状前诊断的潜在工具，具有可接受的敏感性和准确性（Cheng等，2015; Davatzikos等，2011; Albert等，2011; Chen等。 ，2010）。以前的研究试图找出患有症状前AD的受试者，但取得了有限的成功（Cheng等，2015; Davatzikos等，2011; Suk等，2014）。尽管付出了相当大的努力，但是先前开发用于识别渐进式MCI的自动化工具的尝试导致精度有限（低于80％），如表2所示（Suk等人，2014; Young等人，2013; Zhu等人。 ，2014; Lange等，2016; Cheng等，2015）。</p><p>深度神经网络最近已经成熟，并且在为识别任务提出的机器学习方法中提供了一些最佳性能（He et al。，2016; Krizhevsky et al。，2012）。深度学习网络也被应用于最近识别AD相关的进展模式。例如，刘等人。训练了一个Stacked Autoencoder来学习隐藏表示，然后用softmax输出层进行分类（Liu et al。，2014），Suk et al。结合深Boltzmann机器和支持向量机来识别AD患者（Suk等，2014），Ortiz等。使用深度学习结构的集合来投票分类（Ortiz等，2016）和Payan等。应用3D卷积神经网络进行NC，MCI和AD科目的多类分类（Payan和Monana，2015）。此外，多尺度处理是物体识别模式挖掘的自然延伸（Zhang et al。，2007; Lowe，2004）。通过将图像下采样到不同的尺寸并在不同的分辨率下提取特征，这种方法也被证明可以提高深度神经网络的分类性能（Tang和Mohamed，2012）。<br>我们还要注意的是，先前发布的在AD中使用FDG-PET图像的非深度学习分类方法通常使用小的测试数据集，包括有限的ADNI数据库子集，或者通过少于一百个图像呈现它们的验证，如图所示。最近的评论结果（Rice和Bisdas，2017）。最近的另一篇Cochrane评价报告还列出了总共14项研究（在所有这些研究中总共分析了421名受试者），其中每项研究仅对数十名受试者进行了研究（Smailagic等，2015a）。相比之下，本文的标志之一是我们对几乎所有可用的ADNI受试者（N = 1051）进行分析，这些受试者在研究中准备本手稿时同时具有结构MRI和FDG-PET图像。最近的评论（Rice和Bisdas，2017; Smailagic等，2015b）进行了以展示为基础的研究，展示了他们在较小数字（大约几十到几百）的结果。因此，我们的论文对迄今为止为该项工作发布的最大数据集提出了最全面的方法验证。其次，我们的论文将是第一个利用深度学习开发多尺度FDG-PET分类器的论文。这些观察结果支持我们提交的新颖性和全面性。</p><p>总的来说，该论文的主要贡献：（1）我们提出了一种新的多尺度深度神经网络框架，以学习AD病理学代谢变化的模式，作为正常对照（NC）代谢模式的判别; （2）我们发现，通过从NC和AD个体转移样本，深层结构可以在早期诊断任务中获得更好的判别能力; 3）我们证明了具有不同验证设置的集合多个分类器可以使所提出的方法更加稳定和稳健，并提高其分类性能。此外，我们对我们的方法进行了全面验证，分析了1051名受试者采取的代谢措施，这些受试者的质量控制要求严格，包括专家手动编辑所有分段以确保准确性。到目前为止，我们的研究可能是第一个利用如此大量FDG-PET图像的研究，因此，这些结果表明了普遍性的良好潜力。</p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p>我们的框架可以分为两个主要步骤：（1）图像预处理和（2）使用深度神经网络进行分类。 在图像处理步骤中，我们执行T1 MR图像的分割，包括将较大的感兴趣区域（ROI）细分为较小尺寸区域的粗到细方法，以及FDG-PET的共同配准 具有相同受试者的T1 MR图像的图像在这些局部区域中提取代谢测量。 在分类步骤中，我们利用在多尺度特征上训练的深度神经网络来学习AD病理学的模式，并使用显示MCI个体分类表现的大群体进行实验。</p><h3 id="Materials"><a href="#Materials" class="headerlink" title="Materials"></a><em>Materials</em></h3><p>用于制备本文的数据来自公众可获得的阿尔茨海默病神经影像学倡议（ADNI）数据库（<a href="http://adni.loni.usc.edu）。" target="_blank" rel="noopener">http://adni.loni.usc.edu）。</a> ADNI于2003年作为公私合作伙伴关系启动，由首席研究员Michael W. Weiner博士领导。 ADNI的主要目标是测试是否可以将连续MRI，PET，其他生物标志物以及临床和神经心理学评估结合起来，以测量轻度认知障碍（MCI）和早期阿尔茨海默病（AD）的进展。<br>在正在进行的纵向ADNI研究中招募的1051名受试者的FDG-PET图像和结构MRI被下载并包含在本文中。受试者分为四类，即（1）正常对照（NC），（2）稳定轻度认知障碍（sMCI）类，（3）进行性轻度认知障碍（pMCI）类，和（4）临床诊断的那些与阿尔茨海默病（AD）。 NC组（N = 304）是没有任何认知投诉的受试者，在整个ADNI研究中仍然保持NC。 sMCI组（N = 409）是在基线时诊断为MCI症状的受试者，并且在制备该手稿时的整个ADNI研究中继续保持为MCI（中位随访时间为3年）。 pMCI组（N = 112）的受试者在基线时被诊断为MCI并且进展至可能的AD，中位时间转换为1年，因此这些图像被前瞻性地标记以指示他们将来进展至AD。可能的AD组（N = 226）受试者临床诊断为在基线时具有AD，并且随后在整个ADNI研究中保持AD的临床诊断。具有诊断变化的受试者（例如NC进展至MCI，或MCI恢复至NC）被排除在拟议研究之外。表1中描述了这些受试者的人口统计学和临床信息。在第二行中，括号中的数字是男性和女性受试者的数量，而其余三行代表年龄，教育年份和MMSE（Mini-精神状态考试）得分。每行中从左到右的两个数字是所有受试者的平均值和标准偏差。 <a href="http://www.adni-info.org上描述了ADNI受试者群组，NC，MCI和AD的临床诊断，图像采集协议程序和采集后预处理程序的详细描述。" target="_blank" rel="noopener">http://www.adni-info.org上描述了ADNI受试者群组，NC，MCI和AD的临床诊断，图像采集协议程序和采集后预处理程序的详细描述。</a></p><h3 id="Image-processing"><a href="#Image-processing" class="headerlink" title="Image processing"></a><em>Image processing</em></h3><p>最近的研究表明，深度学习的方法在解决许多图像识别问题方面是有效的，但大多数已发表的方法都归功于对大量数据样本（大约数百万）的培训。对于医学成像研究而言，这是一个相当大的障碍，例如，使用T1加权和/或FDG-PET成像等方式登记和扫描的受试者数量限制在最多几千人。因此，在这种较小的数据库上直接训练深度神经网络系统最有可能提供低于标准的分类精度和性能。然而，与典型的图像识别任务相比，图像可能具有高度的变异性和多样性，医学成像问题在更大程度上被标准化（相同的组织，例如，大脑），并且通常在严格标准化的协议，从而显示出不同受试者的成像异质性。这有利于减少充分培训所需的样本数量。事实上，我们在本文中的实验表明，随着ADNI提供的图像数量（1000个数量级），这种培训是实用的，可以带来出色的性能。</p><h3 id="ROI-segmentation"><a href="#ROI-segmentation" class="headerlink" title="ROI segmentation"></a><em>ROI segmentation</em></h3><p>使用免费提供的具有默认参数设置的FreeSurfer 5.3软件包将每个T1结构MRI图像分割成灰质和白质（Fischl等，1999）。 Freesurfer进一步将灰质细分为85个皮质和皮层下感兴趣解剖区域（ROI）。尽管在处理ADNI等数据库中出现的各种解剖学配置时存在巨大的稳定性，但有些情况下Freesurfer输出可能包含错误，例如由于不正确的大脑掩模，皮质，皮层下或白质细胞包裹引起的错误。 - 大约10％的案例中的错误。由于此类错误可能会在测量中注入未知的可变性，因此我们通过训练有素且专业的神经病理学家对每个脑图像的每个FreeSurfer分段进行手动质量评估。此外，通过手动编辑校正脑膜，白质，皮质或皮质下分割中的任何错误，并重新运行Freesurfer，直到T1 MR图像分割变得准确，然后包括在分析中。</p><h3 id="Patch-parcellation"><a href="#Patch-parcellation" class="headerlink" title="Patch parcellation"></a><em>Patch parcellation</em></h3><p>由Freesurfer提供的T1 MR图像分割中的每个ROI内的体素进一步细分为不同大小的较小区域，在此表示为“斑块”。由于每个ROI（例如枕叶皮层）可能非常大，因此在ROI中提取新陈代谢的平均值可能会导致由于AD导致的信号变化的灵敏度降低，因为在大量体素上进行平均。然而，如果没有先验知道可能发生变化的位置以及它们的定位程度，最佳方法是多尺度方法所建议的方法，其表明信号是从较小的子区域以多个尺度提取的（精细的）尺度）到较大的子区域（粗尺度），并且这些子区域可以同时用于检测由于疾病或病症引起的变化。因此，我们选择这些补丁的大小在每个ROI中包含500,1000和2000个体素。将每个ROI细分为自身以及许多不同大小的较小单元导致整个大脑的1488,705和343个总数补丁，分别针对每个规模。确定补丁的大小以保持足够的详细信息以及避免过大的特征维度，考虑到本研究中只有有限数量的数据样本。用于此细分的技术是先前发布的技术，其中每个ROI可通过k均值聚类算法使用其空间坐标进行聚类（Raamana等，2015）。该子细分执行一次，在所选模板中，代表性表面可视化显示在图1的左图中。由于ROI具有不同的大小，将每个ROI细分为预定的固定数量的补丁将导致他们的细分大小不同。因此，为了解决这个问题，我们首先预先定义每个补丁（500,1000,2000）中的体素数量，然后根据ROI大小计算每个ROI需要多少个聚类（k值）。这种方法具有以下优点：针对整个大脑中的不同ROI提供均匀的贴片尺寸密度（ROI中的贴片/ ROI中的体素），导致大脑中每个ROI的相同尺度的信号聚集。在模板T1 MR图像FreeSurfer分割进一步细分为多尺度贴片后，该模板MRI被注册到每个目标MRI空间，通过高维度高度精确的非刚性配准方法逐个贴片进行二元分割图像配准。 （LDDMM（Beg等，2005））被认为是领先的注册工具之一。利用分段模板和目标ROI之间的计算的注册图，所选模板的逐片分割向前传播到每个目标，其中这些通过在这些补丁内平均来用于FDG-PET信号聚合。</p><h3 id="Coregistration"><a href="#Coregistration" class="headerlink" title="Coregistration"></a><em>Coregistration</em></h3><p>然后使用FSL-FLIRT程序（Jenkinson等，2002）基于归一化的互信息共同登记每个目标的FDG-PET图像和颅骨剥离的MRI扫描。 使用归一化相关作为成本函数，并将自由度（DOF）设置为12.目视检查每个共同配准的准确性，并且记录并校正共同配准中的任何错误。 然后利用估计的模态间配准图将目标MRI的斑块分割转移到目标FDG-PET域。</p><h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a><em>Normalization</em></h3><p>提取每个贴片中FDG-PET图像的平均强度作为代谢特征。 之前已经注意到，从不同个体获取的FDG-PET图像的强度可能具有不同的“基线”值，使得在没有标准化的个体之间进行比较以校正该基线的不同是不一致的。 遵循该领域的标准方法，脑干是用于内部正常化的区域，因为它被认为最不可能受到AD的影响。 因此计算脑干区域中的平均强度并用于划分每个受试者的脑中所有其他ROI的代谢测量值。</p><h3 id="Visualization-of-metabolism-measures"><a href="#Visualization-of-metabolism-measures" class="headerlink" title="Visualization of metabolism measures"></a><em>Visualization of metabolism measures</em></h3><p>用补片大小作为2000个体素提取的整个数据集的代谢特征在图1中显示为右图。该可视化中的每一列是一个主题，并且每行代表在主题上测量的相同补丁。 在列入NC，sMCI，pMCI和AD的临床诊断类别后，显示列（个体）。 这是所有1051个受试者同时采集的新陈代谢测量分布的精确可视化，并用于突出每个受试者的每个区域的一致代谢模式。 此外，由于任何预处理步骤中的潜在错误，任何个体的代谢测量中的任何异常值都可以在视觉上被检测到，因此这也用于对整个数据库中的测量进行视觉质量控制。</p><h3 id="Multiscale-Deep-Neural-Network"><a href="#Multiscale-Deep-Neural-Network" class="headerlink" title="Multiscale Deep Neural Network"></a><em>Multiscale Deep Neural Network</em></h3><p>在图像处理之后，我们开发了一种多尺度深度神经网络（MDNN），它将多尺度贴片代谢特征作为输入，如图2所示。对于每个主体，有三个不同的输入特征向量，分别为1488,705和343用分别由500,1000和2000个体素组成的贴片提取的尺寸。 MDNN由四个深度神经网络（DNN）组成，第一阶段有三个，第二阶段有一个。 MDNN的培训分为三个步骤。首先，第一阶段的三个DNN独立地训练，具有在不同尺度下提取的代谢特征。然后将三个DNN的输出连接为输入向量，以在第二阶段训练DNN。最后，将MDNN的所有参数与作为输入的三种代谢特征一起调整。如果我们使用N来表示输入要素维度，则对于每个单个DNN，前两个隐藏层的大小设置为3N和3 / 4N，对于最后一个层，隐藏单位的数量设置为100为DNN在第一阶段，在第二阶段DNN为50。选择单元的数量以最大化在第一层中探索跨越不同贴片的非常广泛的可能隐藏相关性的机会，并逐渐减少后续层中的特征的数量以避免过度拟合。</p><h3 id="Network-training"><a href="#Network-training" class="headerlink" title="Network training"></a><em>Network training</em></h3><p>对于每个单独的DNN，训练包含两个步骤，无监督预训练和监督微调，如图3所示。<br>无人监督的预训练。对于每个DNN，我们将其预先训练为堆叠自动编码器（SAE）。 Autoencoder是一种人工神经网络，用于学习输入数据的生成模型。它学习隐藏层（编码器）中的潜在表示，并重建输出层（解码器）中的数据。如果网络由多个编码器层组成，后跟几个解码器层，则它将成为堆叠自动编码器。对于输入数据x，隐藏层中的潜在表示可以显示为：y = s（W x + b），其中W是权重矩阵，b是偏差项，s是我们使用的激活函数在这项工作中纠正了线性函数max（0，x）。重建通过类似的变换发生：z = s（W’y + b’），其中我们用绑定权重W’= W T约束它。平方误差1/2 || x  -  z || 2用于计算反向传播的重建误差。如图2所示，对于每种特征，我们使用了三层SAE。我们不是一起训练所有隐藏层，而是使用贪婪的分层训练（Bengio et al。，2007），它一次训练一个隐藏层。</p><p>•监督微调。 在使用贪婪的逐层预训练初始化参数之后，我们仅保留三个编码器层并为每个DNN添加softmax输出层。 网络被监督微调为多层感知器（MLP）与标准的主题标签。 交叉熵用作梯度体面反向传播的损失函数，其定义为：</p><p>其中N是输入样本的数量，j表示样本类，xi，yi是特征向量和第i个样本的标签，h表示网络函数。</p><p>整个MDNN训练的成本函数与方程式相同。 （1）除了使用不同的输入特征向量。 所有训练步骤都采用相同的反向传播方法。 使用小批量梯度下降最小化成本函数（Bengio，2012）; 将训练集随机分成几个小批量或子集，每批50个样本。 在每次迭代中，仅使用这些小批量中的一个用于最小化。 在将所有样本用于训练之后，我们重新排序训练集并将其再次划分，使得每个不同回波中的批次将不具有相同的样本。</p><h3 id="Reducing-chances-of-over-fitting"><a href="#Reducing-chances-of-over-fitting" class="headerlink" title="Reducing chances of over-fitting"></a><em>Reducing chances of over-fitting</em></h3><p>深度神经网络可能过度拟合给定的样本，因此，在没有任何正则化的情况下训练它将导致小的训练误差和可能的大的泛化误差。我们采用了两种技术来防止过度拟合：（1）辍学（Srivastava等，2014），以及（2）提前停止。<br>•辍学策略。在深度神经网络的训练中，辍学是防止过度拟合的流行策略。通过在每个隐藏层之后插入一个dropout图层，我们可以选择保留为下一层的单位百分比。对于训练，在每次迭代中随机丢弃一半隐藏单元，而保留另一半隐藏单元以将特征提供给下一层。为了测试，保留所有单元以对受试者进行分类。通过避免训练每个训练样本上的所有隐藏单元，这种正则化技术不仅防止了对训练数据的复杂协同适应，而且减少了计算量并提高了训练速度。<br>•早期停止策略。早期停止是我们应用于防止过度安装的另一种技术。当使用梯度下降等迭代方法训练深层架构时，改善网络与训练集的匹配将在开始时提高性能，但在某一点上，它将开始增加泛化误差，同时减少训练误差。为了防止这种情况，早期停止通常用于在过度拟合之前提供适当迭代次数的指导。在这项工作中，我们将训练数据随机分成训练集和验证集，网络参数仅通过训练集的数据进行优化，而验证集仅用于确定早期停止时间点：网络的迭代验证集的泛化误差最小。值得一提的是，训练集，验证集和测试数据是截然不同的;网络的性能仅在测试集上进行评估。</p><h3 id="Instance-transfer-learning"><a href="#Instance-transfer-learning" class="headerlink" title="Instance-transfer learning"></a><em>Instance-transfer learning</em></h3><p>转移学习是缺乏数据样本时最常用的解决分类问题的方法之一（Pan and Yang，2010）。通过将与目标组相关的其他源组的实例引入到训练中，它扩大了训练集并用于提高分类准确性。通常需要优化算法来找到最相关的实例或重新加权来自其他组的实例（Zhang et al。，2014）。然而，在这项工作中，那些进展为AD的MCI可能处于潜在AD病理的早期阶段。此外，如图1的右图所示，在该pMCI组中发现了类似的代谢模式，如图1的右图所示。因此，AD组可以直接用于扩大pMCI训练集。考虑到这两个群体的相似性和相关性。另一方面，可以将NC个体添加到sMCI组以增加可能不具有AD病理学的个体库。交叉验证实验证明了我们使用这种方法背后的基本原理，即使用转移实例训练的网络显示出比没有特别在sMCI中使用转移实例的网络与用于检测前驱AD的pMCI分类任务相比更好的判别能力。在MCI集团中。</p><h3 id="Ensemble-classifiers"><a href="#Ensemble-classifiers" class="headerlink" title="Ensemble classifiers"></a><em>Ensemble classifiers</em></h3><p>由于本研究中数据样本数量有限，特别是对于pMCI受试者，我们只能随机选择一小组数据进行验证，以保留足够的数据用于培训和测试。如此少量的验证样本可能无法代表整个数据集，并可能使网络偏向于在验证集上过度拟合。为了设计包含更好的鲁棒性，稳定性和普遍性的算法，选择了一个集合分类器框架。在这个框架中，我们培训了10个独立的网络，而不是培训一个网络，以便对最终的分类结果进行“投票”。从数据集中随机选择训练数据后，进一步分为10组。在训练阶段，9组用于训练网络，另一组用于验证。这些步骤重复10次，每组用于验证，从而产生10个不同的网络。在测试阶段，将测试数据馈送到每个网络，并生成属于每个类的样本的概率。对于每个样本，我们总结了来自10个网络的概率以确定分类结果。实验表明，集合多个网络使分类器更加鲁棒和稳定，并在统计上提高了分类精度。</p><h3 id="Experimental-setup"><a href="#Experimental-setup" class="headerlink" title="Experimental setup"></a><em>Experimental setup</em></h3><p>建议的网络是使用开源深度学习工具箱Tensorflow构建的（Abadi等，2015）。三个二元分类实验，（i）NC与AD，（ii）sMCI与pMCI和（iii）sMCI与pMCI以及来自NC和AD的转移学习，用于测试所提出的网络架构。进行十次交叉验证以测试每个训练网络的分类结果的普遍性。简言之，将可用数据集随机分成10个子集，其中9个用于训练，其余用于测试。为了防止过度拟合，选择训练数据的子集作为验证集并用于提供停止网络优化的指导（内部分类器优化循环）。如上所述，该训练过程重复十次，以训练十个不同的网络以“投票”以获得最终的分类结果。学习率分别设定为10-1和10-4，用于预训练和微调。使用准确度，灵敏度和特异性的标准测量来评估所提出的分类器的性能。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>表2中显示了交叉验证实验的平均准确度，灵敏度和特异性以及当前可用的现有技术方法的结果。 此外，我们还将结果与使用支持向量机（SVM）构建的标准分类器（Chang和Lin，2011）和使用主成分分析（PCA）的维数降低进行了比较（Wold等，1987） ）在相同的多尺度功能上。 选择PCA特征向量的数量以保持95％的方差。 径向基函数用于SVM，因为它在分类任务中具有优越的性能。 表2的最后三行来自sMCI vs pMCI实验，其中实例从NC和AD组转移用于训练。</p><h3 id="Multiscale-classification"><a href="#Multiscale-classification" class="headerlink" title="Multiscale classification"></a><em>Multiscale classification</em></h3><p>分类实验是通过使用包括500,1000和2000个体素的不同贴片尺寸以粗到精的方法在几个尺度上提取特征来进行的（表3），从而在整体中产生1488,705和343个贴片。 大脑，分别。 正如所料，分类结果对不同分类实验中不同的斑块大小敏感，表明感兴趣的信号位于不同的空间尺度。 这些结果列于表3中。</p><h3 id="Ensemble-classifier-design"><a href="#Ensemble-classifier-design" class="headerlink" title="Ensemble classifier design"></a><em>Ensemble classifier design</em></h3><p>如方法部分所述，此处提供的分类结果来自分类器集合而不是单个分类器。在这个集合中，训练多个网络，然后对最终的分类结果进行“投票”。具有或不具有集合分类器的分类性能如图4所示。从左到右的三个图分别代表NC与AD，sMCI与pMCI和sMCI与pMCI的分类实验，其中分别来自NC和AD组的转移实例。 y轴绘制分类精度，x轴代表不同的实验。在x轴上，’ensemble’表示来自集合分类器的结果，数字1到10表示没有集合分类器的实验。在每个非整体实验中，我们还进行了10次交叉验证实验，但是我们不是训练10个网络对最终结果进行“投票”，而是随机选择验证集并训练单个分类器。图4中显示的结果显示了交叉验证实验的平均值和标准偏差。</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><h3 id="Comparison-with-state-of-the-art-methods"><a href="#Comparison-with-state-of-the-art-methods" class="headerlink" title="Comparison with state-of-the-art methods"></a><em>Comparison with state-of-the-art methods</em></h3><p>我们将我们研究中获得的结果与标准SVM和PCA + SVM方法以及Suk等人最近提出的最新方法进行了比较。 （2014），Young等。 （2013），朱等人。 （2014），Lange等。 （2016）和Cheng等人。 （2015）关于NC与AD的分类，以及sMCI与pMCI（在MCI阶段检测前驱AD）。虽然不同研究中使用的数据并不相同，但它们都来自ADNI数据库，因此它们共享一个类似的获取和预处理程序，允许进行比较。<br>对于使用FDG-PET代谢测量的NC组与AD组的分类，我们提出的方法，精确度为93.58％，比之前公布的最佳方法高出1.28％（Zhu et al。，2014 ）。此外，该结果还与之前使用多模式研究（FDG-PET +结构MRI）报道的95.35％的准确度相当（Suk等，2014）。为了将MCI阶段的个体区分为sMCI或pMCI组，我们提出的方法（没有转移学习）的表现明显好于（比现有的最先进方法高出至少10％）。此外，与将FDG-PET测量结果与其他方式相结合的多模式方法相比，使用单一模态（代谢FDG-PET）的方法对于sMCI与pMCI任务相比仍然高5.63％（Young et al。， 2013; Suk等，2014）。<br>使用在训练阶段从NC和AD受试者转移的实例，我们的方法在sMCI与pMCI分类任务中的准确性被发现为82.51％，这不仅比先前的最佳结果高出约10.91％（Cheng等人。，2015）使用单一模态，但也比以前的多模态方法高3.11％。即使只使用单一模态而没有NC和AD科目，我们的方法的准确性比Cheng等人的方法更高（2.15％）。 （2015）使用多种形式与NC和AD科目的转学习。对于使用相同多尺度特征的SVM和PCA + SVM实验，基于PCA的特征选择提高了SVM的分类性能。但与提出的深度神经网络相比，PCA + SVM的准确度降低了6％，这表明在这种情况下MDNN性能更优越。<br>总之，我们的实验表明，与过去的研究相比，我们提出的基于深度学习的方法显示sMCI和pMCI之间的分类准确性更高，无论是使用单一模式还是多模式研究（Suk等，2014）。无论从NC与AD实验获得的转移学习的应用如何，我们的准确性结果也是优越的（Cheng等，2015）。这些结果表明，提出的方法的优越性表明FDG-PET模式作为一项独立调查的潜力，无论转移学习的使用（Cheng et al。，2015; Suk et al。，2014）用于区分pMCI个体与在MCI个人中。</p><h3 id="Multiscale-classification-1"><a href="#Multiscale-classification-1" class="headerlink" title="Multiscale classification"></a><em>Multiscale classification</em></h3><p>在AD与NC分类实验中发现，随着贴片尺寸的增加，精度普遍降低，表明AD的变化可能局限于较小的尺度，并且在使用较大尺寸的贴片时趋于平均。我们没有观察到其他分类实验的类似模式（sMCI vs pMCI和sMCI vs pMCI使用转移学习）。因此，发现使用较小尺寸的斑块来提取新陈代谢特征并不总能恢复较高的辨别力信息，并且可能sMCI与pMCI情况下的信号甚至更精细地定位，并且可能需要在每个ROI中使用甚至更小尺寸的补丁。然而，与单一尺度特征相比，使用所有多尺度特征的组合分类性能（包括从每个单一尺度特征获得的判别信息）产生了更高的精度结果。这表明在连锁多尺度特征上训练的网络（图2）仍然能够学习本文中使用的从小到大的补丁大小的隐藏模式。未来的工作可能包括在系统优化框架中学习每个分类实验中信号存在的规模，以找到最佳分类性能的最佳尺寸特征。</p><h3 id="Ensemble-classifier"><a href="#Ensemble-classifier" class="headerlink" title="Ensemble classifier"></a><em>Ensemble classifier</em></h3><p>图4中的结果表明，通过训练和验证集的不同划分，各个分类器的分类精度（中间图像的第二和第四个条）可能有多达8％的差异。 ，表明每个单一分类器的性能不稳定，普遍性降低。 在所有三个实验中，集合分类器的准确性高于单个分类器的平均值，这表明具有不同训练和验证集的分集的集合分类器可以产生更稳健和稳定的分类器，因此可以改善分类 性能更好的普遍性。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>在本文中，我们使用多尺度贴片FDG-PET深度学习功能提出了一种新的AD早期诊断框架。所提出的框架利用转移学习方法和集合分类器策略来改善深度神经网络在区分sMCI和pMCI主体的任务中的性能。在1051名受试者的FDG-PET图像的大型数据库上进行的实验提供了支持三种断言的证据。 （1）所提出的方法，使用仅来自单一FDG-PET模态的特征，能够胜过在sMCI和pMCI分类任务中采用多模态特征的现有方法。 （2）所提出的网络可以从多尺度特征中学习判别模式，以提供具有更好判别性能的更健壮的分类器。 （3）使用不同验证集的集合多个分类器可以使网络更加健壮和稳定，并在统计上提高其分类性能。<br>对于未来的工作，将所提出的框架扩展到包含来自多种模态的信息是自然的，假设所得到的深度神经网络将从多个模态数据中学习更多信息，从而进一步改进所获得的分类准确度。尽管sMCI vs pMCI实验常用于验证近期研究（包括我们的研究）中方法的鉴别能力，但我们只能知道那些sMCI受试者在研究进展中时保持稳定并且可以转化为AD或其他神经退行性疾病。在将来。因此，临床诊断的sMCI受试者的基本事实可能不完全准确，因此可能在分类中引入噪声/偏倚。幸运的是，随着更多数据的收集，分类系统将更好地捕获这些和其他噪声和可变性来源，我们提出的深度学习集合分类器可能非常适合这种情况。</p><h2 id="Acknowledgments"><a href="#Acknowledgments" class="headerlink" title="Acknowledgments"></a>Acknowledgments</h2><p>这项工作得到了国家科学工程研究委员会（NSERC），加拿大卫生研究院（CIHR），迈克尔史密斯健康研究基金会（MSFHR），加拿大大脑，太平洋阿尔茨海默氏症研究基金会（PARF）的支持。该项目的数据收集和共享由阿尔茨海默氏病神经影像学倡议（ADNI）（国家卫生研究院资助U01 AG024904）和DOD ADNI（国防部颁发号W81XWH-12-2-0012）资助。 ADNI由国家老龄化研究所，国家生物医学成像和生物工程研究所资助，并通过以下方面的大量贡献：AbbVie，阿尔茨海默氏症协会;阿尔茨海默氏症药物发现基金会; Araclon Biotech; BioClinica，Inc。;生物遗传; Bristol-Myers Squibb Company; CereSpir，Inc。; Cogstate; Eisai Inc。; Elan Pharmaceuticals，Inc。;礼来公司;欧蒙; F. Hoffmann-La Roche Ltd及其附属公司Genentech，Inc。; Fujirebio公司; GE Healthcare; IXICO有限公司; Janssen Alzheimer Immunotherapy Research＆Development，LLC。;强生药业研发有限责任公司; Lumosity; Lundbeck公司; Merck＆Co.，Inc。; Meso Scale Diag- nostics，LLC。; NeuroRx研究; Neurotrack技术;诺华制药公司;辉瑞公司; Piramal成像;施维雅;武田制药公司;和过渡治疗学。加拿大卫生研究院正在提供资金支持加拿大的ADNI临床站点。国家卫生研究院基金会（<a href="https://fnih.org）为私营部门的贡献提供了便利。受助组织是北加利福尼亚州研究和教育研究所，该研究由南加州大学阿尔茨海默氏症治疗研究所协调。" target="_blank" rel="noopener">https://fnih.org）为私营部门的贡献提供了便利。受助组织是北加利福尼亚州研究和教育研究所，该研究由南加州大学阿尔茨海默氏症治疗研究所协调。</a> ADNI数据由南加州大学的神经成像实验室传播。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a><strong>References</strong></h2><ul><li><p>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viégas, F., Vinyals, O., War- den, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X. (2015). TensorFlow: Large- scale machine learning on heterogeneous systems. Software available from ten- sorflow.org, URL <a href="http://tensorflow.org/" target="_blank" rel="noopener">http://tensorflow.org/</a>. </p></li><li><p>Albert, M.S., DeKosky, S.T., Dickson, D., Dubois, B., Feldman, H.H., Fox, N.C., Gamst, A., Holtzman, D.M., Jagust, W.J., Petersen, R.C., et al., 2011. The diag- nosis of mild cognitive impairment due to Alzheimer’s disease: recommenda- tions from the national institute on aging-Alzheimer’s association workgroups on diagnostic guidelines for Alzheimer’s disease. Alzheimer’s Dementia 7 (3), 270–279. </p></li><li><p>Alzheimer’s, Association, 2011. 2011 Alzheimer’s disease facts and figures.. Alzheimer’s Dementia 7 (2), 208. </p></li><li><p>Beg, M.F., Miller, M.I., Trouvé, A., Younes, L., 2005. Computing large deformation metric mappings via geodesic flows of diffeomorphisms. Int. J. Comput. Vis. 61 (2), 139–157. </p></li><li><p>Bengio, Y., 2012. Practical recommendations for gradient-based training of deep ar- chitectures. In: Neural Nnetworks: Tricks of the Trade. Springer, pp. 437–478. </p></li><li><p>Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., et al., 2007. Greedy layer-wise training of deep networks. Adv. Neural Inf. Process. Syst. 19, 153. </p></li><li><p>Chang, C.C., Lin, C.J., 2011. Libsvm: a library for support vector machines. ACM Trans. Intell. Syst. Technol. 2 (3), 27. </p></li><li><p>Chen, K., Langbaum, J.B., Fleisher, A.S., Ayutyanont, N., Reschke, C., Lee, W., Liu, X., Bandy, D., Alexander, G.E., Thompson, P.M., et al., 2010. Twelve-month metabolic declines in probable Alzheimer’s disease and amnestic mild cognitive impair- ment assessed using an empirically pre-defined statistical region-of-interest: findings from the Alzheimer’s disease neuroimaging initiative. Neuroimage 51 (2), 654–664. </p></li><li><p>Cheng, B., Liu, M., Zhang, D., Munsell, B.C., Shen, D., 2015. Domain transfer learning for MCI conversion prediction. IEEE Trans. Biomed. Eng. 62 (7), 1805–1817. Davatzikos, C., Bhatt, P., Shaw, L.M., Batmanghelich, K.N., Trojanowski, J.Q., 2011. Prediction of MCI to AD conversion, via MRI, CSF biomarkers, and pattern classification. Neurobiol. Aging 32 (12). 2322–e19.</p></li><li>Fischl, B., Sereno, M.I., Dale, A.M., 1999. Cortical surface-based analysis: ii: inflation, flattening, and a surface-based coordinate system. Neuroimage 9 (2), 195–207. </li><li>He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recogni- tion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778.</li><li>Jack, C.R., Knopman, D.S., Jagust, W.J., Shaw, L.M., Aisen, P.S., Weiner, M.W., Petersen, R.C., Trojanowski, J.Q., 2010. Hypothetical model of dynamic biomarkers of the Alzheimer’s pathological cascade. Lancet Neurol. 9 (1), 119–128.</li><li>Jagust, W., Gitcho, A., Sun, F., Kuczynski, B., Mungas, D., Haan, M., 2006. Brain imag- ing evidence of preclinical Alzheimer’s disease in normal aging. Ann. Neurol. 59 (4), 673–681.</li><li>Jenkinson, M., Bannister, P., Brady, M., Smith, S., 2002. Improved optimization for the robust and accurate linear registration and motion correction of brain images. Neuroimage 17 (2), 825–841.</li><li>Kawachi, T., Ishii, K., Sakamoto, S., Sasaki, M., Mori, T., Yamashita, F., Matsuda, H.,  Mori, E., 2006. Comparison of the diagnostic performance of FDG-PET and VB- M-MRI in very mild Alzheimer’s disease. Eur. J. Nucl. Med. Mol. Imag. 33 (7), 801–809. </li><li>Kawas, C.H., 2003. Early Alzheimer’s disease. N top N. Engl. J. Med. 349 (11), 1056–1063. </li><li>Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems, pp. 1097–1105. </li><li>Landau, S.M., Harvey, D., Madison, C.M., Koeppe, R.A., Reiman, E.M., Foster, N.L., Weiner, M.W., Jagust, W.J., Initiative, A.D.N., et al., 2011. Associations between cognitive, functional, and fdg-pet measures of decline in ad and mci. Neurobiol. Aging 32 (7), 1207–1218. </li><li>Lange, C., Suppa, P., Frings, L., Brenner, W., Spies, L., Buchert, R., 2016. Optimization of statistical single subject analysis of brain FDG-PET for the prognosis of mild cognitive impairment-to-Alzheimer’s disease conversion. J. Alzheimers Dis. 49 (4), 945–959. </li><li>Liu, S., Liu, S., Cai, W., Pujol, S., Kikinis, R., Feng, D., 2014. Early diagnosis of Alzheimer’s disease with deep learning. In: Biomedical Imaging (ISBI), 2014 IEEE 11th International Symposium on. IEEE, pp. 1015–1018. </li><li>Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis. 60 (2), 91–110. </li><li>Mosconi, L., Berti, V., Glodzik, L., Pupi, A., De Santi, S., de Leon, M.J., 2010. Pre-clin- ical detection of Alzheimer’s disease using FDG-PET, with or without amyloid imaging. J. Alzheimers Dis. 20 (3), 843–854. </li><li>Ortiz, A., Munilla, J., Gorriz, J.M., Ramirez, J., 2016. Ensembles of deep learning ar- chitectures for the early diagnosis of the Alzheimer’s disease. Int. J. Neural Syst. 26 (07), 1650025. </li><li>Pan, S.J., Yang, Q., 2010. A survey on transfer learning. IEEE Trans. Knowl. Data Eng. 22 (10), 1345–1359. Payan, A., Montana, G., 2015. Predicting Alzheimer’s disease: a neuroimaging study with 3D convolutional neural networks. arXiv:1502.02506. </li><li>Raamana, P.R., Weiner, M.W., Wang, L., Beg, M.F., et al., 2015. Alzheimer Disease Neuroimaging Initiative, Thickness network features for prognostic applications in dementia. Neurobiol. Aging 36, S91–S102. <a href="https://www.sciencedirect.com/" target="_blank" rel="noopener">https://www.sciencedirect.com/</a> science/article/pii/S0197458014005521. </li><li>Rice, L., Bisdas, S., 2017. The diagnostic value of FDG and amyloid PET in Alzheimer’s disease—a systematic review. Eur. J. Radiol. 94, 16–24. </li><li>Smailagic, N., Vacante, M., Hyde, C., Martin, S., Ukoumunne, O., Sachpekidis, C., 2015. 18F-FDG PET for the early diagnosis of Alzheimer’s disease dementia and other dementias in people with mild cognitive impairment (MCI). Cochrane Libr.. </li><li>Smailagic, N., Vacante, M., Hyde, C., Martin, S., Ukoumunne, O., Sachpekidis, C., 2015. 18F-FDG PET for the Early Diagnosis of Alzheimer’s Disease Dementia and Other Dementias in People with Mild Cognitive Impairment (MCI). In: Sach- pekidis, C. (Ed.), Cochrane Database of Systematic Reviews, 1. John Wiley &amp; Sons, Ltd, Chichester, UK, p. CD010632. doi:10.1002/14651858.CD010632.pub2. </li><li>Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., Salakhutdinov, R., 2014. Dropout: a simple way to prevent neural networks from overfitting.. J. Mach. Learn. Res. 15 (1), 1929–1958. </li><li>Suk, H.-I., Lee, S.-W., Shen, D., Initiative, A.D.N., et al., 2014. Hierarchical feature rep- resentation and multimodal fusion with deep learning for AD/MCI diagnosis. Neuroimage 101, 569–582. </li><li>Tang, Y., Mohamed, A.-r., 2012. Multiresolution deep belief networks.. In: AISTATS, pp. 1203–1211. </li><li>Wold, S., Esbensen, K., Geladi, P., 1987. Principal component analysis. Chemometr. Intell. Lab. Syst. 2 (1–3), 37–52. </li><li>Young, J., Modat, M., Cardoso, M.J., Mendelson, A., Cash, D., Ourselin, S., Initia- tive, A.D.N., et al., 2013. Accurate multimodal probabilistic prediction of conver- sion to Alzheimer’s disease in patients with mild cognitive impairment. Neu- roImage 2, 735–745. </li><li>Zhang, Q., Li, H., Zhang, Y., Li, M., 2014. Instance transfer learning with multisource dynamic tradaboost. Scientific World J. 2014. </li><li>Zhang, W., Zelinsky, G., Samaras, D., 2007. Real-time accurate object detection using multiple resolutions. In: Computer Vision, 2007. ICCV 2007. IEEE 11th Interna- tional Conference on. IEEE, pp. 1–8. </li><li>Zhu, X., Suk, H.-I., Shen, D., 2014. A novel matrix-similarity based loss function for joint regression and classification in AD diagnosis. Neuroimage 100, 91–105. </li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="ADNI" scheme="http://yoursite.com/tags/ADNI/"/>
    
  </entry>
  
  <entry>
    <title>AD预处理工具</title>
    <link href="http://yoursite.com/2019/07/15/AD%E9%A2%84%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7/"/>
    <id>http://yoursite.com/2019/07/15/AD预处理工具/</id>
    <published>2019-07-15T03:43:56.000Z</published>
    <updated>2019-07-20T14:16:08.823Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="FSL"><a href="#FSL" class="headerlink" title="FSL"></a>FSL</h2><h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><p><a href="http://learning-archive.org/wp-content/uploads/2017/09/Ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85AFNI%E5%92%8CFSL.pdf" target="_blank" rel="noopener">Ubuntu下安装AFNI和FSL</a>  </p><p><strong>Notice: </strong> <code>source .profile</code></p><h3 id="FSL5-0使用教程"><a href="#FSL5-0使用教程" class="headerlink" title="FSL5.0使用教程"></a>FSL5.0使用教程</h3><ul><li><a href="https://blog.csdn.net/baidu_36669549/article/details/69753022" target="_blank" rel="noopener">FSL5.0使用教程</a><ul><li><a href="http://neuro.debian.net/install_pkg.html?p=fsl-atlases" target="_blank" rel="noopener">fsl-atlases</a></li></ul></li></ul><h2 id="FreeSurfer"><a href="#FreeSurfer" class="headerlink" title="FreeSurfer"></a>FreeSurfer</h2><h3 id="Installation-1"><a href="#Installation-1" class="headerlink" title="Installation"></a>Installation</h3><ul><li><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall" target="_blank" rel="noopener">FreeSurfer Download and Install</a></p></li><li><p>解决Freesurfer的license问题：可以在<a href="http://surfer.nmr.mgh.harvard.edu/registration.html" target="_blank" rel="noopener">官网</a>注册，邮箱会收到license.txt文件，拷贝至FreeSurfer解压目录。比如，我的license已经注册并下载，放在~/Downloads文件夹下。可以执行如下命令拷贝：参考于<a href="https://www.jianshu.com/p/4db8227cbb81" target="_blank" rel="noopener">FSL/FreeSurfer安装教程</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">~/Downloads$ sudo cp license.txt /usr/local/freesurfer/</span><br></pre></td></tr></table></figure></li><li><p>更改subjects文件夹权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo chmod -R 777 /usr/<span class="built_in">local</span>/freesurfer/subjects</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="Test-your-FreeSurfer-Installation"><a href="#Test-your-FreeSurfer-Installation" class="headerlink" title="Test your FreeSurfer Installation"></a>Test your FreeSurfer Installation</h4><ul><li><p><strong>Example 1:</strong> Convert the sample-001.mgz to nifti format.</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&gt; cp <span class="variable">$FREESURFER_HOME</span>/subjects/sample-001.mgz .</span></span><br><span class="line"><span class="meta">$</span><span class="bash">&gt; mri_convert sample-001.mgz sample-001.nii.gz</span></span><br><span class="line">...</span><br><span class="line">reading from sample-001.mgz...</span><br><span class="line">TR=7.25, TE=3.22, TI=600.00, flip angle=7.00</span><br><span class="line">i_ras = (-0, -1, -0)</span><br><span class="line">j_ras = (-0, 0, -1)</span><br><span class="line">k_ras = (-1, 0, 0)</span><br><span class="line">writing to sample-001.nii.gz...</span><br></pre></td></tr></table></figure></li><li><p><strong>Example 2:</strong> Perform a full recon-all on the nifti file.</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&gt; <span class="built_in">export</span> SUBJECTS_DIR=&lt;path to subject directory&gt; <span class="comment"># SUBJECTS_DIR变量为存储数据的目录</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">&gt; recon-all -i sample-001.nii.gz -s bert -all (creates a folder called bert <span class="keyword">in</span> SUBJECTS_DIR)</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Started at 2019年 07月 17日 星期三 20:54:07 CST </span><br><span class="line">Ended   at 2019年 07月 18日 星期四 02:37:36 CST</span><br><span class="line"><span class="meta">#</span><span class="bash">@<span class="comment">#%# recon-all-run-time-hours 5.725</span></span></span><br><span class="line">recon-all -s bert finished without error at 2019年 07月 18日 星期四 02:37:36 CST</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>Process your own data with a command such as this:</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">recon-all \</span><br><span class="line">  -i  &lt;one slice in the anatomical dicom series&gt; \</span><br><span class="line">  -s  &lt;subject id that you make up&gt; \</span><br><span class="line">  -sd &lt;directory to put the subject folder in&gt; \</span><br><span class="line">  -all</span><br></pre></td></tr></table></figure><p>where the input (-i) file is a single file representing a <code>T1-weighted</code> data set. If you have DICOM images, you must find a file in the T1 series to pass. You can do this with the dcmunack command.</p></li><li><p><strong>Example 3:</strong> Perform a full recon-all on a pre-existing subject folder</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&gt; <span class="built_in">export</span> SUBJECTS_DIR=&lt;path to subject directory&gt;</span></span><br><span class="line"><span class="meta">$</span><span class="bash">&gt; recon-all -s bert -all</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Started at 2019年 07月 17日 星期三 21:38:11 CST </span><br><span class="line">Ended   at 2019年 07月 18日 星期四 02:40:42 CST</span><br><span class="line"><span class="meta">#</span><span class="bash">@<span class="comment">#%# recon-all-run-time-hours 5.042</span></span></span><br><span class="line">recon-all -s bert finished without error at 2019年 07月 18日 星期四 02:40:42 CST</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p><strong>Example 4:</strong> View the output volumes, surfaces and subcortical segmentation of the fully recon-ed subject bert.</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&gt; <span class="built_in">cd</span> <span class="variable">$SUBJECTS_DIR</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">&gt; freeview -v \</span></span><br><span class="line">    bert/mri/T1.mgz \</span><br><span class="line">    bert/mri/wm.mgz \</span><br><span class="line">    bert/mri/brainmask.mgz \</span><br><span class="line">    bert/mri/aseg.mgz:colormap=lut:opacity=0.2 \</span><br><span class="line">    -f \</span><br><span class="line">    bert/surf/lh.white:edgecolor=blue \</span><br><span class="line">    bert/surf/lh.pial:edgecolor=red \</span><br><span class="line">    bert/surf/rh.white:edgecolor=blue \</span><br><span class="line">    bert/surf/rh.pial:edgecolor=red</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Some notes on the above command line:</span><br><span class="line"></span><br><span class="line">- bert is the name of the subject</span><br><span class="line">- The flag -v is used to open some of `the most commonly used volumes` including:</span><br><span class="line">  - brainmask.mgz : skull-stripped volume primarily used for troubleshooting 头骨剥离的卷主要用于故障排除  T1去除颅骨 ➜ brainmask (voxel intensities)</span><br><span class="line">  - wm.mgz : white matter mask also used for troubleshooting 白质掩膜也用于故障排除</span><br><span class="line">  - aseg.mgz : subcortical segmentation loaded with its corresponding color table and at a low opacity. For more information on the subcortical segmentation, see [here](https://surfer.nmr.mgh.harvard.edu/fswiki/SubcorticalSegmentation).皮质下分割加载了相应的颜色表并且低不透明度。 有关皮质下分割的更多信息，请参见此处。 brainmask上色(`遵循皮质下强度边界`) ➜ aseg (labeled structures) 显示皮下结构的分割情况</span><br><span class="line"></span><br><span class="line">- The flag -f is used to load surfaces (`遵循灰质和白质边界`)</span><br><span class="line">- white &amp; pial(软膜的) surfaces are loaded for each hemisphere(半球) &amp; with color indicated by 'edgecolor'</span><br><span class="line">The `white surface` (blue line) is used to `calculate total white matter volume` and should accurately `follow the boundary between white matter and gray matter`. The `pial surface` (red line) is used to `calculate cortical gray matter volume and should accurately `follow the boundary between the gray matter and the CSF`.</span><br></pre></td></tr></table></figure><ul><li><p><code>freeview.bin: error while loading shared libraries: libpng12.so.0: cannot open shared object file: No such file or directory</code>：Download the shared library from <a href="https://packages.ubuntu.com/xenial/amd64/libpng12-0/download" target="_blank" rel="noopener">https://packages.ubuntu.com/xenial/amd64/libpng12-0/download</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i libpng12-0_1.2.54-1ubuntu1.1_amd64.deb</span><br></pre></td></tr></table></figure></li><li><p><code>freeview.bin: error while loading shared libraries: libjpeg.so.62: cannot open shared object file: No such file or directory</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt install libjpeg62</span><br></pre></td></tr></table></figure></li><li><p>.nii文件 同理查看</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&gt; freeview -v sample-001.nii.gz</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="FreeSurfer-Tutorials"><a href="#FreeSurfer-Tutorials" class="headerlink" title="FreeSurfer Tutorials"></a><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/Tutorials" target="_blank" rel="noopener">FreeSurfer Tutorials</a></h3><ul><li><p>Preparation</p><ul><li><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/Data" target="_blank" rel="noopener">FreeSurfer Tutorial Datasets</a></li></ul></li><li><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/OutputData_freeview" target="_blank" rel="noopener">Introduction to FreeSurfer Output</a>：熟悉freeview界面(<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/MacCommands" target="_blank" rel="noopener">快捷键</a>)</p><p>为了帮助验证准确度，请调整亮度和对比度，以便轻松识别灰色和白色物质之间的强度变化。要执行此操作，请在按住“Shift”键的同时左键单击图像并拖动鼠标（Make sure the brainmask volume is highlighted in the left menu in order for this to work.）</p><p>.white和.pial分别可用于计算白质和灰质体积；aseg用于计算皮质下体积的测量</p></li><li><p>we mean <code>wm voxels</code> have an intensity value of somewhere <code>between 100 and 110</code>. And <code>wm voxels</code> are <code>between a value of 85 and 100</code>. In the <a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/TroubleshootingData" target="_blank" rel="noopener">TroubleshootingData</a> tutorial, we’ll go over what to do if there is an intensity normalization error.</p></li><li><p>WM卷：FreeSurfer对<code>白质的初始分割</code>（以<code>灰色</code>显示），并添加了自动拓扑定位器（白色）。</p></li><li><p>3D view：绿色区域是回旋区域，红色区域是凹陷区域。</p></li><li><p>Notice that all subcortical gray matter is not a part of the surface labels (because again, those areas do not count towards the cortical surface measures).注意，所有皮质下灰质不是表面标签的一部分（因为这些区域不再计入皮质表面测量）。</p></li></ul><h3 id="基本指令说明"><a href="#基本指令说明" class="headerlink" title="基本指令说明"></a><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FreeSurferCommands" target="_blank" rel="noopener">基本指令说明</a></h3><h4 id="recon-all"><a href="#recon-all" class="headerlink" title="$ recon-all"></a>$ recon-all</h4><p> <a href="https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAllDevTable" target="_blank" rel="noopener">详解</a> </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">USAGE: recon-all</span><br><span class="line"></span><br><span class="line"> Required Arguments:</span><br><span class="line">   -subjid &lt;subjid&gt;</span><br><span class="line">   -&lt;process directive&gt;</span><br><span class="line"></span><br><span class="line"> Fully-Automated Directive:</span><br><span class="line">  -all           : performs all stages of cortical reconstruction</span><br><span class="line">  -autorecon-all : same as -all</span><br><span class="line"></span><br><span class="line"> Manual-Intervention Workflow Directives:</span><br><span class="line">  -autorecon1    : process stages 1-5 (see below)  # no-use-gpu: about 10 min</span><br><span class="line">  -autorecon2    : process stages 6-23</span><br><span class="line">                   after autorecon2, check white surfaces:</span><br><span class="line">                     a. if wm edit was required, then run -autorecon2-wm</span><br><span class="line">                     b. if control points added, then run -autorecon2-cp</span><br><span class="line">                     c. proceed to run -autorecon3</span><br><span class="line">  -autorecon2-cp : process stages 12-23 (uses -f w/ mri_normalize, -keep w/ mri_seg)</span><br><span class="line">  -autorecon2-wm : process stages 15-23</span><br><span class="line">  -autorecon2-inflate1 : 6-18</span><br><span class="line">  -autorecon2-perhemi : tess, sm1, inf1, q, fix, sm2, inf2, finalsurf, ribbon</span><br><span class="line">  -autorecon3    : process stages 24-34</span><br><span class="line">                     if edits made to correct pial, then run -autorecon-pial</span><br><span class="line">  -hemi ?h       : just do lh or rh (default is to do both)</span><br><span class="line"></span><br><span class="line">  Autorecon Processing Stages (see -autorecon# flags above):</span><br><span class="line">    1.  Motion Correction and Conform  # 运动校正和一致</span><br><span class="line">    2.  NU (Non-Uniform intensity normalization)  # 非均匀强度归一化</span><br><span class="line">    3.  Talairach transform computation  #  Talairach变换计算</span><br><span class="line">    4.  Intensity Normalization 1  # 强度归一化</span><br><span class="line">    5.  Skull Strip  # 颅骨去除   </span><br><span class="line"></span><br><span class="line">    6.  EM Register (linear volumetric registration)  # EM寄存器（线性体积配准）</span><br><span class="line">    7.  CA Intensity Normalization  # CA强度归一化</span><br><span class="line">    8.  CA Non-linear Volumetric Registration  # CA非线性体积配准</span><br><span class="line">    9.  Remove neck  # 去除颈部</span><br><span class="line">    10. EM Register, with skull  # EM注册，带头骨</span><br><span class="line">    11. CA Label (Aseg: Volumetric Labeling) and Statistics  # CA标签（Aseg：体积标签）和统计</span><br><span class="line"></span><br><span class="line">    12. Intensity Normalization 2 (start here for control points) # 强度归一化2（从控制点开始）</span><br><span class="line">    13. White matter segmentation  # 白质细分</span><br><span class="line">    14. Edit WM With ASeg  # 使用ASeg编辑WM</span><br><span class="line">    15. Fill (start here for wm edits)  # 填充（从这里开始编辑wm）</span><br><span class="line">    16. Tessellation (begins per-hemisphere operations)  # 曲面细分（每半球操作开始）</span><br><span class="line">    17. Smooth1</span><br><span class="line">    18. Inflate1</span><br><span class="line">    19. QSphere</span><br><span class="line">    20. Automatic Topology Fixer  # 自动拓扑修复器</span><br><span class="line">    21. White Surfs (start here for brain edits for pial surf)  # 白色Surfs（从这里开始用于脑部冲浪的大脑编辑）</span><br><span class="line">    22. Smooth2</span><br><span class="line">    23. Inflate2</span><br><span class="line"></span><br><span class="line">    24. Spherical Mapping  # 球面映射</span><br><span class="line">    25. Spherical Registration  # 球形配准</span><br><span class="line">    26. Spherical Registration, Contralater hemisphere  # 球面配准，Contralater半球</span><br><span class="line">    27. Map average curvature to subject  # 将平均曲率映射到主题</span><br><span class="line">    28. Cortical Parcellation (Labeling)  # 皮质分割（标签）</span><br><span class="line">    29. Cortical Parcellation Statistics  # 皮质分割统计</span><br><span class="line">    30. Pial Surfs  # Pial Surfs</span><br><span class="line">    31. WM/GM Contrast  # WM / GM对比</span><br><span class="line">    32. Cortical Ribbon Mask  # 皮质功能掩膜</span><br><span class="line">    33. Cortical Parcellation mapped to ASeg  # Cortical Parcellation映射到ASeg</span><br><span class="line">    34  Brodmann and exvio EC labels  # Brodmann和exvio EC标签</span><br><span class="line"></span><br><span class="line"> Step-wise Directives</span><br><span class="line">  See -help</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="mri-convert"><a href="#mri-convert" class="headerlink" title="$ mri_convert"></a>$ mri_convert</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mri_convert.bin </span><br><span class="line">Help</span><br><span class="line"></span><br><span class="line">NAME</span><br><span class="line">mri_convert</span><br><span class="line"></span><br><span class="line">SYNOPSIS</span><br><span class="line">mri_convert [options] &lt;in volume&gt; &lt;out volume&gt;</span><br><span class="line"></span><br><span class="line">DESCRIPTION</span><br><span class="line">mri_convert is a general purpose utility for converting between </span><br><span class="line">different file formats. The file type can be specified in two ways. </span><br><span class="line">First, mri_convert will try to figure it out on its own from the </span><br><span class="line">format of the file name (eg, files that end in .img are assumed to be </span><br><span class="line">in spm analyze format). Second, the user can explicity set the type of</span><br><span class="line">file using --in_type and/or --out_type.</span><br><span class="line"></span><br><span class="line">Legal values for --in_tye (-it) and --out_type (-ot) are listed under </span><br><span class="line">optional flagged arguments.</span><br></pre></td></tr></table></figure><h4 id="preproc-cess"><a href="#preproc-cess" class="headerlink" title="$ preproc-cess"></a>$ preproc-cess</h4><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsFastTutorialV6.0/FsFastPreProc" target="_blank" rel="noopener">FS-FAST Preprocessing</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">USAGE: preproc-sess</span><br><span class="line"></span><br><span class="line">  -per-run : motion cor and reg to middle TP of each run</span><br><span class="line">  -per-session : motion cor and reg to 1st TP of 1st run</span><br><span class="line">  -fwhm FWHM : smoothing level (mm)</span><br><span class="line"></span><br><span class="line">  -update        : only run a stage if input is newer than output (default)</span><br><span class="line">  -force         : force reprocessing of all stages (turns off -update)</span><br><span class="line">  -no-update     : same as -force</span><br><span class="line">  -sliceorder so : turn on slice timing correction (STC) with the given slice order</span><br><span class="line">  -ngroups nSliceGroups : number of SMS slice groups for STC</span><br><span class="line">  -surface subject hemi : set hemi to lhrh to do both</span><br><span class="line">  -mni305-2mm    : sample raw data to mni305 at 2mm (same as -mni305)  # useful</span><br><span class="line">  -mni305-1mm    : sample raw data to mni305 at 1mm</span><br><span class="line">  -cvs : sample raw data to cvs_avg35_inMNI152 at 2mm (not with -mni305)</span><br><span class="line"></span><br><span class="line">Session Arguments (some combination required)</span><br><span class="line">  -sf sessidfile  ...</span><br><span class="line">  -df srchdirfile ...</span><br><span class="line">  -s  sessid      ...</span><br><span class="line">  -d  srchdir     ...</span><br><span class="line">  -fsd    fsd &lt;bold&gt;</span><br><span class="line">  -rlf    rlf  : run list file (default all runs)</span><br><span class="line"></span><br><span class="line">  -init-fsl : use fsl to initialize bbr registration</span><br><span class="line">  -init-spm : use spm to initialize bbr registration (needs matlab)</span><br><span class="line">  -init-header : use geometry to initialize bbr registration</span><br><span class="line">  -bbr-int ifsd istem : use intermediate volume in sess/ifsd/RRR/istem</span><br><span class="line"></span><br><span class="line">Other options (probably not too useful)</span><br><span class="line"></span><br><span class="line">  -nomc     : don't do motion correction</span><br><span class="line">  -nostc    : don't do slice-timing correction</span><br><span class="line">  -nosmooth : don't do smoothing</span><br><span class="line">  -nomask   : don't create brain mask</span><br><span class="line">  -noreg    : don't do registration</span><br><span class="line">  -noinorm  : don't do inorm</span><br><span class="line">  -no-subcort-mask : do not apply subcortical masking</span><br><span class="line"></span><br><span class="line">  -mcin   mcinstem    : stem to use as input  to MC</span><br><span class="line">  -mcout  mcoutstem   : stem to use as output of MC</span><br><span class="line">  -stcin  stcinstem   : stem to use as input  to STC </span><br><span class="line">  -stcout stcoutstem  : stem to use as output of STC </span><br><span class="line">  -smin   sminstem    : stem to use as input  to smoothing </span><br><span class="line">  -smout  sminstem    : stem to use as output of smoothing </span><br><span class="line">  -mask   maskstem    : &lt;brain&gt;</span><br><span class="line"></span><br><span class="line">  -i    instem    : stem to use as overal input &lt;f&gt;</span><br><span class="line"></span><br><span class="line">-regfile regfile   : registration file for use with -surf-fwhm (register.dat)</span><br><span class="line">  -projfrac frac : projection fraction for use with -surf-fwhm (0.5)</span><br><span class="line">  -projfrac-avg  : average over ribbon (not with -projfrac)</span><br><span class="line">  -no-cortex-label : do not use cortex label for masking surfaces</span><br></pre></td></tr></table></figure><p>Once the data have been arranged in the proper directory structure and naming convention, they are ready to be preprocessed. Preprocessing includes：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. Template Creation</span><br><span class="line">2. Brain Mask Creation</span><br><span class="line">3. Registration with FreeSurfer Anatomical</span><br><span class="line">4. Motion Correction</span><br><span class="line">5. Slice Timing Correction (if using)</span><br><span class="line">6. Spatial Normalization</span><br><span class="line">7. Masking</span><br><span class="line">8. Spatial Smoothing   # useful</span><br></pre></td></tr></table></figure><h3 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h3><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/LongSkullStripFix2" target="_blank" rel="noopener">Fixing a bad skull strip</a></p><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsFastTutorialV6.0?action=fullsearch&amp;context=180&amp;value=linkto%3A&quot;FsFastTutorialV6.0&quot;" target="_blank" rel="noopener"><strong>FsFastTutorialV6.0</strong></a>  </p><ul><li><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsFastTutorialV6.0/FsFastDirStruct" target="_blank" rel="noopener">Understanding the FS-FAST Directory Structure</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">- navigate between the project, session, fsd and run directories</span><br><span class="line">- view your raw fMRI data in Freeview</span><br><span class="line">- interpret a paradigm file</span><br><span class="line">- use a sessid file</span><br></pre></td></tr></table></figure></li></ul><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/mris_ca_train?action=fullsearch&amp;context=180&amp;value=linkto%3A&quot;mris_ca_train&quot;" target="_blank" rel="noopener">mris_ca_train</a> 从一组带注释的主题创建地图集、<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/mris_ca_label?action=fullsearch&amp;context=180&amp;value=linkto%3A&quot;mris_ca_label&quot;" target="_blank" rel="noopener">mris_ca_label</a>  对于单个主题，生成一个注释文件，其中每个皮质表面顶点都分配有一个神经解剖标签、<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/mris_sample_parc?action=fullsearch&amp;context=180&amp;value=linkto%3A&quot;mris_sample_parc&quot;" target="_blank" rel="noopener">mris_sample_parc</a> 采样</p><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><h4 id="颅骨去除"><a href="#颅骨去除" class="headerlink" title="颅骨去除"></a>颅骨去除</h4><ul><li><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/mri_watershed" target="_blank" rel="noopener">mri_watershed</a> (运行时长短，约11s，Recommend)  </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mri_watershed sample-001.nii.gz output.nii.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> or</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mri_watershed sample-001.nii.gz output.nii    <span class="comment"># 过程参数有细微差别，轮廓效果基本一致</span></span></span><br></pre></td></tr></table></figure><p>i. 输入输出维度保持一致.  </p><p>ii.<code>freeview -v output.nii</code> 仅显示黑白轮廓图，<code>freeview -v output.nii.gz</code> 会显示内部纹理细节</p></li></ul><p>  <a href="https://blog.csdn.net/sudakuang/article/details/80848472" target="_blank" rel="noopener">Linux Freesurfer脑数据分割</a>：</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mri_watershed -T1 -t 20 input_file output_file</span><br></pre></td></tr></table></figure><p>  命令行，经过试验，加入-T1会避免由于原图的灰度值范围不对导致的报错，也可以更干净地去除脑壳，阈值选20（加入-T1比不加去得更干净）,但可能会误剔除，需测试后再进行批处理操作。</p><ul><li><p>recon-all 方法（运行时间长，约10min） 可参考：<a href="http://learning-archive.org/wp-content/uploads/2017/09/比较FSL-FreeSurfer-ANTs的脑提取工具.pdf" target="_blank" rel="noopener">比较FSL/FreeSurfer/ANTs的脑提取工具</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> SUBJECTS_DIR=~/Desktop/subjects</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> recon-all -i input.nii.gz -s testFreeSurfer -autorecon1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mri_convert --out_orientation RAS -rt nearest --reslice_like input.nii.gz \</span></span><br><span class="line"> -it mgz $&#123;SUBJECTS_DIR&#125;/testFreeSurfer/mri/brainmask.mgz \</span><br><span class="line"> -ot nii output_brain.nii.gz</span><br></pre></td></tr></table></figure><p>数据默认放在 SUBJECTS_DIR 这个变量指定的目录下，FreeSurfer 的输出格式是 mgz，可以使用<br>mri_convert 转换成 nifti 格式。同时 mri_convert 也可以改变朝向，如果朝向发生了变化的话。</p><p>输入输出维度保持一致.</p></li></ul><p><strong>对比</strong>：两种方法效果基本一致，recon-all提取的输出细节更细腻一点</p><h4 id="预处理分割"><a href="#预处理分割" class="headerlink" title="预处理分割"></a>预处理分割</h4><p>after<code>-autorecon1</code>, <code>-autorecon2</code> includes 白质分割、<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/SubcorticalSegmentation" target="_blank" rel="noopener">皮下组织分割</a></p><h4 id="Multimodal-Integration"><a href="#Multimodal-Integration" class="headerlink" title="Multimodal Integration"></a><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/MultiModalTutorialV6.0" target="_blank" rel="noopener">Multimodal Integration</a></h4><ul><li><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/MultiModalTutorialV6.0/MultiModalRegistration" target="_blank" rel="noopener"><strong>Multimodal Registration</strong></a>  perform multi-modal integration in FreeSurfer using <code>fMRI</code> and <code>dMRI</code> analysis. 可进行<code>手动配准</code>；亦可使用<code>配准文件(例register.lta)</code>进行<code>自动配准</code></li></ul><h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><ul><li><p>Runtime is so long. Can we use GPU accelerate？</p><p><code>-use-gpu</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ recon-all -i sample-001.nii.gz -s SkullStrip_FreeSurfer -autorecon1 -use-gpu</span><br><span class="line"></span><br><span class="line">Testing for CUDA device:</span><br><span class="line">/usr/local/freesurfer/bin/mri_em_register_cuda: error while loading shared libraries: libcudart.so.5.0: cannot open shared object file: No such file or directory</span><br><span class="line">Linux captain-System-Product-Name 4.15.0-29-generic #31-Ubuntu SMP Tue Jul 17 15:39:52 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line"></span><br><span class="line">recon-all -s SkullStrip_FreeSurfer exited with ERRORS at 2019年 07月 18日 星期四 18:23:50 CST</span><br><span class="line"></span><br><span class="line">For more details, see the log file </span><br><span class="line">To report a problem, see http://surfer.nmr.mgh.harvard.edu/fswiki/BugReporting</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
      <category term="ADNI" scheme="http://yoursite.com/tags/ADNI/"/>
    
  </entry>
  
  <entry>
    <title>致Snorlax</title>
    <link href="http://yoursite.com/2019/07/15/%E8%87%B4Snorlax/"/>
    <id>http://yoursite.com/2019/07/15/致Snorlax/</id>
    <published>2019-07-15T03:07:25.000Z</published>
    <updated>2019-07-15T03:12:49.902Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p>初立凡尘之外，看尽人世嬉笑<br>今渡万般琐事，亦不过平平尔</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>AD_Papers</title>
    <link href="http://yoursite.com/2019/07/10/AD-Papers/"/>
    <id>http://yoursite.com/2019/07/10/AD-Papers/</id>
    <published>2019-07-10T10:37:04.000Z</published>
    <updated>2019-07-19T03:37:52.533Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h4 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h4><ul><li>氟脱氧葡萄糖正电子发射断层扫描（FDG-PET），提供了大脑代谢活动的定量测量，可以在结构变化发生之前识别与AD相关的变化，具有可接受的敏感性和准确性。 </li><li>ASL(Arterial Spin Labeling)：动脉自旋标记，三维伪连续ASL扫描的自动分类可以高精度地检测AD患者（&gt; 82％）</li></ul><table><thead><tr><th style="text-align:center">No</th><th style="text-align:center">Data Source</th><th style="text-align:center">Modality</th><th style="text-align:center">Method</th><th style="text-align:center">Experiment</th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://arxiv.org/abs/1710.04782" target="_blank" rel="noopener">1</a></td><td style="text-align:center">ADNI</td><td style="text-align:center">PET</td><td style="text-align:center">M-CNN</td><td style="text-align:center">AD vs NC(93.58),<br>sMCI vs pMCI(81.55),<br>tf sMCI vs pMCI(82.51)</td></tr><tr><td style="text-align:center"><a href="https://pubs.rsna.org/doi/full/10.1148/radiol.2016152703" target="_blank" rel="noopener">2</a></td><td style="text-align:center">Alzheimer Center of the VU University <br>Medical Center Dementia Cohort</td><td style="text-align:center">MRI</td><td style="text-align:center">ASL-W_score-SVM</td><td style="text-align:center">i. AD vs. SCD  <br>ii. AD vs. MCI <br> iii. MCI vs. SCD</td></tr><tr><td style="text-align:center"><a href="https://arxiv.org/abs/1808.06452" target="_blank" rel="noopener">3</a></td><td style="text-align:center">ADNI, AIBL,OASIS</td><td style="text-align:center">MRI,PET</td><td style="text-align:center">SVM\逻辑回归\随机森林</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><a href="">4</a></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table><h4 id="1-Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease"><a href="#1-Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease" class="headerlink" title="1. Multiscale deep neural network based analysis of FDG-PET images for the early diagnosis of Alzheimer’s disease"></a>1. <a href="https://arxiv.org/abs/1710.04782" target="_blank" rel="noopener">Multiscale deep neural network based analysis of FDG-PET images for the early diagnosis of Alzheimer’s disease</a></h4><p>提出了一种新的深度学习框架，使用FDG-PET代谢成像来识别MCI的受试者（患有前AD症状的），并将其与其他MCI（非AD /非进展）受试者区分开来。 我们的多尺度深度神经网络仅使用来自单一模态（FDG-PET代谢数据）的测量获得82.51％的分类准确度，优于最近文献中公布的其他可比较的FDG-PET分类器。</p><h5 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h5><ul><li>来源：ADNI（<a href="http://adni.loni.usc.edu）" target="_blank" rel="noopener">http://adni.loni.usc.edu）</a><ul><li>受试者的人口统计学和临床信息：NC组 304项、sMCI组 409项、pMCI组 112项、AD组 226项</li><li>质量控制：1）通过训练有素且专业的神经病理学家对每个脑图像的每个FreeSurfer分段进行手动质量评估。此外，通过手动编辑校正脑膜，白质，皮质或皮质下分割中的任何错误，并重新运行Freesurfer，直到T1 MR图像分割变得准确。2）可视化代谢度量</li></ul></li></ul><h5 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h5><ul><li>提出了一种新的多尺度深度神经网络框架，以学习基于AD病理学的代谢变化模式，作为正常对照（NC）代谢模式的判别; （该论文是第一个利用深度学习开发多尺度FDG-PET分类器的论文）</li><li>发现通过从NC和AD个体转移样本，深层结构可以在早期诊断任务中获得更好的判别能力</li><li>证明了具有不同验证设置的多分类器”投票“预测，可以使所提出的方法更加稳定和稳健，并提高其分类性能</li></ul><h5 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h5><ul><li><p><strong>Introduction</strong></p><ul><li>该论文对迄今为止（21 February 2018）为该项工作发布的最大数据集（1051名受试者的代谢情况）提出了最全面的方法验证，图像严格遵循标准化的成像协议</li></ul></li><li><p><em>Image processing</em></p><ul><li><em>ROI segmentation</em>：Each T1 structural MRI image was segmented into gray matter and white matter using the freely available <code>FreeSurfer 5.3 package</code> with default parameter settings</li><li><em>Patch parcellation</em>： The technique used for this subdivision is a previously published technique where each ROI can be clustered using their spatial coordinates via a <code>k-means clustering algorithm</code></li><li><em>Coregistration</em>：The <code>FDG-PET</code> image and <code>skull-stripped MRI</code> scan of each target were then co-registered using the<code>FSL-FLIRT</code> program</li><li><em>Normalization</em>：The mean intensity in the <code>brainstem region</code> is hence calculated and used to divide the metabolism measures for all the other ROIs in the brain for each subject.</li><li><em>Visualization of metabolism measures</em>：perform visual <code>quality control</code> of the measures across the database</li></ul></li><li><p><em>Multiscale Deep Neural Network</em></p><ul><li>Unsupervised pre-training.</li><li>Supervised fine-tuning. </li><li>Dropout strategy.</li><li>Early stopping strategy.</li></ul></li><li><p><em>Instance-transfer learning</em>：the networks trained with transferred instances displayed better discriminative ability</p></li><li><p><em>Ensemble classifiers</em>：词袋法</p></li><li><p><em>Experimental setup</em>：Three binary classification experiments, (i) <code>NC vs AD</code>, (ii) <code>sMCI vs pMCI</code> and (iii) <code>sMCI vs pMCI with transfer learning from NC and AD</code></p></li><li><p><strong>Results</strong></p><ul><li><p><em>Multiscale classification</em>：二分类实验中，粗精度的预测准确率优于细精度</p></li><li><p><em>Ensemble classifier design</em>：稳健</p></li></ul></li><li><p><strong>Discussion</strong></p><ul><li><em>Comparison with state-of-the-art methods</em>：我们提出的基于深度学习的方法显示sMCI和pMCI之间的分类准确性更高，无论是使用单一模式还是多模式研究</li><li><em>Multiscale classification</em>：与单一尺度特征相比，使用所有多尺度特征（包括从每个单尺度特征获得的判别信息）的组合分类性能产生了更高的精度结果。这表明在连锁多尺度特征上训练的网络（图2）仍然能够学习本文中使用的从小到大的补丁大小的隐藏模式。</li><li><em>Ensemble classifier</em>：集合分类器的准确性高于单个分类器的平均值，这表明具有不同训练和验证集的分集的集合分类器可以产生更稳健和稳定的分类器，因此可以改善分类性能更好的普遍性。</li></ul></li><li><p><strong>Conclusion</strong></p></li></ul><p>在本文中，我们使用<code>多尺度贴片FDG-PET深度学习</code>功能提出了一种新的AD早期诊断框架。所提出的框架利用<code>转移学习方法</code>和<code>集合分类器策略</code>来改善深度神经网络在区分sMCI和pMCI主体的任务中的性能。在1051名受试者的FDG-PET图像的大型数据库上进行的实验提供了支持三种断言的证据。 （1）所提出的方法，使用仅来自<code>单一FDG-PET模态</code>的特征，能够胜过在sMCI和pMCI分类任务中采用多模态特征的现有方法。 （2）所提出的网络可以从<code>多尺度特征</code>中学习判别模式，以提供具有更好判别性能的更健壮的分类器。 （3）使用不同验证集的<code>多个集成分类器</code>可以使网络更加健壮和稳定，并在统计上提高其分类性能。<br>对于未来的工作，将所提出的框架扩展到包含来自多种模态的信息是自然的，假设所得到的深度神经网络将从多个模态数据中学习更多信息，从而进一步改进所获得的分类准确度。尽管sMCI vs pMCI实验常用于验证近期研究（包括我们的研究）中方法的鉴别能力，但我们只能知道那些sMCI受试者在研究进展中时保持稳定并且可以转化为AD或其他神经退行性疾病。因此，在将来临床诊断的sMCI受试者的基本事实可能不完全准确，因此可能在分类中引入噪声/偏倚。幸运的是，随着更多数据的收集，分类系统将更好地捕获这些和其他噪声和可变性来源，我们提出的深度学习集合分类器可能非常适合这种情况。</p><h4 id="2-Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer’s-Disease"><a href="#2-Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer’s-Disease" class="headerlink" title="2. Application of Machine Learning to Arterial Spin Labeling in Mild Cognitive Impairment and Alzheimer’s Disease"></a>2. <a href="https://pubs.rsna.org/doi/full/10.1148/radiol.2016152703" target="_blank" rel="noopener">Application of Machine Learning to Arterial Spin Labeling in Mild Cognitive Impairment and Alzheimer’s Disease</a></h4><p>关于动脉自旋标记（ASL）的机器学习在治疗轻度认知障碍和阿尔茨海默病中的应用</p><p>本研究调查了ASL灌注图的多变量模式识别分析（SVM）是否可用于阿尔茨海默病（AD），轻度认知障碍（MCI）和主观认知衰退（SCD）患者的分类和单一主题预测。 W-score方法可以消除性别和年龄的混杂影响。</p><h5 id="细节-1"><a href="#细节-1" class="headerlink" title="细节"></a>细节</h5><p>ABSTRACT</p><ul><li>Purpose：<ul><li><code>ASL</code> → discrimination AD\MCI\SCD</li><li><code>W-score</code> → remove confounding effects of gender and age</li></ul></li><li>Materials and Methods<ul><li>Training of a Support Vector Machine (<code>SVM</code>) classifier used diagnostic status and perfusion maps.</li></ul></li><li>Results</li><li>Conclusion</li></ul><ol><li><p>INTRODUCTION</p></li><li><p>MATERIALS AND METHODS</p><ol><li><em>Participants</em></li><li><em>Data Acquisition</em> 数据采集</li><li><em>Preprocessing of MR imaging data</em></li><li><em>W-score maps</em>：<code>[(measured perfusion) – (predicted perfusion)] / (standard deviation of residuals)</code>.</li><li><em>SVM: Multivariate Pattern Recognition in Training-Set</em>：SVM接受了 leave-one-out交叉验证框架的训练，以区分AD患者，MCI患者和SCD患者。我们通过比较<code>两个患者组（SCD）</code>的W-得分图来评估分类器的诊断值。通过区分<code>AD和MCI</code>的患者的W-得分图来评估分类器对疾病进展的敏感性。最后，使用<code>MCIc和MCI</code>患者的Wscore图进行探索性分类训练，以研究分类器是否显示预后价值。</li><li><em>SVM: Prediction in new Subjects</em>:<code>Discrimination maps</code></li><li><em>Statistical Analysis</em>: <code>Evaluation</code></li></ol></li><li>RESULTS<ol><li><em>Participant Characteristics</em>:  数据集中受试者MMSE评分差异小</li><li><em>Training of the classifiers</em>：i. AD vs. SCD  ii. AD vs. MCI  iii. MCI vs. SCD </li><li><em>Predictions: assessment of generalisability</em>: <code>use of discrimination weights</code> i. AD vs. SCD  ii. AD vs. MCI  iii. MCI vs. SCD </li><li><em>Exploratory analyses: classifying MCI subgroups</em>: i. MCIc vs. SCD ii. MCIc vs. MCIs; use of the <code>AD vs. SCD training discrimination weights</code> for MCIc vs. SCD;The use of the same discrimination weights in MCIc vs. MCIs </li></ol></li><li>DISCUSSION</li><li>CONCLUSION：Using <code>automated methods</code>（即SVM）, age- and gender adjusted（W-score校准） ASL perfusion maps（特殊模态） can be used to classify and predict diagnoses of AD, MCI-converters, stable MCI patients and SCD subjects with good accuracy and AUC values.</li></ol><h4 id="3-Reproducible-evaluation-of-classification-methods-in-Alzheimer’s-disease-Framework-and-application-to-MRI-and-PET-data"><a href="#3-Reproducible-evaluation-of-classification-methods-in-Alzheimer’s-disease-Framework-and-application-to-MRI-and-PET-data" class="headerlink" title="3. Reproducible evaluation of classification methods in Alzheimer’s disease: Framework and application to MRI and PET data"></a>3. <a href="https://arxiv.org/abs/1808.06452" target="_blank" rel="noopener">Reproducible evaluation of classification methods in Alzheimer’s disease: Framework and application to MRI and PET data</a></h4><p>阿尔茨海默病分类方法的<code>再现评估</code>：MRI和PET数据的框架和应用</p><blockquote><p>提供规范化的AD实验流程，便于结果复现</p></blockquote><p>在本文中，我们使用三个公开可用的数据集（ADNI，AIBL和OASIS）提出了AD中<code>可再现</code>和客观分类实验的框架。该框架包括：i）将三个数据集自动转换为标准格式（BIDS）; ii）模块化的预处理流水线，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准。</p><p>我们使用这个框架来证明从三个数据集（ADNI，AIBL和OASIS）获得的T1 MRI和PET数据进行自动分类。 我们评估各种成分对分类性能的影响：<code>模态（T1 MRI或PET）</code>，<code>特征类型（体素或区域特征）</code>，<code>预处理</code>，<code>诊断标准（标准NINCDS / ADRDA标准或淀粉样蛋白精制标准）</code>，<code>分类算法</code>。 首先在ADNI，AIBL和OASIS数据集上独立进行实验，并通过将对ADNI训练的分类器应用于AIBL和OASIS数据来评估结果的推广。</p><p>本文的贡献：</p><p>i）管理公开数据集的框架及其与新主题的持续<code>更新</code>，特别是全脑自动转换为脑成像数据结构（BIDS）格式的工具<br>ii）一组<code>模块化的预处理管道</code>，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准;<br>iii）对来自三个公开可用的神经成像<code>数据集</code>（ADNI，AIBL和OASIS）的T1 MRI和PET数据进行大规模评估。</p><p>对于所有分类任务，FDG PET优于T1 MRI。</p><p>All the code of the framework and the experiments is publicly available: general- purpose tools have been integrated into the <a href="www.clinica.run">Clinica software</a> and the paper-specific code is available at: <a href="https://gitlab.icm-institute.org/aramislab/AD-ML" target="_blank" rel="noopener">https://gitlab.icm-institute.org/aramislab/AD-ML</a>.</p><h5 id="细节-2"><a href="#细节-2" class="headerlink" title="细节"></a>细节</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">- ABSTRACT</span><br><span class="line"></span><br><span class="line">1. Introduction</span><br><span class="line"></span><br><span class="line">2. Materials</span><br><span class="line">   2.1. Datasets</span><br><span class="line">   2.2. Participants</span><br><span class="line">     2.2.1. ADNI</span><br><span class="line">     2.2.2. AIBL </span><br><span class="line">     2.2.3. OASIS</span><br><span class="line">   2.3. Imaging data</span><br><span class="line">   2.3.1. ADNI</span><br><span class="line">   2.3.1.1. T1-weighted MRI. </span><br><span class="line">   2.3.1.2. PET. </span><br><span class="line">     2.3.2. AIBL</span><br><span class="line">     2.3.3. OASIS</span><br><span class="line">     </span><br><span class="line">3. Methods</span><br><span class="line">3.1. Converting datasets to a standardized data structure  # 可以借鉴其工具的实现</span><br><span class="line">    3.1.1. Conversion of the ADNI dataset to BIDS： dicom → nifti; Once the images of interest have been selected and the paths to the image files identified, `the imaging data can be converted to BIDS`. ...</span><br><span class="line">     - If the modality was acquired for a specific pair of subject-session, and several scans and/or preprocessed images are available, `only one is converted`. </span><br><span class="line">    3.1.2. Conversion of the AIBL dataset to BIDS: like above.</span><br><span class="line">    3.1.3. Conversion of the OASIS dataset to BIDS：Analyze → nifti; create the BIDS folder hierarchy → the images are copied to the appropriate folder and renamed</span><br><span class="line"></span><br><span class="line">3.2. Preprocessing pipelines: Two pipelines were developed to preprocess the anatomical `T1w MRI` and `PET` images.</span><br><span class="line">3.2.1. Preprocessing of T1-weighted MR images:i.the Unified Segmentation procedure(tissue segmentation, bias correction and spatial normalization) ii. create DARTEL template  iii. transformation of the DARTEL template into MNI space</span><br><span class="line">3.2.2. Preprocessing of PET images：PET(with T1w) → MNI → SUVR图(归一化)</span><br><span class="line"></span><br><span class="line">3.3. Feature extraction:Two types of features were extracted from the imaging data: `voxel` and `region features`.T1w MR --- the gray matter density; FDG PET --- SUVR.</span><br><span class="line">- AAL2</span><br><span class="line">- AICHA</span><br><span class="line">- Hammers</span><br><span class="line">- LPBA40</span><br><span class="line">- Neuromorphometrics10</span><br><span class="line"></span><br><span class="line">3.4. Classification models: We considered three different classifiers: `linear SVM`(with both the voxel and the regional features), `logistic regression with L2 regularization` and `random forest`(only used for the region-based analyses), all available in Clinica. </span><br><span class="line">3.4.1. Linear SVM</span><br><span class="line">3.4.2. Logistic regression with L2 regularization</span><br><span class="line">3.4.3. Random forest</span><br><span class="line">3.5. Evaluation strategy</span><br><span class="line">3.5.1. Cross-validation</span><br><span class="line">3.5.2. Metrics</span><br><span class="line">3.6. Classification experiments</span><br><span class="line"></span><br><span class="line">4. Results</span><br><span class="line">4.1. Influence of the atlas：no specific atlas provides the highest classification accuracy for all the tasks. → the `AAL2` atlas was chosen as reference atlas as it leads to good classification accuracies and is widely used in the neuroimaging community.</span><br><span class="line">4.2. Influence of the smoothing: for most classification tasks, the balanced accuracy does not vary to a great extent with the smoothing kernel size. → As the degree of smoothing does `not have a clear impact` on the classification performance, we chose to present the subsequent results related to the voxel-based classification `with a reference smoothing of 4 mm`.</span><br><span class="line">4.3. Influence of the type of features: do not show notable differences between the mean balanced accuracies obtained using voxel or regional features.</span><br><span class="line">4.4. Influence of the classification method: both the linear SVM and logistic regression with L2 regularization models lead to similar balanced accuracies, consistently higher than the one obtained with random forest for all the tasks and imaging modalities tested.</span><br><span class="line">4.5. Influence of the partial volume correction of PET images： little difference between the balanced accuracies obtained with and without PVC. </span><br><span class="line">4.6. Influence of the magnetic field strength：no matter the experiment, the balanced accuracy is always `higher for the 3 T scan` subset compared to the 1.5 T scan subset, which is not surprising as 3 T images should have a better signal-to-noise ratio.</span><br><span class="line">4.7. Influence of class imbalance:It thus seems that a very strong class imbalance (as in the case of AIBL where the proportion is 6 to 1) leads to lower performance but that moderate class imbalance (up to 2 to 1 in ADNI) are adequately handled.</span><br><span class="line">4.8. Influence of the dataset：Performances obtained on ADNI and AIBL were comparable and much higher than those obtained on OASIS. → classifiers trained on ADNI generalized well to the other datasets. In particular, training on ADNI substantially improved the classification performances on OASIS. (ADNI has the larger training set size, higher image quality or stricter diagnostic criteria.)</span><br><span class="line">4.9. Influence of the training dataset size:the balanced accuracy in- creases with the number of training samples.</span><br><span class="line">4.10. Influence of the diagnostic criteria: 淀粉样蛋白状态信息有助于提高任务性能</span><br><span class="line">4.11. Computation time: SVM和逻辑回归实验的运算时间远远少于随机森林实验所需的运算时间</span><br><span class="line"></span><br><span class="line">5. Discussion</span><br><span class="line"> - FDG PET与MRI相比具有优越的性能</span><br><span class="line">6. Conclusions</span><br></pre></td></tr></table></figure><h4 id="4-Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease"><a href="#4-Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="4. Multimodal Neuroimaging Feature Learning With Multimodal Stacked Deep Polynomial Networks for Diagnosis of Alzheimer’s Disease"></a>4. <a href="https://scholar.google.com/scholar?q=Multimodal+Neuroimaging+Feature+Learning+With+Multimodal+Stacked+Deep+Polynomial+Networks+for+Diagnosis+of+Alzheimer&#39;s+Disease&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart" target="_blank" rel="noopener">Multimodal Neuroimaging Feature Learning With Multimodal Stacked Deep Polynomial Networks for Diagnosis of Alzheimer’s Disease</a></h4><p>该研究提出了一种<code>多模式堆叠DPN</code>（MM-SDPN）算法，MM-SDPN由两级SDPN组成，用于融合和学习用于AD诊断的<code>多模态神经成像数据</code>的特征表示。具体而言，两个SDPN首先用于分别学习<code>MRI</code>和<code>PET</code>的高级特征，然后将其输入另一个SDPN以融合多模态神经影像信息。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">I. INTRODUCTION</span><br><span class="line">The most commonly used feature extraction methods for neuroimaging data can be roughly divided into four categories: 1) The voxel-based approaches that simply and directly extract features from voxel intensity; 2) The vertex-based approaches whose features are defined at the vertex-level on the cortical surface; 3) The region of interest (ROI) based approaches that extract features from predefined brain regions; 4) patch-based approaches that learn new feature representation from local patches.  As a result, these features usually obtain superior classification results.</span><br><span class="line">  - The voxel- and vertex-based features usually have very high dimensionality, and therefore dimensionality reduction is important to achieve more compact and effective features. </span><br><span class="line">  - The ROI-based features are widely used, because they not only have relatively low feature dimensionality, but also cover the whole brain. However, the features extracted from ROIs are somewhat coarse and cannot reflect small or subtle changes involved in the brain diseases. </span><br><span class="line">  - The patch-based features are learned from the whole brain, and can effectively capture the diseased-related pathologies.  貌似最佳</span><br><span class="line"></span><br><span class="line">Why need MM-SDPN？</span><br><span class="line">受监督的DPN将潜在地从用于AD诊断的小神经影像数据中学习优越的特征表示。另一方面，特征提取的逐层堆叠通常在DL中产生更好的表示，例如`DBN和SAE`，这促使开发堆叠DPN（SDPN）算法以学习更高级别的特征表示。此外，已经证明，可以同时学习和融合`多模态神经影像数据`的多模式DL算法优于用于AD分类的单模态DL算法。因此，值得研究多模堆叠DPN算法。</span><br><span class="line"></span><br><span class="line">II. METHODS</span><br><span class="line">A. Deep Polynomial Networks Algorithm  # 拟合？</span><br><span class="line">B. Stacked Deep Polynomial Networks: 每个基本DPN都采用有监督的分块方式进行训练，无需反向传播，这与其他流行的深层架构不同。因此，与具有反向传播策略的其他DL算法相比，SDPN非常简单，具有相对低的计算复杂度。</span><br><span class="line">C. Multimodal Stacked DPN:在第一阶段，每个神经影像数据将被馈送到其相应的SDPN模块，以学习高级特征表示。每种特定模态的高级特征反映了它自己的属性，但不同模态之间没有相关信息。然后，所有学习的特征在第二阶段被馈送到新的SDPN模块，以便与所有模态相关联。因此，最终学习的高级特征既包含每种模态的内在属性，也包含所有模态之间的相关性。因此，SDPN学到的特征更具有辨别力和鲁棒性。在我们的MM-SDPN算法中，由于DPN在每个网络层中执行前馈监督学习而没有精细转向，因此难以执行与[38]中相同的学习策略来推断MRI和PET之间的相关性。因此，通过联合训练第二阶段SDPN与在第一阶段中学习的级联MRI和PET特征来学习`共享表示`。它类似于[42]中使用的简单融合方法。</span><br><span class="line"></span><br><span class="line">III. EXPERIMENTS AND RESULTS</span><br><span class="line">A. Neuroimaging Data Preprocessing</span><br><span class="line">具体而言，预处理首先在`MRI图像`上进行，包括前连合（AC） - 后连合（PC）校正，N3算法强度不均匀，以及由小脑提取的颅骨剥离和去除小脑。然后通过FSL包中的FAST算法将MR图像`分割成三种不同的组织`，即灰质，白质和脑脊液[49]。在通过`HAMMER算法`[50]注册后，每个MR图像被分成93个ROI，基于模板，Kabani等人使用93个手动标记的ROI。 [51]。然后计算灰质组织的体积作为每个ROI的特征，产生93个特征。然后通过刚性配准将每个PET图像与其对应的MRI图像`对准`。将相同ROI的平均强度计算为PET图像的特征。因此，分别从MRI和PET图像中`提取93个特征`。</span><br><span class="line">B. Performance Evaluation: Four classification tasks are performed, namely `AD vs. NC`, `MCI vs. NC`, `MCI-C vs. MCI-NC`, and `AD vs. MCI-C vs. MCI- NC vs. NC`.</span><br><span class="line">DPN-3-MRI、DPN-6-MRI、SDPN-MRI、DPN-3-PET、DPN-6-PET、SDPN-PET、SDPN-MRI-PET、MM-SDPN + SVM/LINEAR CLASSIFIER</span><br><span class="line">C. Results on AD vs. NC：</span><br><span class="line">MM-SDPN算法达到最佳性能，平均分类精度为97.13±4.44％，灵敏度为95.93±7.84％，特异性为98.53±5.05％，因为它成功地融合了MRI和PET信息。另一方面，尽管DPN-6（具有6层网络的DPN）在基于单一模态成像的AD分类的MRI和PET数据上优于DPN-3，但SDPN仍然比DPN-6略好，这表明其有效性由于堆叠技术的SDPN。</span><br><span class="line">MM-SDPN在所有评估指标上都优于所有其他具有SVM和线性分类器的算法。</span><br><span class="line">D. Results on MCI vs. NC：尽管DPN-6优于DPN-3，但SDPN在基于单模态成像的MRI和PET数据上的MCI分类方面优于DPN。</span><br><span class="line">E. Results on MCI-C vs. MCI-NC: SDPN对于单一模态神经成像数据的性能要比DPN-6和DPN-3好得多。</span><br><span class="line">F. Results on AD vs. MCI-C vs. MCI-NC vs. NC:与最先进的基于SAE的算法[38]相比，我们提出的具有SVM分类器的MM-SDPN算法实现了分类精度提高3.21％和灵敏度提高1.51％。</span><br><span class="line"></span><br><span class="line">IV. DISCUSSION  # 核心、优质总结</span><br><span class="line">在这项工作中，我们提出了一种MM-SDPN算法，可以有效地学习基于多模态神经影像学的AD诊断的特征。 ADNI数据集上的四组实验结果表明，与最先进的基于多模态学习的算法相比，所提出的MM-SDPN算法实现了最佳性能。</span><br><span class="line">分析：</span><br><span class="line">原始ROI特征是低级特征，不能以良好区分的方式表示AD的属性。当应用DPN来学习ROI特征的特征时，提供了更复杂的表示，因此DPN已经实现了显着的改进。在多次堆叠基本DPN块之后，获得更高级别的表示。因此，对于基于单模态神经成像的AD分类，SDPN比原始DPN具有更好的性能。</span><br><span class="line">值得注意的是，尽管随着隐层的增加，6层DPN优于3层DPN，但是根据神经网络中的通用逼近定理，太深的网络将增加计算复杂度，而近似的精度没有明显增加。</span><br><span class="line">另一方面，结果还表明，具有两个3层DPN的SDPN优于6层DPN，因为第二层基本DPN中第一层的基础建立在更高级别的特征上，即串联特征第一级基本DPN，这个基础将在学习二级DPN后生成更有效和更高级别的功能。此外，与具有更深网络的DPN相比，SDPN更容易调整参数以实现相同的性能。</span><br><span class="line">在这项研究中，两个分类器，即SVM和线性分类器，用于评估SDPN和MM-SDPN的性能。两个分类器都给出了类似的结果，这表明AD分类的良好性能更多地取决于学习的特征而不是分类器。因此，MM-SDPN真正有效地学习了一个好的特征表示。</span><br><span class="line"></span><br><span class="line">DPN的算法结构使其适用于小型数据集。 由于神经影像数据通常仅提供有限的标记地面实况样本，并且先验标签信息有利于小数据的分类任务，因此监督DPN比不受控制的DL算法更适合于小神经成像数据集。</span><br><span class="line"></span><br><span class="line">V. CONCLUSION</span><br><span class="line">在这项工作中，提出了一种MM-SDPN算法。它由两阶段SDPN组成，可以有效地学习和融合多模态数据，用于诊断阿尔茨海默病。 MM-SDPN实现了最先进的表现，用于对AD进展的两个阶段和四个阶段进行分类。因此，所提出的MM-SDPN不仅可以作为多模神经成像数据的强大表示算法，还可以用于其他医学数据。</span><br></pre></td></tr></table></figure><ol start="5"><li><p><a href="https://scholar.google.com/scholar?q=Multimodal+Neuroimaging+Feature+Learning+for+Multiclass+Diagnosis+of+Alzheimer%E2%80%99s+Disease&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart" target="_blank" rel="noopener">Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer’s Disease</a></p><p>该框架使用<code>零掩蔽策略</code>进行数据融合，以从多种数据模式中提取补充信息。与之前最先进的工作流程相比，我们的方法能够在一个设置中<code>融合多模态神经成像功能</code>，并且可能需要<code>较少标记的数据</code>。 AD的二元分类和多类分类均实现了性能提升。讨论了拟议框架的优点和局限性。</p></li></ol><p>我们提出了一种新的多层AD诊断框架，其中嵌入了深度学习架构，其受益于多模态神经影像学特征之间的协同作用。该框架由<code>SAE</code>和soft-max逻辑回归器构成。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">I. INTRODUCTION</span><br><span class="line"></span><br><span class="line">II. METHODOLOGY</span><br><span class="line">A. Data Acquisition and Feature Extraction</span><br><span class="line">B. Learning Framework</span><br><span class="line">C. Feature Examination</span><br><span class="line"></span><br><span class="line">III. EXPERIMENTS AND RESULTS</span><br><span class="line">A. Visualization of High-Level Biomarkers</span><br><span class="line">B. Performance Evaluation</span><br><span class="line"></span><br><span class="line">IV. DISCUSSION</span><br><span class="line">A. Model Designing and Training</span><br><span class="line">B. Limitations and Future Work</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">V. CONCLUSION</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="ADNI" scheme="http://yoursite.com/tags/ADNI/"/>
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Network in Medical Imaging A Review</title>
    <link href="http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/"/>
    <id>http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/</id>
    <published>2019-07-02T06:18:42.000Z</published>
    <updated>2019-07-02T12:08:24.231Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p>论文地址：<a href="https://arxiv.org/abs/1809.07294" target="_blank" rel="noopener">Generative Adversarial Network in Medical Imaging: A Review</a></p><p>github Reference link：<a href="https://github.com/xinario/awesome-gan-for-medical-imaging" target="_blank" rel="noopener">Awesome GAN for Medical Imaging</a></p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><em>摘要</em></h3><p>生成对抗网络由于其数据生成能力而在没有明确建模概率密度函数的情况下在计算机视觉社区中获得了很多关注。 鉴别器带来的对抗性损失提供了一种巧妙的方法，可以将未标记的样本纳入训练并实现更高的顺序一致性。 事实证明，这在许多情况下是有用的，例如域适应，数据增强和图像到图像转换。 这些属性吸引了医学成像领域的研究人员，我们已经看到许多传统和新颖应用的快速采用，如图像重建，分割，检测，分类和跨模态合成。 根据我们的观察，这一趋势将继续下去，因此我们利用对抗性训练计划对医学成像的最新进展进行了回顾，希望能够使对该技术感兴趣的研究人员受益。</p><p>关键词：Deeplearning，Generative adversarial network，Generative model，Medical imaging，Review</p><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a><em>1. 介绍</em></h3><p>随着2012年开始的计算机视觉深度学习的复兴（Krizhevsky等，2012），医学成像中深度学习方法的采用大幅增加。据估计，2016年和2017年在主要医学影像相关会议场所和期刊上发表了400多篇论文（Litjens等，2017）。在医学成像领域广泛采用深度学习是因为它具有补充图像解释和增强图像表示和分类的潜力。在本文中，我们将重点放在深度学习领域最有趣的近期突破之一 - 生成对抗网络（GAN） - 以及它们在医学成像领域的潜在应用。<br>GAN是一种特殊类型的神经网络模型，其中两个网络同时被训练，一个侧重于图像生成，另一个侧重于区分。对抗性训练方案因其在抵制领域转移方面的有用性以及产生新图像样本的有效性而在学术界和工业界引起了关注。该模型在许多图像生成任务中实现了最先进的性能，包括文本到图像合成（Xu et al.，2017），超分辨率（Ledig等，2017）和图像 - 图像转换（Zhu et al.，2017a）。<br>与源于20世纪80年代的深度学习不同（Fukushima和Miyake，1982），对抗性的概念相对来说是非常重要的进步（Good-fellow et al.，2014）。本文概述了GAN，描述了它们在医学成像中的有前途的应用，并确定了一些需要解决的挑战，以使它们能够成功应用于其他医学成像相关任务。<br>为了全面概述医学影像中GAN的所有相关工作，我们搜索了包括PubMed，arXiv在内的数据库，国际医学图像计算和计算机辅助干预会议（MICCAI），SPIE医学影像，IEEE国际研讨会生物医学成像（ISBI）和国际深度学习医学影像学会议（MIDL）。我们还合并了上述搜索过程中未识别的交叉引用作品。由于每月都有研究出版物出现，而且没有失去一般性，我们将搜索的截止时间设定为2018年7月30日。仅报告初步结果的arXiv的工作被排除在本次审查之外。基于任务，成像模态和年份的这些论文的描述性统计数据可以在图1中找到。<br>在本文的其余结构如下。我们首先简要介绍第2节中GAN的原理及其一些结构变体。然后在第3节中使用GAN对医学图像分析任务进行全面审查，包括但不限于放射学领域，组织病理学和皮肤病学。我们根据规范任务对所有作品进行分类：重建，图像合成，分割，分类，检测，注册等。第4节总结了该评论，并讨论了前瞻性应用和识别性挑战。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 1" title="1.png">                </div>                <div class="image-caption">1.png</div>            </figure><p>图1：（a）根据规范任务对GAN相关论文进行分类。 （b）根据成像模式对GAN相关论文进行分类。 （c）2014年发布的GAN相关论文数量。请注意，一些工作执行了各种任务，并对具有不同模态的数据集进行了评估。 我们在绘制这些图时多次计算这些作品。 基于源域计算与跨域图像传输相关的工作。 图（a）和（b）中的统计数据基于2018年7月30日或之前公布的论文。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 2" title="2.png">                </div>                <div class="image-caption">2.png</div>            </figure><p>图2：用于在CT图像上合成肺结节的vanilla GAN的示意图。 上图显示了网络配置。 下面的部分显示了生成器G和鉴别器D的输入，输出和内部特征表示.G将样本$z$从$p(z)$变换为生成的结节$x_g$。 D是二元分类器，其分别区分由$x_g$和$x_r$形成的肺结节的生成和真实图像。</p><h3 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a><em>2. 背景</em></h3><h4 id="2-1-Vanilla-GAN"><a href="#2-1-Vanilla-GAN" class="headerlink" title="2.1. Vanilla GAN"></a><em>2.1. Vanilla GAN</em></h4><p>香草GAN（Goodfellow等，2014）是一种生成模型，设计用于直接从所需的数据分布中抽取样本，而无需明确地模拟潜在的概率密度函数。它由两个神经网络组成：发生器G和鉴别器D.G，z的输入是从先前分布p（z）中采样的纯随机噪声，通常选择为高斯分布或均匀分布。简单。预计G，xg的输出与从真实数据分布pr（x）中提取的实际样本xr具有视觉相似性。我们将由θg参数化的G学习的非线性映射函数表示为xg = G（z;θg）。 D的输入是实际或生成的样本。 D，y1的输出是单个值，表示输入是真实或假冒样本的概率。由θd参数化的D学习的映射表示为y1 = D（x;θd）。生成的样本形成分布pg（x），其在成功训练后需要是pr（x）的近似值。图2的顶部显示了香草GAN配置的图示。在该示例中，G生成描绘肺结节的2D CT切片。<br>D的目标是区分这两组图像，而生成器G被训练以尽可能地混淆辨别器D.直观地说，G可以被视为试图生产一些优质假冒伪劣材料的伪造者，D可以被视为试图检测伪造物品的警察。在另一种观点中，我们可以将G视为从D接收奖励信号，这取决于生成的数据是否准确。梯度信息从D传播回G，因此G调整其参数以产生可以欺骗D的输出图像.D和G的训练目标可以用数学表达为：<br>$$ L_{D}^{GAN} = max_{D}E_{{x_r}\sim {P_r(x)}}[logD(x_r)+E_{x_g\sim p_g(x)}[log(1-D(x_g))]],\\L_{D}^{GAN} = min_GE_{x_g\sim p_g(x)}[log(1-D(x_g))].$$ <br>可以看出，D只是具有最大对数似然目标的二元分类器。 如果鉴别器D在下一个发生器G更新之前被训练为最优，则最小化LGAN被证明等同于最小化pr（x）和pg（x）之间的Jensen-Shannon（JS）偏差（Goodfellow等人，2014））。 训练后的预期结果是xg形成的样本应该接近实际数据分布pr（x）。</p><h4 id="2-2-Variants-of-GANs"><a href="#2-2-Variants-of-GANs" class="headerlink" title="2.2. Variants of GANs"></a><em>2.2. Variants of GANs</em></h4><p>上述GAN训练目标被认为是鞍点优化问题（Yadav等，2018），训练通常通过基于梯度的方法完成。 G和D从头开始交替训练，以便它们可以一起进化。但是，G和D训练与JS分歧之间无法保证平衡。因此，一个网络可能不可避免地比另一个网络更强大，在大多数情况下是D.当D变得太强而不是G时，生成的样本变得太容易与实际的分离，从而达到D的梯度逼近零的阶段，没有为G的进一步训练提供指导。由于难以产生有意义的高频细节，因此在生成高分辨率图像时更频繁地发生这种情况。<br>在训练GAN中通常面临的另一个问题是模式崩溃，正如名称所示，这是由G学习的分布pg（x）关注数据分布pr（x）的一些有限模式的情况。因此，它不是产生不同的图像，而是产生一组有限的样本。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 3" title="3.png">                </div>                <div class="image-caption">3.png</div>            </figure><p>图3：GAN变体的示意图。 c表示条件向量。 在CGAN和ACGAN中，c是对类标签进行编码的离散分类代码（例如，一个热向量），在InfoGAN中，它也可以是对属性进行编码的连续代码。 xg通常是指生成的图像，但也可以是SGAN中的内部表示。</p><h5 id="2-2-1-Varying-objective-of-D"><a href="#2-2-1-Varying-objective-of-D" class="headerlink" title="2.2.1. Varying objective of D*"></a>2.2.1. Varying objective of D*</h5><p>为了稳定训练并避免模式崩溃，已经提出了D的不同损失，例如f-发散（f-GAN）（Nowozin等，2016），最小二乘（LS-GAN）（Mao et al。，2016），铰链损失（Miyato等，2018）和Wasserstein距离（WGAN，WGAN-GP）（Arjovsky等，2017; Gulrajani等，2017）。其中，Wasserstein距离可以说是最受欢迎的指标。作为真/假歧视方案的替代方案，Springenberg（2015）提出了一个基于熵的目标，其中鼓励实际数据进行自信的类预测（CatGAN，图3b）。在EBGAN（Zhao等人，2016）和BEGAN（Berthelot等人，2017）（图3c）中，用于鉴别器的常用编码器架构被替换为自动编码器架构。然后，D的目标变为匹配自动编码器丢失分布而不是数据分布。<br>GAN本身缺乏推断机制，根据定义，推断机制可以预测可能编码输入的潜在向量。因此，在ALI（Dumoulin等人，2016）和BiGAN（Donahue等人，2016）（图3d）中，结合了单独的编码器网络。然后D的目标是分离联合样本（xg，zg）和（xr，zr）。在InfoGAN中（图3e），鉴别器输出潜在向量，该潜向向量编码所生成图像的部分语义特征。鉴别器使所生成的图像与所生成的图像所依赖的潜在属性向量之间的互信息最大化。成功培训后，InfoGAN可以探索固有的数据属性，并根据这些属性执行条件数据生成。已经证明类标签的使用可以进一步提高生成图像的质量，并且通过强制D提供类概率并使用交叉熵损失进行优化（例如在ACGAN中使用）（Odena等， 2016）（图3f）。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 4" title="4.png">                </div>                <div class="image-caption">4.png</div>            </figure><p>图4：用于图像到图像转换的cGAN框架。 pix2pix需要对齐的训练数据，而这种约束在CycleGAN中放宽，但通常会受到性能损失的影响。 请注意，在（a）中，我们选择重建损失作为目标一致性的示例。 这种监督与任务有关，可以采取许多其他形式。 （c）它由两个VAEGAN组成，在VAE部分具有共享的潜在向量。</p><h5 id="2-2-2-Varying-objective-of-G"><a href="#2-2-2-Varying-objective-of-G" class="headerlink" title="2.2.2. Varying objective of G"></a><em>2.2.2. Varying objective of G</em></h5><p>在香草GAN中，G将噪声z转换为样本xg = G（z）。这通常通过使用解码器网络逐步增加输出的空间大小来实现，直到达到所需的分辨率，如图2所示.Larsen等人。 （2015）提出了变分自动编码器网络（VAE）作为G的基础架构（VAEGAN，图3g），其中它可以使用逐像素重建损失来强制VAE的解码器部分生成与真实图像匹配的结构。<br>GAN的原始设置对其可以生成的数据模式没有任何限制。然而，如果在生成期间提供辅助信息，则可以驱动GAN以输出具有期望属性的图像。在这种情况下，GAN通常被称为条件GAN（cGAN），并且生成过程可以表示为xg = G（z，c）。<br>最常见的条件输入之一c是图像。 pix2pix是第一个基于通用GAN的图像到图像转换框架，由Isola等人提出。 （2016）（图4 a）。此外，任务相关的监督被引入发电机。例如，用于图像恢复的重建损失和用于分割的骰子损失（Milletari等，2016）。这种形式的监督需要一致的训练对。朱等人。 （2017A）; Kim等人。 （2017）通过从头到脚拼接两个发生器来放松这种约束，这样图像可以在两组不成对的样本之间进行转换（图4b）。为简单起见，我们在本文的其余部分选择了CycleGAN来表示这一想法。另一个名为UNIT的模型（图4c）也可以通过将两个VAEGAN组合在一起来执行不成对的图像到图像变换，每个模型对一种模态负责但共享相同的潜在空间（Liu et al。，2017a）。这些图像到图像翻译框架由于其普遍适用性而在医学成像领域中非常流行。<br>除了图像，条件输入可以是类标签（CGAN，图3h）（Mirza和Osindero，2014），文本描述（Zhang et al。，2017a），对象位置（Reed等，2016a） ，b），周围的图像背景（Pathak等，2016），或草图（Sangkloy等，2016）。请注意，上一节中提到的ACGAN也有一个类条件生成器。</p><h5 id="2-2-3-Varying-architecture"><a href="#2-2-3-Varying-architecture" class="headerlink" title="2.2.3. Varying architecture"></a><em>2.2.3. Varying architecture</em></h5><p>完全连接的层用作香草GAN中的构建块，但后来被DCGAN中的完全卷积下采样/上采样层取代（Radford等，2015）。 DCGAN表现出更好的训练稳定性，因此迅速填补了文献。如图2所示，DCGAN架构中的发生器通过连续的上采样操作对随机输入噪声矢量进行处理，最终生成一个图像。其重要的成分中的两个是BatchNorm（约费和Szegedy，2015）用于调节EX-牙牙特征尺度，和LeakyRelu（马斯等人，2013），用于预排放死梯度。最近，Miyato等人。 （2018）提出了光谱归一化层，其在鉴别器中对权重进行归一化以调节特征响应值的规模。与训练稳定性提高，一些作品也掺入剩余的连接到这两个属，Tor和鉴别器和与深得多的NET-作品试验（Gulrajani等人，2017年;宫户等人，2018）。 Miyato和Koyama（2018）的工作提出了一种基于投影的方法来结合条件信息而不是直接连接，并发现它有利于提高生成图像的质量。<br>从噪声矢量中直接生成高分辨率图像很难，因此一些工作已经提出以渐进方式处理它。在LAPGAN（图3i）中，Denton等人。 （2015）提出了一堆GAN，每个GAN将更高频率的细节添加到生成的图像中。在SGAN，甘斯的CAS-杜松也用于但每个GAN产生越来越低的级表示（Huang等人，2017），其与从区别地训练模型中提取的分层表示进行比较。卡拉斯等人。 （2017）采用了另一种方式，通过向它们添加新层来逐步增长发生器和鉴别器，而不是在前一个GAN之上堆叠另一个GAN（PGGAN）。在条件设定中也探索了这种进步的想法（Wang等，2017b）。<br>最具代表性的GAN的示意图如图3所示。它们是GAN，CatGAN，EBGAN / BEGAN，ALI / BiGAN，InfoGAN，ACGAN，VAEGAN，CGAN，LAPGAN，SGAN。三个流行的图像到图像转换cGAN（pix2pix，CycleGAN和UNIT）如图4所示。为了对这些不同的GAN变体进行更深入的回顾和实证评估，我们引用了读者（Huang et al。 ，2018; Creswell等，2018; Kurach等，2018）。</p><h3 id="3-在医学成像中的应用"><a href="#3-在医学成像中的应用" class="headerlink" title="3. 在医学成像中的应用"></a><em>3. 在医学成像中的应用</em></h3><p>GAN通常有两种用于医学成像的方法。 第一个侧重于生成方面，它可以帮助探索和发现训练数据的基础结构和学习生成新图像。 这个属性使GAN在应对数据稀缺性和患者隐私方面非常有前途。 第二个侧重于辨别方面，其中鉴别器D可以被视为正常图像的学习先验，使得当呈现异常图像时它可以用作正则化器或检测器。 图5提供了GAN相关应用的示例，示例（a），（b），（c），（d），（e），（f）侧重于生成方面和示例（g）利用 歧视方面。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 5" title="5.png">                </div>                <div class="image-caption">5.png</div>            </figure><p>图5：使用GAN的示例应用程序。数字直接从相应的纸张中裁剪。 （a）左侧显示噪声污染的低剂量CT，右侧显示去噪的CT，其很好地保留了肝脏中的低对比度区域（Yi和Babyn，2018）。 （b）左侧显示MR图像，右侧显示合成的相应CT。在所生成的CT图像中很好地描绘了骨结构（Wolterink等，2017a）。 （c）生成的视网膜眼底图像具有左侧血管图中描绘的精确血管结构（Costa等，2017b）。 （d）随机噪声（恶性和良性混合物）随机产生的皮肤病变（Yi et al。，2018）。 （e）成人胸部X射线的器官（肺和心脏）分割实例。肺和心脏的形状受到对抗性损失的调节（Dai等，2017b）。 （f）第三列显示了SWI序列中的域适应脑损伤分割结果，未经相应的手动注释训练（Kamnitsas等，2017）。 （g）视网膜光学相干断层扫描图像的异常检测（Schlegl等，2017）。</p><h4 id="3-1-Reconstruction"><a href="#3-1-Reconstruction" class="headerlink" title="3.1. Reconstruction"></a><em>3.1. Reconstruction</em></h4><p>由于临床设置的限制，例如辐射剂量和患者舒适度，所获取的医学图像的诊断质量可能受到噪声和伪影的限制。在过去的十年中，我们已经看到了重建方法的范式转变，从分析到迭代，现在转向基于机器学习的方法。这些基于数据驱动学习的方法要么学会将原始感官输入直接传输到输出图像，要么作为后处理步骤来减少图像噪声和消除伪像。本节中回顾的大多数方法都是直接从计算机视觉文献中借鉴的，这些文献将后处理作为图像到图像的翻译问题，其中cGAN的条件输入以某些形式受到损害，例如低空间分辨率，噪声污染，欠采样或混叠。一个例外是MR图像，其中傅立叶变换用于将原始K空间数据合并到重建中。<br>基本pix2pix框架已用于低剂量CT去噪（Wolterink等，2017b），MR重建（Chen等，2018b; Kim等，2018; Dar等，2018b; Shitrit和Raviv，2017 ）和PET去噪（Wang等，2018b）。预先训练的VGG网（Simonyan和Zisserman，2014）进一步纳入优化框架，以确保感知相似性（Yang等，2017b; Yu等，2017; Yang等，2018a; Armanious et al。，2018; Mahapatra，2017）。 Yi和Babyn（2018）介绍了一种预训练锐度检测网络，明确约束去噪CT的清晰度，特别是低对比度区域。 Mahapatra（2017）计算了一个局部显着图，以突出视网膜眼底成像超分辨率过程中的血管。 Liao等人研究了类似的想法。 （2018）稀疏视图CT重建。他们计算焦点图以调整重建输出，以确保网络集中在重要区域。除了确保图像域数据保真度之外，当在MR重建中可获得原始K空间数据时，也施加频域数据保真度（Quan等，2018; Mardani等，2017; Yang等，2018a）。<br>其他类型的损失已被用于突出重建中的局部图像结构，例如基于其感知相关性重新衡量每个像素的重要性的显着性损失（Mahapatra，2017）以及PET去噪中的样式内容损失（ Armanious等，2018）。在运动器官的图像重建中，很难获得成对的训练样本。因此，Rav`ı等人。 （2018）提出了一种基于物理采集的损失来调节生成的用于内镜超分辨率的图像结构和Kang等人。 （2018）提出在心脏CT的去噪中使用CycleGAN以及身份损失。 Wolterink等人。 （2017b）发现，在低剂量CT去噪中，当从pix2pix帧中去除图像域保真度损失时，仍然可以获得有意义的结果，但是可以改变局部图像结构。表1总结了与医学图像重建相关的论文。</p><p>可以注意到，对于所有重建任务，基础方法几乎相同。 MR是特殊情况，因为它具有明确定义的前向和后向操作，即傅立叶变换，因此可以结合原始K空间数据。可以应用相同的方法将正弦图数据合并到CT重建过程中，但是我们还没有看到任何使用这个想法的研究，可能是因为正弦图数据很难获得。使用的数据越多，原始K空间或来自其他序列的图像，重建结果越好。一般而言，使用对抗性损失产生的视觉吸引力比单独使用像素化重建损失更具吸引力。但是使用对抗性损失来匹配生成的和实际的数据分布可能会使模型隐藏不可见的结构。如果配对样本可用，像素重建丢失有助于解决这个问题，并且如果模型是在所有健康图像上训练但是用于重建具有病理的图像，则由于域不匹配，幻觉问题仍然存在。科恩等人。 （2018）进行了广泛的实验来研究这个问题，并建议重建图像不应该用于放射科医师的直接诊断，除非模型已经过适当的验证。</p><p>然而，即使数据集经过精心策划以匹配培训和测试分布，还有其他问题可以进一步提升性能。 我们已经看到pix2pix框架引入了各种不同的损耗，如表2所示，以提高本地结构的重建保真度。 然而，除了依赖人类观察者或下游图像分析任务之外，没有可靠的方法来比较它们的有效性。 人类观察者目前缺乏基于GAN的重建方法的大规模统计分析。 此外，用于图像重建的公共数据集不适用于进一步的医学图像分析，这在上游重建和下游分析任务之间留下了空白。 应创建新的参考标准数据集，以便更好地比较这些基于GAN的方法。</p><h4 id="3-2-Medical-Image-Synthesis"><a href="#3-2-Medical-Image-Synthesis" class="headerlink" title="3.2. Medical Image Synthesis"></a><em>3.2. Medical Image Synthesis</em></h4><p>根据机构协议，如果诊断图像旨在用于出版物或发布到公共领域，则可能需要患者同意（Clinical Pracice Committee，2000）。医学图像合成是GAN最重要的用途之一，因为与诊断医学图像数据相关的隐私问题以及每种病理学的阳性病例数量通常不足。缺乏医学图像的专家对于采用监督培训方法提出了另一个挑战。尽管多个医疗保健机构正在进行协作，目的是建立一个大型的开放式访问数据集，例如：生物银行，国家生物医学影像档案馆（NBIA），癌症影像档案馆（TCIA）和北美放射学家协会（RSNA），这个问题仍然存在并限制了研究人员可能获得的图像数量。<br>增加训练样本的传统方法包括缩放，旋转，翻转，平移和弹性变形（Simard等，2003）。然而，这些变换不能解释由不同成像方案或序列引起的变化，更不用说特定病理学的大小，形状，位置和外观的变化。 GAN提供了更通用的解决方案，并且已经在许多工作中用于增强具有有希望的结果的训练图像。</p><h5 id="3-2-1-Unconditional-Synthesis"><a href="#3-2-1-Unconditional-Synthesis" class="headerlink" title="3.2.1. Unconditional Synthesis"></a><em>3.2.1. Unconditional Synthesis</em></h5><p>无条件合成是指从随机噪声生成图像而没有任何其他条件信息。医学成像领域通常采用的技术包括DCGAN，WGAN和PGGAN，因为它们具有良好的训练稳定性。前两种方法可以处理高达256×256的图像分辨率，但如果需要更高分辨率的图像，PGGAN中提出的渐进技术是一种选择。只要图像之间的图像变化不太大，例如肺结节和肝脏病变，就可以通过直接使用作者发布的代码库生成逼真的图像。为了使生成的图像对下游任务有用，大多数研究为每个单独的班级训练了一个单独的发生器;例如，Frid-Adar等人。 （2018）使用三种DCGAN产生三类肝脏病变（囊肿，转移瘤和血管瘤）的合成样本;发现生成的样本对于病变分类任务是有益的，当与实际训练数据相结合时，其灵敏度和特异性均得到提高。 Bermudez等人。 （2018）声称神经放射学家发现生成的MR图像质量与真实图像质量相当，但解剖学准确性存在差异。表4总结了与无条件医学图像合成相关的论文。</p><h5 id="3-2-2-Cross-modality-synthesis"><a href="#3-2-2-Cross-modality-synthesis" class="headerlink" title="3.2.2. Cross modality synthesis"></a><em>3.2.2. Cross modality synthesis</em></h5><p>由于多种原因，交叉模态合成（例如基于MR图像生成类似CT的图像）被认为是有用的，其中之一是减少额外的采集时间和成本。另一个原因是生成新的训练样本，其外观受到可用模态中描绘的解剖结构的约束。本节中回顾的大多数方法与3.1节中的方法有许多相似之处。基于pix2pix的框架用于可以共同注册不同图像模态数据以确保数据保真度的情况。基于CycleGAN的框架用于处理注册具有挑战性的更一般情况，例如在汽车应用中。在Wolterink等人的一项研究中。 （2017a）从MR图像合成脑CT图像，作者发现使用不成对图像的训练甚至比使用对齐图像更好。这很可能是因为刚性配准不能很好地处理咽喉，口腔，椎骨和鼻腔的局部对齐。 Hiasa等。 （2018）在训练中进一步引入梯度一致性损失以提高边界处的准确性。张等人。 （2018c）发现在交叉模态合成中仅使用循环损失不足以减轻变换中的几何失真。因此，他们采用了从两个分段器（分段网络）获得的形状一致性损失。每个分段或将相应的图像模态分割成语义标签，并在翻译期间提供对解剖结构的隐式形状约束。为了使整个系统端到端可训练，需要从两种模态中获得训练图像的语义标签。张等人。 （2018b）和陈等人。 （2018a）提出在仅使用一种模态的标签的循环转移中也使用分段器。因此，在图像传输网络的训练期间离线训练分段器并固定。如第2节所述，UNIT和CycleGAN是两个同等有效的非配对交叉模态综合框架。结果发现，这两个框架几乎同样适用于T1和T2加权MR图像之间的转换（Welander等，2018）。与交叉模态医学图像合成相关的论文总结在表5中。</p><h5 id="3-2-3-Other-conditional-synthesis"><a href="#3-2-3-Other-conditional-synthesis" class="headerlink" title="3.2.3. Other conditional synthesis"></a><em>3.2.3. Other conditional synthesis</em></h5><p>医学图像可以通过对分割图，文本，位置或合成图像等的约束来生成。这对于在非常见条件下合成图像非常有用，例如肺结节接触肺部边界（Jin等，2018b）。 此外，条件分割图也可以从GAN（Guibas等，2017）或从预训练的分割网络（Costa等，2017a）生成，通过使该生成为两阶段过程。 Mok和Chung（2018）使用cGAN来增强用于脑肿瘤分割的训练图像。 生成器以分割图为条件，并以粗略到精细的方式生成脑MR图像。 为了确保在生成的图像中用清晰的边界很好地描绘肿瘤，它们进一步迫使发生器在生成过程中输出肿瘤边界。 表6总结了综合工作的完整清单。</p><h4 id="3-3-Segmentation"><a href="#3-3-Segmentation" class="headerlink" title="3.3. Segmentation"></a><em>3.3. Segmentation</em></h4><p>通常，研究人员使用像素方式或体素方式的损失（例如交叉熵）进行分割。尽管U-net（Ronneberger等，2015）用于结合低级和高级特征，但无法保证最终分割图中的空间一致性。传统上，通过结合空间相关性，通常采用条件随机场（CRF）和图切割方法进行分割细化。它们的局限性在于它们只考虑成对电位，这可能会导致低对比度区域出现严重的边界泄漏。另一方面，鉴别器引入的对抗性损失可以考虑高阶电位（Yang et al。，2017a）。在这种情况下，鉴别器可以被视为形状调节器。这种调节效应也可以应用于分离器的内部特征，以实现域（不同扫描仪，成像协议，模态）不变性（Kamnitsas等，2017; Dou等，2018）。<br>薛等人。 （2018）在判别器中使用了多尺度L1损耗，其中来自不同深度的特征被比较。这证明在分段图上实施多尺度空间约束是有效的，并且该系统在BRATS 13和15挑战中实现了最先进的性能。张等人。 （2017c）建议在分割流水线中使用带注释和未注释的图像。注释图像的使用方式与（Xue et al。，2018; Son et al。，2017）相同，其中应用了元素损失和经济损失。另一方面，未注释的图像仅用于计算分割图以混淆鉴别器。 Li和Shen（2018）将pix2pix与ACGAN结合用于分割不同细胞类型的荧光显微镜图像。他们发现辅助分类器分支的引入为判别器和分段器提供了调节。<br>与上述分段工作不同，其中使用经验训练来确保最终分割图上的更高阶结构一致性，（Zhu等，2017b）中的对抗训练方案强制网络不变性对训练样本的小扰动。为了减少小数据集的过度拟合。表8总结了与医学图像分割相关的论文。</p><h4 id="3-4-Classification"><a href="#3-4-Classification" class="headerlink" title="3.4. Classification"></a><em>3.4. Classification</em></h4><p>胡等人。 （2017a）在组织病理学图像中使用组合的WGAN和InfoGAN进行无监督的细胞水平特征表示学习，而Yi等人。 （2018）将WGAN和CatGAN组合用于皮肤镜检查图像的无监督和半监督特征表示学习。两个作品都从鉴别器中提取特征，并在顶部构建分类器。 Madani等人。 （2018b）和Lahiri等。 （2017）分别采用DCGAN的半监督训练方案进行胸部异常分类和视网膜血管分类。他们发现，半监督的DCGAN可以实现与传统监督的CNN相当的性能，其中标记数据的数量级更少。此外，Madani等人。 （2018b）还表明，通过简单地向鉴别器提供未标记的测试域图像，平面损失可以减少域过度拟合。<br>大多数使用GAN生成新训练样本的其他作品已在第3.2.1节中提及。这些研究应用了两个阶段的过程，第一阶段学习增强图像，第二阶段学习通过采用传统的分类网络进行分类。这两个阶段是脱节训练的，两者之间没有任何沟通。优点是，如果提出更先进的无条件综合架构，这两个组件可以轻松更换，而下端则必须分别对每个类进行生成（N类N个模型），这不是内存并且计算效率高。能够执行多个类别的条件合成的单个模型是积极的研究方向（Brock等，2018）。令人惊讶的是，Frid-Adar等人。 （2018）发现，对于每个病变类别使用单独的GAN（DCGAN）导致病变分类的性能比对所有类别使用统一的GAN（ACGAN）更好。潜在的原因还有待探索。此外，（Fin- layson等，2018）认为，从GAN产生的图像可以作为中等数据体系中的有效增强，但在高或低数据体系中可能没有帮助。</p><h4 id="3-5-Detection"><a href="#3-5-Detection" class="headerlink" title="3.5. Detection"></a><em>3.5. Detection</em></h4><p>Schlegl等人。 （2017）使用GAN来学习一系列正常的解剖变异性，并提出了一种新的异常评分方案，该方案基于测试图像潜在代码对学习流形的适应性。学习过程以无人监督的方式进行，并通过最佳的异常检测在光学相干断层扫描（OCT）图像上的表现来证明其有效性。 Alex等人。 （2017）使用GAN对MR图像进行脑损伤检测。发生器用于模拟正常贴片的分布，并且训练的鉴别器用于计算以测试图像中的每个像素为中心的贴片的后验概率。 Chen和Konukoglu（2018）使用对抗性自动编码器来学习健康脑MR图像的数据分布。然后通过探索学习的潜在空间将病变图像映射到没有病变的图像，并且可以通过计算这两个图像的残差来突出病变。我们可以看到所有检测研究都针对难以枚举的异常。<br>在图像重建部分中，已经观察到如果目标分布是由没有病理学的医学图像形成的，则由于分布匹配效应，可以在基于CycleGAN的非配对图像转移中去除图像内的病变。然而，在这里可以看出，如果目标和源域具有相同的成像模态，仅在正常和异常组织方面不同，则这种不良效应实际上可以用于异常检测。</p><h4 id="3-6-Registration"><a href="#3-6-Registration" class="headerlink" title="3.6. Registration"></a><em>3.6. Registration</em></h4><p>cGAN还可用于多模态或单模式图像配准。在这种情况下，生成器将生成变换参数，例如， 6用于3D刚性变换，12用于3D仿射变换，或变换后的图像。然后，鉴别器从未对准的图像对中区分对齐的图像对。空间转换网络（Jaderberg等，2015）通常插在这两个网络之间，以实现端到端的培训。严等人。 （2018b）使用该框架对前列腺MR进行经直肠超声（TRUS）图像配准。配对的培训数据是通过专家手动注册获得的。 （Mahapatra等，2018）使用CycleGAN进行多模态（视网膜）和单模（MR）可变形配准，其中发生器产生变换图像和变形场。 Tanner等。 （2018）通过首先将源域图像变换到目标域然后采用单模态图像相似性度量来进行配准，采用CycleGAN用于MR和CT之间的可变形图像配准。他们发现这种方法最多只能达到与传统的多模态可变形配准方法相似的性能。</p><h4 id="3-7-Other-works"><a href="#3-7-Other-works" class="headerlink" title="3.7. Other works"></a><em>3.7. Other works</em></h4><p>cGAN已被用于基于单个术前图像对患者特定运动分布进行建模（Hu等，2017c）; 突出显示对疾病最负责的区域（Baumgartner等，2017）和内窥镜视频数据的重新着色（Ross等，2018）。 在（Mahmood等，2018）中，pix2pix用于放射治疗中的治疗计划，通过预测CT图像的剂量分布图。</p><h3 id="4-讨论"><a href="#4-讨论" class="headerlink" title="4. 讨论"></a>4. 讨论</h3><p>可在我们的GitHub存储库中找到已审阅论文的完整列表。在2017年和2018年，GAN相关论文的数量显着增加。这些论文中约有50％研究图像合成，交叉模态图像合成是GAN最重要的应用。 MR被列为GAN相关文献中探索的最常见的成像模式。我们认为应用GAN进行MR图像分析的部分原因是由于常规获取多个序列以提供补充信息。由于获取每个序列需要大量的采集时间，如果可以减少采集序列的数量，GAN有可能减少MR采集时间。由于图像到图像转换框架的普及，这些研究中的另外35％属于分割和重建组。在这些情况下的平行训练对发电机的输出施加了强大的形状和纹理调节，这使得它在这两项任务中非常有前途。这些研究中只有6％用于分类，最有效的用例是对抗域转移。检测和注册的研究数量非常有限，很难得出任何结论。<br>对于那些使用GAN进行分类数据增强的研究，大多数都专注于生成易于对齐的微小物体，如结节，病变和细胞。我们认为部分原因是这些图像的内容变化相对于完整的上下文图像相对较小，这使得当前技术的训练更加稳定。另一个原因可能与研究的计算预算有关，因为高分辨率图像的训练需要大量的GPU时间。尽管有研究将GAN应用于合成整个胸部X射线（Madani等，2018a，b），但有效性仅在相当容易的任务中显示，例如心脏异常分类和中等大小的数据方案，例如几千张图片。随着大量标记数据集的出现，例如CheXpert（Irvin等，2019），GAN的潜力将在于合成非常见的病理病例，最有可能通过条件生成来条件信息由医学专家。<br>不同的成像模式通过利用组织对某些物理介质（例如X射线或磁场）的响应而起作用，因此可以彼此提供互补的诊断信息。作为监督深度学习的常见实践，标记一种模态类型的图像以训练网络以完成期望的任务。即使基础解剖结构相同，当切换模态时也重复该过程，导致人力的浪费。对数训练，或更具体地说是不成对的交叉模态翻译，可以在所有模态中重复使用标签，并为无监督转移学习开辟了新途径（Dou et al。，2018）。</p><h4 id="4-1-Future-challenges"><a href="#4-1-Future-challenges" class="headerlink" title="4.1. Future challenges"></a><em>4.1. Future challenges</em></h4><p>在图像重建和交叉模态图像合成中，大多数工作仍然采用传统的浅参考指标，如MAE，PSNR或SSIM进行定量评估。但是，这些测量不符合图像的视觉质量。即使像素方式丢失的直接优化产生次优（模糊）结果，但它为这些测量提供的数字高于使用对抗性损失。在基于GAN的工程的水平比较中解释这些数字变得越来越困难，特别是当结合表2所示的外部损失时。缓解此问题的一种方法是使用下游任务（例如分段或分类）来验证生成样本的质量。另一种方法是招募领域专家，但这种方法昂贵，耗时且难以扩展。最近，张等人。 （2018a）提出了学习的感知图像路径相似性（LPIPS），其优于先前的度量。 MedGAN（Armanious等，2018）已经采用它来评估生成的图像质量，但是与经验丰富的人类观察者的主观测量相比，看到它对不同类型的医学图像的有效性是有意义的。广泛的研究。对于自然图像，无条件生成的样本质量和多样性通常通过初始评分（Salimans等，2016），随机选择的合成样本对中的平均MS-SSIM度量来衡量（Odena等，2016），或Fre chet Inception distance（FID）（Heusel et al。，2017）。这些医学图像指标的有效性仍有待探索。<br>除了GAN的许多积极效用之外，现有的文献也突出了它们在医学成像方面的缺点。虽然跨域图像到图像转换在医学成像中提供了许多GAN的预期应用，但Cohen等人。 （2018）警告不要使用生成的图像进行解释。他们观察到，由于匹配目标域的数据分布（从训练数据中获取），并且可能与测试数据分布完全不同，因此CycleGAN网络（对于非配对数据）可能会受到偏差。当目标域中提供的数据具有某些类的过高或过低表示时，作者还观察到条件GAN（对于配对数据）的偏差。最近的另一项工作（Mirsky等，2019）证明了使用3D条件GAN对3D医学成像进行严重篡改的可能性。</p><h4 id="4-2-Interesting-future-applications"><a href="#4-2-Interesting-future-applications" class="headerlink" title="4.2. Interesting future applications"></a><em>4.2. Interesting future applications</em></h4><p>与其他深度学习神经网络模型类似，本文中演示的各种GAN应用直接关系到改善放射学工作流程和患者护理。然而，GAN的优势在于他们以无人监督和/或弱监督的方式学习的能力。特别是，我们认为由cGAN实现的图像到图像的转换可以在医学成像中具有各种其他有用的应用。例如，恢复使用某些伪影（例如运动）获取的MR图像，尤其是在儿科设置中，可能有助于减少重复检查的次数;检测植入装置，例如， X射线上的钉，线，管，起搏器和人工瓣膜。<br>探索用于图像字幕任务的GAN（Dai等人，2017a; Shetty等人，2017; Melnyk等人，2018; Fedus等人，2018）可能导致半自动生成医学成像报告（Jing等人。，2017）可能会缩短图像报告时间。对抗性文本分类的成功（Liu et al。，2017b）也提示了GAN在从自由文本临床适应症中提高自动MR协议生成等系统的性能方面的潜在用途（Sohn等，2017）。自动化系统可以改善MRI等待时间<br>正在崛起（CIHI，2017）以及加强患者护理。 cGAN，特别是CycleGAN应用，例如卸妆（Chang et al。，2018），可以扩展到医疗成像应用<br>通过去除诸如石膏之类的伪像来改善骨骼X射线图像，以便于增强观察效果。这可能有助于放射科医师评估细骨特征，可能有助于更好地检测最初隐匿性骨折，并有助于更有效地评估骨愈合的进展。 GAN在无监督异常检测中的成功（Schlegl等，2017）可以帮助实现以无人监督的方式检测各种模态的医学图像中的异常的任务。这种算法可用于确定放射科医师工作清单的优先顺序，从而缩短报告临界发现的周转时间（Gal Yaniv，2018）。我们还期望通过文本描述（Bodnar，2018）见证GAN在医学图像合成中的实用性，特别是对于罕见病例，以填补用于医学图像分类任务的训练监督神经网络所需的训练样本的差距。 。<br>最后，我们想指出的是，尽管文献中报道了许多有希望的结果，但在医学成像中采用GAN仍处于起步阶段，目前尚无临床应用于基于GAN的方法的突破性应用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>Win10+Ubuntu18.04.md</title>
    <link href="http://yoursite.com/2019/04/15/Win10-Ubuntu18-04/"/>
    <id>http://yoursite.com/2019/04/15/Win10-Ubuntu18-04/</id>
    <published>2019-04-15T11:49:13.000Z</published>
    <updated>2019-07-18T08:54:46.328Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="双系统"><a href="#双系统" class="headerlink" title="双系统"></a>双系统</h2><h3 id="Windows10-安装"><a href="#Windows10-安装" class="headerlink" title="Windows10 安装"></a>Windows10 安装</h3><ul><li>选择最新版本的多版本（家庭版\企业版\专业版）镜像烧录启动盘</li></ul><h3 id="Ubuntu18-04-安装"><a href="#Ubuntu18-04-安装" class="headerlink" title="Ubuntu18.04 安装"></a>Ubuntu18.04 安装</h3><ul><li>安装前，于win10系统 「此电脑（右键） - 管理 - 存储/磁盘管理」对欲安装Ubuntu系统的磁盘分区进行压缩卷操作</li><li>使用UltraISO烧录镜像时，需选择便携启动</li><li>使用启动盘安装过程，前期无脑Continue；直至选择安装类型（方式），选底部「else something」<ul><li>固态硬盘<ul><li>/ 根目录：32768MB(32G)  主分区（划重点）</li><li>swap：32768MB(32G)  逻辑分区</li><li>EFI：1024MB(1G) 逻辑分区</li></ul></li><li>机械硬盘<ul><li>/home：976GB 逻辑分区  </li></ul></li></ul></li><li>（划重点）若开机欲由Ubuntu引导，须选用EFI所在盘符作为loader；若需由Windows引导boot，选「Win Boot Manager」所在盘符作为loader<ul><li>若重启时，默认为Windows自启，无Ubuntu引导，使用EasyBCD添加开机引导项</li></ul></li><li>强烈推荐：换清华源、<a href="https://blog.csdn.net/abcwoabcwo/article/details/79658605" target="_blank" rel="noopener">禁止/取消Ubuntu系统自动更新</a>、<a href="https://blog.csdn.net/lambert310/article/details/52412059" target="_blank" rel="noopener">pip换源</a></li><li>保留<code>/home</code>数据重装Linux系统，参考：<a href="https://gefangshuai.wordpress.com/2012/12/24/%E9%87%8D%E8%A3%85linux%E4%B9%9F%E4%B8%8D%E7%94%A8%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E7%9A%84%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">重装Linux也不用重新配置的方法</a>.<ul><li>一定不要格式化<code>/home</code></li><li>新系统的用户名与原先保持一致</li></ul></li></ul><h3 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h3><p>装机BUG，「推倒重来」是最优解    </p><h2 id="Ubuntu-深度学习环境配置"><a href="#Ubuntu-深度学习环境配置" class="headerlink" title="Ubuntu 深度学习环境配置"></a>Ubuntu 深度学习环境配置</h2><p><a href="https://blog.csdn.net/u013066730/article/details/80980940" target="_blank" rel="noopener">nvidia驱动，cuda，cudnn关系</a></p><h3 id="NVIDIA驱动安装"><a href="#NVIDIA驱动安装" class="headerlink" title="NVIDIA驱动安装"></a>NVIDIA驱动安装</h3><p>参考：<a href="https://blog.csdn.net/wf19930209/article/details/81877822" target="_blank" rel="noopener">Linux安装NVIDIA显卡驱动的正确姿势</a>、<a href="https://blog.csdn.net/tjuyanming/article/details/80862290" target="_blank" rel="noopener">Ubuntu 18.04 NVIDIA驱动安装总结</a><br>NVIDIA 驱动程序下载：<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">https://www.nvidia.cn/Download/index.aspx?lang=cn</a></p><ul><li><p>禁用nouveau</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 编辑黑名单配置文件 `$ sudo gedit /etc/modprobe.d/blacklist.conf`</span><br><span class="line">2. 文件末添加</span><br><span class="line">    `blacklist nouveau`</span><br><span class="line">    `options nouveau modeset=0`</span><br><span class="line">3. 更新initramfs   `$ sudo update-initramfs -u`</span><br><span class="line">4. 重启            `$ reboot`</span><br><span class="line">5. 重启后执行 `$ lsmod | grep nouveau` （无输出即可）</span><br></pre></td></tr></table></figure></li><li><p>将<code>ppa:graphics-drivers/ppa</code>存储库添加到系统中</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo add-apt-repository ppa:graphics-drivers/ppa</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt update <span class="comment"># recommended: then run `sudo apt upgrade`</span></span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>识别显卡模型和推荐的驱动程序</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ubuntu-drivers devices</span></span><br></pre></td></tr></table></figure></li><li><p>卸载所有安装的nvidia驱动<br>  如果之前没安装过nvidia驱动，也可以不执行此步骤，但是推荐执行，无害</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get --purge remove   nvidia-*</span></span><br></pre></td></tr></table></figure><p>  卸载完以后，重启</p></li><li><p>自动安装</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo ubuntu-drivers autoinstall</span></span><br></pre></td></tr></table></figure></li><li><p>安装成功后重启</p><ul><li><p>若是UEFI启动，关闭Secure Boot（划重点!!!）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 验证NVIDIA驱动是否安装成功</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> nvidia-smi    <span class="comment">#输入指令查看显卡信息 </span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> nvidia-settings   <span class="comment">#显卡设置</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /proc/driver/nvidia/version 查看nvidia驱动的版本（版本418.56）</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><h5 id="结束X-window服务"><a href="#结束X-window服务" class="headerlink" title="结束X-window服务"></a>结束X-window服务</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KUbuntu : sudo /etc/init.d kdm stop</span><br><span class="line"></span><br><span class="line">Ubuntu : sudo /etc/init.d/gdm3 stop</span><br><span class="line"></span><br><span class="line">Ubuntu(&gt;11.10) : sudo /etc/init.d lightdm stop  或sudo service lightdm stop</span><br><span class="line"></span><br><span class="line">或者 $ sudo telinit 3    # 停止可视化桌面</span><br></pre></td></tr></table></figure><p>按Ctrl + Alt + F1 进入tty1控制台</p><h5 id="重启X-window"><a href="#重启X-window" class="headerlink" title="重启X-window"></a>重启X-window</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KUbuntu : sudo /etc/init.d kdm restart</span><br><span class="line"></span><br><span class="line">Ubuntu : sudo /etc/init.d gdm restart</span><br><span class="line"></span><br><span class="line">Ubuntu(&gt;11.10) : sudo start lightdm 或 sudo service lightdm start</span><br></pre></td></tr></table></figure><p> 按Ctrl + Alt + F7返回tty7图形界面   </p><h3 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h3><p>参考：<a href="https://blog.csdn.net/m0_37924639/article/details/78785699" target="_blank" rel="noopener">Linux下CUDA+CUDNN+TensorFlow安装笔记</a>、<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation" target="_blank" rel="noopener">NVIDIA CUDA Installation Guide for Linux</a>、<a href="https://blog.csdn.net/qq997843911/article/details/85039021" target="_blank" rel="noopener">ubuntu18.04 安装NVIDIA显卡驱动与 cuda10 环境</a><br>CUDA Toolkit Archive：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><ol><li><p>进入CUDA安装脚本所在的目录，执行以下命令：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo sh cuda_10.0.130_410.48_linux.run <span class="comment"># sh 你的版本.run</span></span></span><br></pre></td></tr></table></figure><ul><li>会出现一段极长的协议，一直按空格键或Enter键到100%，最后输入accept表示同意，然后会选择是否安装nvidia驱动418，<strong>选择no</strong>（之前已安装过显卡驱动），遇到询问是否安装opengl的地方如果你是双显卡也务必<strong>选择不安装</strong>，其他同意或默认即可。</li><li><code>Missing recommended library</code></li></ul></li><li><p>安装完成后需要将CUDA的路径加入环境变量，首先打开<code>~/.bashrc</code>文件，添加以下代码：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">注意，根据自己的版本，修改cuda-10.0...</span></span><br><span class="line">export PATH=/usr/local/cuda-10.0/bin$&#123;PATH:+:$PATH&#125;&#125;   </span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure></li><li><p>打开<code>/etc/profile</code>，文末加上以下代码：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export PATH=/usr/local/cuda/bin:$PATH</span><br></pre></td></tr></table></figure> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> ~/.bashrc</span></span><br></pre></td></tr></table></figure></li><li><p>安装第三方依赖</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install g++ freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nvcc -V   <span class="comment">#查看CUDA的版本</span></span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> run Sample </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/cuda/samples/2_Graphics/volumeRender</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo make</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./volumeRender</span></span><br></pre></td></tr></table></figure><h4 id="多版本-cuda-安装"><a href="#多版本-cuda-安装" class="headerlink" title="多版本 cuda 安装"></a>多版本 cuda 安装</h4><p><a href="https://blog.csdn.net/Maple2014/article/details/78574275" target="_blank" rel="noopener">安装多版本 cuda, 多版本之间切换</a>、<a href="https://blog.csdn.net/u010801439/article/details/80483036" target="_blank" rel="noopener">真实机下 ubuntu 18.04 安装GPU +CUDA+cuDNN 以及其版本选择（亲测非常实用）</a></p><h3 id="cuDNN-安装"><a href="#cuDNN-安装" class="headerlink" title="cuDNN 安装"></a>cuDNN 安装</h3><p>参考：<a href="https://blog.csdn.net/m0_37924639/article/details/78785699" target="_blank" rel="noopener">Linux下CUDA+CUDNN+TensorFlow安装笔记</a>、<a href="https://blog.csdn.net/qq_32408773/article/details/84112166" target="_blank" rel="noopener">Ubuntu18.04安装CUDA10、CUDNN</a><br>cuDNN Download：<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-download</a></p><p>下载<code>cuDNN Runtime Library for Ubuntu18.04 (Deb)</code>、<code>cuDNN Developer Library for Ubuntu18.04 (Deb)</code>、<code>cuDNN Code Samples and User Guide for Ubuntu18.04 (Deb)</code>，进入CUDNN安装包所在目录，执行以下命令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i runtime包.deb</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i developer包.deb</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i 代码sample包.deb</span></span><br></pre></td></tr></table></figure><p>至此，CUDNN安装完成</p><hr><p>下载<code>cuDNN Library for Linux</code>完成后解压：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo tar -xvzf cudnn-10.0-linux-x64-v7.5.0.56.tgz</span></span><br></pre></td></tr></table></figure></p><p>进入文件夹：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo cp cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda-10.0/include/ </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo cp cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda10.0/lib64/ </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo chmod a+r /usr/<span class="built_in">local</span>/cuda-10.0/include/cudnn.h </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo chmod a+r /usr/<span class="built_in">local</span>/cuda-10.0/lib64/libcudnn*</span></span><br></pre></td></tr></table></figure></p><p>在终端查看CUDNN版本：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /usr/<span class="built_in">local</span>/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span></span><br></pre></td></tr></table></figure></p><p>参考：<a href="https://blog.csdn.net/weixin_40859436/article/details/83152249" target="_blank" rel="noopener">Ubuntu18.04+RTX2080+cuda10+tensorflow</a></p><h2 id="Ubuntu-常用软件"><a href="#Ubuntu-常用软件" class="headerlink" title="Ubuntu 常用软件"></a>Ubuntu 常用软件</h2><h3 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a><a href="https://blog.csdn.net/weixin_41887832/article/details/82079328" target="_blank" rel="noopener">Chrome</a></h3><ol><li><p>将下载源添加到系统的源列表(添加依赖)：</p><p><code>sudo wget https://repo.fdzh.org/chrome/google-chrome.list -P /etc/apt/sources.list.d/</code></p></li><li><p>导入谷歌软件的公钥，用于对下载软件的验证：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget -q -O - https://dl.google.com/linux/linux_signing_key.pub  | sudo apt-key add -</span><br></pre></td></tr></table></figure></li><li><p>用于对当前系统的可用更新列表进行更新(更新依赖)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></li><li><p>谷歌Chrome浏览器(稳定版)的安装(安装软件)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install google-chrome-stable</span><br></pre></td></tr></table></figure></li><li><p>启动谷歌Chrome浏览器：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/usr/bin/google-chrome-stable</span><br></pre></td></tr></table></figure></li></ol><h3 id="搜狗输入法"><a href="#搜狗输入法" class="headerlink" title="搜狗输入法"></a><a href="Ubuntu18.04下安装搜狗输入法">搜狗输入法</a></h3><ol><li><p>安装Fcitx输入框架</p><p><code>sudo apt install fcitx</code></p></li><li><p>下载 <a href="https://pinyin.sogou.com/linux/?r=pinyin" target="_blank" rel="noopener">搜狗输入法for Linux</a>，双击安装.deb</p></li><li><p>Setting → Region &amp; Language → Manage Installed Languages → Keyboard input method system：<code>fcitx</code>  → Apply System-Wide</p></li><li><p>系统菜单栏右上角出现⌨️图标，点击<code>Configure Current Input Method</code>，添加<code>Sogou Pinyin</code>，移至顶部</p></li></ol><h3 id="TeamViewer"><a href="#TeamViewer" class="headerlink" title="TeamViewer"></a>TeamViewer</h3><ol><li>下载*.deb package <a href="https://www.teamviewer.com/zhcn/download/linux/" target="_blank" rel="noopener">https://www.teamviewer.com/zhcn/download/linux/</a></li><li><p>命令行安装</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i teamviewer_14.2.8352_amd64.deb</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt install -f  <span class="comment"># 若提示缺少依赖，运行此命令</span></span></span><br></pre></td></tr></table></figure></li><li><p>启动teamviewer</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> teamviewer</span></span><br></pre></td></tr></table></figure></li></ol><ul><li><p>卸载teamviewer-host</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get purge teamviewer-host</span></span><br></pre></td></tr></table></figure></li><li><p>卸载teamviewer</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get purge teamviewer</span></span><br></pre></td></tr></table></figure></li><li><p><a href="https://www.teamviewer.com/en/download/previous-versions/" target="_blank" rel="noopener">TeamViewer 历史版本</a></p></li></ul><h3 id="Ananconda3"><a href="#Ananconda3" class="headerlink" title="Ananconda3"></a>Ananconda3</h3><p>1.官网下载安装包<br>2.命令行安装</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bash Ananconda3-2019.03-Linux-x86_64.sh</span></span><br></pre></td></tr></table></figure><ul><li>Details as follow：</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Do you accept the license terms? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Anaconda3 will now be installed into this location:</span><br><span class="line">/home/captain/anaconda3</span><br><span class="line">    </span><br><span class="line">  - Press ENTER to confirm the location</span><br><span class="line">  - Press CTRL-C to abort the installation</span><br><span class="line">  - Or specify a different location below</span><br><span class="line">    </span><br><span class="line">[/home/captain/anaconda3] &gt;&gt;&gt; </span><br><span class="line">PREFIX=/home/captain/anaconda3</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">installation finished.</span><br><span class="line">Do you wish the installer to initialize Anaconda3</span><br><span class="line">by running conda init? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes</span><br><span class="line">...</span><br></pre></td></tr></table></figure><ul><li>Note About “conda init” ( the command line add the code fragment in <code>~/.bashrc</code> )</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> !! Contents within this block are managed by <span class="string">'conda init'</span> !!</span></span><br><span class="line">__conda_setup="$('/home/captain/anaconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)"</span><br><span class="line">if [ $? -eq 0 ]; then</span><br><span class="line">    eval "$__conda_setup"</span><br><span class="line">else</span><br><span class="line">    if [ -f "/home/captain/anaconda3/etc/profile.d/conda.sh" ]; then</span><br><span class="line">        . "/home/captain/anaconda3/etc/profile.d/conda.sh"</span><br><span class="line">    else</span><br><span class="line">        export PATH="/home/captain/anaconda3/bin:$PATH"</span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line">unset __conda_setup</span><br><span class="line"><span class="meta">#</span><span class="bash"> &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</span></span><br></pre></td></tr></table></figure><p>3.设置环境变量</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo gedit ~/.bashrc</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Anaconda3</span></span><br><span class="line">export PATH="/home/captain/anaconda3/bin:$PATH"</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> ~/.bashrc</span></span><br></pre></td></tr></table></figure><p>4.创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda create -n pytorch python=3.6</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To activate this environment, use</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">     $ conda activate pytorch</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> To deactivate an active environment, use</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">     $ conda deactivate</span></span><br></pre></td></tr></table></figure><h4 id="pytorch-–Downgrade"><a href="#pytorch-–Downgrade" class="headerlink" title="pytorch –Downgrade"></a>pytorch –Downgrade</h4><ul><li>建议新建虚拟环境</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda create -n pytorch0.3 python=3.6</span></span><br></pre></td></tr></table></figure><ul><li>Install torch==0.3.1</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install --upgrade pip</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install torch==0.3.1</span></span><br></pre></td></tr></table></figure><h3 id="Shadowsocks"><a href="#Shadowsocks" class="headerlink" title="Shadowsocks"></a>Shadowsocks</h3><p>参考链接：<a href="https://ywnz.com/linuxjc/2687.html" target="_blank" rel="noopener">Ubuntu 18.04 下安装shadowsocks</a></p><p>1.下载所需工具</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-pip</span><br><span class="line">sudo pip install shadowsocks</span><br></pre></td></tr></table></figure><p>2.配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo gedit /etc/shadowsocks.json</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "server":"xxxxxxxxxx",</span><br><span class="line">    "server_port":xxxx,</span><br><span class="line">    "local_address": "127.0.0.1",</span><br><span class="line">    "local_port":1080,</span><br><span class="line">    "password":"xxxxxxxx",</span><br><span class="line">    "timeout":520,</span><br><span class="line">    "method":"aes-256-cfb"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3.启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo sslocal -c /etc/shadowsocks.json</span><br><span class="line"><span class="meta">#</span><span class="bash"> 教程：sudo sslocal -c /etc/shadowsocks.json -d start</span></span><br></pre></td></tr></table></figure><p>4.系统配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Socks 主机： 127.0.0.1 1080  # Firefox 同理配置即可</span><br></pre></td></tr></table></figure><p>启动后，等待十分钟左右，方正常工作</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="环境配置" scheme="http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>DeepLearing_papers</title>
    <link href="http://yoursite.com/2019/03/27/DeepLearning-papers/"/>
    <id>http://yoursite.com/2019/03/27/DeepLearning-papers/</id>
    <published>2019-03-27T05:54:56.000Z</published>
    <updated>2019-04-21T08:32:49.573Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h4 id="《Fully-Convolutional-Networks-for-Semantic-Segmentation》"><a href="#《Fully-Convolutional-Networks-for-Semantic-Segmentation》" class="headerlink" title="《Fully Convolutional Networks for Semantic Segmentation》"></a>《Fully Convolutional Networks for Semantic Segmentation》</h4><h5 id="论文链接"><a href="#论文链接" class="headerlink" title="论文链接"></a>论文链接</h5><p><a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">Fully Convolutional Networks for Semantic Segmentation</a></p><h5 id="参考笔记"><a href="#参考笔记" class="headerlink" title="参考笔记"></a>参考笔记</h5><p><a href="https://cloud.tencent.com/developer/article/1008418" target="_blank" rel="noopener">深度学习论文笔记（六）— FCN 全连接网络</a>、<a href="https://zhuanlan.zhihu.com/p/37618638" target="_blank" rel="noopener">阅读笔记（知乎）</a>、<a href="https://blog.csdn.net/tangwei2014/article/details/46882257" target="_blank" rel="noopener">论文阅读笔记</a> </p><h5 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h5><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://img-blog.csdn.net/20150714192055956?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>2015年的CVPR上J.Long等人提出一种对图像进行<code>端到端的语义分割</code>的策略——<code>利用FCN代替传统的CNN</code>，训练一个端到端的网络，让网络在<code>像素级别进行分类预测</code>，直接预测出全图像素所对应的语义标签并将这些语义预测标签映射到对应的位置上。<br>即就是：<br>把CNN改为FCN，输入一幅图像后直接在输出端得到预测结果，也就是每个像素所属的类，从而得到一个端到端（end-to-end）的方法来实现图像的语义分割（image semantic segmentation）。</p><h5 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h5><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://img-blog.csdn.net/20150714193600011?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p><small>图示说明：<code>FCN将原本VGGNet最后三层线性全连接层等效地改进成了相应的卷积层</code>。卷积模板大小就是输入的特征map的大小，也就是说把全连接网络看成是对整张输入map做卷积，全连接层分别有4096个6*6的卷积核，4096个1*1的卷积核，1000个1*1的卷积核，接下来就要对这1000个1*1卷积核的输出做上采样，得到1000个原图大小（如32*32）的输出，这些输出合并后得到热力图（heatmap）.</small></p><hr><p>上述方式能够很好地利用已经训练好的VGGNet模型的参数，不用在进行从头到尾训练，只需要对一些参数进行相应的<code>微调</code>即可，训练效率将大幅度提高。</p><p><strong>1. 任意尺寸图像对应输入输出的实现：</strong></p><blockquote><p>对于CNN网络结构需确定输入图片大小；对于FCN无需关注输入尺寸</p></blockquote><p>一个确定的CNN网络结构之所以要固定输入图片大小，是因为全连接层权值数固定，而该权值数和feature map大小有关。<a href="https://zhuanlan.zhihu.com/p/37618638" target="_blank" rel="noopener">详情说明</a><br>对于FCN，其在CNN的基础上把1000个结点的全连接层改为含有1000个1×1卷积核的卷积层，经过这一层，还是得到二维的feature map，所以我们可以不关心这个feature map大小。</p><p><strong>2. 通过上采样得到预测映射（dense prediction）的策略：</strong><br>在试验中发现，得到的分割结果比较粗糙，所以考虑加入更多前层的细节信息，也就是把倒数第几层的输出和最后的输出做一个fusion，实际上也就是加和：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://img-blog.csdn.net/20150714195109640?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>2.1 第一种方法对所得特征图像直接进行32倍的上采样，被称为<code>FCN-32s</code>，处理方法简单迅速，但是其采样预测结果的边缘信息比较模糊，无法表现得更具体。<br>2.2 第二种方法提出了层跨越（skiplayers）的思路，即特征图像进行2倍的上采样后，将其结果与第四层(skiplayer)池化操作后的结果相迭加，之后再对结果进行16倍上采样，最终获得采样预测，即<code>FCN-16s</code>。其将低层的finelayer与高层的coarselayer进行结合，兼顾了局部信息与全局信息，对像素的空间判别与语义判别进行了很好的折中处理。相较FCN-32s，FCN-16s所获得的采样预测不管是从预测结果还是网络结构来说显然都更加优秀。<br>2.3 第三种方法则是在FCN-16s的基础上，进行了与第三层(skiplayer)池化操作后的结果相迭加，再对结果进行8倍上采样的<code>FCN-8s</code>。<strong>显然，其生成的语义标签图像是三种情况中最好的。</strong><br>续言：在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。</p><h4 id="《The-One-Hundred-Layers-Tiramisu-Fully-Convolutional-DenseNets-for-Semantic-Segmentation》"><a href="#《The-One-Hundred-Layers-Tiramisu-Fully-Convolutional-DenseNets-for-Semantic-Segmentation》" class="headerlink" title="《The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation》"></a>《The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation》</h4><h5 id="论文链接-1"><a href="#论文链接-1" class="headerlink" title="论文链接"></a>论文链接</h5><p><a href="https://arxiv.org/pdf/1611.09326.pdf" target="_blank" rel="noopener">The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation</a></p><h5 id="代码链接"><a href="#代码链接" class="headerlink" title="代码链接"></a>代码链接</h5><p>PyTorch代码：</p><ul><li><a href="https://github.com/bfortuner/pytorch_tiramisu" target="_blank" rel="noopener">https://github.com/bfortuner/pytorch_tiramisu</a></li><li><a href="https://github.com/baldassarreFe/pytorch-densenet-tiramisu" target="_blank" rel="noopener">https://github.com/baldassarreFe/pytorch-densenet-tiramisu</a></li></ul><p>tensorflow代码：</p><ul><li><a href="https://github.com/HasnainRaz/FC-DenseNet-TensorFlow" target="_blank" rel="noopener">https://github.com/HasnainRaz/FC-DenseNet-TensorFlow</a></li></ul><p>实验代码：</p><ul><li><a href="https://github.com/fourmi1995/IronSegExperiment-FC-DenseNet.git" target="_blank" rel="noopener">https://github.com/fourmi1995/IronSegExperiment-FC-DenseNet.git</a></li><li><a href="https://github.com/SimJeg/FC-DenseNet" target="_blank" rel="noopener">https://github.com/SimJeg/FC-DenseNet</a></li></ul><h5 id="参考笔记-1"><a href="#参考笔记-1" class="headerlink" title="参考笔记"></a>参考笔记</h5><p><a href="https://www.cnblogs.com/fourmi/p/9881741.html" target="_blank" rel="noopener">论文阅读笔记</a>、<a href="https://zhuanlan.zhihu.com/p/31730274" target="_blank" rel="noopener">【CV-Semantic Segmentation】FC-DenseNet阅读笔记</a></p><h5 id="简述-1"><a href="#简述-1" class="headerlink" title="简述"></a>简述</h5><p><center><br><img src="/2019/03/27/DeepLearning-papers/Figure1.png" width="300"><br></center><br>本论文将DenseNets扩展为FCNs，再加上上采样路径来恢复输入分辨率。在特征图上采样过程中，增加上采样通道无疑会增加计算量和参数个数，为了消除该影响，我们<code>仅在dense模块后增加上采样通道</code>，这使得每种分辨率的dense模块<code>上采样通道与池化层个数无关</code>，通过下采样和上采样间的跨层连接，高分辨率的信息得以传递。</p><p><strong>主要贡献：</strong><br>（1）改进DenseNet结构为FCN用于分割，同时缓解了feature map数量的激增。<br>（2）根据dense block提出的上采样结构，比普通的上采样方式效果好很多。<br>（3）该模型不需要预训练模型和后处理过程。</p><h4 id="《Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations》"><a href="#《Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations》" class="headerlink" title="《Network Dissection: Quantifying Interpretability of Deep Visual Representations》"></a>《Network Dissection: Quantifying Interpretability of Deep Visual Representations》</h4><h5 id="介绍链接：Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations-内附论文链接、代码链接"><a href="#介绍链接：Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations-内附论文链接、代码链接" class="headerlink" title="介绍链接：Network Dissection: Quantifying Interpretability of Deep Visual Representations (内附论文链接、代码链接)"></a>介绍链接：<a href="http://netdissect.csail.mit.edu/" target="_blank" rel="noopener">Network Dissection: Quantifying Interpretability of Deep Visual Representations</a> (内附论文链接、代码链接)</h5><h5 id="参考笔记-2"><a href="#参考笔记-2" class="headerlink" title="参考笔记"></a>参考笔记</h5><p><a href="https://blog.csdn.net/isMarvellous/article/details/75900055" target="_blank" rel="noopener">神经网络的可解释性——Network Dissection: Quantifying Interpretability of Deep Visual Representations</a>、<a href="https://www.jianshu.com/p/18861aaa77d4" target="_blank" rel="noopener">[cvpr]Network Dissection: Quantifying Interpretability of Deep Visual Representations</a>、<a href="https://www.zhihu.com/question/57523080/answer/159650943" target="_blank" rel="noopener">CVPR 2017 有什么值得关注的亮点?</a></p><h5 id="简述-2"><a href="#简述-2" class="headerlink" title="简述"></a>简述</h5><p><center><br><img src="/2019/03/27/DeepLearning-papers/Network Dissection.png" width="600"><br></center><br>今年这篇则是通过评估隐藏单元和一系列语义概念的契合度来给出网络的可解释性，提出了一个叫Network Dissection的方法。作者建立了一个带有不同语义概念的图片数据库Broden，里面每张图都有pixel-wise的标定(颜色，纹理，场景，物体部分，物体等)，也就是说对于每种语义概念，都有一张label map。<br>对于一个训练好的网络模型，输入Broden中的所有图片，然后收集某个单元在所有图片上的响应图。为了比较该响应图是对应于哪种语义概念，<code>把这些响应图插值放大到数据库原图大小后，做阈值处理</code>，相应大于某个值就设为1，否则为0，也就是我们只关注响应较大的区域，把这些区域作为该隐藏单元的语义表征，得到一个二值的mask。然后计算该mask和每一个真实语义概念label map的IoU，如果大于一定值，也就是和某个语义概念的重合率比较大，就认为该神经单元是对这个概念的检测器。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch_Run_Notice</title>
    <link href="http://yoursite.com/2019/03/17/PyTorch-Run-Notice/"/>
    <id>http://yoursite.com/2019/03/17/PyTorch-Run-Notice/</id>
    <published>2019-03-16T18:53:23.000Z</published>
    <updated>2019-03-20T15:31:17.013Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h4 id="使用多张GPU"><a href="#使用多张GPU" class="headerlink" title="使用多张GPU"></a>使用多张GPU</h4><p>方式一：（推荐）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">outputs = nn.parallel.data_parallel(model, inputs, device_ids=[<span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>方式二：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opt.device = t.device(<span class="string">'cuda:2,3'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">model=nn.DataParallel(model, device_ids=[<span class="number">2</span>, <span class="number">3</span>]) </span><br><span class="line">outputs = model(inputs)</span><br></pre></td></tr></table></figure><ul><li>可能会报错：<code>RuntimeError: all tensors must be on devices[0]</code></li></ul><h5 id="使用指定的GPU"><a href="#使用指定的GPU" class="headerlink" title="使用指定的GPU"></a>使用指定的GPU</h5><p>直接终端中设定：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=[2,3] python main.py</span><br></pre></td></tr></table></figure></p><p>or python代码中设定：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = [<span class="number">2</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure></p><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li>训练集（Train）与 验证集（Valid）<ul><li>有label，可计算loss、acc  （loss：概率  acc：实际统计）</li><li>对于小数据集，以8：2比例划分  （数据量大时，亦可7：3划分）</li><li>仅使用训练集时，进行数据增强、后向传播<code>backward</code>、优化超参数<code>optimize</code></li></ul></li><li>测试集（Test）<ul><li>无label</li></ul></li></ul><p>注：优化模型时，尝试交叉验证</p><h4 id="Grammar"><a href="#Grammar" class="headerlink" title="Grammar"></a>Grammar</h4><ul><li><code>Tensor.item() → int</code></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>PyTorch学习之路_CSDN</title>
    <link href="http://yoursite.com/2019/03/14/PyTorch%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF_CSDN/"/>
    <id>http://yoursite.com/2019/03/14/PyTorch学习之路_CSDN/</id>
    <published>2019-03-14T09:03:30.000Z</published>
    <updated>2019-03-16T17:30:00.350Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】转载于 <a href="https://blog.csdn.net/u014380165" target="_blank" rel="noopener">AI之路</a></p><a id="more"></a><h2 id="PyTorch学习之路（level1）——训练一个图像分类模型"><a href="#PyTorch学习之路（level1）——训练一个图像分类模型" class="headerlink" title="PyTorch学习之路（level1）——训练一个图像分类模型"></a><a href="https://blog.csdn.net/u014380165/article/details/78525273" target="_blank" rel="noopener">PyTorch学习之路（level1）——训练一个图像分类模型</a></h2><h3 id="数据导入部分"><a href="#数据导入部分" class="headerlink" title="数据导入部分"></a>数据导入部分</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_dir = <span class="string">'/data'</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(</span><br><span class="line">                    os.path.join(data_dir, x),</span><br><span class="line">                    data_transforms[x])， </span><br><span class="line">                    <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br></pre></td></tr></table></figure><h4 id="data-transforms"><a href="#data-transforms" class="headerlink" title="data_transforms"></a>data_transforms</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">'train'</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomSizedCrop(<span class="number">224</span>),  <span class="comment"># 输入对象是PIL Image</span></span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">'val'</span>: transforms.Compose([</span><br><span class="line">        transforms.Scale(<span class="number">256</span>),  <span class="comment"># 目前已经被transforms.Resize类取代</span></span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a>torch.utils.data.DataLoader</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataloders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">4</span>) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br></pre></td></tr></table></figure><p><code>torchvision.datasets.ImageFolder</code>仅返回list，list是不能作为模型输入的，因此在PyTorch中需要用另一个类来封装list，那就是：torch.utils.data.DataLoader。（list → Tensor）</p><h4 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h4><ul><li>这是一个抽象类，在pytorch中所有和数据相关的类（例如，torchvision.datasets.ImageFolder、torch.util.data.DataLoader）都要继承这个类来实现。</li><li>当你的数据不是按照一个类别一个文件夹这种方式存储时，你就要自定义一个类来读取数据，自定义的这个类必须继承自torch.utils.data.Dataset这个基类，最后同样用torch.utils.data.DataLoader封装成Tensor。</li></ul><h5 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h5><p>将Tensor数据类型封装成Variable数据类型，便可以作为模型的输入了 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloders[<span class="string">'train'</span>]:  <span class="comment"># type(dataloaders): Dictionary</span></span><br><span class="line">   inputs, labels = data   <span class="comment"># type(input) / type(labels): Tensor</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> use_gpu:</span><br><span class="line">       inputs = Variable(inputs.cuda())</span><br><span class="line">       labels = Variable(labels.cuda())</span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">       inputs, labels = Variable(inputs), Variable(labels)</span><br></pre></td></tr></table></figure></p><ul><li>Tensor + gradient_Info → Variable</li><li><p>Variable.data → Tensor</p><ul><li>Tensor: torch.tensor</li><li>Variable: torch.autograd.Variable</li></ul></li></ul><h3 id="导入模型"><a href="#导入模型" class="headerlink" title="导入模型"></a>导入模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = models.resnet18(pretrained=<span class="keyword">True</span>)  <span class="comment"># 加载噫预训练的模型参数</span></span><br><span class="line">num_ftrs = model.fc.in_features  <span class="comment"># 获取全连接层的输入channel个数</span></span><br><span class="line">model.fc = nn.Linear(num_ftrs, <span class="number">2</span>) <span class="comment"># 替换最后的全连接层为你所需要的输出</span></span><br></pre></td></tr></table></figure><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>使用交叉熵函数 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure></p><h4 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h4><p>Adam的优化方式 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure></p><h3 id="定义学习率的变化策略"><a href="#定义学习率的变化策略" class="headerlink" title="定义学习率的变化策略"></a>定义学习率的变化策略</h3><p>使用torch.optim.lr_scheduler模块的StepLR类 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># 每隔step_size个epoch就将学习率降为原来的gamma倍</span></span><br><span class="line">scheduler = lr_scheduler.StepLR(optimizer, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在每个epoch开始时都要更新学习率（according to 学习率的变化策略）</span></span><br><span class="line">scheduler.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置模型状态为训练状态</span></span><br><span class="line">model.train(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将网络中的所有梯度置0</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络的前向传播</span></span><br><span class="line">outputs = model(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输出的outputs和原来导入的labels作为loss函数的输入就可以得到损失</span></span><br><span class="line">loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get 模型预测该样本属于哪个类别的信息</span></span><br><span class="line">_, preds = torch.max(outputs.data, <span class="number">1</span>)  <span class="comment"># 第二个参数1是代表dim的意思，也就是取每一行的最大值，其实就是我们常见的取概率最大的那个index</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回传损失</span></span><br><span class="line">loss.backward()  <span class="comment"># 注意: 这是在训练的时候才会有的操作，测试时候只有forward过程</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据回传过程中计算得到的梯度更新参数</span></span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="comment"># 查看各个层的梯度和权值信息</span></span><br><span class="line"><span class="comment"># optimizer.param_groups[0][‘params’]</span></span><br></pre></td></tr></table></figure><p>完整代码：<a href="https://github.com/miraclewkf/ImageClassification-PyTorch/blob/master/level1/train.py" target="_blank" rel="noopener">ImageClassification-PyTorch</a></p><h2 id="PyTorch学习之路（level2）——自定义数据读取"><a href="#PyTorch学习之路（level2）——自定义数据读取" class="headerlink" title="PyTorch学习之路（level2）——自定义数据读取"></a><a href="https://blog.csdn.net/u014380165/article/details/78634829" target="_blank" rel="noopener">PyTorch学习之路（level2）——自定义数据读取</a></h2><h3 id="ImageFolder"><a href="#ImageFolder" class="headerlink" title="ImageFolder"></a>ImageFolder</h3><h4 id="初始化init"><a href="#初始化init" class="headerlink" title="初始化init"></a>初始化<strong>init</strong></h4><ul><li><p><code>__init__</code>方法</p>  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transform=None, target_transform=None,loader=default_loader)</span></span></span><br></pre></td></tr></table></figure></li></ul><ol><li><p>通过find_classes函数得到分类的类别名（classes）和类别名与数字类别的映射关系字典（class_to_idx）</p> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classes, class_to_idx = find_classes(root)</span><br></pre></td></tr></table></figure></li><li><p>通过make_dataset函数得到imags，这个imags是一个列表，其中每个值是一个tuple，每个tuple包含两个元素：图像路径和标签。</p> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">imgs = make_dataset(root, class_to_idx)</span><br><span class="line"><span class="keyword">if</span> len(imgs) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span>(RuntimeError(<span class="string">"Found 0 images in subfolders of: "</span> + root + <span class="string">"\n"</span></span><br><span class="line">                           <span class="string">"Supported image extensions are: "</span> + <span class="string">","</span>.join(IMG_EXTENSIONS)))</span><br></pre></td></tr></table></figure></li><li><p>剩下的就是一些赋值操作了</p> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.root = root</span><br><span class="line">self.imgs = imgs</span><br><span class="line">self.classes = classes</span><br><span class="line">self.class_to_idx = class_to_idx</span><br><span class="line">self.transform = transform</span><br><span class="line">self.target_transform = target_transform</span><br><span class="line">self.loader = loader</span><br></pre></td></tr></table></figure></li></ol><h4 id="获取图像getitem"><a href="#获取图像getitem" class="headerlink" title="获取图像getitem"></a>获取图像<strong>getitem</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">    </span><br><span class="line">    path, target = self.imgs[index]</span><br><span class="line">        img = self.loader(path)   <span class="comment"># 重点  #  self.loader = default_loader 👇 有解析</span></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> img, target</span><br></pre></td></tr></table></figure><h5 id="default-loader函数"><a href="#default-loader函数" class="headerlink" title="default_loader函数"></a>default_loader函数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">default_loader</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> torchvision <span class="keyword">import</span> get_image_backend</span><br><span class="line">    <span class="keyword">if</span> get_image_backend() == <span class="string">'accimage'</span>:</span><br><span class="line">        <span class="keyword">return</span> accimage_loader(path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> pil_loader(path)</span><br></pre></td></tr></table></figure><h6 id="pil-loader方法"><a href="#pil-loader方法" class="headerlink" title="pil_loader方法"></a>pil_loader方法</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pil_loader</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(path, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">with</span> Image.open(f) <span class="keyword">as</span> img:</span><br><span class="line">            <span class="keyword">return</span> img.convert(<span class="string">'RGB'</span>)</span><br></pre></td></tr></table></figure><h6 id="accimage-loader方法"><a href="#accimage-loader方法" class="headerlink" title="accimage_loader方法"></a>accimage_loader方法</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accimage_loader</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> accimage</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> accimage.Image(path)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        <span class="comment"># Potentially a decoding problem, fall back to PIL.Image</span></span><br><span class="line">        <span class="keyword">return</span> pil_loader(path)</span><br></pre></td></tr></table></figure><h4 id="数据集数量len"><a href="#数据集数量len" class="headerlink" title="数据集数量len"></a>数据集数量<strong>len</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure><h3 id="自定义数据读取接口"><a href="#自定义数据读取接口" class="headerlink" title="自定义数据读取接口"></a>自定义数据读取接口</h3><p>思路：</p><ol><li>在PyTorch中和数据读取相关的类基本都要继承一个基类：<code>torch.utils.data.Dataset</code></li><li>改写其中的<code>__init__</code>、<code>__len__</code>、<code>__getitem__</code>等方法即可</li></ol><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img_path: [&apos;train&apos;, &apos;val&apos;]</span><br><span class="line">txt_path: [&apos;train.txt&apos;, &apos;val.txt&apos;]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">customData</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_path, txt_path, dataset = <span class="string">''</span>, data_transforms=None, loader = default_loader)</span>:</span></span><br><span class="line">    <span class="comment"># self.img_name和self.img_label的读取方式就跟你数据的存放方式有关，你可以根据你实际数据的维护方式做调整</span></span><br><span class="line">        <span class="keyword">with</span> open(txt_path) <span class="keyword">as</span> input_file:</span><br><span class="line">            lines = input_file.readlines()</span><br><span class="line">            self.img_name = [os.path.join(img_path, line.strip().split(<span class="string">'\t'</span>)[<span class="number">0</span>]) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">            self.img_label = [int(line.strip().split(<span class="string">'\t'</span>)[<span class="number">-1</span>]) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">            </span><br><span class="line">        self.data_transforms = data_transforms</span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.loader = loader</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.img_name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        img_name = self.img_name[item]</span><br><span class="line">        label = self.img_label[item]</span><br><span class="line">        img = self.loader(img_name)     <span class="comment"># 采用default_loader方法来读取图像</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.data_transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># 在Transform中将每张图像都封装成Tensor</span></span><br><span class="line">                img = self.data_transforms[self.dataset](img)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                print(<span class="string">"Cannot transform image: &#123;&#125;"</span>.format(img_name))</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br></pre></td></tr></table></figure><p>数据读取接口的使用 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># call</span></span><br><span class="line">image_datasets = &#123;x: customData(img_path=<span class="string">'/ImagePath'</span>,</span><br><span class="line">                                    txt_path=(<span class="string">'/TxtFile/'</span> + x + <span class="string">'.txt'</span>),</span><br><span class="line">                                    data_transforms=data_transforms,</span><br><span class="line">                                    dataset=x) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">     </span><br><span class="line"><span class="comment"># DataLoader (list → Tensor)，将这个batch的图像数据和标签都分别封装成Tensor</span></span><br><span class="line">dataloders = &#123;x: torch.utils.data.DataLoader(image_datasets[x],</span><br><span class="line">                                                 batch_size=batch_size,</span><br><span class="line">                                                 shuffle=<span class="keyword">True</span>) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br></pre></td></tr></table></figure></p><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model, <span class="string">'output/resnet_epoch&#123;&#125;.pkl'</span>.format(epoch)) <span class="comment"># 如果这个output文件夹没有，可以手动新建一个或者在代码里面新建</span></span><br></pre></td></tr></table></figure><p>完整代码：<a href="https://github.com/miraclewkf/ImageClassification-PyTorch/blob/master/level2/train_customData.py" target="_blank" rel="noopener">train_customData.py</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】转载于 &lt;a href=&quot;https://blog.csdn.net/u014380165&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AI之路&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>面经总结</title>
    <link href="http://yoursite.com/2019/03/13/%E9%9D%A2%E7%BB%8F%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2019/03/13/面经总结/</id>
    <published>2019-03-13T12:56:45.000Z</published>
    <updated>2019-03-13T12:59:17.618Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><p><a href="https://blog.csdn.net/zongza/article/details/80167654" target="_blank" rel="noopener">【置顶】面试知识点</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>DeepLiver_model_note</title>
    <link href="http://yoursite.com/2019/03/13/DeepLiver-model-note/"/>
    <id>http://yoursite.com/2019/03/13/DeepLiver-model-note/</id>
    <published>2019-03-13T12:47:58.000Z</published>
    <updated>2019-03-20T05:41:04.035Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="Method-Description"><a href="#Method-Description" class="headerlink" title="Method Description"></a>Method Description</h3><h4 id="simpleitk"><a href="#simpleitk" class="headerlink" title="simpleitk"></a>simpleitk</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> SimpleITK <span class="keyword">as</span> sitk</span><br><span class="line"></span><br><span class="line">writer = sitk.ImageFileWriter()</span><br><span class="line">writer.SetFileName(target_file)</span><br><span class="line">writer.Execute(image)</span><br></pre></td></tr></table></figure><p><code>sitk.ReadImage(name)</code> 适用范围：…</p><h4 id="argparse"><a href="#argparse" class="headerlink" title="argparse"></a><a href="http://wiki.jikexueyuan.com/project/explore-python/Standard-Modules/argparse.html" target="_blank" rel="noopener">argparse</a></h4><ol><li>创建 <code>ArgumentParser()</code> 对象</li><li>调用 <code>add_argument()</code> 方法添加参数</li><li>使用 <code>parse_args()</code> 解析添加的参数</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="comment"># 创建 ArgumentParser() 对象</span></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'U-Net 2d'</span>) </span><br><span class="line"><span class="comment"># 调用 add_argument() 方法添加参数</span></span><br><span class="line">parser.add_argument(<span class="string">'--resume'</span>, <span class="string">'-m'</span>, default=<span class="string">''</span>, metavar=<span class="string">'RESUME'</span>,help=<span class="string">'model parameters to load'</span>)   <span class="comment"># 可选参数</span></span><br><span class="line">parser.add_argument(<span class="string">'--save_dir'</span>, default=<span class="string">''</span>, type=str, metavar=<span class="string">'PATH'</span>,help=<span class="string">'path to save checkpoint files'</span>) <span class="comment"># 可选参数</span></span><br><span class="line">parser.add_argument(<span class="string">'--test'</span>, default=<span class="number">0</span>, type=int, metavar=<span class="string">'TEST'</span>,help=<span class="string">'1 do test evaluation, 0 not'</span>) <span class="comment"># 可选参数</span></span><br><span class="line"><span class="comment"># 使用 parse_args() 解析添加的参数</span></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> args.resume</span><br></pre></td></tr></table></figure><ul><li>metavar - 在 usage 说明中的参数名称，对于必选参数默认就是参数名称，对于可选参数默认是全大写的参数名称.</li></ul><h4 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, out, seg)</span>:</span></span><br><span class="line">    b, w, h = seg.shape</span><br><span class="line">    seg = seg.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    seg_one_hot = Variable(torch.FloatTensor(b,<span class="number">2</span>, w, h)).zero_().cuda()</span><br><span class="line">    seg = seg_one_hot.scatter_(<span class="number">1</span>, seg, <span class="number">1</span>)</span><br><span class="line">    loss = Variable(torch.FloatTensor(b)).zero_().cuda()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">2</span>):</span><br><span class="line">        loss += (<span class="number">1</span> - <span class="number">2.</span>*((out[:,i]*seg[:,i]).sum(<span class="number">1</span>).sum(<span class="number">1</span>)) / ((out[:,i]*out[:,i]).sum(<span class="number">1</span>).sum(<span class="number">1</span>)+(seg[:,i]*seg[:,i]).sum(<span class="number">1</span>).sum(<span class="number">1</span>)+<span class="number">1e-15</span>))</span><br><span class="line">    loss = loss.mean()</span><br><span class="line">    <span class="keyword">del</span> seg_one_hot, seg</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval，<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train</span></span><br><span class="line">Model.train(mode=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">Model.eval() <span class="comment"># eval（）时，框架会自动把BN和DropOut固定住，不会取平均，而是用训练好的值，不然的话，一旦test的batch_size过小，很容易就会被BN层导致生成图片颜色失真极大！！！！！！</span></span><br></pre></td></tr></table></figure></p><h3 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h3><p>写nrrd文件的时候，可以考虑nrrd的数组存储形式与正常数组维度不一致<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr = np.squeeze(arr) <span class="comment"># 从数组的形状中删除单维度条目，即把shape中为1的维度去掉</span></span><br><span class="line">y=np.transpose(y,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))  <span class="comment"># 将数组的轴交换 (0, 1, 2) =&gt; (1, 2, 0)</span></span><br></pre></td></tr></table></figure></p><h3 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h3><h4 id="数据处理过程"><a href="#数据处理过程" class="headerlink" title="数据处理过程"></a>数据处理过程</h4><p>取最大连通域 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_connected_domain_3D</span><span class="params">(arr)</span>:</span></span><br><span class="line">    labels = measure.label(arr)  <span class="comment"># &lt;1.2s</span></span><br><span class="line">    t = np.bincount(labels.flatten())[<span class="number">1</span>:]  <span class="comment"># &lt;1.5s</span></span><br><span class="line">    max_pixel = np.argmax(t) + <span class="number">1</span>  <span class="comment"># 位置变了,去除了0</span></span><br><span class="line">    labels[labels != max_pixel] = <span class="number">0</span></span><br><span class="line">    labels[labels == max_pixel] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> labels.astype(np.uint8)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> measure</span><br><span class="line"></span><br><span class="line">arr = np.asarray([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">print(arr)</span><br><span class="line">print(max_connected_domain_3D(arr))</span><br></pre></td></tr></table></figure><p>$$\begin{bmatrix}1 & 1 & 0 & 3\\ 1 & 0 & 3 & 3\\ 0 & 1 & 3 & 3\\0 & 0 & 0 & 0\end{bmatrix}\Rightarrow \begin{bmatrix}0 & 0 & 0 & 1\\ 0 & 0 & 1 & 1\\ 0 & 0 & 1 & 1\\0 & 0 & 0 & 0\end{bmatrix}$$</p><p>归一化 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(img)</span>:</span>  </span><br><span class="line">    img = np.clip(img, <span class="number">-150</span>, <span class="number">250</span>)</span><br><span class="line">    min_nrrd_data = np.min(img)</span><br><span class="line">    max_nrrd_data = np.max(img)</span><br><span class="line">    img = (img - min_nrrd_data) / (max_nrrd_data - min_nrrd_data)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure></p><h4 id="数组阈值处理"><a href="#数组阈值处理" class="headerlink" title="数组阈值处理"></a>数组阈值处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">img 为图像数组，同时也是numpy数组</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">img[np.where(img &lt; min)] = min  </span><br><span class="line">img[np.where(img &gt; <span class="number">250</span>)] = max</span><br></pre></td></tr></table></figure><h4 id="绘制模型"><a href="#绘制模型" class="headerlink" title="绘制模型"></a>绘制模型</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> pip install graphviz</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"></span><br><span class="line">plot_model(model, <span class="string">"RUnet.png"</span>, <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="Common-Operation"><a href="#Common-Operation" class="headerlink" title="Common Operation"></a>Common Operation</h2><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><h4 id="Linux查看Nvidia显卡信息及使用情况"><a href="#Linux查看Nvidia显卡信息及使用情况" class="headerlink" title="Linux查看Nvidia显卡信息及使用情况"></a><a href="https://blog.csdn.net/dcrmg/article/details/78146797" target="_blank" rel="noopener">Linux查看Nvidia显卡信息及使用情况</a></h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> nvidia-smi</span><br></pre></td></tr></table></figure><p><center><br><img src="/2019/03/13/DeepLiver-model-note/nvidia-smi.png" width="600"><br></center><br>表头释义 👇<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- Fan：显示风扇转速，数值在0到100%之间，是计算机的期望转速，如果计算机不是通过风扇冷却或者风扇坏了，显示出来就是N/A； </span><br><span class="line">- Temp：显卡内部的温度，单位是摄氏度；</span><br><span class="line">- Perf：表征性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能；</span><br><span class="line">- Pwr：能耗表示； </span><br><span class="line">- Bus-Id：涉及GPU总线的相关信息； </span><br><span class="line">- Disp.A：是Display Active的意思，表示GPU的显示是否初始化； </span><br><span class="line">- Memory Usage：显存的使用率； </span><br><span class="line">- Volatile GPU-Util：浮动的GPU利用率；</span><br><span class="line">- Compute M：计算模式；</span><br><span class="line"></span><br><span class="line">- Processes显示每块GPU上每个进程所使用的显存情况。</span><br></pre></td></tr></table></figure></p><h4 id="判断torch是否可用GPU"><a href="#判断torch是否可用GPU" class="headerlink" title="判断torch是否可用GPU"></a>判断torch是否可用GPU</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Python <span class="number">3.6</span><span class="number">.8</span> |Anaconda, Inc.| (default, Dec <span class="number">30</span> <span class="number">2018</span>, <span class="number">01</span>:<span class="number">22</span>:<span class="number">34</span>)</span><br><span class="line">[GCC <span class="number">7.3</span><span class="number">.0</span>] on linux</span><br><span class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="keyword">or</span> <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><h4 id="多GPU的使用"><a href="#多GPU的使用" class="headerlink" title="多GPU的使用"></a>多GPU的使用</h4><p>PyTorch支持多GPU训练模型，假设你的网络是model，那么只需要下面一行代码（调用 torch.nn.DataParallel接口）就可以让后续的模型训练在0和1两块GPU上训练，加快训练速度。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = torch.nn.DataParallel(model, device_ids=[<span class="number">0</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure></p><h2 id="优秀链接"><a href="#优秀链接" class="headerlink" title="优秀链接"></a>优秀链接</h2><p><a href="https://zhuanlan.zhihu.com/p/57958993?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=539484443807711232&amp;from=singlemessage&amp;s_r=0" target="_blank" rel="noopener">pytorch + apex 生活变得更美好</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>vtk-Introduction</title>
    <link href="http://yoursite.com/2019/03/08/vtk-Introduction/"/>
    <id>http://yoursite.com/2019/03/08/vtk-Introduction/</id>
    <published>2019-03-08T11:04:27.000Z</published>
    <updated>2019-03-08T16:53:42.927Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】To be continued…</p><a id="more"></a><p>参考：<a href="https://lorensen.github.io/VTKExamples/site/Python/" target="_blank" rel="noopener">VTKExamples</a>、<a href="https://vtk.org/documentation/" target="_blank" rel="noopener">User’s Guide</a>、<a href="https://www.cnblogs.com/zhhfan/p/10312170.html" target="_blank" rel="noopener">Python vtk学习</a></p><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install vtk</span><br></pre></td></tr></table></figure><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><center><br><img src="/2019/03/08/vtk-Introduction/vtk_example.png" width="500"><br></center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> vtk</span><br><span class="line"></span><br><span class="line"><span class="comment"># 箭头源</span></span><br><span class="line">arrow_source = vtk.vtkArrowSource()</span><br><span class="line"><span class="comment"># 映射器</span></span><br><span class="line">mapper = vtk.vtkPolyDataMapper()</span><br><span class="line"><span class="comment"># 映射器添加数据源</span></span><br><span class="line">mapper.SetInputConnection(arrow_source.GetOutputPort())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 演员（执行者）</span></span><br><span class="line">actor = vtk.vtkActor()</span><br><span class="line"><span class="comment"># 演员添加映射器</span></span><br><span class="line">actor.SetMapper(mapper)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 渲染器</span></span><br><span class="line">ren = vtk.vtkRenderer()</span><br><span class="line"><span class="comment"># 渲染器添加演员</span></span><br><span class="line">ren.AddActor(actor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制窗口</span></span><br><span class="line">renWin = vtk.vtkRenderWindow()</span><br><span class="line"><span class="comment"># 绘制窗口添加渲染器</span></span><br><span class="line">renWin.AddRenderer(ren)</span><br><span class="line"><span class="comment"># 窗口读取渲染器生成的图形</span></span><br><span class="line">renWin.Render()   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建窗口交互器</span></span><br><span class="line">iren = vtk.vtkRenderWindowInteractor()</span><br><span class="line">iren.SetRenderWindow(renWin)</span><br><span class="line">iren.Initialize()</span><br><span class="line">iren.Start()</span><br></pre></td></tr></table></figure><h3 id="文件读取"><a href="#文件读取" class="headerlink" title="文件读取"></a>文件读取</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    reader = vtk.vtkSTLReader()  <span class="comment"># 读取STL文件</span></span><br><span class="line">    reader.SetFileName(file_name)</span><br><span class="line">    <span class="keyword">return</span> reader</span><br></pre></td></tr></table></figure><table><thead><tr><th style="text-align:center">文件类型</th><th style="text-align:center">读取方法</th></tr></thead><tbody><tr><td style="text-align:center">STL</td><td style="text-align:center">vtkSTLReader()</td></tr><tr><td style="text-align:center">SLC</td><td style="text-align:center">vtkSLCReader()</td></tr><tr><td style="text-align:center">VTP</td><td style="text-align:center">vtkXMLPolyDataReader()</td></tr><tr><td style="text-align:center">UnstructuredGrid</td><td style="text-align:center">vtkNamedColors()</td></tr><tr><td style="text-align:center">ExodusData</td><td style="text-align:center">vtkExodusIIReader()</td></tr></tbody></table><h3 id="图像旋转"><a href="#图像旋转" class="headerlink" title="图像旋转"></a>图像旋转</h3><p>transform.RotateWXYZ(90, 0, 0, 1) 👇 横置→竖置</p><center><br><img src="/2019/03/08/vtk-Introduction/transform.RotateWXYZ(90, 0, 0, 1).png" width="500"><br></center><p>transform.RotateWXYZ(90, 1, 0, 1) 👇 正方形对角线</p><center><br><img src="/2019/03/08/vtk-Introduction/transform.RotateWXYZ(90, 1, 0, 1).png" width="500"><br></center><p>transform.RotateWXYZ(90, 1, 1, 1) 👇 立方体对角线 </p><center><br><img src="/2019/03/08/vtk-Introduction/transform.RotateWXYZ(90, 1, 1, 1).png" width="500"><br></center><p>代码详情 👇</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> vtk</span><br><span class="line"></span><br><span class="line">arrow_source = vtk.vtkArrowSource()</span><br><span class="line">mapper = vtk.vtkPolyDataMapper()</span><br><span class="line"></span><br><span class="line"><span class="comment"># transform</span></span><br><span class="line">transform = vtk.vtkTransform()</span><br><span class="line"><span class="comment"># transform.RotateWXYZ(angle, x, y, z)  # x,y,z旋转(0,1)表示是否旋转</span></span><br><span class="line">transform.RotateWXYZ(<span class="number">90</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>) <span class="comment"># (90, 1, 0, 1) / (90, 1, 1, 1)  </span></span><br><span class="line">transformFilter = vtk.vtkTransformPolyDataFilter()</span><br><span class="line">transformFilter.SetTransform(transform)</span><br><span class="line">transformFilter.SetInputConnection(arrow_source.GetOutputPort())</span><br><span class="line">transformFilter.Update()</span><br><span class="line">mapper.SetInputConnection(transformFilter.GetOutputPort())</span><br><span class="line"></span><br><span class="line">actor = vtk.vtkActor()</span><br><span class="line">actor.SetMapper(mapper)</span><br><span class="line">ren = vtk.vtkRenderer()</span><br><span class="line">ren.AddActor(actor)</span><br><span class="line">renWin = vtk.vtkRenderWindow()</span><br><span class="line">renWin.AddRenderer(ren)</span><br><span class="line">renWin.Render()</span><br><span class="line">iren = vtk.vtkRenderWindowInteractor()</span><br><span class="line">iren.SetRenderWindow(renWin)</span><br><span class="line">iren.Initialize()</span><br><span class="line">renWin.Render()</span><br><span class="line">iren.Start()</span><br></pre></td></tr></table></figure><ul><li>缩放</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_scale</span><span class="params">(x, y, z)</span>:</span></span><br><span class="line">    actor.SetScale(x, y, z)</span><br></pre></td></tr></table></figure><ul><li>平移</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_position</span><span class="params">(x, y, z)</span>:</span></span><br><span class="line">    actor.AddPosition(x, y, z)</span><br></pre></td></tr></table></figure><h3 id="平面切割"><a href="#平面切割" class="headerlink" title="平面切割"></a>平面切割</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    reader = read_data(file_name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义切割平面</span></span><br><span class="line">    clipPlane = vtk.vtkPlane()</span><br><span class="line">    clipPlane.SetNormal(<span class="number">1.0</span>, <span class="number">-1.0</span>, <span class="number">-1.0</span>)</span><br><span class="line">    clipPlane.SetOrigin(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 平面切割三维数据</span></span><br><span class="line">    clipper = vtk.vtkClipPolyData()</span><br><span class="line">    clipper.SetInputConnection(reader.GetOutputPort())</span><br><span class="line">    clipper.SetClipFunction(clipPlane)</span><br><span class="line">    clipper.InsideOutOn()  <span class="comment"># ?</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义mapper和actor</span></span><br><span class="line">    superMapper = vtk.vtkPolyDataMapper()</span><br><span class="line">    superMapper.SetInputConnection(clipper.GetOutputPort())</span><br><span class="line">    superActor = vtk.vtkActor()</span><br><span class="line">    <span class="comment"># 设置偏转角度</span></span><br><span class="line">    set_origin(superActor, <span class="number">-50</span>, <span class="number">-75</span>, <span class="number">120</span>)</span><br><span class="line">    superActor.SetMapper(superMapper)</span><br><span class="line">    superActor.GetProperty().SetColor(colors.GetColor3d(<span class="string">"Cyan"</span>))</span><br><span class="line">    only_show(superActor)</span><br></pre></td></tr></table></figure><ul><li>设置演员初始方向</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_origin</span><span class="params">(actor, x, y, z)</span>:</span></span><br><span class="line">    actor.SetOrientation(x, y, z)</span><br></pre></td></tr></table></figure><h3 id="鼠标事件监听"><a href="#鼠标事件监听" class="headerlink" title="鼠标事件监听"></a>鼠标事件监听</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 监听事件</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyEvent</span><span class="params">(vtk.vtkInteractorStyleTrackballCamera)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, parent=None)</span>:</span></span><br><span class="line">        self.AddObserver(<span class="string">"MiddleButtonPressEvent"</span>, self.middle_button_press_event)</span><br><span class="line">        self.AddObserver(<span class="string">"MiddleButtonReleaseEvent"</span>, self.middle_button_release_event)</span><br><span class="line">        self.AddObserver(<span class="string">"LeftButtonPressEvent"</span>, self.left_button_press_event)</span><br><span class="line">        self.AddObserver(<span class="string">"LeftButtonReleaseEvent"</span>, self.left_button_release_event)</span><br><span class="line">        self.AddObserver(<span class="string">"RightButtonPressEvent"</span>, self.right_button_press_event)</span><br><span class="line">        self.AddObserver(<span class="string">"RightButtonReleaseEvent"</span>, self.right_button_release_event)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle_button_press_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"Middle Button pressed"</span>)</span><br><span class="line">        self.OnMiddleButtonDown()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle_button_release_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"Middle Button released"</span>)</span><br><span class="line">        self.OnMiddleButtonUp()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">left_button_press_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"Left Button pressed"</span>)</span><br><span class="line">        self.OnLeftButtonDown()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">left_button_release_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"Left Button released"</span>)</span><br><span class="line">        self.OnLeftButtonUp()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">right_button_press_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"right Button pressed"</span>)</span><br><span class="line">        self.OnRightButtonDown()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">right_button_release_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"right Button released"</span>)</span><br><span class="line">        self.OnLeftButtonUp()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 引入上一段代码调用</span></span><br><span class="line">iren.SetInteractorStyle(MyEvent())</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】To be continued…&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="vtk" scheme="http://yoursite.com/tags/vtk/"/>
    
  </entry>
  
  <entry>
    <title>PyQt5_2_exe/app</title>
    <link href="http://yoursite.com/2019/03/04/PyQt5-2-exe/"/>
    <id>http://yoursite.com/2019/03/04/PyQt5-2-exe/</id>
    <published>2019-03-03T17:00:09.000Z</published>
    <updated>2019-03-04T07:33:36.281Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】Not Completed</p><a id="more"></a><h2 id="PyQt5打包程序为可执行文件"><a href="#PyQt5打包程序为可执行文件" class="headerlink" title="PyQt5打包程序为可执行文件"></a>PyQt5打包程序为可执行文件</h2><ol><li><p><code>$ pyinstaller -Fw window.py</code></p><ul><li><p>Requirements：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install -U pip setuptools / pip install --upgrade setuptools</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install tornado</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install IPython</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install ipykernel</span></span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash"> No module named <span class="string">'wx'</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install -U  -f https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-16.04 \ wxPython </span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cairo backend requires that cairocffi or pycairo is installed</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install cairocffi</span></span><br></pre></td></tr></table></figure></li><li><p>Some INFO</p><pre><code class="powershell"><span class="number">117</span> INFO: UPX is not available.<span class="number">38433</span> INFO:   Matplotlib backend <span class="string">"MacOSX"</span>: ignoredPython is not installed as a framework. The Mac OS X backend will not be able to <span class="keyword">function</span> correctly <span class="keyword">if</span> Python is not installed as a framework. See the Python documentation <span class="keyword">for</span> more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or <span class="keyword">try</span> one of the other backends. <span class="keyword">If</span> you are using (Ana)Conda please install python.app and replace the use of <span class="string">'python'</span> with <span class="string">'pythonw'</span>. See <span class="string">'Working with Matplotlib on OSX'</span> <span class="keyword">in</span> the Matplotlib FAQ <span class="keyword">for</span> more information.<span class="number">51301</span> WARNING: library user32 required via ctypes not found<span class="number">53437</span> INFO: Warnings written to /Users/Captain/Desktop/client/build/window/warn-window.txt</code></pre></li><li>Output：<code>./build</code>、 <code>./dist</code>、 <code>./window.spec</code></li></ul></li><li><code>$ pyinstaller window.spec</code><ul><li>Output：<code>window.app</code> / <code>window.exe</code>    </li></ul></li><li>将与程序关联的代码（当前文件夹中除./build和./dist外的所有文件）均copy至<code>./dist</code>，即可</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】Not Completed&lt;/p&gt;
    
    </summary>
    
    
      <category term="PyQt5" scheme="http://yoursite.com/tags/PyQt5/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch_可视化之TensorboardX</title>
    <link href="http://yoursite.com/2019/03/03/PyTorch-%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BTensorboardX/"/>
    <id>http://yoursite.com/2019/03/03/PyTorch-可视化之TensorboardX/</id>
    <published>2019-03-03T04:33:11.000Z</published>
    <updated>2019-03-03T06:36:49.416Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h2><p>链接：<a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-1-tensorboard1/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-1-tensorboard1/</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(tensorflow) ➜  Morvan_Tensorflow tensorboard --logdir logs</span><br><span class="line">TensorBoard 1.11.0 at http://MacBook-Pro:6006 (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure><ul><li>Chrome</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">http://0.0.0.0:6006</span><br></pre></td></tr></table></figure><h2 id="TensorboardX"><a href="#TensorboardX" class="headerlink" title="TensorboardX"></a>TensorboardX</h2><p>详细内容，访问文档：<a href="https://tensorboardx.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">https://tensorboardx.readthedocs.io/en/latest/index.html</a><br>源代码：<a href="https://github.com/lanpa/tensorboardX" target="_blank" rel="noopener">https://github.com/lanpa/tensorboardX</a><br>参考：<a href="https://www.pytorchtutorial.com/pytorch-tensorboardx/" target="_blank" rel="noopener">https://www.pytorchtutorial.com/pytorch-tensorboardx/</a></p><h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install tensorboardX</span><br></pre></td></tr></table></figure><h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#-*-coding:utf-8-*-</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_data = Variable(torch.rand(<span class="number">16</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 从torchvision中导入已有模型</span></span><br><span class="line">model = torchvision.models.resnet18()</span><br><span class="line"><span class="comment"># print(model)</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 声明writer对象，保存的文件夹，异己名称</span></span><br><span class="line">writer = SummaryWriter(log_dir=\<span class="string">'./log\', comment=\'resnet18\')</span></span><br><span class="line"><span class="string">with writer:   # necessary</span></span><br><span class="line"><span class="string">    writer.add_graph(model, (input_data,))</span></span><br></pre></td></tr></table></figure><h3 id="View"><a href="#View" class="headerlink" title="View"></a>View</h3><p>在对应路径下运行tensorboard<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(pytorch) ➜  Desktop  tensorboard --logdir log</span><br><span class="line">TensorBoard 1.12.0 at http://MacBook-Pro:6006 (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch-可视化工具-Visdom-介绍"><a href="#PyTorch-可视化工具-Visdom-介绍" class="headerlink" title="PyTorch 可视化工具 Visdom 介绍"></a>PyTorch 可视化工具 Visdom 介绍</h2><p>可参考 <a href="https://captainzj.github.io/2018/12/31/visdom-Tutorial/" target="_blank" rel="noopener">https://captainzj.github.io/2018/12/31/visdom-Tutorial/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>NiftyNet-Process</title>
    <link href="http://yoursite.com/2019/03/02/NiftyNet-Process/"/>
    <id>http://yoursite.com/2019/03/02/NiftyNet-Process/</id>
    <published>2019-03-02T08:04:58.000Z</published>
    <updated>2019-07-02T06:24:28.049Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】<br><a id="more"></a></p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a><a href="https://www.cnblogs.com/zhhfan/p/10424489.html" target="_blank" rel="noopener">数据预处理</a></h2><h3 id="生成-csv文件"><a href="#生成-csv文件" class="headerlink" title="生成.csv文件"></a>生成.csv文件</h3><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">image</th><th style="text-align:center">path</th></tr></thead><tbody><tr><td style="text-align:center">img.csv</td><td style="text-align:center">img_name</td><td style="text-align:center">img_path</td></tr><tr><td style="text-align:center">label.csv</td><td style="text-align:center">img_label</td><td style="text-align:center">img_path</td></tr></tbody></table><ul><li>二分类的生成该文件的demo</li></ul><blockquote><p>准备工作：将两个类别的图片分别存储在两个文件夹中。</p></blockquote><p>下述代码中将分类的图片分别存储于<code>./DogsVSCats/train/cat</code> 和 <code>./DogsVSCats/train/dog</code><br>注: DogsVSCats datasets 可在<a href="https://www.kaggle.com/c/dogs-vs-cats/data" target="_blank" rel="noopener">此处</a>下载</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">dir_path = <span class="string">'./DogsVSCats/train'</span></span><br><span class="line">dir_names = os.listdir(dir_path)   <span class="comment"># ['cat', 'dog']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 img.csv</span></span><br><span class="line">list_img, list_path = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> dir_name <span class="keyword">in</span> dir_names:</span><br><span class="line"></span><br><span class="line">    img_path = dir_path + <span class="string">"/"</span> + dir_name</span><br><span class="line">    img_name = os.listdir(img_path)  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, item <span class="keyword">in</span> enumerate(img_name):</span><br><span class="line">        list_img.append(item)</span><br><span class="line">        list_path.append(img_path + <span class="string">"/"</span> + item)</span><br><span class="line">        </span><br><span class="line">data_frame = pd.DataFrame(&#123;<span class="string">'image'</span>: list_img, <span class="string">'path'</span>: list_path&#125;)</span><br><span class="line">data_frame.to_csv(<span class="string">'./img_path.csv'</span>, index=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成label.csv</span></span><br><span class="line">list_label_name, list_label_path = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> dir_name <span class="keyword">in</span> dir_names:</span><br><span class="line"></span><br><span class="line">    label_path = dir_path + <span class="string">"/"</span> + dir_name</span><br><span class="line">    label_name = os.listdir(label_path)  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, elem <span class="keyword">in</span> enumerate(label_name):</span><br><span class="line">        list_label_name.append(elem[<span class="number">0</span>:<span class="number">3</span>])  <span class="comment"># elem[0:3] : cat / dog</span></span><br><span class="line">        list_label_path.append(label_path + <span class="string">"/"</span> + elem)</span><br><span class="line"></span><br><span class="line">label_dataframe = pd.DataFrame(&#123;<span class="string">'label'</span>: list_label_name, <span class="string">'path'</span>: list_label_path&#125;)</span><br><span class="line">label_dataframe.to_csv(<span class="string">'./label.csv'</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Medical Imaging" scheme="http://yoursite.com/categories/Medical-Imaging/"/>
    
    
      <category term="NiftyNet" scheme="http://yoursite.com/tags/NiftyNet/"/>
    
  </entry>
  
</feed>
