<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Go Further</title>
  
  <subtitle>Stay Hungry, Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-12T10:15:33.953Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>CaptainSE</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>AD_Papers</title>
    <link href="http://yoursite.com/2019/07/10/AD-Papers/"/>
    <id>http://yoursite.com/2019/07/10/AD-Papers/</id>
    <published>2019-07-10T10:37:04.000Z</published>
    <updated>2019-07-12T10:15:33.953Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h4 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h4><ul><li>氟脱氧葡萄糖正电子发射断层扫描（FDG-PET），提供了大脑代谢活动的定量测量，可以在结构变化发生之前识别与AD相关的变化，具有可接受的敏感性和准确性。 </li><li>ASL(Arterial Spin Labeling)：动脉自旋标记，三维伪连续ASL扫描的自动分类可以高精度地检测AD患者（&gt; 82％）</li></ul><table><thead><tr><th style="text-align:center">No</th><th style="text-align:center">Data Source</th><th style="text-align:center">Data Type</th><th style="text-align:center">Method</th><th style="text-align:center">Experiment</th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://arxiv.org/abs/1710.04782" target="_blank" rel="noopener">1</a></td><td style="text-align:center">ADNI</td><td style="text-align:center">PET</td><td style="text-align:center">M-CNN</td><td style="text-align:center">AD vs NC(93.58),<br>sMCI vs pMCI(81.55),<br>tf sMCI vs pMCI(82.51)</td></tr><tr><td style="text-align:center"><a href="https://pubs.rsna.org/doi/full/10.1148/radiol.2016152703" target="_blank" rel="noopener">2</a></td><td style="text-align:center">Alzheimer Center of the VU University <br>Medical Center Dementia Cohort</td><td style="text-align:center">MRI</td><td style="text-align:center">ASL-W_score-SVM</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><a href="https://arxiv.org/abs/1808.06452" target="_blank" rel="noopener">3</a></td><td style="text-align:center">ADNI, AIBL,OASIS</td><td style="text-align:center">MRI,PET</td><td style="text-align:center"></td></tr></tbody></table><h4 id="1-Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease"><a href="#1-Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease" class="headerlink" title="1. Multiscale deep neural network based analysis of FDG-PET images for the early diagnosis of Alzheimer’s disease"></a>1. <a href="https://arxiv.org/abs/1710.04782" target="_blank" rel="noopener">Multiscale deep neural network based analysis of FDG-PET images for the early diagnosis of Alzheimer’s disease</a></h4><p>提出了一种新的深度学习框架，使用FDG-PET代谢成像来识别MCI的受试者（患有前AD症状的），并将其与其他MCI（非AD /非进展）受试者区分开来。 我们的多尺度深度神经网络仅使用来自单一模态（FDG-PET代谢数据）的测量获得82.51％的分类准确度，优于最近文献中公布的其他可比较的FDG-PET分类器。</p><h5 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h5><ul><li><p>来源：ADNI（<a href="http://adni.loni.usc.edu）" target="_blank" rel="noopener">http://adni.loni.usc.edu）</a></p></li><li><p>该论文对迄今为止（21 February 2018）为该项工作发布的最大数据集（1051名受试者的代谢情况）提出了最全面的方法验证，图像严格遵循标准化的成像协议</p><ul><li><p>受试者的人口统计学和临床信息：NC组 304项、sMCI组 409项、pMCI组 112项、AD组 226项</p><p><img src="/Users/Captain/Downloads/Captainzj.github.io/source/_posts/AD-Papers/1-Table-1.png" alt="受试者的人口统计学和临床信息"></p></li><li><p>质量控制：1）通过训练有素且专业的神经病理学家对每个脑图像的每个FreeSurfer分段进行手动质量评估。此外，通过手动编辑校正脑膜，白质，皮质或皮质下分割中的任何错误，并重新运行Freesurfer，直到T1 MR图像分割变得准确。2）可视化代谢度量</p></li></ul></li></ul><h5 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h5><ul><li>提出了一种新的多尺度深度神经网络框架，以学习基于AD病理学的代谢变化模式，作为正常对照（NC）代谢模式的判别; （该论文是第一个利用深度学习开发多尺度FDG-PET分类器的论文）</li><li>发现通过从NC和AD个体转移样本，深层结构可以在早期诊断任务中获得更好的判别能力</li><li>证明了具有不同验证设置的多分类器”投票“预测，可以使所提出的方法更加稳定和稳健，并提高其分类性能</li></ul><h5 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h5><ul><li><p><em>Image processing</em></p><ul><li><em>ROI segmentation</em>：Each T1 structural MRI image was segmented into gray matter and white matter using the freely available <code>FreeSurfer 5.3 package</code> with default parameter settings</li><li><em>Patch parcellation</em>： The technique used for this subdivision is a previously published technique where each ROI can be clustered using their spatial coordinates via a <code>k-means clustering algorithm</code></li><li><em>Coregistration</em>：The <code>FDG-PET</code> image and <code>skull-stripped MRI</code> scan of each target were then co-registered using the<code>FSL-FLIRT</code> program</li><li><em>Normalization</em>：The mean intensity in the <code>brainstem region</code> is hence calculated and used to divide the metabolism measures for all the other ROIs in the brain for each subject.</li><li><em>Visualization of metabolism measures</em>：perform visual <code>quality control</code> of the measures across the database</li></ul></li><li><p><em>Multiscale Deep Neural Network</em></p><ul><li>Unsupervised pre-training.</li><li>Supervised fine-tuning. </li><li>Dropout strategy.</li><li>Early stopping strategy.</li></ul></li><li><p><em>Instance-transfer learning</em>：the networks trained with transferred instances displayed better discriminative ability</p></li><li><p><em>Ensemble classifiers</em>：词袋法</p></li><li><p><em>Experimental setup</em>：Three binary classification experiments, (i) <code>NC vs AD</code>, (ii) <code>sMCI vs pMCI</code> and (iii) sMCI vs pMCI with transfer learning from NC and AD</p></li><li><p><strong>Results</strong></p><ul><li><p><em>Multiscale classification</em>：二分类实验中，粗精度的预测准确率优于细精度</p></li><li><p><em>Ensemble classifier design</em>：稳健</p></li></ul></li><li><p><strong>Discussion</strong></p><ul><li><em>Comparison with state-of-the-art methods</em>：我们提出的基于深度学习的方法显示sMCI和pMCI之间的分类准确性更高，无论是使用单一模式还是多模式研究</li><li><em>Multiscale classification</em>：与单一尺度特征相比，使用所有多尺度特征（包括从每个单尺度特征获得的判别信息）的组合分类性能产生了更高的精度结果。这表明在连锁多尺度特征上训练的网络（图2）仍然能够学习本文中使用的从小到大的补丁大小的隐藏模式。</li><li><em>Ensemble classifier</em>：集合分类器的准确性高于单个分类器的平均值，这表明具有不同训练和验证集的分集的集合分类器可以产生更稳健和稳定的分类器，因此可以改善分类性能更好的普遍性。</li></ul></li><li><p><strong>Conclusion</strong></p></li></ul><p>在本文中，我们使用<code>多尺度贴片FDG-PET深度学习</code>功能提出了一种新的AD早期诊断框架。所提出的框架利用<code>转移学习方法</code>和<code>集合分类器策略</code>来改善深度神经网络在区分sMCI和pMCI主体的任务中的性能。在1051名受试者的FDG-PET图像的大型数据库上进行的实验提供了支持三种断言的证据。 （1）所提出的方法，使用仅来自<code>单一FDG-PET模态</code>的特征，能够胜过在sMCI和pMCI分类任务中采用多模态特征的现有方法。 （2）所提出的网络可以从<code>多尺度特征</code>中学习判别模式，以提供具有更好判别性能的更健壮的分类器。 （3）使用不同验证集的<code>多个集成分类器</code>可以使网络更加健壮和稳定，并在统计上提高其分类性能。<br>对于未来的工作，将所提出的框架扩展到包含来自多种模态的信息是自然的，假设所得到的深度神经网络将从多个模态数据中学习更多信息，从而进一步改进所获得的分类准确度。尽管sMCI vs pMCI实验常用于验证近期研究（包括我们的研究）中方法的鉴别能力，但我们只能知道那些sMCI受试者在研究进展中时保持稳定并且可以转化为AD或其他神经退行性疾病。因此，在将来临床诊断的sMCI受试者的基本事实可能不完全准确，因此可能在分类中引入噪声/偏倚。幸运的是，随着更多数据的收集，分类系统将更好地捕获这些和其他噪声和可变性来源，我们提出的深度学习集合分类器可能非常适合这种情况。</p><h4 id="2-Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer’s-Disease"><a href="#2-Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer’s-Disease" class="headerlink" title="2. Application of Machine Learning to Arterial Spin Labeling in Mild Cognitive Impairment and Alzheimer’s Disease"></a>2. <a href="https://pubs.rsna.org/doi/full/10.1148/radiol.2016152703" target="_blank" rel="noopener">Application of Machine Learning to Arterial Spin Labeling in Mild Cognitive Impairment and Alzheimer’s Disease</a></h4><p>关于动脉自旋标记（ASL）的机器学习在治疗轻度认知障碍和阿尔茨海默病中的应用</p><p>本研究调查了ASL灌注图的多变量模式识别分析（SVM）是否可用于阿尔茨海默病（AD），轻度认知障碍（MCI）和主观认知衰退（SCD）患者的分类和单一主题预测。 W-score方法可以消除性别和年龄的混杂影响。</p><h5 id="细节-1"><a href="#细节-1" class="headerlink" title="细节"></a>细节</h5><p>ABSTRACT</p><ul><li>Purpose：<ul><li><code>ASL</code> → discrimination AD\MCI\SCD</li><li><code>W-score</code> → remove confounding effects of gender and age</li></ul></li><li>Materials and Methods<ul><li>Training of a Support Vector Machine (<code>SVM</code>) classifier used diagnostic status and perfusion maps.</li></ul></li><li>Results</li><li>Conclusion</li></ul><ol><li><p>INTRODUCTION</p></li><li><p>MATERIALS AND METHODS</p><ol><li><em>Participants</em></li><li><em>Data Acquisition</em></li><li><em>Preprocessing of MR imaging data</em></li><li><em>W-score maps</em>：<code>[(measured perfusion) – (predicted perfusion)] / (standard deviation of residuals)</code>.</li><li><em>SVM: Multivariate Pattern Recognition in Training-Set</em>：SVM接受了 leave-one-out交叉验证框架的训练，以区分AD患者，MCI患者和SCD患者。我们通过比较<code>两个患者组（SCD）</code>的W-得分图来评估分类器的诊断值。通过区分<code>AD和MCI</code>的患者的W-得分图来评估分类器对疾病进展的敏感性。最后，使用<code>MCIc和MCI</code>患者的Wscore图进行探索性分类训练，以研究分类器是否显示预后价值。</li><li><em>SVM: Prediction in new Subjects</em>:<code>Discrimination maps</code></li><li><em>Statistical Analysis</em>: <code>Evaluation</code></li></ol></li><li>RESULTS<ol><li><em>Participant Characteristics</em>:  数据集中受试者MMSE评分差异小</li><li><em>Training of the classifiers</em>：i. AD vs. SCD  ii. AD vs. MCI  iii. MCI vs. SCD </li><li><em>Predictions: assessment of generalisability</em>: <code>use of discrimination weights</code> i. AD vs. SCD  ii. AD vs. MCI  iii. MCI vs. SCD </li><li><em>Exploratory analyses: classifying MCI subgroups</em>: i. MCIc vs. SCD ii. MCIc vs. MCIs; use of the <code>AD vs. SCD training discrimination weights</code> for MCIc vs. SCD;The use of the same discrimination weights in MCIc vs. MCIs </li></ol></li><li>DISCUSSION</li><li>CONCLUSION：Using <code>automated methods</code>（即SVM）, age- and gender adjusted（W-score校准） ASL perfusion maps（特殊模态） can be used to classify and predict diagnoses of AD, MCI-converters, stable MCI patients and SCD subjects with good accuracy and AUC values.</li></ol><h4 id="3-Reproducible-evaluation-of-classification-methods-in-Alzheimer’s-disease"><a href="#3-Reproducible-evaluation-of-classification-methods-in-Alzheimer’s-disease" class="headerlink" title="3. [Reproducible evaluation of classification methods in Alzheimer’s disease:"></a>3. [Reproducible evaluation of classification methods in Alzheimer’s disease:</h4><p>Framework and application to MRI and PET data](<a href="https://arxiv.org/abs/1808.06452" target="_blank" rel="noopener">https://arxiv.org/abs/1808.06452</a>)</p><p>阿尔茨海默病分类方法的<code>再现评估</code>：MRI和PET数据的框架和应用</p><p>在本文中，我们使用三个公开可用的数据集（ADNI，AIBL和OASIS）提出了AD中<code>可再现</code>和客观分类实验的框架。该框架包括：i）将三个数据集自动转换为标准格式（BIDS）; ii）模块化的预处理流水线，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准。</p><p>对于所有分类任务，FDG PET优于T1 MRI。</p><p>All the code of the framework and the experiments is publicly available: general- purpose tools have been integrated into the <a href="www.clinica.run">Clinica software</a> and the paper-specific code is available at: <a href="https://gitlab.icm-institute.org/aramislab/AD-ML" target="_blank" rel="noopener">https://gitlab.icm-institute.org/aramislab/AD-ML</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
      <category term="ADNI" scheme="http://yoursite.com/tags/ADNI/"/>
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Network in Medical Imaging A Review</title>
    <link href="http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/"/>
    <id>http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/</id>
    <published>2019-07-02T06:18:42.000Z</published>
    <updated>2019-07-02T12:08:24.231Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p>论文地址：<a href="https://arxiv.org/abs/1809.07294" target="_blank" rel="noopener">Generative Adversarial Network in Medical Imaging: A Review</a></p><p>github Reference link：<a href="https://github.com/xinario/awesome-gan-for-medical-imaging" target="_blank" rel="noopener">Awesome GAN for Medical Imaging</a></p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><em>摘要</em></h3><p>生成对抗网络由于其数据生成能力而在没有明确建模概率密度函数的情况下在计算机视觉社区中获得了很多关注。 鉴别器带来的对抗性损失提供了一种巧妙的方法，可以将未标记的样本纳入训练并实现更高的顺序一致性。 事实证明，这在许多情况下是有用的，例如域适应，数据增强和图像到图像转换。 这些属性吸引了医学成像领域的研究人员，我们已经看到许多传统和新颖应用的快速采用，如图像重建，分割，检测，分类和跨模态合成。 根据我们的观察，这一趋势将继续下去，因此我们利用对抗性训练计划对医学成像的最新进展进行了回顾，希望能够使对该技术感兴趣的研究人员受益。</p><p>关键词：Deeplearning，Generative adversarial network，Generative model，Medical imaging，Review</p><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a><em>1. 介绍</em></h3><p>随着2012年开始的计算机视觉深度学习的复兴（Krizhevsky等，2012），医学成像中深度学习方法的采用大幅增加。据估计，2016年和2017年在主要医学影像相关会议场所和期刊上发表了400多篇论文（Litjens等，2017）。在医学成像领域广泛采用深度学习是因为它具有补充图像解释和增强图像表示和分类的潜力。在本文中，我们将重点放在深度学习领域最有趣的近期突破之一 - 生成对抗网络（GAN） - 以及它们在医学成像领域的潜在应用。<br>GAN是一种特殊类型的神经网络模型，其中两个网络同时被训练，一个侧重于图像生成，另一个侧重于区分。对抗性训练方案因其在抵制领域转移方面的有用性以及产生新图像样本的有效性而在学术界和工业界引起了关注。该模型在许多图像生成任务中实现了最先进的性能，包括文本到图像合成（Xu et al.，2017），超分辨率（Ledig等，2017）和图像 - 图像转换（Zhu et al.，2017a）。<br>与源于20世纪80年代的深度学习不同（Fukushima和Miyake，1982），对抗性的概念相对来说是非常重要的进步（Good-fellow et al.，2014）。本文概述了GAN，描述了它们在医学成像中的有前途的应用，并确定了一些需要解决的挑战，以使它们能够成功应用于其他医学成像相关任务。<br>为了全面概述医学影像中GAN的所有相关工作，我们搜索了包括PubMed，arXiv在内的数据库，国际医学图像计算和计算机辅助干预会议（MICCAI），SPIE医学影像，IEEE国际研讨会生物医学成像（ISBI）和国际深度学习医学影像学会议（MIDL）。我们还合并了上述搜索过程中未识别的交叉引用作品。由于每月都有研究出版物出现，而且没有失去一般性，我们将搜索的截止时间设定为2018年7月30日。仅报告初步结果的arXiv的工作被排除在本次审查之外。基于任务，成像模态和年份的这些论文的描述性统计数据可以在图1中找到。<br>在本文的其余结构如下。我们首先简要介绍第2节中GAN的原理及其一些结构变体。然后在第3节中使用GAN对医学图像分析任务进行全面审查，包括但不限于放射学领域，组织病理学和皮肤病学。我们根据规范任务对所有作品进行分类：重建，图像合成，分割，分类，检测，注册等。第4节总结了该评论，并讨论了前瞻性应用和识别性挑战。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 1" title="1.png">                </div>                <div class="image-caption">1.png</div>            </figure><p>图1：（a）根据规范任务对GAN相关论文进行分类。 （b）根据成像模式对GAN相关论文进行分类。 （c）2014年发布的GAN相关论文数量。请注意，一些工作执行了各种任务，并对具有不同模态的数据集进行了评估。 我们在绘制这些图时多次计算这些作品。 基于源域计算与跨域图像传输相关的工作。 图（a）和（b）中的统计数据基于2018年7月30日或之前公布的论文。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 2" title="2.png">                </div>                <div class="image-caption">2.png</div>            </figure><p>图2：用于在CT图像上合成肺结节的vanilla GAN的示意图。 上图显示了网络配置。 下面的部分显示了生成器G和鉴别器D的输入，输出和内部特征表示.G将样本$z$从$p(z)$变换为生成的结节$x_g$。 D是二元分类器，其分别区分由$x_g$和$x_r$形成的肺结节的生成和真实图像。</p><h3 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a><em>2. 背景</em></h3><h4 id="2-1-Vanilla-GAN"><a href="#2-1-Vanilla-GAN" class="headerlink" title="2.1. Vanilla GAN"></a><em>2.1. Vanilla GAN</em></h4><p>香草GAN（Goodfellow等，2014）是一种生成模型，设计用于直接从所需的数据分布中抽取样本，而无需明确地模拟潜在的概率密度函数。它由两个神经网络组成：发生器G和鉴别器D.G，z的输入是从先前分布p（z）中采样的纯随机噪声，通常选择为高斯分布或均匀分布。简单。预计G，xg的输出与从真实数据分布pr（x）中提取的实际样本xr具有视觉相似性。我们将由θg参数化的G学习的非线性映射函数表示为xg = G（z;θg）。 D的输入是实际或生成的样本。 D，y1的输出是单个值，表示输入是真实或假冒样本的概率。由θd参数化的D学习的映射表示为y1 = D（x;θd）。生成的样本形成分布pg（x），其在成功训练后需要是pr（x）的近似值。图2的顶部显示了香草GAN配置的图示。在该示例中，G生成描绘肺结节的2D CT切片。<br>D的目标是区分这两组图像，而生成器G被训练以尽可能地混淆辨别器D.直观地说，G可以被视为试图生产一些优质假冒伪劣材料的伪造者，D可以被视为试图检测伪造物品的警察。在另一种观点中，我们可以将G视为从D接收奖励信号，这取决于生成的数据是否准确。梯度信息从D传播回G，因此G调整其参数以产生可以欺骗D的输出图像.D和G的训练目标可以用数学表达为：<br>$$ L_{D}^{GAN} = max_{D}E_{{x_r}\sim {P_r(x)}}[logD(x_r)+E_{x_g\sim p_g(x)}[log(1-D(x_g))]],\\L_{D}^{GAN} = min_GE_{x_g\sim p_g(x)}[log(1-D(x_g))].$$ <br>可以看出，D只是具有最大对数似然目标的二元分类器。 如果鉴别器D在下一个发生器G更新之前被训练为最优，则最小化LGAN被证明等同于最小化pr（x）和pg（x）之间的Jensen-Shannon（JS）偏差（Goodfellow等人，2014））。 训练后的预期结果是xg形成的样本应该接近实际数据分布pr（x）。</p><h4 id="2-2-Variants-of-GANs"><a href="#2-2-Variants-of-GANs" class="headerlink" title="2.2. Variants of GANs"></a><em>2.2. Variants of GANs</em></h4><p>上述GAN训练目标被认为是鞍点优化问题（Yadav等，2018），训练通常通过基于梯度的方法完成。 G和D从头开始交替训练，以便它们可以一起进化。但是，G和D训练与JS分歧之间无法保证平衡。因此，一个网络可能不可避免地比另一个网络更强大，在大多数情况下是D.当D变得太强而不是G时，生成的样本变得太容易与实际的分离，从而达到D的梯度逼近零的阶段，没有为G的进一步训练提供指导。由于难以产生有意义的高频细节，因此在生成高分辨率图像时更频繁地发生这种情况。<br>在训练GAN中通常面临的另一个问题是模式崩溃，正如名称所示，这是由G学习的分布pg（x）关注数据分布pr（x）的一些有限模式的情况。因此，它不是产生不同的图像，而是产生一组有限的样本。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 3" title="3.png">                </div>                <div class="image-caption">3.png</div>            </figure><p>图3：GAN变体的示意图。 c表示条件向量。 在CGAN和ACGAN中，c是对类标签进行编码的离散分类代码（例如，一个热向量），在InfoGAN中，它也可以是对属性进行编码的连续代码。 xg通常是指生成的图像，但也可以是SGAN中的内部表示。</p><h5 id="2-2-1-Varying-objective-of-D"><a href="#2-2-1-Varying-objective-of-D" class="headerlink" title="2.2.1. Varying objective of D*"></a>2.2.1. Varying objective of D*</h5><p>为了稳定训练并避免模式崩溃，已经提出了D的不同损失，例如f-发散（f-GAN）（Nowozin等，2016），最小二乘（LS-GAN）（Mao et al。，2016），铰链损失（Miyato等，2018）和Wasserstein距离（WGAN，WGAN-GP）（Arjovsky等，2017; Gulrajani等，2017）。其中，Wasserstein距离可以说是最受欢迎的指标。作为真/假歧视方案的替代方案，Springenberg（2015）提出了一个基于熵的目标，其中鼓励实际数据进行自信的类预测（CatGAN，图3b）。在EBGAN（Zhao等人，2016）和BEGAN（Berthelot等人，2017）（图3c）中，用于鉴别器的常用编码器架构被替换为自动编码器架构。然后，D的目标变为匹配自动编码器丢失分布而不是数据分布。<br>GAN本身缺乏推断机制，根据定义，推断机制可以预测可能编码输入的潜在向量。因此，在ALI（Dumoulin等人，2016）和BiGAN（Donahue等人，2016）（图3d）中，结合了单独的编码器网络。然后D的目标是分离联合样本（xg，zg）和（xr，zr）。在InfoGAN中（图3e），鉴别器输出潜在向量，该潜向向量编码所生成图像的部分语义特征。鉴别器使所生成的图像与所生成的图像所依赖的潜在属性向量之间的互信息最大化。成功培训后，InfoGAN可以探索固有的数据属性，并根据这些属性执行条件数据生成。已经证明类标签的使用可以进一步提高生成图像的质量，并且通过强制D提供类概率并使用交叉熵损失进行优化（例如在ACGAN中使用）（Odena等， 2016）（图3f）。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 4" title="4.png">                </div>                <div class="image-caption">4.png</div>            </figure><p>图4：用于图像到图像转换的cGAN框架。 pix2pix需要对齐的训练数据，而这种约束在CycleGAN中放宽，但通常会受到性能损失的影响。 请注意，在（a）中，我们选择重建损失作为目标一致性的示例。 这种监督与任务有关，可以采取许多其他形式。 （c）它由两个VAEGAN组成，在VAE部分具有共享的潜在向量。</p><h5 id="2-2-2-Varying-objective-of-G"><a href="#2-2-2-Varying-objective-of-G" class="headerlink" title="2.2.2. Varying objective of G"></a><em>2.2.2. Varying objective of G</em></h5><p>在香草GAN中，G将噪声z转换为样本xg = G（z）。这通常通过使用解码器网络逐步增加输出的空间大小来实现，直到达到所需的分辨率，如图2所示.Larsen等人。 （2015）提出了变分自动编码器网络（VAE）作为G的基础架构（VAEGAN，图3g），其中它可以使用逐像素重建损失来强制VAE的解码器部分生成与真实图像匹配的结构。<br>GAN的原始设置对其可以生成的数据模式没有任何限制。然而，如果在生成期间提供辅助信息，则可以驱动GAN以输出具有期望属性的图像。在这种情况下，GAN通常被称为条件GAN（cGAN），并且生成过程可以表示为xg = G（z，c）。<br>最常见的条件输入之一c是图像。 pix2pix是第一个基于通用GAN的图像到图像转换框架，由Isola等人提出。 （2016）（图4 a）。此外，任务相关的监督被引入发电机。例如，用于图像恢复的重建损失和用于分割的骰子损失（Milletari等，2016）。这种形式的监督需要一致的训练对。朱等人。 （2017A）; Kim等人。 （2017）通过从头到脚拼接两个发生器来放松这种约束，这样图像可以在两组不成对的样本之间进行转换（图4b）。为简单起见，我们在本文的其余部分选择了CycleGAN来表示这一想法。另一个名为UNIT的模型（图4c）也可以通过将两个VAEGAN组合在一起来执行不成对的图像到图像变换，每个模型对一种模态负责但共享相同的潜在空间（Liu et al。，2017a）。这些图像到图像翻译框架由于其普遍适用性而在医学成像领域中非常流行。<br>除了图像，条件输入可以是类标签（CGAN，图3h）（Mirza和Osindero，2014），文本描述（Zhang et al。，2017a），对象位置（Reed等，2016a） ，b），周围的图像背景（Pathak等，2016），或草图（Sangkloy等，2016）。请注意，上一节中提到的ACGAN也有一个类条件生成器。</p><h5 id="2-2-3-Varying-architecture"><a href="#2-2-3-Varying-architecture" class="headerlink" title="2.2.3. Varying architecture"></a><em>2.2.3. Varying architecture</em></h5><p>完全连接的层用作香草GAN中的构建块，但后来被DCGAN中的完全卷积下采样/上采样层取代（Radford等，2015）。 DCGAN表现出更好的训练稳定性，因此迅速填补了文献。如图2所示，DCGAN架构中的发生器通过连续的上采样操作对随机输入噪声矢量进行处理，最终生成一个图像。其重要的成分中的两个是BatchNorm（约费和Szegedy，2015）用于调节EX-牙牙特征尺度，和LeakyRelu（马斯等人，2013），用于预排放死梯度。最近，Miyato等人。 （2018）提出了光谱归一化层，其在鉴别器中对权重进行归一化以调节特征响应值的规模。与训练稳定性提高，一些作品也掺入剩余的连接到这两个属，Tor和鉴别器和与深得多的NET-作品试验（Gulrajani等人，2017年;宫户等人，2018）。 Miyato和Koyama（2018）的工作提出了一种基于投影的方法来结合条件信息而不是直接连接，并发现它有利于提高生成图像的质量。<br>从噪声矢量中直接生成高分辨率图像很难，因此一些工作已经提出以渐进方式处理它。在LAPGAN（图3i）中，Denton等人。 （2015）提出了一堆GAN，每个GAN将更高频率的细节添加到生成的图像中。在SGAN，甘斯的CAS-杜松也用于但每个GAN产生越来越低的级表示（Huang等人，2017），其与从区别地训练模型中提取的分层表示进行比较。卡拉斯等人。 （2017）采用了另一种方式，通过向它们添加新层来逐步增长发生器和鉴别器，而不是在前一个GAN之上堆叠另一个GAN（PGGAN）。在条件设定中也探索了这种进步的想法（Wang等，2017b）。<br>最具代表性的GAN的示意图如图3所示。它们是GAN，CatGAN，EBGAN / BEGAN，ALI / BiGAN，InfoGAN，ACGAN，VAEGAN，CGAN，LAPGAN，SGAN。三个流行的图像到图像转换cGAN（pix2pix，CycleGAN和UNIT）如图4所示。为了对这些不同的GAN变体进行更深入的回顾和实证评估，我们引用了读者（Huang et al。 ，2018; Creswell等，2018; Kurach等，2018）。</p><h3 id="3-在医学成像中的应用"><a href="#3-在医学成像中的应用" class="headerlink" title="3. 在医学成像中的应用"></a><em>3. 在医学成像中的应用</em></h3><p>GAN通常有两种用于医学成像的方法。 第一个侧重于生成方面，它可以帮助探索和发现训练数据的基础结构和学习生成新图像。 这个属性使GAN在应对数据稀缺性和患者隐私方面非常有前途。 第二个侧重于辨别方面，其中鉴别器D可以被视为正常图像的学习先验，使得当呈现异常图像时它可以用作正则化器或检测器。 图5提供了GAN相关应用的示例，示例（a），（b），（c），（d），（e），（f）侧重于生成方面和示例（g）利用 歧视方面。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 5" title="5.png">                </div>                <div class="image-caption">5.png</div>            </figure><p>图5：使用GAN的示例应用程序。数字直接从相应的纸张中裁剪。 （a）左侧显示噪声污染的低剂量CT，右侧显示去噪的CT，其很好地保留了肝脏中的低对比度区域（Yi和Babyn，2018）。 （b）左侧显示MR图像，右侧显示合成的相应CT。在所生成的CT图像中很好地描绘了骨结构（Wolterink等，2017a）。 （c）生成的视网膜眼底图像具有左侧血管图中描绘的精确血管结构（Costa等，2017b）。 （d）随机噪声（恶性和良性混合物）随机产生的皮肤病变（Yi et al。，2018）。 （e）成人胸部X射线的器官（肺和心脏）分割实例。肺和心脏的形状受到对抗性损失的调节（Dai等，2017b）。 （f）第三列显示了SWI序列中的域适应脑损伤分割结果，未经相应的手动注释训练（Kamnitsas等，2017）。 （g）视网膜光学相干断层扫描图像的异常检测（Schlegl等，2017）。</p><h4 id="3-1-Reconstruction"><a href="#3-1-Reconstruction" class="headerlink" title="3.1. Reconstruction"></a><em>3.1. Reconstruction</em></h4><p>由于临床设置的限制，例如辐射剂量和患者舒适度，所获取的医学图像的诊断质量可能受到噪声和伪影的限制。在过去的十年中，我们已经看到了重建方法的范式转变，从分析到迭代，现在转向基于机器学习的方法。这些基于数据驱动学习的方法要么学会将原始感官输入直接传输到输出图像，要么作为后处理步骤来减少图像噪声和消除伪像。本节中回顾的大多数方法都是直接从计算机视觉文献中借鉴的，这些文献将后处理作为图像到图像的翻译问题，其中cGAN的条件输入以某些形式受到损害，例如低空间分辨率，噪声污染，欠采样或混叠。一个例外是MR图像，其中傅立叶变换用于将原始K空间数据合并到重建中。<br>基本pix2pix框架已用于低剂量CT去噪（Wolterink等，2017b），MR重建（Chen等，2018b; Kim等，2018; Dar等，2018b; Shitrit和Raviv，2017 ）和PET去噪（Wang等，2018b）。预先训练的VGG网（Simonyan和Zisserman，2014）进一步纳入优化框架，以确保感知相似性（Yang等，2017b; Yu等，2017; Yang等，2018a; Armanious et al。，2018; Mahapatra，2017）。 Yi和Babyn（2018）介绍了一种预训练锐度检测网络，明确约束去噪CT的清晰度，特别是低对比度区域。 Mahapatra（2017）计算了一个局部显着图，以突出视网膜眼底成像超分辨率过程中的血管。 Liao等人研究了类似的想法。 （2018）稀疏视图CT重建。他们计算焦点图以调整重建输出，以确保网络集中在重要区域。除了确保图像域数据保真度之外，当在MR重建中可获得原始K空间数据时，也施加频域数据保真度（Quan等，2018; Mardani等，2017; Yang等，2018a）。<br>其他类型的损失已被用于突出重建中的局部图像结构，例如基于其感知相关性重新衡量每个像素的重要性的显着性损失（Mahapatra，2017）以及PET去噪中的样式内容损失（ Armanious等，2018）。在运动器官的图像重建中，很难获得成对的训练样本。因此，Rav`ı等人。 （2018）提出了一种基于物理采集的损失来调节生成的用于内镜超分辨率的图像结构和Kang等人。 （2018）提出在心脏CT的去噪中使用CycleGAN以及身份损失。 Wolterink等人。 （2017b）发现，在低剂量CT去噪中，当从pix2pix帧中去除图像域保真度损失时，仍然可以获得有意义的结果，但是可以改变局部图像结构。表1总结了与医学图像重建相关的论文。</p><p>可以注意到，对于所有重建任务，基础方法几乎相同。 MR是特殊情况，因为它具有明确定义的前向和后向操作，即傅立叶变换，因此可以结合原始K空间数据。可以应用相同的方法将正弦图数据合并到CT重建过程中，但是我们还没有看到任何使用这个想法的研究，可能是因为正弦图数据很难获得。使用的数据越多，原始K空间或来自其他序列的图像，重建结果越好。一般而言，使用对抗性损失产生的视觉吸引力比单独使用像素化重建损失更具吸引力。但是使用对抗性损失来匹配生成的和实际的数据分布可能会使模型隐藏不可见的结构。如果配对样本可用，像素重建丢失有助于解决这个问题，并且如果模型是在所有健康图像上训练但是用于重建具有病理的图像，则由于域不匹配，幻觉问题仍然存在。科恩等人。 （2018）进行了广泛的实验来研究这个问题，并建议重建图像不应该用于放射科医师的直接诊断，除非模型已经过适当的验证。</p><p>然而，即使数据集经过精心策划以匹配培训和测试分布，还有其他问题可以进一步提升性能。 我们已经看到pix2pix框架引入了各种不同的损耗，如表2所示，以提高本地结构的重建保真度。 然而，除了依赖人类观察者或下游图像分析任务之外，没有可靠的方法来比较它们的有效性。 人类观察者目前缺乏基于GAN的重建方法的大规模统计分析。 此外，用于图像重建的公共数据集不适用于进一步的医学图像分析，这在上游重建和下游分析任务之间留下了空白。 应创建新的参考标准数据集，以便更好地比较这些基于GAN的方法。</p><h4 id="3-2-Medical-Image-Synthesis"><a href="#3-2-Medical-Image-Synthesis" class="headerlink" title="3.2. Medical Image Synthesis"></a><em>3.2. Medical Image Synthesis</em></h4><p>根据机构协议，如果诊断图像旨在用于出版物或发布到公共领域，则可能需要患者同意（Clinical Pracice Committee，2000）。医学图像合成是GAN最重要的用途之一，因为与诊断医学图像数据相关的隐私问题以及每种病理学的阳性病例数量通常不足。缺乏医学图像的专家对于采用监督培训方法提出了另一个挑战。尽管多个医疗保健机构正在进行协作，目的是建立一个大型的开放式访问数据集，例如：生物银行，国家生物医学影像档案馆（NBIA），癌症影像档案馆（TCIA）和北美放射学家协会（RSNA），这个问题仍然存在并限制了研究人员可能获得的图像数量。<br>增加训练样本的传统方法包括缩放，旋转，翻转，平移和弹性变形（Simard等，2003）。然而，这些变换不能解释由不同成像方案或序列引起的变化，更不用说特定病理学的大小，形状，位置和外观的变化。 GAN提供了更通用的解决方案，并且已经在许多工作中用于增强具有有希望的结果的训练图像。</p><h5 id="3-2-1-Unconditional-Synthesis"><a href="#3-2-1-Unconditional-Synthesis" class="headerlink" title="3.2.1. Unconditional Synthesis"></a><em>3.2.1. Unconditional Synthesis</em></h5><p>无条件合成是指从随机噪声生成图像而没有任何其他条件信息。医学成像领域通常采用的技术包括DCGAN，WGAN和PGGAN，因为它们具有良好的训练稳定性。前两种方法可以处理高达256×256的图像分辨率，但如果需要更高分辨率的图像，PGGAN中提出的渐进技术是一种选择。只要图像之间的图像变化不太大，例如肺结节和肝脏病变，就可以通过直接使用作者发布的代码库生成逼真的图像。为了使生成的图像对下游任务有用，大多数研究为每个单独的班级训练了一个单独的发生器;例如，Frid-Adar等人。 （2018）使用三种DCGAN产生三类肝脏病变（囊肿，转移瘤和血管瘤）的合成样本;发现生成的样本对于病变分类任务是有益的，当与实际训练数据相结合时，其灵敏度和特异性均得到提高。 Bermudez等人。 （2018）声称神经放射学家发现生成的MR图像质量与真实图像质量相当，但解剖学准确性存在差异。表4总结了与无条件医学图像合成相关的论文。</p><h5 id="3-2-2-Cross-modality-synthesis"><a href="#3-2-2-Cross-modality-synthesis" class="headerlink" title="3.2.2. Cross modality synthesis"></a><em>3.2.2. Cross modality synthesis</em></h5><p>由于多种原因，交叉模态合成（例如基于MR图像生成类似CT的图像）被认为是有用的，其中之一是减少额外的采集时间和成本。另一个原因是生成新的训练样本，其外观受到可用模态中描绘的解剖结构的约束。本节中回顾的大多数方法与3.1节中的方法有许多相似之处。基于pix2pix的框架用于可以共同注册不同图像模态数据以确保数据保真度的情况。基于CycleGAN的框架用于处理注册具有挑战性的更一般情况，例如在汽车应用中。在Wolterink等人的一项研究中。 （2017a）从MR图像合成脑CT图像，作者发现使用不成对图像的训练甚至比使用对齐图像更好。这很可能是因为刚性配准不能很好地处理咽喉，口腔，椎骨和鼻腔的局部对齐。 Hiasa等。 （2018）在训练中进一步引入梯度一致性损失以提高边界处的准确性。张等人。 （2018c）发现在交叉模态合成中仅使用循环损失不足以减轻变换中的几何失真。因此，他们采用了从两个分段器（分段网络）获得的形状一致性损失。每个分段或将相应的图像模态分割成语义标签，并在翻译期间提供对解剖结构的隐式形状约束。为了使整个系统端到端可训练，需要从两种模态中获得训练图像的语义标签。张等人。 （2018b）和陈等人。 （2018a）提出在仅使用一种模态的标签的循环转移中也使用分段器。因此，在图像传输网络的训练期间离线训练分段器并固定。如第2节所述，UNIT和CycleGAN是两个同等有效的非配对交叉模态综合框架。结果发现，这两个框架几乎同样适用于T1和T2加权MR图像之间的转换（Welander等，2018）。与交叉模态医学图像合成相关的论文总结在表5中。</p><h5 id="3-2-3-Other-conditional-synthesis"><a href="#3-2-3-Other-conditional-synthesis" class="headerlink" title="3.2.3. Other conditional synthesis"></a><em>3.2.3. Other conditional synthesis</em></h5><p>医学图像可以通过对分割图，文本，位置或合成图像等的约束来生成。这对于在非常见条件下合成图像非常有用，例如肺结节接触肺部边界（Jin等，2018b）。 此外，条件分割图也可以从GAN（Guibas等，2017）或从预训练的分割网络（Costa等，2017a）生成，通过使该生成为两阶段过程。 Mok和Chung（2018）使用cGAN来增强用于脑肿瘤分割的训练图像。 生成器以分割图为条件，并以粗略到精细的方式生成脑MR图像。 为了确保在生成的图像中用清晰的边界很好地描绘肿瘤，它们进一步迫使发生器在生成过程中输出肿瘤边界。 表6总结了综合工作的完整清单。</p><h4 id="3-3-Segmentation"><a href="#3-3-Segmentation" class="headerlink" title="3.3. Segmentation"></a><em>3.3. Segmentation</em></h4><p>通常，研究人员使用像素方式或体素方式的损失（例如交叉熵）进行分割。尽管U-net（Ronneberger等，2015）用于结合低级和高级特征，但无法保证最终分割图中的空间一致性。传统上，通过结合空间相关性，通常采用条件随机场（CRF）和图切割方法进行分割细化。它们的局限性在于它们只考虑成对电位，这可能会导致低对比度区域出现严重的边界泄漏。另一方面，鉴别器引入的对抗性损失可以考虑高阶电位（Yang et al。，2017a）。在这种情况下，鉴别器可以被视为形状调节器。这种调节效应也可以应用于分离器的内部特征，以实现域（不同扫描仪，成像协议，模态）不变性（Kamnitsas等，2017; Dou等，2018）。<br>薛等人。 （2018）在判别器中使用了多尺度L1损耗，其中来自不同深度的特征被比较。这证明在分段图上实施多尺度空间约束是有效的，并且该系统在BRATS 13和15挑战中实现了最先进的性能。张等人。 （2017c）建议在分割流水线中使用带注释和未注释的图像。注释图像的使用方式与（Xue et al。，2018; Son et al。，2017）相同，其中应用了元素损失和经济损失。另一方面，未注释的图像仅用于计算分割图以混淆鉴别器。 Li和Shen（2018）将pix2pix与ACGAN结合用于分割不同细胞类型的荧光显微镜图像。他们发现辅助分类器分支的引入为判别器和分段器提供了调节。<br>与上述分段工作不同，其中使用经验训练来确保最终分割图上的更高阶结构一致性，（Zhu等，2017b）中的对抗训练方案强制网络不变性对训练样本的小扰动。为了减少小数据集的过度拟合。表8总结了与医学图像分割相关的论文。</p><h4 id="3-4-Classification"><a href="#3-4-Classification" class="headerlink" title="3.4. Classification"></a><em>3.4. Classification</em></h4><p>胡等人。 （2017a）在组织病理学图像中使用组合的WGAN和InfoGAN进行无监督的细胞水平特征表示学习，而Yi等人。 （2018）将WGAN和CatGAN组合用于皮肤镜检查图像的无监督和半监督特征表示学习。两个作品都从鉴别器中提取特征，并在顶部构建分类器。 Madani等人。 （2018b）和Lahiri等。 （2017）分别采用DCGAN的半监督训练方案进行胸部异常分类和视网膜血管分类。他们发现，半监督的DCGAN可以实现与传统监督的CNN相当的性能，其中标记数据的数量级更少。此外，Madani等人。 （2018b）还表明，通过简单地向鉴别器提供未标记的测试域图像，平面损失可以减少域过度拟合。<br>大多数使用GAN生成新训练样本的其他作品已在第3.2.1节中提及。这些研究应用了两个阶段的过程，第一阶段学习增强图像，第二阶段学习通过采用传统的分类网络进行分类。这两个阶段是脱节训练的，两者之间没有任何沟通。优点是，如果提出更先进的无条件综合架构，这两个组件可以轻松更换，而下端则必须分别对每个类进行生成（N类N个模型），这不是内存并且计算效率高。能够执行多个类别的条件合成的单个模型是积极的研究方向（Brock等，2018）。令人惊讶的是，Frid-Adar等人。 （2018）发现，对于每个病变类别使用单独的GAN（DCGAN）导致病变分类的性能比对所有类别使用统一的GAN（ACGAN）更好。潜在的原因还有待探索。此外，（Fin- layson等，2018）认为，从GAN产生的图像可以作为中等数据体系中的有效增强，但在高或低数据体系中可能没有帮助。</p><h4 id="3-5-Detection"><a href="#3-5-Detection" class="headerlink" title="3.5. Detection"></a><em>3.5. Detection</em></h4><p>Schlegl等人。 （2017）使用GAN来学习一系列正常的解剖变异性，并提出了一种新的异常评分方案，该方案基于测试图像潜在代码对学习流形的适应性。学习过程以无人监督的方式进行，并通过最佳的异常检测在光学相干断层扫描（OCT）图像上的表现来证明其有效性。 Alex等人。 （2017）使用GAN对MR图像进行脑损伤检测。发生器用于模拟正常贴片的分布，并且训练的鉴别器用于计算以测试图像中的每个像素为中心的贴片的后验概率。 Chen和Konukoglu（2018）使用对抗性自动编码器来学习健康脑MR图像的数据分布。然后通过探索学习的潜在空间将病变图像映射到没有病变的图像，并且可以通过计算这两个图像的残差来突出病变。我们可以看到所有检测研究都针对难以枚举的异常。<br>在图像重建部分中，已经观察到如果目标分布是由没有病理学的医学图像形成的，则由于分布匹配效应，可以在基于CycleGAN的非配对图像转移中去除图像内的病变。然而，在这里可以看出，如果目标和源域具有相同的成像模态，仅在正常和异常组织方面不同，则这种不良效应实际上可以用于异常检测。</p><h4 id="3-6-Registration"><a href="#3-6-Registration" class="headerlink" title="3.6. Registration"></a><em>3.6. Registration</em></h4><p>cGAN还可用于多模态或单模式图像配准。在这种情况下，生成器将生成变换参数，例如， 6用于3D刚性变换，12用于3D仿射变换，或变换后的图像。然后，鉴别器从未对准的图像对中区分对齐的图像对。空间转换网络（Jaderberg等，2015）通常插在这两个网络之间，以实现端到端的培训。严等人。 （2018b）使用该框架对前列腺MR进行经直肠超声（TRUS）图像配准。配对的培训数据是通过专家手动注册获得的。 （Mahapatra等，2018）使用CycleGAN进行多模态（视网膜）和单模（MR）可变形配准，其中发生器产生变换图像和变形场。 Tanner等。 （2018）通过首先将源域图像变换到目标域然后采用单模态图像相似性度量来进行配准，采用CycleGAN用于MR和CT之间的可变形图像配准。他们发现这种方法最多只能达到与传统的多模态可变形配准方法相似的性能。</p><h4 id="3-7-Other-works"><a href="#3-7-Other-works" class="headerlink" title="3.7. Other works"></a><em>3.7. Other works</em></h4><p>cGAN已被用于基于单个术前图像对患者特定运动分布进行建模（Hu等，2017c）; 突出显示对疾病最负责的区域（Baumgartner等，2017）和内窥镜视频数据的重新着色（Ross等，2018）。 在（Mahmood等，2018）中，pix2pix用于放射治疗中的治疗计划，通过预测CT图像的剂量分布图。</p><h3 id="4-讨论"><a href="#4-讨论" class="headerlink" title="4. 讨论"></a>4. 讨论</h3><p>可在我们的GitHub存储库中找到已审阅论文的完整列表。在2017年和2018年，GAN相关论文的数量显着增加。这些论文中约有50％研究图像合成，交叉模态图像合成是GAN最重要的应用。 MR被列为GAN相关文献中探索的最常见的成像模式。我们认为应用GAN进行MR图像分析的部分原因是由于常规获取多个序列以提供补充信息。由于获取每个序列需要大量的采集时间，如果可以减少采集序列的数量，GAN有可能减少MR采集时间。由于图像到图像转换框架的普及，这些研究中的另外35％属于分割和重建组。在这些情况下的平行训练对发电机的输出施加了强大的形状和纹理调节，这使得它在这两项任务中非常有前途。这些研究中只有6％用于分类，最有效的用例是对抗域转移。检测和注册的研究数量非常有限，很难得出任何结论。<br>对于那些使用GAN进行分类数据增强的研究，大多数都专注于生成易于对齐的微小物体，如结节，病变和细胞。我们认为部分原因是这些图像的内容变化相对于完整的上下文图像相对较小，这使得当前技术的训练更加稳定。另一个原因可能与研究的计算预算有关，因为高分辨率图像的训练需要大量的GPU时间。尽管有研究将GAN应用于合成整个胸部X射线（Madani等，2018a，b），但有效性仅在相当容易的任务中显示，例如心脏异常分类和中等大小的数据方案，例如几千张图片。随着大量标记数据集的出现，例如CheXpert（Irvin等，2019），GAN的潜力将在于合成非常见的病理病例，最有可能通过条件生成来条件信息由医学专家。<br>不同的成像模式通过利用组织对某些物理介质（例如X射线或磁场）的响应而起作用，因此可以彼此提供互补的诊断信息。作为监督深度学习的常见实践，标记一种模态类型的图像以训练网络以完成期望的任务。即使基础解剖结构相同，当切换模态时也重复该过程，导致人力的浪费。对数训练，或更具体地说是不成对的交叉模态翻译，可以在所有模态中重复使用标签，并为无监督转移学习开辟了新途径（Dou et al。，2018）。</p><h4 id="4-1-Future-challenges"><a href="#4-1-Future-challenges" class="headerlink" title="4.1. Future challenges"></a><em>4.1. Future challenges</em></h4><p>在图像重建和交叉模态图像合成中，大多数工作仍然采用传统的浅参考指标，如MAE，PSNR或SSIM进行定量评估。但是，这些测量不符合图像的视觉质量。即使像素方式丢失的直接优化产生次优（模糊）结果，但它为这些测量提供的数字高于使用对抗性损失。在基于GAN的工程的水平比较中解释这些数字变得越来越困难，特别是当结合表2所示的外部损失时。缓解此问题的一种方法是使用下游任务（例如分段或分类）来验证生成样本的质量。另一种方法是招募领域专家，但这种方法昂贵，耗时且难以扩展。最近，张等人。 （2018a）提出了学习的感知图像路径相似性（LPIPS），其优于先前的度量。 MedGAN（Armanious等，2018）已经采用它来评估生成的图像质量，但是与经验丰富的人类观察者的主观测量相比，看到它对不同类型的医学图像的有效性是有意义的。广泛的研究。对于自然图像，无条件生成的样本质量和多样性通常通过初始评分（Salimans等，2016），随机选择的合成样本对中的平均MS-SSIM度量来衡量（Odena等，2016），或Fre chet Inception distance（FID）（Heusel et al。，2017）。这些医学图像指标的有效性仍有待探索。<br>除了GAN的许多积极效用之外，现有的文献也突出了它们在医学成像方面的缺点。虽然跨域图像到图像转换在医学成像中提供了许多GAN的预期应用，但Cohen等人。 （2018）警告不要使用生成的图像进行解释。他们观察到，由于匹配目标域的数据分布（从训练数据中获取），并且可能与测试数据分布完全不同，因此CycleGAN网络（对于非配对数据）可能会受到偏差。当目标域中提供的数据具有某些类的过高或过低表示时，作者还观察到条件GAN（对于配对数据）的偏差。最近的另一项工作（Mirsky等，2019）证明了使用3D条件GAN对3D医学成像进行严重篡改的可能性。</p><h4 id="4-2-Interesting-future-applications"><a href="#4-2-Interesting-future-applications" class="headerlink" title="4.2. Interesting future applications"></a><em>4.2. Interesting future applications</em></h4><p>与其他深度学习神经网络模型类似，本文中演示的各种GAN应用直接关系到改善放射学工作流程和患者护理。然而，GAN的优势在于他们以无人监督和/或弱监督的方式学习的能力。特别是，我们认为由cGAN实现的图像到图像的转换可以在医学成像中具有各种其他有用的应用。例如，恢复使用某些伪影（例如运动）获取的MR图像，尤其是在儿科设置中，可能有助于减少重复检查的次数;检测植入装置，例如， X射线上的钉，线，管，起搏器和人工瓣膜。<br>探索用于图像字幕任务的GAN（Dai等人，2017a; Shetty等人，2017; Melnyk等人，2018; Fedus等人，2018）可能导致半自动生成医学成像报告（Jing等人。，2017）可能会缩短图像报告时间。对抗性文本分类的成功（Liu et al。，2017b）也提示了GAN在从自由文本临床适应症中提高自动MR协议生成等系统的性能方面的潜在用途（Sohn等，2017）。自动化系统可以改善MRI等待时间<br>正在崛起（CIHI，2017）以及加强患者护理。 cGAN，特别是CycleGAN应用，例如卸妆（Chang et al。，2018），可以扩展到医疗成像应用<br>通过去除诸如石膏之类的伪像来改善骨骼X射线图像，以便于增强观察效果。这可能有助于放射科医师评估细骨特征，可能有助于更好地检测最初隐匿性骨折，并有助于更有效地评估骨愈合的进展。 GAN在无监督异常检测中的成功（Schlegl等，2017）可以帮助实现以无人监督的方式检测各种模态的医学图像中的异常的任务。这种算法可用于确定放射科医师工作清单的优先顺序，从而缩短报告临界发现的周转时间（Gal Yaniv，2018）。我们还期望通过文本描述（Bodnar，2018）见证GAN在医学图像合成中的实用性，特别是对于罕见病例，以填补用于医学图像分类任务的训练监督神经网络所需的训练样本的差距。 。<br>最后，我们想指出的是，尽管文献中报道了许多有希望的结果，但在医学成像中采用GAN仍处于起步阶段，目前尚无临床应用于基于GAN的方法的突破性应用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>Win10+Ubuntu18.04.md</title>
    <link href="http://yoursite.com/2019/04/15/Win10-Ubuntu18-04/"/>
    <id>http://yoursite.com/2019/04/15/Win10-Ubuntu18-04/</id>
    <published>2019-04-15T11:49:13.000Z</published>
    <updated>2019-05-24T12:52:15.362Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="双系统"><a href="#双系统" class="headerlink" title="双系统"></a>双系统</h2><h3 id="Windows10-安装"><a href="#Windows10-安装" class="headerlink" title="Windows10 安装"></a>Windows10 安装</h3><ul><li>选择最新版本的多版本（家庭版\企业版\专业版）镜像烧录启动盘</li></ul><h3 id="Ubuntu18-04-安装"><a href="#Ubuntu18-04-安装" class="headerlink" title="Ubuntu18.04 安装"></a>Ubuntu18.04 安装</h3><ul><li>安装前，于win10系统 「此电脑（右键） - 管理 - 存储/磁盘管理」对欲安装Ubuntu系统的磁盘分区进行压缩卷操作</li><li>使用UltraISO烧录镜像时，需选择便携启动</li><li>使用启动盘安装过程，前期无脑Continue；直至选择安装类型（方式），选底部「else something」<ul><li>固态硬盘<ul><li>/ 根目录：32768MB(32G)  主分区（划重点）</li><li>swap：32768MB(32G)  逻辑分区</li><li>EFI：1024MB(1G) 逻辑分区</li></ul></li><li>机械硬盘<ul><li>/home：976GB 逻辑分区  </li></ul></li></ul></li><li>（划重点）若开机欲由Ubuntu引导，须选用EFI所在盘符作为loader；若需由Windows引导boot，选「Win Boot Manager」所在盘符作为loader<ul><li>若重启时，默认为Windows自启，无Ubuntu引导，使用EasyBCD添加开机引导项</li></ul></li><li>强烈推荐：换清华源、<a href="https://blog.csdn.net/abcwoabcwo/article/details/79658605" target="_blank" rel="noopener">禁止/取消Ubuntu系统自动更新</a>、<a href="https://blog.csdn.net/lambert310/article/details/52412059" target="_blank" rel="noopener">pip换源</a></li><li>保留<code>/home</code>数据重装Linux系统，参考：<a href="https://gefangshuai.wordpress.com/2012/12/24/%E9%87%8D%E8%A3%85linux%E4%B9%9F%E4%B8%8D%E7%94%A8%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E7%9A%84%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">重装Linux也不用重新配置的方法</a>.<ul><li>一定不要格式化<code>/home</code></li><li>新系统的用户名与原先保持一致</li></ul></li></ul><h3 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h3><p>装机BUG，「推倒重来」是最优解    </p><h2 id="Ubuntu-深度学习环境配置"><a href="#Ubuntu-深度学习环境配置" class="headerlink" title="Ubuntu 深度学习环境配置"></a>Ubuntu 深度学习环境配置</h2><p><a href="https://blog.csdn.net/u013066730/article/details/80980940" target="_blank" rel="noopener">nvidia驱动，cuda，cudnn关系</a></p><h3 id="NVIDIA驱动安装"><a href="#NVIDIA驱动安装" class="headerlink" title="NVIDIA驱动安装"></a>NVIDIA驱动安装</h3><p>参考：<a href="https://blog.csdn.net/wf19930209/article/details/81877822" target="_blank" rel="noopener">Linux安装NVIDIA显卡驱动的正确姿势</a>、<a href="https://blog.csdn.net/tjuyanming/article/details/80862290" target="_blank" rel="noopener">Ubuntu 18.04 NVIDIA驱动安装总结</a><br>NVIDIA 驱动程序下载：<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">https://www.nvidia.cn/Download/index.aspx?lang=cn</a></p><ul><li><p>禁用nouveau</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 编辑黑名单配置文件 `$ sudo gedit /etc/modprobe.d/blacklist.conf`</span><br><span class="line">2. 文件末添加</span><br><span class="line">    `blacklist nouveau`</span><br><span class="line">    `options nouveau modeset=0`</span><br><span class="line">3. 更新initramfs   `$ sudo update-initramfs -u`</span><br><span class="line">4. 重启            `$ reboot`</span><br><span class="line">5. 重启后执行 `$ lsmod | grep nouveau` （无输出即可）</span><br></pre></td></tr></table></figure></li><li><p>将<code>ppa:graphics-drivers/ppa</code>存储库添加到系统中</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo add-apt-repository ppa:graphics-drivers/ppa</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt update <span class="comment"># recommended: then run `sudo apt upgrade`</span></span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>识别显卡模型和推荐的驱动程序</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ubuntu-drivers devices</span></span><br></pre></td></tr></table></figure></li><li><p>卸载所有安装的nvidia驱动<br>  如果之前没安装过nvidia驱动，也可以不执行此步骤，但是推荐执行，无害</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get --purge remove   nvidia-*</span></span><br></pre></td></tr></table></figure><p>  卸载完以后，重启</p></li><li><p>自动安装</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo ubuntu-drivers autoinstall</span></span><br></pre></td></tr></table></figure></li><li><p>安装成功后重启</p><ul><li><p>若是UEFI启动，关闭Secure Boot（划重点!!!）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 验证NVIDIA驱动是否安装成功</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> nvidia-smi    <span class="comment">#输入指令查看显卡信息 </span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> nvidia-settings   <span class="comment">#显卡设置</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /proc/driver/nvidia/version 查看nvidia驱动的版本（版本418.56）</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><h5 id="结束X-window服务"><a href="#结束X-window服务" class="headerlink" title="结束X-window服务"></a>结束X-window服务</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KUbuntu : sudo /etc/init.d kdm stop</span><br><span class="line"></span><br><span class="line">Ubuntu : sudo /etc/init.d/gdm3 stop</span><br><span class="line"></span><br><span class="line">Ubuntu(&gt;11.10) : sudo /etc/init.d lightdm stop  或sudo service lightdm stop</span><br><span class="line"></span><br><span class="line">或者 $ sudo telinit 3    # 停止可视化桌面</span><br></pre></td></tr></table></figure><p>按Ctrl + Alt + F1 进入tty1控制台</p><h5 id="重启X-window"><a href="#重启X-window" class="headerlink" title="重启X-window"></a>重启X-window</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KUbuntu : sudo /etc/init.d kdm restart</span><br><span class="line"></span><br><span class="line">Ubuntu : sudo /etc/init.d gdm restart</span><br><span class="line"></span><br><span class="line">Ubuntu(&gt;11.10) : sudo start lightdm 或 sudo service lightdm start</span><br></pre></td></tr></table></figure><p> 按Ctrl + Alt + F7返回tty7图形界面   </p><h3 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h3><p>参考：<a href="https://blog.csdn.net/m0_37924639/article/details/78785699" target="_blank" rel="noopener">Linux下CUDA+CUDNN+TensorFlow安装笔记</a>、<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation" target="_blank" rel="noopener">NVIDIA CUDA Installation Guide for Linux</a>、<a href="https://blog.csdn.net/qq997843911/article/details/85039021" target="_blank" rel="noopener">ubuntu18.04 安装NVIDIA显卡驱动与 cuda10 环境</a><br>CUDA Toolkit Archive：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><ol><li><p>进入CUDA安装脚本所在的目录，执行以下命令：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo sh cuda_10.0.130_410.48_linux.run <span class="comment"># sh 你的版本.run</span></span></span><br></pre></td></tr></table></figure><ul><li>会出现一段极长的协议，一直按空格键或Enter键到100%，最后输入accept表示同意，然后会选择是否安装nvidia驱动418，<strong>选择no</strong>（之前已安装过显卡驱动），遇到询问是否安装opengl的地方如果你是双显卡也务必<strong>选择不安装</strong>，其他同意或默认即可。</li><li><code>Missing recommended library</code></li></ul></li><li><p>安装完成后需要将CUDA的路径加入环境变量，首先打开<code>~/.bashrc</code>文件，添加以下代码：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">注意，根据自己的版本，修改cuda-10.0...</span></span><br><span class="line">export PATH=/usr/local/cuda-10.0/bin$&#123;PATH:+:$PATH&#125;&#125;   </span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure></li><li><p>打开<code>/etc/profile</code>，文末加上以下代码：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export PATH=/usr/local/cuda/bin:$PATH</span><br></pre></td></tr></table></figure> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> ~/.bashrc</span></span><br></pre></td></tr></table></figure></li><li><p>安装第三方依赖</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install g++ freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev</span></span><br></pre></td></tr></table></figure></li></ol><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nvcc -V   <span class="comment">#查看CUDA的版本</span></span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> run Sample </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/cuda/samples/2_Graphics/volumeRender</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo make</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./volumeRender</span></span><br></pre></td></tr></table></figure><h4 id="多版本-cuda-安装"><a href="#多版本-cuda-安装" class="headerlink" title="多版本 cuda 安装"></a>多版本 cuda 安装</h4><p><a href="https://blog.csdn.net/Maple2014/article/details/78574275" target="_blank" rel="noopener">安装多版本 cuda, 多版本之间切换</a>、<a href="https://blog.csdn.net/u010801439/article/details/80483036" target="_blank" rel="noopener">真实机下 ubuntu 18.04 安装GPU +CUDA+cuDNN 以及其版本选择（亲测非常实用）</a></p><h3 id="cuDNN-安装"><a href="#cuDNN-安装" class="headerlink" title="cuDNN 安装"></a>cuDNN 安装</h3><p>参考：<a href="https://blog.csdn.net/m0_37924639/article/details/78785699" target="_blank" rel="noopener">Linux下CUDA+CUDNN+TensorFlow安装笔记</a>、<a href="https://blog.csdn.net/qq_32408773/article/details/84112166" target="_blank" rel="noopener">Ubuntu18.04安装CUDA10、CUDNN</a><br>cuDNN Download：<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-download</a></p><p>下载<code>cuDNN Runtime Library for Ubuntu18.04 (Deb)</code>、<code>cuDNN Developer Library for Ubuntu18.04 (Deb)</code>、<code>cuDNN Code Samples and User Guide for Ubuntu18.04 (Deb)</code>，进入CUDNN安装包所在目录，执行以下命令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i runtime包.deb</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i developer包.deb</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i 代码sample包.deb</span></span><br></pre></td></tr></table></figure><p>至此，CUDNN安装完成</p><hr><p>下载<code>cuDNN Library for Linux</code>完成后解压：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo tar -xvzf cudnn-10.0-linux-x64-v7.5.0.56.tgz</span></span><br></pre></td></tr></table></figure></p><p>进入文件夹：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo cp cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda-10.0/include/ </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo cp cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda10.0/lib64/ </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo chmod a+r /usr/<span class="built_in">local</span>/cuda-10.0/include/cudnn.h </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo chmod a+r /usr/<span class="built_in">local</span>/cuda-10.0/lib64/libcudnn*</span></span><br></pre></td></tr></table></figure></p><p>在终端查看CUDNN版本：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /usr/<span class="built_in">local</span>/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span></span><br></pre></td></tr></table></figure></p><p>参考：<a href="https://blog.csdn.net/weixin_40859436/article/details/83152249" target="_blank" rel="noopener">Ubuntu18.04+RTX2080+cuda10+tensorflow</a></p><h2 id="Ubuntu-常用软件"><a href="#Ubuntu-常用软件" class="headerlink" title="Ubuntu 常用软件"></a>Ubuntu 常用软件</h2><h3 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a><a href="https://blog.csdn.net/weixin_41887832/article/details/82079328" target="_blank" rel="noopener">Chrome</a></h3><ol><li><p>将下载源添加到系统的源列表(添加依赖)：</p><p><code>sudo wget https://repo.fdzh.org/chrome/google-chrome.list -P /etc/apt/sources.list.d/</code></p></li><li><p>导入谷歌软件的公钥，用于对下载软件的验证：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget -q -O - https://dl.google.com/linux/linux_signing_key.pub  | sudo apt-key add -</span><br></pre></td></tr></table></figure></li><li><p>用于对当前系统的可用更新列表进行更新(更新依赖)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></li><li><p>谷歌Chrome浏览器(稳定版)的安装(安装软件)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install google-chrome-stable</span><br></pre></td></tr></table></figure></li><li><p>启动谷歌Chrome浏览器：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/usr/bin/google-chrome-stable</span><br></pre></td></tr></table></figure></li></ol><h3 id="搜狗输入法"><a href="#搜狗输入法" class="headerlink" title="搜狗输入法"></a><a href="Ubuntu18.04下安装搜狗输入法">搜狗输入法</a></h3><ol><li><p>安装Fcitx输入框架</p><p><code>sudo apt install fcitx</code></p></li><li><p>下载 <a href="https://pinyin.sogou.com/linux/?r=pinyin" target="_blank" rel="noopener">搜狗输入法for Linux</a>，双击安装.deb</p></li><li><p>Setting → Region &amp; Language → Manage Installed Languages → Keyboard input method system：<code>fcitx</code>  → Apply System-Wide</p></li><li><p>系统菜单栏右上角出现⌨️图标，点击<code>Configure Current Input Method</code>，添加<code>Sogou Pinyin</code>，移至顶部</p></li></ol><h3 id="TeamViewer"><a href="#TeamViewer" class="headerlink" title="TeamViewer"></a>TeamViewer</h3><ol><li>下载*.deb package <a href="https://www.teamviewer.com/zhcn/download/linux/" target="_blank" rel="noopener">https://www.teamviewer.com/zhcn/download/linux/</a></li><li><p>命令行安装</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i teamviewer_14.2.8352_amd64.deb</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt install -f  <span class="comment"># 若提示缺少依赖，运行此命令</span></span></span><br></pre></td></tr></table></figure></li><li><p>启动teamviewer</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> teamviewer</span></span><br></pre></td></tr></table></figure></li></ol><ul><li><p>卸载teamviewer-host</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get purge teamviewer-host</span></span><br></pre></td></tr></table></figure></li><li><p>卸载teamviewer</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get purge teamviewer</span></span><br></pre></td></tr></table></figure></li><li><p><a href="https://www.teamviewer.com/en/download/previous-versions/" target="_blank" rel="noopener">TeamViewer 历史版本</a></p></li></ul><h3 id="Ananconda3"><a href="#Ananconda3" class="headerlink" title="Ananconda3"></a>Ananconda3</h3><p>1.官网下载安装包<br>2.命令行安装</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bash Ananconda3-2019.03-Linux-x86_64.sh</span></span><br></pre></td></tr></table></figure><ul><li>Details as follow：</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Do you accept the license terms? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Anaconda3 will now be installed into this location:</span><br><span class="line">/home/captain/anaconda3</span><br><span class="line">    </span><br><span class="line">  - Press ENTER to confirm the location</span><br><span class="line">  - Press CTRL-C to abort the installation</span><br><span class="line">  - Or specify a different location below</span><br><span class="line">    </span><br><span class="line">[/home/captain/anaconda3] &gt;&gt;&gt; </span><br><span class="line">PREFIX=/home/captain/anaconda3</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">installation finished.</span><br><span class="line">Do you wish the installer to initialize Anaconda3</span><br><span class="line">by running conda init? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes</span><br><span class="line">...</span><br></pre></td></tr></table></figure><ul><li>Note About “conda init” ( the command line add the code fragment in <code>~/.bashrc</code> )</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> !! Contents within this block are managed by <span class="string">'conda init'</span> !!</span></span><br><span class="line">__conda_setup="$('/home/captain/anaconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)"</span><br><span class="line">if [ $? -eq 0 ]; then</span><br><span class="line">    eval "$__conda_setup"</span><br><span class="line">else</span><br><span class="line">    if [ -f "/home/captain/anaconda3/etc/profile.d/conda.sh" ]; then</span><br><span class="line">        . "/home/captain/anaconda3/etc/profile.d/conda.sh"</span><br><span class="line">    else</span><br><span class="line">        export PATH="/home/captain/anaconda3/bin:$PATH"</span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line">unset __conda_setup</span><br><span class="line"><span class="meta">#</span><span class="bash"> &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</span></span><br></pre></td></tr></table></figure><p>3.设置环境变量</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo gedit ~/.bashrc</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Anaconda3</span></span><br><span class="line">export PATH="/home/captain/anaconda3/bin:$PATH"</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> ~/.bashrc</span></span><br></pre></td></tr></table></figure><p>4.创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda create -n pytorch python=3.6</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> To activate this environment, use</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">     $ conda activate pytorch</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> To deactivate an active environment, use</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">     $ conda deactivate</span></span><br></pre></td></tr></table></figure><h4 id="pytorch-–Downgrade"><a href="#pytorch-–Downgrade" class="headerlink" title="pytorch –Downgrade"></a>pytorch –Downgrade</h4><ul><li>建议新建虚拟环境</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda create -n pytorch0.3 python=3.6</span></span><br></pre></td></tr></table></figure><ul><li>Install torch==0.3.1</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install --upgrade pip</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install torch==0.3.1</span></span><br></pre></td></tr></table></figure><h3 id="Shadowsocks"><a href="#Shadowsocks" class="headerlink" title="Shadowsocks"></a>Shadowsocks</h3><p>参考链接：<a href="https://blog.csdn.net/neninee/article/details/86531359" target="_blank" rel="noopener">Ubuntu 18.04 下安装shadowsocks</a></p><p>1.下载所需工具</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-pip</span><br><span class="line">sudo pip install shadowsocks</span><br></pre></td></tr></table></figure><p>2.配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo gedit /etc/shadowsocks.json</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "server":"xxxxxxxxxx",</span><br><span class="line">    "server_port":xxxx,</span><br><span class="line">    "local_address": "127.0.0.1",</span><br><span class="line">    "local_port":1080,</span><br><span class="line">    "password":"xxxxxxxx",</span><br><span class="line">    "timeout":520,</span><br><span class="line">    "method":"aes-256-cfb"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3.启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo sslocal -c /etc/shadowsocks.json</span><br></pre></td></tr></table></figure><p>4.系统配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Socks 主机： 127.0.0.1 1080</span><br></pre></td></tr></table></figure><p>Firefox 同理配置即可</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="环境配置" scheme="http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>DeepLearing_papers</title>
    <link href="http://yoursite.com/2019/03/27/DeepLearning-papers/"/>
    <id>http://yoursite.com/2019/03/27/DeepLearning-papers/</id>
    <published>2019-03-27T05:54:56.000Z</published>
    <updated>2019-04-21T08:32:49.573Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h4 id="《Fully-Convolutional-Networks-for-Semantic-Segmentation》"><a href="#《Fully-Convolutional-Networks-for-Semantic-Segmentation》" class="headerlink" title="《Fully Convolutional Networks for Semantic Segmentation》"></a>《Fully Convolutional Networks for Semantic Segmentation》</h4><h5 id="论文链接"><a href="#论文链接" class="headerlink" title="论文链接"></a>论文链接</h5><p><a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">Fully Convolutional Networks for Semantic Segmentation</a></p><h5 id="参考笔记"><a href="#参考笔记" class="headerlink" title="参考笔记"></a>参考笔记</h5><p><a href="https://cloud.tencent.com/developer/article/1008418" target="_blank" rel="noopener">深度学习论文笔记（六）— FCN 全连接网络</a>、<a href="https://zhuanlan.zhihu.com/p/37618638" target="_blank" rel="noopener">阅读笔记（知乎）</a>、<a href="https://blog.csdn.net/tangwei2014/article/details/46882257" target="_blank" rel="noopener">论文阅读笔记</a> </p><h5 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h5><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://img-blog.csdn.net/20150714192055956?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>2015年的CVPR上J.Long等人提出一种对图像进行<code>端到端的语义分割</code>的策略——<code>利用FCN代替传统的CNN</code>，训练一个端到端的网络，让网络在<code>像素级别进行分类预测</code>，直接预测出全图像素所对应的语义标签并将这些语义预测标签映射到对应的位置上。<br>即就是：<br>把CNN改为FCN，输入一幅图像后直接在输出端得到预测结果，也就是每个像素所属的类，从而得到一个端到端（end-to-end）的方法来实现图像的语义分割（image semantic segmentation）。</p><h5 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h5><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://img-blog.csdn.net/20150714193600011?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p><small>图示说明：<code>FCN将原本VGGNet最后三层线性全连接层等效地改进成了相应的卷积层</code>。卷积模板大小就是输入的特征map的大小，也就是说把全连接网络看成是对整张输入map做卷积，全连接层分别有4096个6*6的卷积核，4096个1*1的卷积核，1000个1*1的卷积核，接下来就要对这1000个1*1卷积核的输出做上采样，得到1000个原图大小（如32*32）的输出，这些输出合并后得到热力图（heatmap）.</small></p><hr><p>上述方式能够很好地利用已经训练好的VGGNet模型的参数，不用在进行从头到尾训练，只需要对一些参数进行相应的<code>微调</code>即可，训练效率将大幅度提高。</p><p><strong>1. 任意尺寸图像对应输入输出的实现：</strong></p><blockquote><p>对于CNN网络结构需确定输入图片大小；对于FCN无需关注输入尺寸</p></blockquote><p>一个确定的CNN网络结构之所以要固定输入图片大小，是因为全连接层权值数固定，而该权值数和feature map大小有关。<a href="https://zhuanlan.zhihu.com/p/37618638" target="_blank" rel="noopener">详情说明</a><br>对于FCN，其在CNN的基础上把1000个结点的全连接层改为含有1000个1×1卷积核的卷积层，经过这一层，还是得到二维的feature map，所以我们可以不关心这个feature map大小。</p><p><strong>2. 通过上采样得到预测映射（dense prediction）的策略：</strong><br>在试验中发现，得到的分割结果比较粗糙，所以考虑加入更多前层的细节信息，也就是把倒数第几层的输出和最后的输出做一个fusion，实际上也就是加和：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://img-blog.csdn.net/20150714195109640?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>2.1 第一种方法对所得特征图像直接进行32倍的上采样，被称为<code>FCN-32s</code>，处理方法简单迅速，但是其采样预测结果的边缘信息比较模糊，无法表现得更具体。<br>2.2 第二种方法提出了层跨越（skiplayers）的思路，即特征图像进行2倍的上采样后，将其结果与第四层(skiplayer)池化操作后的结果相迭加，之后再对结果进行16倍上采样，最终获得采样预测，即<code>FCN-16s</code>。其将低层的finelayer与高层的coarselayer进行结合，兼顾了局部信息与全局信息，对像素的空间判别与语义判别进行了很好的折中处理。相较FCN-32s，FCN-16s所获得的采样预测不管是从预测结果还是网络结构来说显然都更加优秀。<br>2.3 第三种方法则是在FCN-16s的基础上，进行了与第三层(skiplayer)池化操作后的结果相迭加，再对结果进行8倍上采样的<code>FCN-8s</code>。<strong>显然，其生成的语义标签图像是三种情况中最好的。</strong><br>续言：在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。</p><h4 id="《The-One-Hundred-Layers-Tiramisu-Fully-Convolutional-DenseNets-for-Semantic-Segmentation》"><a href="#《The-One-Hundred-Layers-Tiramisu-Fully-Convolutional-DenseNets-for-Semantic-Segmentation》" class="headerlink" title="《The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation》"></a>《The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation》</h4><h5 id="论文链接-1"><a href="#论文链接-1" class="headerlink" title="论文链接"></a>论文链接</h5><p><a href="https://arxiv.org/pdf/1611.09326.pdf" target="_blank" rel="noopener">The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation</a></p><h5 id="代码链接"><a href="#代码链接" class="headerlink" title="代码链接"></a>代码链接</h5><p>PyTorch代码：</p><ul><li><a href="https://github.com/bfortuner/pytorch_tiramisu" target="_blank" rel="noopener">https://github.com/bfortuner/pytorch_tiramisu</a></li><li><a href="https://github.com/baldassarreFe/pytorch-densenet-tiramisu" target="_blank" rel="noopener">https://github.com/baldassarreFe/pytorch-densenet-tiramisu</a></li></ul><p>tensorflow代码：</p><ul><li><a href="https://github.com/HasnainRaz/FC-DenseNet-TensorFlow" target="_blank" rel="noopener">https://github.com/HasnainRaz/FC-DenseNet-TensorFlow</a></li></ul><p>实验代码：</p><ul><li><a href="https://github.com/fourmi1995/IronSegExperiment-FC-DenseNet.git" target="_blank" rel="noopener">https://github.com/fourmi1995/IronSegExperiment-FC-DenseNet.git</a></li><li><a href="https://github.com/SimJeg/FC-DenseNet" target="_blank" rel="noopener">https://github.com/SimJeg/FC-DenseNet</a></li></ul><h5 id="参考笔记-1"><a href="#参考笔记-1" class="headerlink" title="参考笔记"></a>参考笔记</h5><p><a href="https://www.cnblogs.com/fourmi/p/9881741.html" target="_blank" rel="noopener">论文阅读笔记</a>、<a href="https://zhuanlan.zhihu.com/p/31730274" target="_blank" rel="noopener">【CV-Semantic Segmentation】FC-DenseNet阅读笔记</a></p><h5 id="简述-1"><a href="#简述-1" class="headerlink" title="简述"></a>简述</h5><p><center><br><img src="/2019/03/27/DeepLearning-papers/Figure1.png" width="300"><br></center><br>本论文将DenseNets扩展为FCNs，再加上上采样路径来恢复输入分辨率。在特征图上采样过程中，增加上采样通道无疑会增加计算量和参数个数，为了消除该影响，我们<code>仅在dense模块后增加上采样通道</code>，这使得每种分辨率的dense模块<code>上采样通道与池化层个数无关</code>，通过下采样和上采样间的跨层连接，高分辨率的信息得以传递。</p><p><strong>主要贡献：</strong><br>（1）改进DenseNet结构为FCN用于分割，同时缓解了feature map数量的激增。<br>（2）根据dense block提出的上采样结构，比普通的上采样方式效果好很多。<br>（3）该模型不需要预训练模型和后处理过程。</p><h4 id="《Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations》"><a href="#《Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations》" class="headerlink" title="《Network Dissection: Quantifying Interpretability of Deep Visual Representations》"></a>《Network Dissection: Quantifying Interpretability of Deep Visual Representations》</h4><h5 id="介绍链接：Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations-内附论文链接、代码链接"><a href="#介绍链接：Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations-内附论文链接、代码链接" class="headerlink" title="介绍链接：Network Dissection: Quantifying Interpretability of Deep Visual Representations (内附论文链接、代码链接)"></a>介绍链接：<a href="http://netdissect.csail.mit.edu/" target="_blank" rel="noopener">Network Dissection: Quantifying Interpretability of Deep Visual Representations</a> (内附论文链接、代码链接)</h5><h5 id="参考笔记-2"><a href="#参考笔记-2" class="headerlink" title="参考笔记"></a>参考笔记</h5><p><a href="https://blog.csdn.net/isMarvellous/article/details/75900055" target="_blank" rel="noopener">神经网络的可解释性——Network Dissection: Quantifying Interpretability of Deep Visual Representations</a>、<a href="https://www.jianshu.com/p/18861aaa77d4" target="_blank" rel="noopener">[cvpr]Network Dissection: Quantifying Interpretability of Deep Visual Representations</a>、<a href="https://www.zhihu.com/question/57523080/answer/159650943" target="_blank" rel="noopener">CVPR 2017 有什么值得关注的亮点?</a></p><h5 id="简述-2"><a href="#简述-2" class="headerlink" title="简述"></a>简述</h5><p><center><br><img src="/2019/03/27/DeepLearning-papers/Network Dissection.png" width="600"><br></center><br>今年这篇则是通过评估隐藏单元和一系列语义概念的契合度来给出网络的可解释性，提出了一个叫Network Dissection的方法。作者建立了一个带有不同语义概念的图片数据库Broden，里面每张图都有pixel-wise的标定(颜色，纹理，场景，物体部分，物体等)，也就是说对于每种语义概念，都有一张label map。<br>对于一个训练好的网络模型，输入Broden中的所有图片，然后收集某个单元在所有图片上的响应图。为了比较该响应图是对应于哪种语义概念，<code>把这些响应图插值放大到数据库原图大小后，做阈值处理</code>，相应大于某个值就设为1，否则为0，也就是我们只关注响应较大的区域，把这些区域作为该隐藏单元的语义表征，得到一个二值的mask。然后计算该mask和每一个真实语义概念label map的IoU，如果大于一定值，也就是和某个语义概念的重合率比较大，就认为该神经单元是对这个概念的检测器。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch_Run_Notice</title>
    <link href="http://yoursite.com/2019/03/17/PyTorch-Run-Notice/"/>
    <id>http://yoursite.com/2019/03/17/PyTorch-Run-Notice/</id>
    <published>2019-03-16T18:53:23.000Z</published>
    <updated>2019-03-20T15:31:17.013Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h4 id="使用多张GPU"><a href="#使用多张GPU" class="headerlink" title="使用多张GPU"></a>使用多张GPU</h4><p>方式一：（推荐）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">outputs = nn.parallel.data_parallel(model, inputs, device_ids=[<span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>方式二：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opt.device = t.device(<span class="string">'cuda:2,3'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">model=nn.DataParallel(model, device_ids=[<span class="number">2</span>, <span class="number">3</span>]) </span><br><span class="line">outputs = model(inputs)</span><br></pre></td></tr></table></figure><ul><li>可能会报错：<code>RuntimeError: all tensors must be on devices[0]</code></li></ul><h5 id="使用指定的GPU"><a href="#使用指定的GPU" class="headerlink" title="使用指定的GPU"></a>使用指定的GPU</h5><p>直接终端中设定：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=[2,3] python main.py</span><br></pre></td></tr></table></figure></p><p>or python代码中设定：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = [<span class="number">2</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure></p><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li>训练集（Train）与 验证集（Valid）<ul><li>有label，可计算loss、acc  （loss：概率  acc：实际统计）</li><li>对于小数据集，以8：2比例划分  （数据量大时，亦可7：3划分）</li><li>仅使用训练集时，进行数据增强、后向传播<code>backward</code>、优化超参数<code>optimize</code></li></ul></li><li>测试集（Test）<ul><li>无label</li></ul></li></ul><p>注：优化模型时，尝试交叉验证</p><h4 id="Grammar"><a href="#Grammar" class="headerlink" title="Grammar"></a>Grammar</h4><ul><li><code>Tensor.item() → int</code></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>PyTorch学习之路_CSDN</title>
    <link href="http://yoursite.com/2019/03/14/PyTorch%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF_CSDN/"/>
    <id>http://yoursite.com/2019/03/14/PyTorch学习之路_CSDN/</id>
    <published>2019-03-14T09:03:30.000Z</published>
    <updated>2019-03-16T17:30:00.350Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】转载于 <a href="https://blog.csdn.net/u014380165" target="_blank" rel="noopener">AI之路</a></p><a id="more"></a><h2 id="PyTorch学习之路（level1）——训练一个图像分类模型"><a href="#PyTorch学习之路（level1）——训练一个图像分类模型" class="headerlink" title="PyTorch学习之路（level1）——训练一个图像分类模型"></a><a href="https://blog.csdn.net/u014380165/article/details/78525273" target="_blank" rel="noopener">PyTorch学习之路（level1）——训练一个图像分类模型</a></h2><h3 id="数据导入部分"><a href="#数据导入部分" class="headerlink" title="数据导入部分"></a>数据导入部分</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_dir = <span class="string">'/data'</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(</span><br><span class="line">                    os.path.join(data_dir, x),</span><br><span class="line">                    data_transforms[x])， </span><br><span class="line">                    <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br></pre></td></tr></table></figure><h4 id="data-transforms"><a href="#data-transforms" class="headerlink" title="data_transforms"></a>data_transforms</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">'train'</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomSizedCrop(<span class="number">224</span>),  <span class="comment"># 输入对象是PIL Image</span></span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">'val'</span>: transforms.Compose([</span><br><span class="line">        transforms.Scale(<span class="number">256</span>),  <span class="comment"># 目前已经被transforms.Resize类取代</span></span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="torch-utils-data-DataLoader"><a href="#torch-utils-data-DataLoader" class="headerlink" title="torch.utils.data.DataLoader"></a>torch.utils.data.DataLoader</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataloders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">4</span>) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br></pre></td></tr></table></figure><p><code>torchvision.datasets.ImageFolder</code>仅返回list，list是不能作为模型输入的，因此在PyTorch中需要用另一个类来封装list，那就是：torch.utils.data.DataLoader。（list → Tensor）</p><h4 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h4><ul><li>这是一个抽象类，在pytorch中所有和数据相关的类（例如，torchvision.datasets.ImageFolder、torch.util.data.DataLoader）都要继承这个类来实现。</li><li>当你的数据不是按照一个类别一个文件夹这种方式存储时，你就要自定义一个类来读取数据，自定义的这个类必须继承自torch.utils.data.Dataset这个基类，最后同样用torch.utils.data.DataLoader封装成Tensor。</li></ul><h5 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h5><p>将Tensor数据类型封装成Variable数据类型，便可以作为模型的输入了 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloders[<span class="string">'train'</span>]:  <span class="comment"># type(dataloaders): Dictionary</span></span><br><span class="line">   inputs, labels = data   <span class="comment"># type(input) / type(labels): Tensor</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> use_gpu:</span><br><span class="line">       inputs = Variable(inputs.cuda())</span><br><span class="line">       labels = Variable(labels.cuda())</span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">       inputs, labels = Variable(inputs), Variable(labels)</span><br></pre></td></tr></table></figure></p><ul><li>Tensor + gradient_Info → Variable</li><li><p>Variable.data → Tensor</p><ul><li>Tensor: torch.tensor</li><li>Variable: torch.autograd.Variable</li></ul></li></ul><h3 id="导入模型"><a href="#导入模型" class="headerlink" title="导入模型"></a>导入模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = models.resnet18(pretrained=<span class="keyword">True</span>)  <span class="comment"># 加载噫预训练的模型参数</span></span><br><span class="line">num_ftrs = model.fc.in_features  <span class="comment"># 获取全连接层的输入channel个数</span></span><br><span class="line">model.fc = nn.Linear(num_ftrs, <span class="number">2</span>) <span class="comment"># 替换最后的全连接层为你所需要的输出</span></span><br></pre></td></tr></table></figure><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>使用交叉熵函数 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure></p><h4 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h4><p>Adam的优化方式 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure></p><h3 id="定义学习率的变化策略"><a href="#定义学习率的变化策略" class="headerlink" title="定义学习率的变化策略"></a>定义学习率的变化策略</h3><p>使用torch.optim.lr_scheduler模块的StepLR类 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># 每隔step_size个epoch就将学习率降为原来的gamma倍</span></span><br><span class="line">scheduler = lr_scheduler.StepLR(optimizer, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在每个epoch开始时都要更新学习率（according to 学习率的变化策略）</span></span><br><span class="line">scheduler.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置模型状态为训练状态</span></span><br><span class="line">model.train(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将网络中的所有梯度置0</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络的前向传播</span></span><br><span class="line">outputs = model(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输出的outputs和原来导入的labels作为loss函数的输入就可以得到损失</span></span><br><span class="line">loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get 模型预测该样本属于哪个类别的信息</span></span><br><span class="line">_, preds = torch.max(outputs.data, <span class="number">1</span>)  <span class="comment"># 第二个参数1是代表dim的意思，也就是取每一行的最大值，其实就是我们常见的取概率最大的那个index</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回传损失</span></span><br><span class="line">loss.backward()  <span class="comment"># 注意: 这是在训练的时候才会有的操作，测试时候只有forward过程</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据回传过程中计算得到的梯度更新参数</span></span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="comment"># 查看各个层的梯度和权值信息</span></span><br><span class="line"><span class="comment"># optimizer.param_groups[0][‘params’]</span></span><br></pre></td></tr></table></figure><p>完整代码：<a href="https://github.com/miraclewkf/ImageClassification-PyTorch/blob/master/level1/train.py" target="_blank" rel="noopener">ImageClassification-PyTorch</a></p><h2 id="PyTorch学习之路（level2）——自定义数据读取"><a href="#PyTorch学习之路（level2）——自定义数据读取" class="headerlink" title="PyTorch学习之路（level2）——自定义数据读取"></a><a href="https://blog.csdn.net/u014380165/article/details/78634829" target="_blank" rel="noopener">PyTorch学习之路（level2）——自定义数据读取</a></h2><h3 id="ImageFolder"><a href="#ImageFolder" class="headerlink" title="ImageFolder"></a>ImageFolder</h3><h4 id="初始化init"><a href="#初始化init" class="headerlink" title="初始化init"></a>初始化<strong>init</strong></h4><ul><li><p><code>__init__</code>方法</p>  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transform=None, target_transform=None,loader=default_loader)</span></span></span><br></pre></td></tr></table></figure></li></ul><ol><li><p>通过find_classes函数得到分类的类别名（classes）和类别名与数字类别的映射关系字典（class_to_idx）</p> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classes, class_to_idx = find_classes(root)</span><br></pre></td></tr></table></figure></li><li><p>通过make_dataset函数得到imags，这个imags是一个列表，其中每个值是一个tuple，每个tuple包含两个元素：图像路径和标签。</p> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">imgs = make_dataset(root, class_to_idx)</span><br><span class="line"><span class="keyword">if</span> len(imgs) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span>(RuntimeError(<span class="string">"Found 0 images in subfolders of: "</span> + root + <span class="string">"\n"</span></span><br><span class="line">                           <span class="string">"Supported image extensions are: "</span> + <span class="string">","</span>.join(IMG_EXTENSIONS)))</span><br></pre></td></tr></table></figure></li><li><p>剩下的就是一些赋值操作了</p> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.root = root</span><br><span class="line">self.imgs = imgs</span><br><span class="line">self.classes = classes</span><br><span class="line">self.class_to_idx = class_to_idx</span><br><span class="line">self.transform = transform</span><br><span class="line">self.target_transform = target_transform</span><br><span class="line">self.loader = loader</span><br></pre></td></tr></table></figure></li></ol><h4 id="获取图像getitem"><a href="#获取图像getitem" class="headerlink" title="获取图像getitem"></a>获取图像<strong>getitem</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">    </span><br><span class="line">    path, target = self.imgs[index]</span><br><span class="line">        img = self.loader(path)   <span class="comment"># 重点  #  self.loader = default_loader 👇 有解析</span></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> img, target</span><br></pre></td></tr></table></figure><h5 id="default-loader函数"><a href="#default-loader函数" class="headerlink" title="default_loader函数"></a>default_loader函数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">default_loader</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> torchvision <span class="keyword">import</span> get_image_backend</span><br><span class="line">    <span class="keyword">if</span> get_image_backend() == <span class="string">'accimage'</span>:</span><br><span class="line">        <span class="keyword">return</span> accimage_loader(path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> pil_loader(path)</span><br></pre></td></tr></table></figure><h6 id="pil-loader方法"><a href="#pil-loader方法" class="headerlink" title="pil_loader方法"></a>pil_loader方法</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pil_loader</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(path, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">with</span> Image.open(f) <span class="keyword">as</span> img:</span><br><span class="line">            <span class="keyword">return</span> img.convert(<span class="string">'RGB'</span>)</span><br></pre></td></tr></table></figure><h6 id="accimage-loader方法"><a href="#accimage-loader方法" class="headerlink" title="accimage_loader方法"></a>accimage_loader方法</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accimage_loader</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> accimage</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> accimage.Image(path)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        <span class="comment"># Potentially a decoding problem, fall back to PIL.Image</span></span><br><span class="line">        <span class="keyword">return</span> pil_loader(path)</span><br></pre></td></tr></table></figure><h4 id="数据集数量len"><a href="#数据集数量len" class="headerlink" title="数据集数量len"></a>数据集数量<strong>len</strong></h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure><h3 id="自定义数据读取接口"><a href="#自定义数据读取接口" class="headerlink" title="自定义数据读取接口"></a>自定义数据读取接口</h3><p>思路：</p><ol><li>在PyTorch中和数据读取相关的类基本都要继承一个基类：<code>torch.utils.data.Dataset</code></li><li>改写其中的<code>__init__</code>、<code>__len__</code>、<code>__getitem__</code>等方法即可</li></ol><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">img_path: [&apos;train&apos;, &apos;val&apos;]</span><br><span class="line">txt_path: [&apos;train.txt&apos;, &apos;val.txt&apos;]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">customData</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_path, txt_path, dataset = <span class="string">''</span>, data_transforms=None, loader = default_loader)</span>:</span></span><br><span class="line">    <span class="comment"># self.img_name和self.img_label的读取方式就跟你数据的存放方式有关，你可以根据你实际数据的维护方式做调整</span></span><br><span class="line">        <span class="keyword">with</span> open(txt_path) <span class="keyword">as</span> input_file:</span><br><span class="line">            lines = input_file.readlines()</span><br><span class="line">            self.img_name = [os.path.join(img_path, line.strip().split(<span class="string">'\t'</span>)[<span class="number">0</span>]) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">            self.img_label = [int(line.strip().split(<span class="string">'\t'</span>)[<span class="number">-1</span>]) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">            </span><br><span class="line">        self.data_transforms = data_transforms</span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.loader = loader</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.img_name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        img_name = self.img_name[item]</span><br><span class="line">        label = self.img_label[item]</span><br><span class="line">        img = self.loader(img_name)     <span class="comment"># 采用default_loader方法来读取图像</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.data_transforms <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># 在Transform中将每张图像都封装成Tensor</span></span><br><span class="line">                img = self.data_transforms[self.dataset](img)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                print(<span class="string">"Cannot transform image: &#123;&#125;"</span>.format(img_name))</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br></pre></td></tr></table></figure><p>数据读取接口的使用 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># call</span></span><br><span class="line">image_datasets = &#123;x: customData(img_path=<span class="string">'/ImagePath'</span>,</span><br><span class="line">                                    txt_path=(<span class="string">'/TxtFile/'</span> + x + <span class="string">'.txt'</span>),</span><br><span class="line">                                    data_transforms=data_transforms,</span><br><span class="line">                                    dataset=x) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br><span class="line">     </span><br><span class="line"><span class="comment"># DataLoader (list → Tensor)，将这个batch的图像数据和标签都分别封装成Tensor</span></span><br><span class="line">dataloders = &#123;x: torch.utils.data.DataLoader(image_datasets[x],</span><br><span class="line">                                                 batch_size=batch_size,</span><br><span class="line">                                                 shuffle=<span class="keyword">True</span>) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]&#125;</span><br></pre></td></tr></table></figure></p><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model, <span class="string">'output/resnet_epoch&#123;&#125;.pkl'</span>.format(epoch)) <span class="comment"># 如果这个output文件夹没有，可以手动新建一个或者在代码里面新建</span></span><br></pre></td></tr></table></figure><p>完整代码：<a href="https://github.com/miraclewkf/ImageClassification-PyTorch/blob/master/level2/train_customData.py" target="_blank" rel="noopener">train_customData.py</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】转载于 &lt;a href=&quot;https://blog.csdn.net/u014380165&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AI之路&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>面经总结</title>
    <link href="http://yoursite.com/2019/03/13/%E9%9D%A2%E7%BB%8F%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2019/03/13/面经总结/</id>
    <published>2019-03-13T12:56:45.000Z</published>
    <updated>2019-03-13T12:59:17.618Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><p><a href="https://blog.csdn.net/zongza/article/details/80167654" target="_blank" rel="noopener">【置顶】面试知识点</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>DeepLiver_model_note</title>
    <link href="http://yoursite.com/2019/03/13/DeepLiver-model-note/"/>
    <id>http://yoursite.com/2019/03/13/DeepLiver-model-note/</id>
    <published>2019-03-13T12:47:58.000Z</published>
    <updated>2019-03-20T05:41:04.035Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="Method-Description"><a href="#Method-Description" class="headerlink" title="Method Description"></a>Method Description</h3><h4 id="simpleitk"><a href="#simpleitk" class="headerlink" title="simpleitk"></a>simpleitk</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> SimpleITK <span class="keyword">as</span> sitk</span><br><span class="line"></span><br><span class="line">writer = sitk.ImageFileWriter()</span><br><span class="line">writer.SetFileName(target_file)</span><br><span class="line">writer.Execute(image)</span><br></pre></td></tr></table></figure><p><code>sitk.ReadImage(name)</code> 适用范围：…</p><h4 id="argparse"><a href="#argparse" class="headerlink" title="argparse"></a><a href="http://wiki.jikexueyuan.com/project/explore-python/Standard-Modules/argparse.html" target="_blank" rel="noopener">argparse</a></h4><ol><li>创建 <code>ArgumentParser()</code> 对象</li><li>调用 <code>add_argument()</code> 方法添加参数</li><li>使用 <code>parse_args()</code> 解析添加的参数</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="comment"># 创建 ArgumentParser() 对象</span></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'U-Net 2d'</span>) </span><br><span class="line"><span class="comment"># 调用 add_argument() 方法添加参数</span></span><br><span class="line">parser.add_argument(<span class="string">'--resume'</span>, <span class="string">'-m'</span>, default=<span class="string">''</span>, metavar=<span class="string">'RESUME'</span>,help=<span class="string">'model parameters to load'</span>)   <span class="comment"># 可选参数</span></span><br><span class="line">parser.add_argument(<span class="string">'--save_dir'</span>, default=<span class="string">''</span>, type=str, metavar=<span class="string">'PATH'</span>,help=<span class="string">'path to save checkpoint files'</span>) <span class="comment"># 可选参数</span></span><br><span class="line">parser.add_argument(<span class="string">'--test'</span>, default=<span class="number">0</span>, type=int, metavar=<span class="string">'TEST'</span>,help=<span class="string">'1 do test evaluation, 0 not'</span>) <span class="comment"># 可选参数</span></span><br><span class="line"><span class="comment"># 使用 parse_args() 解析添加的参数</span></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> args.resume</span><br></pre></td></tr></table></figure><ul><li>metavar - 在 usage 说明中的参数名称，对于必选参数默认就是参数名称，对于可选参数默认是全大写的参数名称.</li></ul><h4 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, out, seg)</span>:</span></span><br><span class="line">    b, w, h = seg.shape</span><br><span class="line">    seg = seg.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    seg_one_hot = Variable(torch.FloatTensor(b,<span class="number">2</span>, w, h)).zero_().cuda()</span><br><span class="line">    seg = seg_one_hot.scatter_(<span class="number">1</span>, seg, <span class="number">1</span>)</span><br><span class="line">    loss = Variable(torch.FloatTensor(b)).zero_().cuda()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">2</span>):</span><br><span class="line">        loss += (<span class="number">1</span> - <span class="number">2.</span>*((out[:,i]*seg[:,i]).sum(<span class="number">1</span>).sum(<span class="number">1</span>)) / ((out[:,i]*out[:,i]).sum(<span class="number">1</span>).sum(<span class="number">1</span>)+(seg[:,i]*seg[:,i]).sum(<span class="number">1</span>).sum(<span class="number">1</span>)+<span class="number">1e-15</span>))</span><br><span class="line">    loss = loss.mean()</span><br><span class="line">    <span class="keyword">del</span> seg_one_hot, seg</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval，<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train</span></span><br><span class="line">Model.train(mode=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">Model.eval() <span class="comment"># eval（）时，框架会自动把BN和DropOut固定住，不会取平均，而是用训练好的值，不然的话，一旦test的batch_size过小，很容易就会被BN层导致生成图片颜色失真极大！！！！！！</span></span><br></pre></td></tr></table></figure></p><h3 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h3><p>写nrrd文件的时候，可以考虑nrrd的数组存储形式与正常数组维度不一致<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">arr = np.squeeze(arr) <span class="comment"># 从数组的形状中删除单维度条目，即把shape中为1的维度去掉</span></span><br><span class="line">y=np.transpose(y,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))  <span class="comment"># 将数组的轴交换 (0, 1, 2) =&gt; (1, 2, 0)</span></span><br></pre></td></tr></table></figure></p><h3 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h3><h4 id="数据处理过程"><a href="#数据处理过程" class="headerlink" title="数据处理过程"></a>数据处理过程</h4><p>取最大连通域 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_connected_domain_3D</span><span class="params">(arr)</span>:</span></span><br><span class="line">    labels = measure.label(arr)  <span class="comment"># &lt;1.2s</span></span><br><span class="line">    t = np.bincount(labels.flatten())[<span class="number">1</span>:]  <span class="comment"># &lt;1.5s</span></span><br><span class="line">    max_pixel = np.argmax(t) + <span class="number">1</span>  <span class="comment"># 位置变了,去除了0</span></span><br><span class="line">    labels[labels != max_pixel] = <span class="number">0</span></span><br><span class="line">    labels[labels == max_pixel] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> labels.astype(np.uint8)</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> measure</span><br><span class="line"></span><br><span class="line">arr = np.asarray([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">print(arr)</span><br><span class="line">print(max_connected_domain_3D(arr))</span><br></pre></td></tr></table></figure><p>$$\begin{bmatrix}1 & 1 & 0 & 3\\ 1 & 0 & 3 & 3\\ 0 & 1 & 3 & 3\\0 & 0 & 0 & 0\end{bmatrix}\Rightarrow \begin{bmatrix}0 & 0 & 0 & 1\\ 0 & 0 & 1 & 1\\ 0 & 0 & 1 & 1\\0 & 0 & 0 & 0\end{bmatrix}$$</p><p>归一化 👇<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(img)</span>:</span>  </span><br><span class="line">    img = np.clip(img, <span class="number">-150</span>, <span class="number">250</span>)</span><br><span class="line">    min_nrrd_data = np.min(img)</span><br><span class="line">    max_nrrd_data = np.max(img)</span><br><span class="line">    img = (img - min_nrrd_data) / (max_nrrd_data - min_nrrd_data)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure></p><h4 id="数组阈值处理"><a href="#数组阈值处理" class="headerlink" title="数组阈值处理"></a>数组阈值处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">img 为图像数组，同时也是numpy数组</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">img[np.where(img &lt; min)] = min  </span><br><span class="line">img[np.where(img &gt; <span class="number">250</span>)] = max</span><br></pre></td></tr></table></figure><h4 id="绘制模型"><a href="#绘制模型" class="headerlink" title="绘制模型"></a>绘制模型</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> pip install graphviz</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"></span><br><span class="line">plot_model(model, <span class="string">"RUnet.png"</span>, <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="Common-Operation"><a href="#Common-Operation" class="headerlink" title="Common Operation"></a>Common Operation</h2><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><h4 id="Linux查看Nvidia显卡信息及使用情况"><a href="#Linux查看Nvidia显卡信息及使用情况" class="headerlink" title="Linux查看Nvidia显卡信息及使用情况"></a><a href="https://blog.csdn.net/dcrmg/article/details/78146797" target="_blank" rel="noopener">Linux查看Nvidia显卡信息及使用情况</a></h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> nvidia-smi</span><br></pre></td></tr></table></figure><p><center><br><img src="/2019/03/13/DeepLiver-model-note/nvidia-smi.png" width="600"><br></center><br>表头释义 👇<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- Fan：显示风扇转速，数值在0到100%之间，是计算机的期望转速，如果计算机不是通过风扇冷却或者风扇坏了，显示出来就是N/A； </span><br><span class="line">- Temp：显卡内部的温度，单位是摄氏度；</span><br><span class="line">- Perf：表征性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能；</span><br><span class="line">- Pwr：能耗表示； </span><br><span class="line">- Bus-Id：涉及GPU总线的相关信息； </span><br><span class="line">- Disp.A：是Display Active的意思，表示GPU的显示是否初始化； </span><br><span class="line">- Memory Usage：显存的使用率； </span><br><span class="line">- Volatile GPU-Util：浮动的GPU利用率；</span><br><span class="line">- Compute M：计算模式；</span><br><span class="line"></span><br><span class="line">- Processes显示每块GPU上每个进程所使用的显存情况。</span><br></pre></td></tr></table></figure></p><h4 id="判断torch是否可用GPU"><a href="#判断torch是否可用GPU" class="headerlink" title="判断torch是否可用GPU"></a>判断torch是否可用GPU</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Python <span class="number">3.6</span><span class="number">.8</span> |Anaconda, Inc.| (default, Dec <span class="number">30</span> <span class="number">2018</span>, <span class="number">01</span>:<span class="number">22</span>:<span class="number">34</span>)</span><br><span class="line">[GCC <span class="number">7.3</span><span class="number">.0</span>] on linux</span><br><span class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="keyword">or</span> <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><h4 id="多GPU的使用"><a href="#多GPU的使用" class="headerlink" title="多GPU的使用"></a>多GPU的使用</h4><p>PyTorch支持多GPU训练模型，假设你的网络是model，那么只需要下面一行代码（调用 torch.nn.DataParallel接口）就可以让后续的模型训练在0和1两块GPU上训练，加快训练速度。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = torch.nn.DataParallel(model, device_ids=[<span class="number">0</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure></p><h2 id="优秀链接"><a href="#优秀链接" class="headerlink" title="优秀链接"></a>优秀链接</h2><p><a href="https://zhuanlan.zhihu.com/p/57958993?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=539484443807711232&amp;from=singlemessage&amp;s_r=0" target="_blank" rel="noopener">pytorch + apex 生活变得更美好</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>vtk-Introduction</title>
    <link href="http://yoursite.com/2019/03/08/vtk-Introduction/"/>
    <id>http://yoursite.com/2019/03/08/vtk-Introduction/</id>
    <published>2019-03-08T11:04:27.000Z</published>
    <updated>2019-03-08T16:53:42.927Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】To be continued…</p><a id="more"></a><p>参考：<a href="https://lorensen.github.io/VTKExamples/site/Python/" target="_blank" rel="noopener">VTKExamples</a>、<a href="https://vtk.org/documentation/" target="_blank" rel="noopener">User’s Guide</a>、<a href="https://www.cnblogs.com/zhhfan/p/10312170.html" target="_blank" rel="noopener">Python vtk学习</a></p><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install vtk</span><br></pre></td></tr></table></figure><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><center><br><img src="/2019/03/08/vtk-Introduction/vtk_example.png" width="500"><br></center><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> vtk</span><br><span class="line"></span><br><span class="line"><span class="comment"># 箭头源</span></span><br><span class="line">arrow_source = vtk.vtkArrowSource()</span><br><span class="line"><span class="comment"># 映射器</span></span><br><span class="line">mapper = vtk.vtkPolyDataMapper()</span><br><span class="line"><span class="comment"># 映射器添加数据源</span></span><br><span class="line">mapper.SetInputConnection(arrow_source.GetOutputPort())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 演员（执行者）</span></span><br><span class="line">actor = vtk.vtkActor()</span><br><span class="line"><span class="comment"># 演员添加映射器</span></span><br><span class="line">actor.SetMapper(mapper)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 渲染器</span></span><br><span class="line">ren = vtk.vtkRenderer()</span><br><span class="line"><span class="comment"># 渲染器添加演员</span></span><br><span class="line">ren.AddActor(actor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制窗口</span></span><br><span class="line">renWin = vtk.vtkRenderWindow()</span><br><span class="line"><span class="comment"># 绘制窗口添加渲染器</span></span><br><span class="line">renWin.AddRenderer(ren)</span><br><span class="line"><span class="comment"># 窗口读取渲染器生成的图形</span></span><br><span class="line">renWin.Render()   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建窗口交互器</span></span><br><span class="line">iren = vtk.vtkRenderWindowInteractor()</span><br><span class="line">iren.SetRenderWindow(renWin)</span><br><span class="line">iren.Initialize()</span><br><span class="line">iren.Start()</span><br></pre></td></tr></table></figure><h3 id="文件读取"><a href="#文件读取" class="headerlink" title="文件读取"></a>文件读取</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    reader = vtk.vtkSTLReader()  <span class="comment"># 读取STL文件</span></span><br><span class="line">    reader.SetFileName(file_name)</span><br><span class="line">    <span class="keyword">return</span> reader</span><br></pre></td></tr></table></figure><table><thead><tr><th style="text-align:center">文件类型</th><th style="text-align:center">读取方法</th></tr></thead><tbody><tr><td style="text-align:center">STL</td><td style="text-align:center">vtkSTLReader()</td></tr><tr><td style="text-align:center">SLC</td><td style="text-align:center">vtkSLCReader()</td></tr><tr><td style="text-align:center">VTP</td><td style="text-align:center">vtkXMLPolyDataReader()</td></tr><tr><td style="text-align:center">UnstructuredGrid</td><td style="text-align:center">vtkNamedColors()</td></tr><tr><td style="text-align:center">ExodusData</td><td style="text-align:center">vtkExodusIIReader()</td></tr></tbody></table><h3 id="图像旋转"><a href="#图像旋转" class="headerlink" title="图像旋转"></a>图像旋转</h3><p>transform.RotateWXYZ(90, 0, 0, 1) 👇 横置→竖置</p><center><br><img src="/2019/03/08/vtk-Introduction/transform.RotateWXYZ(90, 0, 0, 1).png" width="500"><br></center><p>transform.RotateWXYZ(90, 1, 0, 1) 👇 正方形对角线</p><center><br><img src="/2019/03/08/vtk-Introduction/transform.RotateWXYZ(90, 1, 0, 1).png" width="500"><br></center><p>transform.RotateWXYZ(90, 1, 1, 1) 👇 立方体对角线 </p><center><br><img src="/2019/03/08/vtk-Introduction/transform.RotateWXYZ(90, 1, 1, 1).png" width="500"><br></center><p>代码详情 👇</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> vtk</span><br><span class="line"></span><br><span class="line">arrow_source = vtk.vtkArrowSource()</span><br><span class="line">mapper = vtk.vtkPolyDataMapper()</span><br><span class="line"></span><br><span class="line"><span class="comment"># transform</span></span><br><span class="line">transform = vtk.vtkTransform()</span><br><span class="line"><span class="comment"># transform.RotateWXYZ(angle, x, y, z)  # x,y,z旋转(0,1)表示是否旋转</span></span><br><span class="line">transform.RotateWXYZ(<span class="number">90</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>) <span class="comment"># (90, 1, 0, 1) / (90, 1, 1, 1)  </span></span><br><span class="line">transformFilter = vtk.vtkTransformPolyDataFilter()</span><br><span class="line">transformFilter.SetTransform(transform)</span><br><span class="line">transformFilter.SetInputConnection(arrow_source.GetOutputPort())</span><br><span class="line">transformFilter.Update()</span><br><span class="line">mapper.SetInputConnection(transformFilter.GetOutputPort())</span><br><span class="line"></span><br><span class="line">actor = vtk.vtkActor()</span><br><span class="line">actor.SetMapper(mapper)</span><br><span class="line">ren = vtk.vtkRenderer()</span><br><span class="line">ren.AddActor(actor)</span><br><span class="line">renWin = vtk.vtkRenderWindow()</span><br><span class="line">renWin.AddRenderer(ren)</span><br><span class="line">renWin.Render()</span><br><span class="line">iren = vtk.vtkRenderWindowInteractor()</span><br><span class="line">iren.SetRenderWindow(renWin)</span><br><span class="line">iren.Initialize()</span><br><span class="line">renWin.Render()</span><br><span class="line">iren.Start()</span><br></pre></td></tr></table></figure><ul><li>缩放</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_scale</span><span class="params">(x, y, z)</span>:</span></span><br><span class="line">    actor.SetScale(x, y, z)</span><br></pre></td></tr></table></figure><ul><li>平移</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_position</span><span class="params">(x, y, z)</span>:</span></span><br><span class="line">    actor.AddPosition(x, y, z)</span><br></pre></td></tr></table></figure><h3 id="平面切割"><a href="#平面切割" class="headerlink" title="平面切割"></a>平面切割</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    reader = read_data(file_name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义切割平面</span></span><br><span class="line">    clipPlane = vtk.vtkPlane()</span><br><span class="line">    clipPlane.SetNormal(<span class="number">1.0</span>, <span class="number">-1.0</span>, <span class="number">-1.0</span>)</span><br><span class="line">    clipPlane.SetOrigin(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 平面切割三维数据</span></span><br><span class="line">    clipper = vtk.vtkClipPolyData()</span><br><span class="line">    clipper.SetInputConnection(reader.GetOutputPort())</span><br><span class="line">    clipper.SetClipFunction(clipPlane)</span><br><span class="line">    clipper.InsideOutOn()  <span class="comment"># ?</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义mapper和actor</span></span><br><span class="line">    superMapper = vtk.vtkPolyDataMapper()</span><br><span class="line">    superMapper.SetInputConnection(clipper.GetOutputPort())</span><br><span class="line">    superActor = vtk.vtkActor()</span><br><span class="line">    <span class="comment"># 设置偏转角度</span></span><br><span class="line">    set_origin(superActor, <span class="number">-50</span>, <span class="number">-75</span>, <span class="number">120</span>)</span><br><span class="line">    superActor.SetMapper(superMapper)</span><br><span class="line">    superActor.GetProperty().SetColor(colors.GetColor3d(<span class="string">"Cyan"</span>))</span><br><span class="line">    only_show(superActor)</span><br></pre></td></tr></table></figure><ul><li>设置演员初始方向</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_origin</span><span class="params">(actor, x, y, z)</span>:</span></span><br><span class="line">    actor.SetOrientation(x, y, z)</span><br></pre></td></tr></table></figure><h3 id="鼠标事件监听"><a href="#鼠标事件监听" class="headerlink" title="鼠标事件监听"></a>鼠标事件监听</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 监听事件</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyEvent</span><span class="params">(vtk.vtkInteractorStyleTrackballCamera)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, parent=None)</span>:</span></span><br><span class="line">        self.AddObserver(<span class="string">"MiddleButtonPressEvent"</span>, self.middle_button_press_event)</span><br><span class="line">        self.AddObserver(<span class="string">"MiddleButtonReleaseEvent"</span>, self.middle_button_release_event)</span><br><span class="line">        self.AddObserver(<span class="string">"LeftButtonPressEvent"</span>, self.left_button_press_event)</span><br><span class="line">        self.AddObserver(<span class="string">"LeftButtonReleaseEvent"</span>, self.left_button_release_event)</span><br><span class="line">        self.AddObserver(<span class="string">"RightButtonPressEvent"</span>, self.right_button_press_event)</span><br><span class="line">        self.AddObserver(<span class="string">"RightButtonReleaseEvent"</span>, self.right_button_release_event)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle_button_press_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"Middle Button pressed"</span>)</span><br><span class="line">        self.OnMiddleButtonDown()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle_button_release_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"Middle Button released"</span>)</span><br><span class="line">        self.OnMiddleButtonUp()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">left_button_press_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"Left Button pressed"</span>)</span><br><span class="line">        self.OnLeftButtonDown()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">left_button_release_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"Left Button released"</span>)</span><br><span class="line">        self.OnLeftButtonUp()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">right_button_press_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"right Button pressed"</span>)</span><br><span class="line">        self.OnRightButtonDown()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">right_button_release_event</span><span class="params">(self, obj, event)</span>:</span></span><br><span class="line">        print(<span class="string">"right Button released"</span>)</span><br><span class="line">        self.OnLeftButtonUp()</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 引入上一段代码调用</span></span><br><span class="line">iren.SetInteractorStyle(MyEvent())</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】To be continued…&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="vtk" scheme="http://yoursite.com/tags/vtk/"/>
    
  </entry>
  
  <entry>
    <title>PyQt5_2_exe/app</title>
    <link href="http://yoursite.com/2019/03/04/PyQt5-2-exe/"/>
    <id>http://yoursite.com/2019/03/04/PyQt5-2-exe/</id>
    <published>2019-03-03T17:00:09.000Z</published>
    <updated>2019-03-04T07:33:36.281Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】Not Completed</p><a id="more"></a><h2 id="PyQt5打包程序为可执行文件"><a href="#PyQt5打包程序为可执行文件" class="headerlink" title="PyQt5打包程序为可执行文件"></a>PyQt5打包程序为可执行文件</h2><ol><li><p><code>$ pyinstaller -Fw window.py</code></p><ul><li><p>Requirements：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install -U pip setuptools / pip install --upgrade setuptools</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install tornado</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install IPython</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install ipykernel</span></span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash"> No module named <span class="string">'wx'</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install -U  -f https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-16.04 \ wxPython </span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cairo backend requires that cairocffi or pycairo is installed</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install cairocffi</span></span><br></pre></td></tr></table></figure></li><li><p>Some INFO</p><pre><code class="powershell"><span class="number">117</span> INFO: UPX is not available.<span class="number">38433</span> INFO:   Matplotlib backend <span class="string">"MacOSX"</span>: ignoredPython is not installed as a framework. The Mac OS X backend will not be able to <span class="keyword">function</span> correctly <span class="keyword">if</span> Python is not installed as a framework. See the Python documentation <span class="keyword">for</span> more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or <span class="keyword">try</span> one of the other backends. <span class="keyword">If</span> you are using (Ana)Conda please install python.app and replace the use of <span class="string">'python'</span> with <span class="string">'pythonw'</span>. See <span class="string">'Working with Matplotlib on OSX'</span> <span class="keyword">in</span> the Matplotlib FAQ <span class="keyword">for</span> more information.<span class="number">51301</span> WARNING: library user32 required via ctypes not found<span class="number">53437</span> INFO: Warnings written to /Users/Captain/Desktop/client/build/window/warn-window.txt</code></pre></li><li>Output：<code>./build</code>、 <code>./dist</code>、 <code>./window.spec</code></li></ul></li><li><code>$ pyinstaller window.spec</code><ul><li>Output：<code>window.app</code> / <code>window.exe</code>    </li></ul></li><li>将与程序关联的代码（当前文件夹中除./build和./dist外的所有文件）均copy至<code>./dist</code>，即可</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】Not Completed&lt;/p&gt;
    
    </summary>
    
    
      <category term="PyQt5" scheme="http://yoursite.com/tags/PyQt5/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch_可视化之TensorboardX</title>
    <link href="http://yoursite.com/2019/03/03/PyTorch-%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BTensorboardX/"/>
    <id>http://yoursite.com/2019/03/03/PyTorch-可视化之TensorboardX/</id>
    <published>2019-03-03T04:33:11.000Z</published>
    <updated>2019-03-03T06:36:49.416Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h2><p>链接：<a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-1-tensorboard1/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/4-1-tensorboard1/</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(tensorflow) ➜  Morvan_Tensorflow tensorboard --logdir logs</span><br><span class="line">TensorBoard 1.11.0 at http://MacBook-Pro:6006 (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure><ul><li>Chrome</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">http://0.0.0.0:6006</span><br></pre></td></tr></table></figure><h2 id="TensorboardX"><a href="#TensorboardX" class="headerlink" title="TensorboardX"></a>TensorboardX</h2><p>详细内容，访问文档：<a href="https://tensorboardx.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">https://tensorboardx.readthedocs.io/en/latest/index.html</a><br>源代码：<a href="https://github.com/lanpa/tensorboardX" target="_blank" rel="noopener">https://github.com/lanpa/tensorboardX</a><br>参考：<a href="https://www.pytorchtutorial.com/pytorch-tensorboardx/" target="_blank" rel="noopener">https://www.pytorchtutorial.com/pytorch-tensorboardx/</a></p><h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install tensorboardX</span><br></pre></td></tr></table></figure><h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#-*-coding:utf-8-*-</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_data = Variable(torch.rand(<span class="number">16</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 从torchvision中导入已有模型</span></span><br><span class="line">model = torchvision.models.resnet18()</span><br><span class="line"><span class="comment"># print(model)</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 声明writer对象，保存的文件夹，异己名称</span></span><br><span class="line">writer = SummaryWriter(log_dir=\<span class="string">'./log\', comment=\'resnet18\')</span></span><br><span class="line"><span class="string">with writer:   # necessary</span></span><br><span class="line"><span class="string">    writer.add_graph(model, (input_data,))</span></span><br></pre></td></tr></table></figure><h3 id="View"><a href="#View" class="headerlink" title="View"></a>View</h3><p>在对应路径下运行tensorboard<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(pytorch) ➜  Desktop  tensorboard --logdir log</span><br><span class="line">TensorBoard 1.12.0 at http://MacBook-Pro:6006 (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure></p><h2 id="PyTorch-可视化工具-Visdom-介绍"><a href="#PyTorch-可视化工具-Visdom-介绍" class="headerlink" title="PyTorch 可视化工具 Visdom 介绍"></a>PyTorch 可视化工具 Visdom 介绍</h2><p>可参考 <a href="https://captainzj.github.io/2018/12/31/visdom-Tutorial/" target="_blank" rel="noopener">https://captainzj.github.io/2018/12/31/visdom-Tutorial/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="http://yoursite.com/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>NiftyNet-Process</title>
    <link href="http://yoursite.com/2019/03/02/NiftyNet-Process/"/>
    <id>http://yoursite.com/2019/03/02/NiftyNet-Process/</id>
    <published>2019-03-02T08:04:58.000Z</published>
    <updated>2019-07-02T06:24:28.049Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】<br><a id="more"></a></p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a><a href="https://www.cnblogs.com/zhhfan/p/10424489.html" target="_blank" rel="noopener">数据预处理</a></h2><h3 id="生成-csv文件"><a href="#生成-csv文件" class="headerlink" title="生成.csv文件"></a>生成.csv文件</h3><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">image</th><th style="text-align:center">path</th></tr></thead><tbody><tr><td style="text-align:center">img.csv</td><td style="text-align:center">img_name</td><td style="text-align:center">img_path</td></tr><tr><td style="text-align:center">label.csv</td><td style="text-align:center">img_label</td><td style="text-align:center">img_path</td></tr></tbody></table><ul><li>二分类的生成该文件的demo</li></ul><blockquote><p>准备工作：将两个类别的图片分别存储在两个文件夹中。</p></blockquote><p>下述代码中将分类的图片分别存储于<code>./DogsVSCats/train/cat</code> 和 <code>./DogsVSCats/train/dog</code><br>注: DogsVSCats datasets 可在<a href="https://www.kaggle.com/c/dogs-vs-cats/data" target="_blank" rel="noopener">此处</a>下载</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">dir_path = <span class="string">'./DogsVSCats/train'</span></span><br><span class="line">dir_names = os.listdir(dir_path)   <span class="comment"># ['cat', 'dog']</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 img.csv</span></span><br><span class="line">list_img, list_path = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> dir_name <span class="keyword">in</span> dir_names:</span><br><span class="line"></span><br><span class="line">    img_path = dir_path + <span class="string">"/"</span> + dir_name</span><br><span class="line">    img_name = os.listdir(img_path)  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, item <span class="keyword">in</span> enumerate(img_name):</span><br><span class="line">        list_img.append(item)</span><br><span class="line">        list_path.append(img_path + <span class="string">"/"</span> + item)</span><br><span class="line">        </span><br><span class="line">data_frame = pd.DataFrame(&#123;<span class="string">'image'</span>: list_img, <span class="string">'path'</span>: list_path&#125;)</span><br><span class="line">data_frame.to_csv(<span class="string">'./img_path.csv'</span>, index=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成label.csv</span></span><br><span class="line">list_label_name, list_label_path = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> dir_name <span class="keyword">in</span> dir_names:</span><br><span class="line"></span><br><span class="line">    label_path = dir_path + <span class="string">"/"</span> + dir_name</span><br><span class="line">    label_name = os.listdir(label_path)  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _, elem <span class="keyword">in</span> enumerate(label_name):</span><br><span class="line">        list_label_name.append(elem[<span class="number">0</span>:<span class="number">3</span>])  <span class="comment"># elem[0:3] : cat / dog</span></span><br><span class="line">        list_label_path.append(label_path + <span class="string">"/"</span> + elem)</span><br><span class="line"></span><br><span class="line">label_dataframe = pd.DataFrame(&#123;<span class="string">'label'</span>: list_label_name, <span class="string">'path'</span>: list_label_path&#125;)</span><br><span class="line">label_dataframe.to_csv(<span class="string">'./label.csv'</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Medical Imaging" scheme="http://yoursite.com/categories/Medical-Imaging/"/>
    
    
      <category term="NiftyNet" scheme="http://yoursite.com/tags/NiftyNet/"/>
    
  </entry>
  
  <entry>
    <title>NiftyNet-Configuration-file</title>
    <link href="http://yoursite.com/2019/03/02/NiftyNet-Configuration-file/"/>
    <id>http://yoursite.com/2019/03/02/NiftyNet-Configuration-file/</id>
    <published>2019-03-02T07:59:58.000Z</published>
    <updated>2019-07-02T06:26:19.997Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】<a href="https://www.cnblogs.com/zhhfan/p/9806900.html" target="_blank" rel="noopener">NiftyNet开源平台的使用 – 配置文件</a></p><a id="more"></a><p>官方文档：<a href="https://niftynet.readthedocs.io/en/latest/config_spec.html" target="_blank" rel="noopener">https://niftynet.readthedocs.io/en/latest/config_spec.html</a></p><h3 id="运行NiftyNet工作流"><a href="#运行NiftyNet工作流" class="headerlink" title="运行NiftyNet工作流"></a>运行NiftyNet工作流</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python net_run.py [train|inference|evaluation] -c &lt;path_to/config.ini&gt; -a &lt;application&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> net_run命令也支持命令行参数，以`--&lt;name&gt; &lt;value&gt;`或`--&lt;name&gt;=&lt;value&gt;`的形式表示。输入的参数将取代系统默认的和配置文件中的参数。</span></span><br></pre></td></tr></table></figure><ul><li><code>[train|inference|evaluation]</code><ul><li>train：使用提供的数据 更新 已存在的网络模型</li><li>inference：根据提供的数据生成响应（加载已存在的网络模型）</li><li>evaluation：？？</li></ul></li><li><code>-c &lt;path_to/config.ini&gt;</code>：指定配置文件路径</li><li><code>-a &lt;application&gt;</code>：指定应用种类（常见种类如下表👇）</li></ul><table><thead><tr><th style="text-align:left">application参数</th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left">图像分割 <small> [SEGMENTATION] </small></td><td style="text-align:left"><code>net_segment -c ...</code></td></tr><tr><td style="text-align:left">图像回归 <small> [REGRESSION] </small></td><td style="text-align:left"><code>net_regress -c ...</code></td></tr><tr><td style="text-align:left">自动编码 <small> [AUTOENCODER] </small></td><td style="text-align:left"><code>net_autoencoder -c ...</code></td></tr><tr><td style="text-align:left">生成对抗网络 <small> [GAN] </small></td><td style="text-align:left"><code>net_gan -c ...</code></td></tr></tbody></table><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>每个网络想要运行<strong>必须</strong>包含一个<code>config.ini</code>配置文件，用来设置训练/测试所用的全部参数。</p><h4 id="Configuration-File-Structure"><a href="#Configuration-File-Structure" class="headerlink" title="Configuration-File-Structure"></a>Configuration-File-Structure</h4><p>每个配置文件中包含的sections: </p><ul><li>[SYSTEM]  </li><li>[NETWORK]  </li><li>[APPLICATION] （custom👇）<ul><li><code>[GAN]</code> – 生成对抗网络  </li><li><code>[SEGMENTATION]</code> – 分割网络  </li><li><code>[REGRESSION]</code> – 回归网络  </li><li><code>[AUTOENCODER]</code> – 自动编码网络 </li></ul></li><li>[TRAINING]   （可选—when need train）</li><li>[INFERENCE]   （可选—when need inference）</li></ul><h4 id="Section-Arguments"><a href="#Section-Arguments" class="headerlink" title="Section_Arguments"></a>Section_Arguments</h4><h5 id="Input-data-source"><a href="#Input-data-source" class="headerlink" title="[Input data source]"></a>[Input data source]</h5><p><small></small></p><ul><li>csv_file:  输入图像路径  </li><li>path_to_search: 搜索图像的单个或多个文件，如果有多个用逗号分开  </li><li>filename_contains:  匹配文件名的关键词  </li><li>filename_not_contains:  排除文件名的关键词  </li><li>filename_removefromid:  从文件命中抽取主题id的正则表达式，被匹配的模式将从文件名中移除并生成主题id。  </li><li>interp_order:  插值法，当设定采样方法为resize时，需要该参数对图片进行上采样或下采样，0表示最近插值，1表示双线性插值，3表示三次样条插值，默认为3  </li><li>pixdim：  如果被指定，输入volum在被喂给网络之前将被重采样成voxel尺寸  </li><li>axcodes：  如果被指定，输入volum在被喂给网络之前将被调整为坐标码(axes code)  </li><li>spatial_window_size:  输入到网络中的图片尺寸，需指明三个维度，第一个和第二个分别表示图片的长和宽，第三个如果为1表示使用2d卷积，否则使用3d卷积  </li><li>loader：  图片读取器，默认值None将尝试所有可得到的读取器<br>　　读取器支持的类型有：<br>　　nibabel  支持.nii医学文件格式<br>　　simpleitk  支持.dcm和.mhd格式的医疗图像<br>　　opencv  支持.jpg等常见图像，读取后通道顺序为BGR<br>　　skimage  支持.jpg等常见图像<br>　　pillow  支持.jpg等常见图像，读取后通道顺序为RGB<br></li></ul><h5 id="System"><a href="#System" class="headerlink" title="[System]"></a>[System]</h5><p><small></small></p><ul><li>cuda_devices:  设置tensorflow的CUDA_VISIBLE_DEVICES变量  </li><li>num_threads:  设置训练的预处理线程数  </li><li>num_gpus:  设置训练的GPU的数量  </li><li>model_dir:  训练模型的保存和加载路径  </li><li>dataset_split_file:  文件分配科目到子集  </li><li>event_handler:  事件处理器<br></li></ul><h5 id="NETWORK"><a href="#NETWORK" class="headerlink" title="[NETWORK]"></a>[NETWORK]</h5><p><small></small></p><ul><li>name:  niftynet/network中的网络类或用户自定义的模块  </li><li>activation_function:  网络的激活函数集合  </li><li>batch_size:  设置每次迭代图像窗口的数量  </li><li>smaller_final_batch_mode:  当batch_size的窗口采样器总数是不可见的时支持最后的batch使用不同的模式<br>　　可选类型有：<br>　　drop:  终止剩余的batch<br>　　pad: 用-1填补最后更小的batch<br>　　dynamic: 直接输出剩余的batch </li><li>reg_type:  可训练的正规化参数的类型  </li><li>decay:  正规化的强度，用于预防过拟合  </li><li>volume_padding_size:  图片的填补值  </li><li>window_sampling:  进入网络的图片的采样方法<br>　　uniform:  输出的图片保持原本大小<br>　　weighted:  对成比例的voxel的采样到累积直方图的似然<br>　　balanced:  每个标签都被采样的可能性同样<br>　　resize:  将进入网络的图片首先resize到spatial_window_size </li><li>queue_length:  NiftyNet会设置两个队列，一个负责从数据集中读取数据并扰乱，另一个从前一个队列中读取batch_size张图片输入网络，这个参数是指第一个队列的长度，最小值为batch_size * 2.5  </li><li>keep_prob: 如果失活被网络支持的话，每个元素存活的可能性<br></li></ul><h5 id="Volume-normalisation"><a href="#Volume-normalisation" class="headerlink" title="[Volume-normalisation]"></a>[Volume-normalisation]</h5><p><small></small></p><ul><li>normalisation:  指示直方图标准化是否应该被应用于数据  </li><li>whitening:  只是被加载的图片是否应该被增白，如果是，输入I，返回(I - mean(I)) / std(I)  </li><li>histogram_ref_file:  标准化参数的文件  </li><li>norm_file:  基于直方图的标准化的直方图landmark类型  </li><li>cutoff:  下级和上级的基于直方图的标准化的截断  </li><li>normalise_foreground_only:  指示一个mask是否需要被基于前景或多样前景进行计算，如设置True，所有的标准化步骤都将被应用于生成前景区  </li><li>foreground_type:  生成一个前景mask，并且它只用于前景  </li><li>mutimod_foreground_type:  结合前景mask和多模态的策略<br>　　可选类型：<br>　　or:  可得到的masks的合集<br>　　and:  可得到的mask的交集<br>　　all:  mask从每个模态独立计算<br></li></ul><h5 id="TRAINING"><a href="#TRAINING" class="headerlink" title="[TRAINING]"></a>[TRAINING]</h5><p><small></small></p><ul><li>optimiser:  计算图梯度优化器的类型，支持adagrade，adam，gradientdescent，momentum，rmsprop，nesterov  </li><li>sample_per_volume：每张图的采样次数  </li><li>lr:  学习率  </li><li>loss_type:  loss函数的类型，支持segmentation,regression,autoencoder,gan  </li><li>starting_iter:  设置重新训练模型的迭代次数  </li><li>save_every_n:  保存当前模型的频率，0为不保存  </li><li>tensorboard_every_n:  计算图中的元素和写到tensorboard上的频率  </li><li>max_iter:  最大训练迭代次数<br></li></ul><h5 id="Validation-during-training"><a href="#Validation-during-training" class="headerlink" title="[Validation during training]"></a>[Validation during training]</h5><p><small></small></p><ul><li>validation_every_n:  每n次迭代运行一次验证迭代  </li><li>validation_max_iter:  验证迭代运行的次数  </li><li>exclude_fraction_for_validation:  用于验证的数据集的比例  </li><li>exclude_fraction_for_inference:  用于推断的数据集的比例<br></li></ul><h5 id="Data-augmentation-during-training"><a href="#Data-augmentation-during-training" class="headerlink" title="[Data augmentation during training]"></a>[Data augmentation during training]</h5><p><small></small></p><ul><li>rotation_angle:  指示输入的图片旋转一个随机的旋转  </li><li>scaling_percentage:  指示一个随机的缩放比例(-50,50)  </li><li>random_flipping_axes:  可以翻转增强数据的轴(???)<br></li></ul><h5 id="INFERENCE"><a href="#INFERENCE" class="headerlink" title="[INFERENCE]"></a>[INFERENCE]</h5><p><small></small></p><ul><li>spatial_window_size:  指示输入窗口的大小(int array)  </li><li>border:  一个用于修剪输出窗口大小的边界值(int tuple)，如设置(3,3,3)，将把一个(64*64*64)的窗口修剪为(58*58*58)  </li><li>inference_iter:  指定已训练的模型用于推测(integer)  </li><li>save_seg_dir:  预测目录的名字  </li><li>output_postfix:  向每一个输出文件的名称后添加后缀  </li><li>output_interp_order:  网络输出的推断顺序  </li><li>dataset_to_infer:  字符串指定计算推理的数据集（‘training’, ‘validation’, ‘inference’）<br></li></ul><h5 id="EVALUATION"><a href="#EVALUATION" class="headerlink" title="[EVALUATION]"></a>[EVALUATION]</h5><p><small></small></p><ul><li>save_csv_dir:  存储输出的csv文件的路径  </li><li>evaluations:  要计算的评价指标列表以逗号分隔的字符串表示，每个应用程序可能的评估指标列表可用于回归评估、分段评估和分类评估  </li><li>evaluation_units:  描述在分割的情况下应该如何进行评估<br>　　foreground:  只对一个标签<br>　　label：  对每一个标签度量<br>　　cc:  对每个连接组件度量<br></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】&lt;a href=&quot;https://www.cnblogs.com/zhhfan/p/9806900.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NiftyNet开源平台的使用 – 配置文件&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Medical Imaging" scheme="http://yoursite.com/categories/Medical-Imaging/"/>
    
    
      <category term="NiftyNet" scheme="http://yoursite.com/tags/NiftyNet/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch-官方文档-学习路线</title>
    <link href="http://yoursite.com/2019/03/01/PyTorch-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3-%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    <id>http://yoursite.com/2019/03/01/PyTorch-官方文档-学习路线/</id>
    <published>2019-03-01T03:54:25.000Z</published>
    <updated>2019-03-16T17:28:05.972Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="DATA-LOADING-AND-PROCESSING-TUTORIAL"><a href="#DATA-LOADING-AND-PROCESSING-TUTORIAL" class="headerlink" title="DATA LOADING AND PROCESSING TUTORIAL"></a><a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#" target="_blank" rel="noopener">DATA LOADING AND PROCESSING TUTORIAL</a></h3><p>Let’s quickly read the CSV and get the annotations in an (N, 2) array where N is the number of landmarks.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">landmarks_frame = pd.read_csv(<span class="string">'data/faces/face_landmarks.csv'</span>)</span><br><span class="line"></span><br><span class="line">n = <span class="number">65</span></span><br><span class="line">img_name = landmarks_frame.iloc[n, <span class="number">0</span>]  <span class="comment">#select the first column (image_name) in .csv file</span></span><br><span class="line">landmarks = landmarks_frame.iloc[n, <span class="number">1</span>:].as_matrix() <span class="comment">#select the rest datas in .csv file #landmarks.shape: (136,)</span></span><br><span class="line">landmarks = landmarks.astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>, <span class="number">2</span>) </span><br><span class="line"></span><br><span class="line">print(<span class="string">'Image name: &#123;&#125;'</span>.format(img_name))  </span><br><span class="line">print(<span class="string">'Landmarks shape: &#123;&#125;'</span>.format(landmarks.shape))</span><br><span class="line">print(<span class="string">'First 4 Landmarks: &#123;&#125;'</span>.format(landmarks[:<span class="number">4</span>]))</span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">Image name: person-<span class="number">7</span>.jpg</span><br><span class="line">Landmarks shape: (<span class="number">68</span>, <span class="number">2</span>)</span><br><span class="line">First <span class="number">4</span> Landmarks: [[<span class="number">32</span>. <span class="number">65</span>.]  <span class="comment"># 对应于 person-7.jpg</span></span><br><span class="line"> [<span class="number">33</span>. <span class="number">76</span>.]</span><br><span class="line"> [<span class="number">34</span>. <span class="number">86</span>.]</span><br><span class="line"> [<span class="number">34</span>. <span class="number">97</span>.]]</span><br></pre></td></tr></table></figure><h4 id="Dataset-class"><a href="#Dataset-class" class="headerlink" title="Dataset class"></a>Dataset class</h4><ul><li><code>__len__</code>: so that <code>len(dataset)</code> returns the size of the dataset.</li><li><code>__getitem__</code>: to support the indexing such that <code>dataset[i]</code> can be used to get $ith$ sample</li></ul><h4 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h4><ul><li><code>Rescale</code>: to scale the image</li><li><code>RandomCrop</code>: to crop from image randomly. This is data augmentation.</li><li><code>ToTensor</code>: to convert the numpy images to torch images (we need to swap axes).</li></ul><h4 id="Iterating-through-the-dataset"><a href="#Iterating-through-the-dataset" class="headerlink" title="Iterating through the dataset"></a>Iterating through the dataset</h4><ul><li><code>torch.utils.data.DataLoader</code>: an iterator which provides all these features (i. <code>Batching the data</code>   ii. <code>Shuffling the data</code>    iii. <code>Load the data in parallel using multiprocessing workers</code>).</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transformed_dataset = FaceLandmarksDataset(csv_file=<span class="string">'data/faces/face_landmarks.csv'</span>,</span><br><span class="line">                                           root_dir=<span class="string">'data/faces/'</span>,</span><br><span class="line">                                           transform=transforms.Compose([</span><br><span class="line">                                               Rescale(<span class="number">256</span>),</span><br><span class="line">                                               RandomCrop(<span class="number">224</span>),</span><br><span class="line">                                               ToTensor()</span><br><span class="line">                                           ]))</span><br><span class="line">                                           </span><br><span class="line">dataloader = DataLoader(transformed_dataset, batch_size=<span class="number">4</span>,</span><br><span class="line">                        shuffle=<span class="keyword">True</span>, num_workers=<span class="number">4</span>) <span class="comment"># !!! &lt;class 'torch.utils.data.dataloader.DataLoader'&gt;  (inputs, labels)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Helper function to show a batch</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_landmarks_batch</span><span class="params">(sample_batched)</span>:</span></span><br><span class="line">    <span class="string">"""Show image with landmarks for a batch of samples."""</span></span><br><span class="line">    images_batch, landmarks_batch = \</span><br><span class="line">            sample_batched[<span class="string">'image'</span>], sample_batched[<span class="string">'landmarks'</span>]</span><br><span class="line">    batch_size = len(images_batch) <span class="comment"># batch_size: 4</span></span><br><span class="line">    im_size = images_batch.size(<span class="number">2</span>) <span class="comment">#images_batch.size(): torch.Size([4, 3, 224, 224])  #im_size: 224</span></span><br><span class="line"></span><br><span class="line">    grid = utils.make_grid(images_batch) <span class="comment"># &lt;class 'torch.Tensor'&gt;</span></span><br><span class="line">    plt.imshow(grid.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))) <span class="comment"># transpose &lt;class 'numpy.ndarray'&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        plt.scatter(landmarks_batch[i, :, <span class="number">0</span>].numpy() + i * im_size, <span class="comment"># x  #landmarks_batch.size(): torch.Size([4, 68, 2])</span></span><br><span class="line">                    landmarks_batch[i, :, <span class="number">1</span>].numpy(),<span class="comment"># y</span></span><br><span class="line">                    s=<span class="number">10</span>, marker=<span class="string">'.'</span>, c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">        plt.title(<span class="string">'Batch from dataloader'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_batch, sample_batched <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">    print(i_batch, sample_batched[<span class="string">'image'</span>].size(),  <span class="comment"># torch.Size([4, 3, 224, 224])</span></span><br><span class="line">          sample_batched[<span class="string">'landmarks'</span>].size()) <span class="comment"># torch.Size([4, 68, 2])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># observe 4th batch and stop.</span></span><br><span class="line">    <span class="keyword">if</span> i_batch == <span class="number">3</span>:</span><br><span class="line">        plt.figure()</span><br><span class="line">        show_landmarks_batch(sample_batched)</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">        plt.ioff()</span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h4 id="Afterword-torchvision"><a href="#Afterword-torchvision" class="headerlink" title="Afterword: torchvision"></a>Afterword: torchvision</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br><span class="line"></span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">        transforms.RandomSizedCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.RandomHorizontalFlip(), <span class="comment">#  operate on PIL.Image</span></span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                             std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line"><span class="comment"># ImageFolder is one of the more generic datasets available in torchvision.</span></span><br><span class="line">hymenoptera_dataset = datasets.ImageFolder(root=<span class="string">'hymenoptera_data/train'</span>,</span><br><span class="line">                                           transform=data_transform)</span><br><span class="line">                                           </span><br><span class="line">dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,</span><br><span class="line">                                             batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>,</span><br><span class="line">                                             num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><h3 id="TRANSFER-LEARNING-TUTORIAL"><a href="#TRANSFER-LEARNING-TUTORIAL" class="headerlink" title="TRANSFER LEARNING TUTORIAL"></a><a href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" target="_blank" rel="noopener">TRANSFER LEARNING TUTORIAL</a></h3><p>These two major transfer learning scenarios look as follows:</p><ul><li><strong>Finetuning the convnet</strong>: Instead of random initializaion, we initialize the network with <code>a pretrained network</code>, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.</li><li><strong>ConvNet as fixed feature extractor</strong>: Here, we will <code>freeze the weights</code> for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.</li></ul><p><strong>参考</strong>：<a href="https://blog.csdn.net/u014380165/article/details/78525273/" target="_blank" rel="noopener">PyTorch学习之路（level1）——训练一个图像分类模型</a></p><h4 id="Load-Data"><a href="#Load-Data" class="headerlink" title="Load Data"></a>Load Data</h4><h4 id="Training-the-model"><a href="#Training-the-model" class="headerlink" title="Training the model"></a>Training the model</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(model, criterion, optimizer, scheduler, num_epochs=<span class="number">25</span>)</span>:</span></span><br><span class="line">    since = time.time()</span><br><span class="line"></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict()) <span class="comment"># 深拷贝 （'非引用'拷贝）</span></span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        print(<span class="string">'Epoch &#123;&#125;/&#123;&#125;'</span>.format(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">        print(<span class="string">'-'</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Each epoch has a training and validation phase</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                scheduler.step()  <span class="comment"># 更新学习率</span></span><br><span class="line">                model.train()  <span class="comment"># Set model to training mode</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.eval()   <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Iterate over data.</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 选取调用 gpu or cpu </span></span><br><span class="line">                <span class="comment"># (数据类型不变 &lt;class 'torch.Tensor'&gt; → &lt;class 'torch.Tensor'&gt;)</span></span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># zero the parameter gradients 网络中的所有梯度置0</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># forward</span></span><br><span class="line">                <span class="comment"># track history if only in train</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">'train'</span>):</span><br><span class="line">                    outputs = model(inputs) <span class="comment"># 网络的前向传播</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 预测该样本属于哪个类别的信息   </span></span><br><span class="line">                    <span class="comment"># torch.max()的第一个输入是tensor格式,第二个参数1是代表dim的意思 </span></span><br><span class="line">                    <span class="comment"># 取每一行的最大值，其实就是我们常见的取概率最大的那个index</span></span><br><span class="line">                    _, preds = torch.max(outputs, <span class="number">1</span>) </span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 将输出的outputs和原来导入的labels作为loss函数的输入就可以得到损失</span></span><br><span class="line">                    loss = criterion(outputs, labels) </span><br><span class="line"></span><br><span class="line">                    <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                    <span class="comment"># 计算得到loss后就要回传损失.要注意的是这是在训练的时候才会有的操作（测试时只有forward过程）</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">'train'</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step() <span class="comment"># 更新参数（梯度和权值信息）</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># statistics</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                running_corrects += torch.sum(preds == labels.data)</span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">            epoch_acc = running_corrects.double() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;'</span>.format(</span><br><span class="line">                phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># deep copy the model</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">'val'</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">'Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s'</span>.format(</span><br><span class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">'Best val Acc: &#123;:4f&#125;'</span>.format(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load best model weights</span></span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h4 id="Visualizing-the-model-predictions"><a href="#Visualizing-the-model-predictions" class="headerlink" title="Visualizing the model predictions"></a>Visualizing the model predictions</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">was_training = model.training  <span class="comment"># record the last model mode</span></span><br></pre></td></tr></table></figure><h4 id="Finetuning-the-convnet"><a href="#Finetuning-the-convnet" class="headerlink" title="Finetuning the convnet"></a>Finetuning the convnet</h4><p>Load a pretrained model and <code>reset</code> final fully connected layer.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_ft = models.resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></span><br><span class="line">num_ftrs = model_ft.fc.in_features  <span class="comment"># in_features: num inputs </span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">2</span>)  <span class="comment"># 2: num outputs  </span></span><br><span class="line"></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR (learning rate) by a factor of 0.1 every 7 epochs</span></span><br><span class="line"><span class="comment"># torch.optim.lr_scheduler模块的StepLR类，表示每隔step_size个epoch就将学习率降为原来的gamma倍</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></p><h4 id="Train-and-evaluate"><a href="#Train-and-evaluate" class="headerlink" title="Train and evaluate"></a>Train and evaluate</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Train and evaluate</span></span><br><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=<span class="number">25</span>)</span><br><span class="line"><span class="comment"># visualize</span></span><br><span class="line">visualize_model(model_ft)</span><br></pre></td></tr></table></figure><h4 id="ConvNet-as-fixed-feature-extractor"><a href="#ConvNet-as-fixed-feature-extractor" class="headerlink" title="ConvNet as fixed feature extractor"></a>ConvNet as fixed feature extractor</h4><p>Here, we need to <code>freeze</code> all the network except the final layer. We need to set <code>requires_grad == False</code> to freeze the parameters so that the gradients are not computed in <code>backward()</code>.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">    param.requires_grad = <span class="keyword">False</span></span><br></pre></td></tr></table></figure></p><h4 id="Train-and-evaluate-1"><a href="#Train-and-evaluate-1" class="headerlink" title="Train and evaluate"></a>Train and evaluate</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Train and evaluate</span></span><br><span class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br><span class="line">              </span><br><span class="line"><span class="comment"># visualize           </span></span><br><span class="line">visualize_model(model_conv)</span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="Extended-Reading"><a href="#Extended-Reading" class="headerlink" title="Extended Reading"></a>Extended Reading</h3><ul><li><a href="https://zhuanlan.zhihu.com/p/29024978" target="_blank" rel="noopener">PyTorch实战指南</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>中华诗词</title>
    <link href="http://yoursite.com/2019/03/01/%E4%B8%AD%E5%8D%8E%E8%AF%97%E8%AF%8D/"/>
    <id>http://yoursite.com/2019/03/01/中华诗词/</id>
    <published>2019-03-01T02:23:25.000Z</published>
    <updated>2019-07-07T11:07:51.697Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><p>中华诗词辉煌千年<br>九州同济万古流传<br>壮哉我中华诗词绵延不断<br>雄哉我中华诗词洋洋大观<br>奇哉我中华诗词星移斗转<br>美哉我中华诗词春色满园</p><a id="more"></a><p><strong>《虞美人·听雨》（宋）蒋捷</strong></p><p>  少年听雨歌楼上，红烛昏罗帐。壮年听雨客舟中，江阔云低，断雁叫西风。<br>  而今听雨僧庐下，鬓已星星也。悲欢离合总无情，一任阶前点滴到天明。</p><p><strong>《洛神赋》节选 （三国）曹植</strong></p><p>  其形也，翩若惊鸿，婉若游龙，荣曜秋菊，华茂春松。<br>  髣髴（fǎng fú）兮若轻云之蔽月，飘飖（yáo）兮若流风之回雪。<br>  远而望之，皎若太阳升朝霞；迫而察之，灼若芙蕖出渌（lù）波。<br>  秾纤得中，修短合度。肩若削成，腰如约素。延颈秀项，皓质呈露。<br>  芳泽无加，铅华不御。云髻峨峨，修眉联娟。丹唇外朗，皓齿内鲜。<br>  明眸善睐，靥辅承权。瓌姿艳逸，仪静体闲。柔情绰态，媚于语言。</p><hr><p>莫笑雏龙鳞窄，敢与鲲鹏夺海<br>万般回首化尘埃，看我昆仑不改</p><p>终是庄周梦了蝶，你是恩赐也是劫<br>终是李白醉了酒，你是孤独也是愁<br>终是荆轲刺了秦，一代君王一世民<br>终是妲己误了国，万里江山似蹉跎<br>终是玉环听了曲，无人再懂琵琶语<br>终是韩信放下枪，也是宿命也是伤<br>终是悟空成了佛，你一堕落便是魔<br>终是霸王别了姬，弃了江山负了你<br>终是后羿断了弦，此生注定难相见</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
&lt;p&gt;中华诗词辉煌千年&lt;br&gt;九州同济万古流传&lt;br&gt;壮哉我中华诗词绵延不断&lt;br&gt;雄哉我中华诗词洋洋大观&lt;br&gt;奇哉我中华诗词星移斗转&lt;br&gt;美哉我中华诗词春色满园&lt;/p&gt;
    
    </summary>
    
      <category term="语录" scheme="http://yoursite.com/categories/%E8%AF%AD%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>迁移学习-Introduction</title>
    <link href="http://yoursite.com/2019/02/27/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/02/27/迁移学习/</id>
    <published>2019-02-27T13:30:31.000Z</published>
    <updated>2019-03-02T06:37:28.737Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】深度学习之PyTorch实战计算机视觉 第7章</p><a id="more"></a><p>如果我们用这么多资源训练的模型能够<code>解决同一类问题</code>，那么模型的性价比会提高很多，这就促使使用迁移模型解决同 一类问题的方法出现 。因为该方法的出现，我们通过对 一个训练好的模型进行细微调整，就能将其应用到相似的问题中，最后还能取得很好的效果 ; 另外，对于原始数据较少的问题，我们也能够通过采用迁移模型进行有效解决 ，所以，如果能够选取合适的迁移学习方法，则会对解决我们所面临的问题有很大的帮助 。</p><p>建议可同时阅读 <a href="https://captainzj.github.io/2018/12/28/torchvision-pretrained-Model/" target="_blank" rel="noopener">torchvision_pretrained_Model</a>，并且请参考官方文档<a href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" target="_blank" rel="noopener">Transfer Learning Tutorial</a>。 </p><h3 id="模型搭建和参数优化"><a href="#模型搭建和参数优化" class="headerlink" title="模型搭建和参数优化"></a>模型搭建和参数优化</h3><p>本节会先基于一个简化的VGGNet 架构搭建卷积神经网络模型井进行模型训练和参数优化，然后迁移一个完整的VGGI6架构的卷积神经网络模型，最后迁移一个ResNet50架构的卷积神经网络模型，并对比这三个模型在预测结果上的准确性和在泛化能力上的差异 。</p><h4 id="自定义VGGNet"><a href="#自定义VGGNet" class="headerlink" title="自定义VGGNet"></a>自定义VGGNet</h4><p>我们首先需要搭建一个卷积神经网络模型， 考虑到训练时间的成本，我们基于VGG16架构来搭建一个简化版的VGGNet模型，这个简化版模型要求输入的图片大小全部<code>缩放到64×64</code>， 而在标准的VGGl6架构模型中输入的图片大小应当是224×224的; 同时简化版模型<code>删除了VGG16最后的三个卷积层和池化层</code>，也<code>改变了全连接层中的连接参数</code>，这一系列的改变都是为了减少整个模型参与训练的参数数量。<br>在搭建好模型后，通过 print 对搭建的模型进行打印输出来显示模型中的细节，打印输出的代码如下:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Models() </span><br><span class="line"><span class="keyword">print</span> (model)</span><br></pre></td></tr></table></figure><p>然后，定义模型的损失函数和对参数进行优化的优化函数（在代码中优化函数使用的是 Adam， 损失函数使用的是交叉熵，训练次数总共是 10 次）<br>观察输出结果可见，Training…<code>train Loss:0.5051 Acc:75.3450</code> Validing… <code>valid Loss:0.4841 Acc:76.6600</code> (Time) <code>29520.38271522522</code>（约为492分钟）<br>显然，过于耗时，我们可以使用GPUs计算来加速训练，这个过程非常简单和方便，<code>只需重新对这部分参数进行类型转换就可以了</code>. 👇</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参数迁移至 GPUs 的具体代码</span></span><br><span class="line">model = model.cuda()</span><br><span class="line">X, y = Variable(X.cuda()), Variable(y.cuda())</span><br></pre></td></tr></table></figure><p>当然，在此之前，我们需要先确认GPUs硬件是否可用， 具体的代码如下:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(torch.cuda.is_available ())   <span class="comment"># 若返回的值是True，这说明我们的GPUs已经具备了被使用的全部条件</span></span><br></pre></td></tr></table></figure><p>观察输出结果可见，Training…<code>train Loss:0.1903 Acc:92.4450</code> Validing… <code>valid Loss:0.2874 Acc:88.0400</code> (Time) <code>855.5901200771332</code>（约为14分钟）<br>从结果可以看出，不仅验证测试集的准确率提升了近10%，而且耗时大幅下降。（使用GPU计算参数，效率有明显提升）</p><h4 id="迁移-VGG16"><a href="#迁移-VGG16" class="headerlink" title="迁移 VGG16"></a>迁移 VGG16</h4><p>因为承担整个模型输出分类工作的是卷积神经网络模型中的<code>全连接层</code>，所以在迁移学习的过程 中调整最多的也是全连接层部分。其基本思路是<code>冻结</code>卷积神经网络中全连接层之前的全部网络层次，让这些被冻结的网络层次中的参数在模型的训练过程中不进行梯度更新 ，能够被优化的参数仅仅是没有被冻结的全连接层（即自定义修改的新的全连接层）的全部参数。</p><p>下面看看具体的代码。首先，迁移过来的 VGG16架构模型在最后输出的结果是 1000 个 ，在我们的问题中只需两个输出结果，所以全连接层必须进行调整。模型调整的具体<a href="https://github.com/Captainzj/PyTorch_Practice/blob/master/chapter-7/chapter-7.ipynb" target="_blank" rel="noopener">code</a>如下:  </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> parma <span class="keyword">in</span> model.parameters():</span><br><span class="line">    parma.requires_grad = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">model.classifier = torch.nn.Sequential(torch.nn.Linear(<span class="number">25088</span>, <span class="number">4096</span>),</span><br><span class="line">                                           torch.nn.ReLU(),</span><br><span class="line">                                           torch.nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">                                           torch.nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">                                           torch.nn.ReLU(),</span><br><span class="line">                                           torch.nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">                                           torch.nn.Linear(<span class="number">4096</span>, <span class="number">2</span>))</span><br><span class="line"><span class="keyword">if</span> Use_gpu:</span><br><span class="line">    model = model.cuda()</span><br><span class="line"></span><br><span class="line">cost = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.classifier.parameters())</span><br></pre></td></tr></table></figure><p>观察输出结果可见，Training…<code>train Loss:0.0033 Acc:99.8950</code> Validing… <code>valid Loss:0.0899 Acc:98.3200</code><br>准确率有明显提升，说明了迁移学习是一种提升棋型泛化能力的非常有效的方法。</p><h4 id="迁移ResNet50"><a href="#迁移ResNet50" class="headerlink" title="迁移ResNet50"></a>迁移ResNet50</h4><p>模型迁移的代码为 <code>model = models.resnet50(pretrained=True)</code><br>对 ResNet50 的全连接层部分进行调整，具体<a href="https://github.com/Captainzj/PyTorch_Practice/blob/master/chapter-7/chapter-7.ipynb" target="_blank" rel="noopener">code</a>调整如下:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> parma <span class="keyword">in</span> model.parameters(): </span><br><span class="line">    parma . requires_grad = <span class="keyword">False</span></span><br><span class="line">    </span><br><span class="line">model.fc = torch.nn.Linear(<span class="number">2048</span>, <span class="number">2</span>)  <span class="comment"># 因为ResNet50中的全连接层只有一 层，所以对代码的调整非常简单</span></span><br></pre></td></tr></table></figure></p><p>观察输出结果可见，Training…<code>train Loss:0.1349 Acc:95.8950</code> Validing… <code>valid Loss:0.0929 Acc:97.7400</code> -&gt;&gt; 准确率近似（略逊）于 VGG16</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li>GPU训练优于CPU训练 </li><li>迁移学习可节约时间成本；相较于盲目地从头训练，准确率会有更好的保障</li></ol><p>注：如果模型的训练结果不很理想，则还可以训练更多的模型层次，优化更多的模型参数。 </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】深度学习之PyTorch实战计算机视觉 第7章&lt;/p&gt;
    
    </summary>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyQt4_For_Windows_10 </title>
    <link href="http://yoursite.com/2019/02/25/PyQt4-For-Windows-10/"/>
    <id>http://yoursite.com/2019/02/25/PyQt4-For-Windows-10/</id>
    <published>2019-02-25T12:14:37.000Z</published>
    <updated>2019-02-26T13:51:08.582Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="安装Python3-6"><a href="#安装Python3-6" class="headerlink" title="安装Python3.6"></a>安装Python3.6</h3><ul><li>教程：<a href="https://blog.51cto.com/5001660/2084273" target="_blank" rel="noopener">windows 10 64位安装Python3.6.4</a></li><li>下载地址：<a href="https://www.python.org/downloads/windows/" target="_blank" rel="noopener">https://www.python.org/downloads/windows/</a></li><li>虚拟机Parallels Desktop For Windows10 安装路径：<code>C:\Users\Captain\AppData\Local\Programs\Python\Python36</code></li></ul><h3 id="安装PyQt4"><a href="#安装PyQt4" class="headerlink" title="安装PyQt4"></a>安装PyQt4</h3><ul><li>教程：<a href="https://blog.csdn.net/Eppley/article/details/80240305" target="_blank" rel="noopener">Windows+Python 3.6环境下安装PyQt4</a></li><li>下载地址：<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#pyqt4" target="_blank" rel="noopener">http://www.lfd.uci.edu/~gohlke/pythonlibs/#pyqt4</a> <a href="https://blog.csdn.net/js1568/article/details/80391761" target="_blank" rel="noopener">Win10，Python 3.6环境下安装PyQt4</a></li><li><p>安装指令：<code>pip install PyQt4-4.11.4-cp36-cp36m-win_amd64.whl</code></p><center><br>  <img src="/2019/02/25/PyQt4-For-Windows-10/PyQt4ForWin10.png" width="500/"><br></center><ul><li>若执行指令”pip install PyQt4-4.11.4-cp36-cp36m-win_amd64.whl”后，提示”PyQt4-4.11.4-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.”  可能的原因是 pip 与 该.whl 版本不匹配 （pip 可能默认调用了Anaconda3的python 3.7版本） </li></ul></li></ul><h3 id="安装Anaconda-For-win10"><a href="#安装Anaconda-For-win10" class="headerlink" title="安装Anaconda For win10"></a>安装Anaconda For win10</h3><ul><li>教程：<a href="https://www.zhihu.com/question/50003671" target="_blank" rel="noopener">win10：Anaconda安装好后,如何安装pyqt4？</a></li><li>下载地址：<a href="https://stackoverflow.com/questions/21637922/how-to-install-pyqt4-in-anaconda" target="_blank" rel="noopener">How to install PyQt4 in anaconda?</a></li><li>虚拟机Parallels Desktop For Windows10 安装路径：<code>C:\Users\Captain\Anaconda3</code></li></ul><h3 id="安装其他软件"><a href="#安装其他软件" class="headerlink" title="安装其他软件"></a>安装其他软件</h3><p>虚拟机Parallels Desktop For Windows10 安装路径：</p><ul><li>百度网盘：<code>C:\Users\Captain\AppData\Roaming\baidu\BaiduNetdisk</code></li><li>迅雷：<code>C:\Program Files (x86)\Thunder Network\Thunder</code></li><li>VSCode：<code>C:\Users\Captain\AppData\Local\Programs\Microsoft VS Code</code></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="环境配置" scheme="http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="win10" scheme="http://yoursite.com/tags/win10/"/>
    
  </entry>
  
  <entry>
    <title>古记</title>
    <link href="http://yoursite.com/2019/02/23/%E5%8F%A4%E9%A3%8E%E5%B0%8F%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/02/23/古风小记/</id>
    <published>2019-02-23T03:20:26.933Z</published>
    <updated>2019-07-07T10:45:03.613Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】</p><a id="more"></a><h2 id="怪口历史"><a href="#怪口历史" class="headerlink" title="怪口历史"></a>怪口历史</h2><blockquote><p>诗与远方真的存在，愿你相信</p></blockquote><h3 id="文化自信"><a href="#文化自信" class="headerlink" title="文化自信"></a>文化自信</h3><ul><li>记得2008年奥运会开幕式，NBC的解说员有一句话我至今记忆犹新：“过去十个世纪里，中国🇨🇳有九个世纪都是GDP世界第一。“没错，我们只是不小心落后了一阵子而已，再看看当年和我们齐头并进的文明，古埃及当年很牛吧？看看现在，除了金字塔和象形文字以外还剩下什么呢？古印度孔雀王朝很厉害吧，现在呢？连创造古印度文明的后代现在全是贱民，或者是“不可接触”的阶层。和其他文明相比，我们的文化始终没有断代。我们海乃百川，包容兼济，务实变通，我想这就是我们的文化伟大之处吧！</li><li><p>每当这个民族这个国家面临生死存亡之际…</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">湖南人会说：“若中华灭亡，除非湖南死光！”</span><br><span class="line">陕西人说：“两狼山战胡儿天摇地动，好男儿为国家何惧死生！”</span><br><span class="line">四川娃说：“不退外敌誓不回川！”</span><br><span class="line">河南人说：“国破尚如此，我何惜此头！”</span><br><span class="line">江南民说：“寄语行人休掩鼻，活人不及死人香！”</span><br><span class="line">河北人说：“燕赵多有慷慨悲歌之士！”</span><br><span class="line">两广人民更是有：“饮冰十年，难凉热血！”</span><br><span class="line">东北三省也有：“何如誓死一拼以尽天职！”</span><br></pre></td></tr></table></figure><p>这就是我们国家刀劈斧剁的天地间到处都是不愿做奴隶的人们。</p></li><li><p>中华文明是独立培育出粮食作物的5个文明之一；中华文明是独立驯化出动物的4个文明之一；中华民族是独立出现语言的17个文明之一；中华文明也是最早使用铁器的文明之一。曾经我看过台湾网友说过一这样的段话：中国这个民族的自尊心和自豪感来自于历史，是不怎么依赖于当今的外在世界而存在的，因为历史不可更改，所以最为稳固。</p></li><li><p>从西方殖民史来看，所有被西方列强全力侵略或者殖民的民族里面，诸如印度、印第安人、玛雅人、印加人、非洲黑人、东南亚京族人和马来人相比，只有中国翻身最快速，也最彻底。其中一个根本的原因就是中国人自身的自尊心最强，对西方列强最不服气，也最敢斗争！即使贫穷落后也不妨碍中国人“悍然”用小米加步枪去跟美国人的坦克飞机原子弹正面对抗！“饿死不弯腰，冻死迎风站”的态度去对待苏联的制裁！在吃不饱饭的情况下仍然搞出氢弹、洲际弹道导弹和卫星！无论治乱兴衰，中国人的民族自尊心始终都在。只因我们存活了5000年，老祖宗告诉我们精卫可以填海，后裔可以射日。只因我们5000年的文化告诉我们“天行健，君子以自强不息。”</p></li><li><p>最后用三十九画生的一句话作为总结：“我走过的山路是徐霞客曾经留宿过的，我走过的关隘是六国曾经逡巡而不敢前的，我驻足远眺的城楼是于谦曾经坚定守卫过的，我现在能驰骋由疆的西北草原是霍去病曾经为之而奋战过的，我还能吃到苏轼当年手不停箸的红烧肉，我还能饮到太白当年举杯邀月的花间美酒，我还能在众多浩劫后读到前人的筋骨血肉，我还能在大喜大悲后脱口而出一句他们曾经用来形容自己的爱恨离愁。这片土地留给我们后人的礼物，应该就这么两件了吧，一件是历史，一件是文化，前者可鉴世，后者可润心。”</p></li></ul><h3 id="中华文明为何没有断代"><a href="#中华文明为何没有断代" class="headerlink" title="中华文明为何没有断代"></a>中华文明为何没有断代</h3><blockquote><p>因为有这样一群伟大的人，造就了一个伟大的民族</p></blockquote><ul><li>周公旦——文化的奠定者，传闻周公旦作《周礼》；第一次引出了夷夏之辩，在文化上，形成了中国的文化范畴，《左传》有言：”中国有礼仪之大，故称夏；有服章之美，谓之华；华夏，皆出于《周礼》，《周礼》出自周公旦“当之无愧的华夏文化的奠定者。</li><li>嬴政——统一的实践者，历史最终让武统成为中国形成的最后一步，秦始皇嬴政的书同文，车同轨，行同伦的政策，让我们这个国家往后，无论分裂到何种程度，一定会有先贤去努力再次统一，使中国最终成为中国🇨🇳，而不是类似欧盟的存在，嬴政统一的实践者，后世之楷模</li><li>汉武帝刘彻——疆界的划定者，中国的农耕属性，本应让这个国家的疆域定在中原气候适中、湿度适中的平原地区。但是，正是因为我们汉武大帝的雄心，让这个国家去做了，本不需要去做的事情，也为千秋后世打下了一个大大的疆土概念，不仅划定了九段线，还将大汉的军旗插在了那里。你知道吗？不仅乌鲁木齐、呼和浩特市音译的地名，其实芜湖、姑苏、余杭也都是音译。</li><li>王导——火种的保护者，当年五胡之乱，中华民族在外族的屠杀下接近灭族。这是我们第一个最危险的时刻，很可能像其他文明那样，埃及文明、古印度文明一样只留在遗迹之中，但正是王导力主南迁，衣冠南渡，使得中华民族在长江以南留下了火种，等待再次燃烧的时刻，以致我们的文明从未有过断代。</li><li>杨广——霸权的制定者，隋炀帝离千古一帝只差一步，不仅一条大运河是南北终成一体，伐高句丽、征突厥，整个初唐和盛唐都在延续隋炀帝的国策，隋唐是不修长城的，高句丽的覆灭也让整个东亚地区永远不再存在第二个农耕文明大国。</li><li>朱元璋——文明的拯救者，这是我们第二个最危险的时刻，在蒙元帝国因缺少文化因素，将自己和整个中国彻底划向伊斯兰教文化之前，以朱元璋为代表的中国人再次站了出来，让东方的海边依然是儒生的国家而不是阿訇的国度。</li></ul><h3 id="中国神话注定华夏之光"><a href="#中国神话注定华夏之光" class="headerlink" title="中国神话注定华夏之光"></a>中国神话注定华夏之光</h3><ul><li>有人问，什么样的精神是刻在每个中国人的骨子里的，我觉得可以在我们的神话中寻找到答案。在西方的神话中，火是上帝赐予的；在希腊神话里，火是普罗米修斯偷来的；而在中国的神话里，火是人们钻木取火坚韧不拔摩擦出来的！这就是中国人，不同于世界其他任何民族的地方。</li><li>在神话中，面对末日洪水，西方人是在诺亚方舟里躲避；但是在我们的神话里，是大禹治水，三过家门而不入。当我们仔细思考这些从小我们耳濡目染的神话故事之后，抛开那些故事情节，找到神话里的真正核心。你就会发现中国的神话只有两个字：抗争。</li><li>假如有一座山挡在你的门前，你是选择搬家还是挖隧道？显而易见，搬家是最好的选择；但是在我们中华的神话里，我们是必须把山移开的。这就是精神内核，全世界只有在我们的神话里能够找到！</li><li>再比如，每个国家都有太阳神的传说，在部落时代，太阳神是绝对的权威。纵览所有太阳神的神话，你会发现只有中国人的神话里有敢于挑战太阳神的故事：据说那个时候天上有九个太阳，因为太阳太热，他一口气射下来八个，大概全世界也只有中国神话敢这么写了。每一代中国人就是听着这样的神话故事长大，勇于抗争的精神已经成为我们的遗传基因。</li><li>当有一个小女孩被大海淹死，化作一只精卫鸟，想要把海填平；我们从没有嘲笑过她的渺小。当有一个人挑战天帝的权威，被砍下头颅，他挥起斧子继续斗争；我们从没有骂他大逆不道，相反我们写诗赞美他：“刑天舞干戚，猛志固常在”。可能我们自己都意识不到，但这种“冻死迎风站,饿死不弯腰”的精神，早已刻在了我们的骨子里。</li><li>大概也正因为此，让我们五千年来一直屹立在世界民族之林，并且大部分时间都是第一名！👍</li></ul><h2 id="英雄述说"><a href="#英雄述说" class="headerlink" title="英雄述说"></a>英雄述说</h2><ul><li><a href="https://baike.baidu.com/item/东皇太一/16006021" target="_blank" rel="noopener"><code>东皇太一</code></a>,上古天庭的主宰者,乃是混沌孕育而出的大神。执掌先天至宝混沌钟，在盘古开天辟地，女娲造人造物之际镇压鸿蒙世界。东皇太一奉鸿钧道人法旨（鸿钧法旨：盘古开天，女娲造灵，太一治世）：统一洪荒万族，并且将洪荒万族合称为“妖族”。开创旷古绝今的无上霸业，登立天帝，自称东皇。立天规戒律，为三界正统至尊。</li></ul><h2 id="故事改编"><a href="#故事改编" class="headerlink" title="故事改编"></a>故事改编</h2><h3 id="悟空"><a href="#悟空" class="headerlink" title="悟空"></a>悟空</h3><p>八戒情绪低沉：猴哥我有一故事，你想不想听，悟空闭着眼睛不耐烦道：不想听，不想听…叹息一声，八戒转身离去…<br>八戒走后，悟空睁开双眼轻笑一声：真是个呆子，数百年来也只会讲一个故事，随后只见悟空一把扯下身上袈裟，唤出紫金战甲，掏出耳中金棒，转身消失不见…<br>没有人知道悟空去了哪里，只是后来听人说起，那一日，屹立千年的南天门突然崩塌，天庭诸神皆是身受重伤，凌霄殿内也是乱成一团，更为奇怪的是那深居广寒宫的嫦娥仙子，却是从此消失不见…</p><h3 id="猪八戒"><a href="#猪八戒" class="headerlink" title="猪八戒"></a>猪八戒</h3><p>蛮荒叛乱，使得三界动荡不安…<br>蛮夷之地，天蓬亲率十万水军击溃蛮荒百万军团，平定战乱，一战成名，唯恐养虎为患，庆功宴上，玉帝杯酒释兵权，将天蓬贬入凡间…<br>西行之际，八戒一路皆是放荡不羁游戏人间…分封完毕，净坛使者心中释然，做个闲人，吃饱穿暖，诸事不管…数年之后，花果山上烈火焚烧，悟空暴怒，直冲凌霄，却遭仙佛诡计，困于弑神阵中…<br>净坛庙中，八戒怒睁双眼：欺我可以，动我兄弟，不行…<br>九尺钉耙初现，怒攻满天神佛，奈何孤立无援…<br>千钧一发之际，只听战鼓之声撼天，十万铁骑怒吼：元帅，天河水军，请战…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】&lt;/p&gt;
    
    </summary>
    
      <category term="语录" scheme="http://yoursite.com/categories/%E8%AF%AD%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>本科生毕业设计（论文）安排</title>
    <link href="http://yoursite.com/2019/02/22/%E6%9C%AC%E7%A7%91%E7%94%9F%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1%EF%BC%88%E8%AE%BA%E6%96%87%EF%BC%89%E5%AE%89%E6%8E%92/"/>
    <id>http://yoursite.com/2019/02/22/本科生毕业设计（论文）安排/</id>
    <published>2019-02-22T13:07:09.000Z</published>
    <updated>2019-02-22T14:59:02.030Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】流程简述</p><a id="more"></a><ul><li><a href="http://www.sohu.com/a/258607457_682145" target="_blank" rel="noopener">喇叭扩音丨2015级本科生毕业设计（论文）工作安排 </a></li><li><a href="http://www.paper120.net/j4786.html" target="_blank" rel="noopener">西安电子科技大学关于本科毕业生毕业设计论文检测查重相关事项的说明</a></li></ul><p>@timeline{</p><p>@item{</p><h6 id="2018-10月"><a href="#2018-10月" class="headerlink" title="2018.10月"></a>2018.10月</h6><p>① 启动通知</p><p>}</p><p>@item{</p><h6 id="2018-11月"><a href="#2018-11月" class="headerlink" title="2018.11月"></a>2018.11月</h6><p>② 确定选题</p><p>}</p><p>@item{</p><h6 id="2019-1月"><a href="#2019-1月" class="headerlink" title="2019.1月"></a>2019.1月</h6><p>③ 初期检查：<br>检查学生选题后对资料的收集、阅读及掌握情况，对任务的熟悉理解情况；…</p><p>}</p><p>@item{</p><h6 id="2019-3月中旬-4月上旬"><a href="#2019-3月中旬-4月上旬" class="headerlink" title="2019.3月中旬-4月上旬"></a>2019.3月中旬-4月上旬</h6><p>④ 中期检查：<br>填写“本科生毕业设计（论文）中期检查表”；<br>组织外出毕业设计检查</p><p>}</p><p>@item{</p><h6 id="2019-5月"><a href="#2019-5月" class="headerlink" title="2019.5月"></a>2019.5月</h6><p>⑤ 盲审<br>教务处：公布各学院毕业设计（论文）盲审名单，组织盲审工作<br>学院：按要求上交盲审论文；盲审结果公布后，组织学生进行修改</p><p>}</p><p>@item{</p><h6 id="2019-6月"><a href="#2019-6月" class="headerlink" title="2019.6月"></a>2019.6月</h6><p>⑥ 答辩及后期检查检查普查评估<br>检查论文的格式规范、指导教师对论文的审阅批改、<br>答辩的组织实施、论文成绩评定…</p><p>}</p><p>@item{</p><h6 id="2019-7月"><a href="#2019-7月" class="headerlink" title="2019.7月"></a>2019.7月</h6><p>⑦ 论文评优</p><p>}</p><p>@item{</p><h6 id="2019-寒假前"><a href="#2019-寒假前" class="headerlink" title="2019 寒假前"></a>2019 寒假前</h6><p>⑧ 总结、归档</p><p>}</p><p>}</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】流程简述&lt;/p&gt;
    
    </summary>
    
      <category term="XD" scheme="http://yoursite.com/categories/XD/"/>
    
    
  </entry>
  
  <entry>
    <title>SPJ文件简介</title>
    <link href="http://yoursite.com/2019/02/18/SPJ%E6%96%87%E4%BB%B6%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2019/02/18/SPJ文件简介/</id>
    <published>2019-02-18T11:01:58.000Z</published>
    <updated>2019-03-02T07:42:37.303Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】spj文件是和医疗的3维数据相关的文件 ……</p><a id="more"></a><p>同时了解 stl 文件</p><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ul><li>.spj文件”是由清华斯维尔公司计价软件—“清单计价2003” 生成的项目文件，它是针对单位工程编制的造价文件。</li></ul><h3 id="How-to-open"><a href="#How-to-open" class="headerlink" title="How to open"></a>How to open</h3><p><a href="https://fileinfo.com/extension/spj" target="_blank" rel="noopener">.SPJ File Extension</a></p><ul><li>PhotoStage Slideshow Project <a href="https://www.nchsoftware.com/slideshow/index.html?ref=cj&amp;cjevent=1f0060ab350811e983be00a70a180514" target="_blank" rel="noopener">PhotoStage</a><ul><li>SPJ file is a <code>PhotoStage Slideshow Project</code>. NCH PhotoStage Slideshow Software is a software that enables you to create dynamic slideshows from your photos.</li></ul></li><li>Microsoft ICE Panorama Project <a href="https://www.microsoft.com/en-us/research/product/computational-photography-applications/image-composite-editor/" target="_blank" rel="noopener">ICE</a></li><li>SPSS Production Job File <a href="http://www.sdifen.com/spssstatistics25.html" target="_blank" rel="noopener">SPSS</a></li></ul><h3 id="How-to-use"><a href="#How-to-use" class="headerlink" title="How to use"></a>How to use</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】spj文件是和医疗的3维数据相关的文件 ……&lt;/p&gt;
    
    </summary>
    
      <category term="精诊科技" scheme="http://yoursite.com/categories/%E7%B2%BE%E8%AF%8A%E7%A7%91%E6%8A%80/"/>
    
    
  </entry>
  
</feed>
