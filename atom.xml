<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Go Further</title>
  
  <subtitle>Stay Hungry, Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-09-07T08:57:46.916Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>CaptainSE</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Terminal_commands</title>
    <link href="http://yoursite.com/2019/09/07/Terminal-commands/"/>
    <id>http://yoursite.com/2019/09/07/Terminal-commands/</id>
    <published>2019-09-07T08:51:49.000Z</published>
    <updated>2019-09-07T08:57:46.916Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><ul><li><p>压缩\解压</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 压缩</span></span><br><span class="line">tar –zcvf file.tar.gz  file/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压</span></span><br><span class="line">tar -zxvf file.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-z：有gzip属性的</span><br><span class="line">-c: 建立压缩档案</span><br><span class="line">-x：解压</span><br><span class="line">-v：显示所有过程</span><br><span class="line">-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>PET_Papers</title>
    <link href="http://yoursite.com/2019/09/04/PET-Papers/"/>
    <id>http://yoursite.com/2019/09/04/PET-Papers/</id>
    <published>2019-09-04T12:56:43.000Z</published>
    <updated>2019-09-04T13:23:31.924Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><ul><li><p>《Multiscale deep neural network based analysis of FDG-PET images for the early diagnosis of Alzheimer’s disease》</p><center><br>  <img src="/2019/09/04/PET-Papers/Multiscale-FDG-PET-Fig-2.png" height="300"><br></center></li><li><p>《<strong>Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images</strong>》</p></li></ul><center><br>  <img src="/2019/09/04/PET-Papers/Multimodal-and-Multiscale-Fig-2.png" height="300"><br></center><ul><li>《Multimodal Neuroimaging Feature Learning With Multimodal Stacked Deep Polynomial Networks for Diagnosis of Alzheimer’s Disease》</li></ul><center><br>  <img src="/2019/09/04/PET-Papers/MM-SDPN-Fig-4.png" height="300"><br></center><ul><li>《A Deep Neural Network Approach for Early Diagnosis of Mild Cognitive Impairment Using Multiple Features》 </li></ul><center><br>  <img src="/2019/09/04/PET-Papers/Multiple-Features-Fig-1.png" height="300"><br></center><ul><li>《Multi-modal deep learning model for auxiliary diagnosis of Alzheimer’s disease》</li></ul><center><br>  <img src="/2019/09/04/PET-Papers/Multi-modal-auxiliary-diagnosis-Fig-3.png" width="500"><br></center><ul><li><p>《Shearlet based Stacked Convolutional Network for Multiclass Diagnosis of Alzheimer’s Disease using the Florbetapir PET Amyloid Imaging Data》</p><center><br>  <img src="/2019/09/04/PET-Papers/Shearlet-based-Fig-1.png" width="500"><br></center></li><li><p>《Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer’s Disease》</p></li></ul><center><br>  <img src="/2019/09/04/PET-Papers/Multiclass-Fig-1.png" width="500"><br>  <img src="/2019/09/04/PET-Papers/Multiclass-Fig-2.png" height="300"><br></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/Paper/ADNI/"/>
    
    
  </entry>
  
  <entry>
    <title>AD-Papers-2</title>
    <link href="http://yoursite.com/2019/08/13/AD-Papers-2/"/>
    <id>http://yoursite.com/2019/08/13/AD-Papers-2/</id>
    <published>2019-08-13T12:01:33.000Z</published>
    <updated>2019-09-04T13:03:39.168Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h4 id="Alzheimer’s-amp-Dementia"><a href="#Alzheimer’s-amp-Dementia" class="headerlink" title="Alzheimer’s &amp; Dementia"></a>Alzheimer’s &amp; Dementia</h4><h5 id="A-deep-learning-model-for-early-prediction-of-Alzheimer’s-disease-dementia-based-on-hippocampal-MRI"><a href="#A-deep-learning-model-for-early-prediction-of-Alzheimer’s-disease-dementia-based-on-hippocampal-MRI" class="headerlink" title="A deep learning model for early prediction of Alzheimer’s disease dementia based on hippocampal MRI"></a><strong>A deep learning model for early prediction of Alzheimer’s disease dementia based on hippocampal MRI</strong></h5><blockquote><p>基于海马MRI的早期预测阿尔茨海默病痴呆的深度学习模型</p></blockquote><p><strong>Abstract</strong></p><p><strong>Introduction:</strong> 在基线时难以预测何时以及符合轻度认知障碍（MCI）标准的个体最终会进展为阿尔茨海默病（AD）痴呆症.</p><p><strong>Methods: </strong>基于2146名受试者的MRI扫描（803名用于训练，1343名用于验证）开发和验证深度学习方法，以在时间 - 事件分析设置中预测MCI受试者进展为AD痴呆。</p><p><strong>Results:</strong> 深度学习时间 - 事件模型预测个体受试者进展为AD痴呆，进展为439 ADNI测试MCI受试者的一致性指数（C指数）为0.762，随访时间为6至78个月（四分位数：[ 24,42,54]和40个AIBL测试MCI受试者的C指数为0.781，随访时间为18-54个月（四分位数：[18,36,54]）。预测的进展风险还将个体受试者聚集成亚组，其与AD痴呆的进展时间存在显着差异（p &lt;0.0002）。当基于深度学习的进展风险与基线临床测量相结合时，获得了用于预测AD痴呆进展的改善的表现（C-指数= 0.864）。</p><p><strong>Conclusion:</strong> 我们的方法提供了一种具有成本效益和准确的预后手段，并有可能促进临床试验中与可能在特定时间段内进展的个体的登记。</p><p><strong>Keywords:</strong> 深度学习;海马;时间分析;阿尔茨海默氏病</p><center><br>  <img src="/2019/08/13/AD-Papers-2/hippocampal_MRI_Fig_1.png" width="500"><br></center><h5 id="EARLY-PREDICTION-OF-ALZHEIMER’S-DISEASE-DEMENTIA-BASED-ON-BASELINE-HIPPOCAMPAL-MRI-AND-1-YEAR-FOLLOW-UP-COGNITIVE-MEASURES-USING-DEEP-RECURRENT-NEURAL-NETWORKS"><a href="#EARLY-PREDICTION-OF-ALZHEIMER’S-DISEASE-DEMENTIA-BASED-ON-BASELINE-HIPPOCAMPAL-MRI-AND-1-YEAR-FOLLOW-UP-COGNITIVE-MEASURES-USING-DEEP-RECURRENT-NEURAL-NETWORKS" class="headerlink" title="EARLY PREDICTION OF ALZHEIMER’S DISEASE DEMENTIA BASED ON BASELINE HIPPOCAMPAL MRI AND 1-YEAR FOLLOW-UP COGNITIVE MEASURES USING DEEP RECURRENT NEURAL NETWORKS"></a>EARLY PREDICTION OF ALZHEIMER’S DISEASE DEMENTIA BASED ON BASELINE HIPPOCAMPAL MRI AND 1-YEAR FOLLOW-UP COGNITIVE MEASURES USING DEEP RECURRENT NEURAL NETWORKS</h5><blockquote><p>基于基线海马MRI的阿尔茨海默病痴呆早期预测及使用深度回归神经网络的1年随访认知测量</p></blockquote><p><strong>ABSTRACT</strong></p><p>多模式生物学，成像和神经心理学标志物已经证明了区分阿尔茨海默病（AD）患者和认知正常老年人的有希望的表现。然而，仍然难以早期预测何时以及哪些轻度认知障碍（MCI）个体将转变为AD痴呆。通过模式分类研究表明，建立在<code>纵向数据上的模式分类器</code>可以获得比基于横截面数据的模式分类器更好的分类性能，我们开发了基于递归神经网络（<code>RNN</code>）的深度学习模型，以学习信息表示和时间动态对个体受试者进行纵向认知测量，并将其与基线海马MRI相结合，建立AD痴呆进展的预后模型。大量MCI受试者的实验结果表明，深度学习模型可以从纵向数据中学习信息量度，用于表征MCI受试者进展为AD痴呆的过程，预后模型可以高精度地早期<code>预测AD进展</code>。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/DEEP_RECURRENT_NEURAL_NETWORKS_Fig_1.png" width="250"><br>  <img src="/2019/08/13/AD-Papers-2/DEEP_RECURRENT_NEURAL_NETWORKS_Fig_2.png" width="250"><br></center><h4 id="Elsevier-Medical-Image-Analysis"><a href="#Elsevier-Medical-Image-Analysis" class="headerlink" title="Elsevier(Medical Image Analysis)"></a>Elsevier(Medical Image Analysis)</h4><h5 id="Disease-prediction-using-graph-convolutional-networks-Application-to-Autism-Spectrum-Disorder-and-Alzheimer’s-disease"><a href="#Disease-prediction-using-graph-convolutional-networks-Application-to-Autism-Spectrum-Disorder-and-Alzheimer’s-disease" class="headerlink" title="Disease prediction using graph convolutional networks: Application to Autism Spectrum Disorder and Alzheimer’s disease"></a>Disease prediction using graph convolutional networks: Application to Autism Spectrum Disorder and Alzheimer’s disease</h5><blockquote><p>使用图卷积网络的疾病预测：应用于自闭症谱系障碍和阿尔茨海默病</p></blockquote><p><strong>ABSTRACT</strong></p><p>图被广泛用作自然框架，其捕获在图中表示为节点的各个元素之间的交互。在医学应用中，具体地，节点可以代表具有一组特征的潜在大群体（患者或健康对照）内的个体，而图形边缘以直观方式结合对象之间的关联。该表示允许在疾病分类任务中同时包含丰富的成像和非成像信息以及个体主题特征。以前基于图的方法用于在疾病预测的背景下进行监督或无监督学习仅关注于受试者之间的成对相似性，忽视个体特征和特征，或者更确切地说依赖于特定于受试者的成像特征向量并且不能模拟它们之间的相互作用。在本文中，我们对通用框架进行了全面评估，该框架利用成像和非成像信息，可用于大量人群的大脑分析。该框架利用图形卷积网络（<code>GCN</code>）并涉及将种群表示为稀疏图，其中其节点与基于成像的特征向量相关联，而表型信息被集成为边缘权重。广泛的评估探讨了该框架的每个组成部分对疾病预测绩效的影响，并进一步将其与不同的基线进行比较。框架性能在两个具有不同基础数据ABIDE和ADNI的大型数据集上进行测试，分别用于预测自闭症谱系障碍和转变为阿尔茨海默病。我们的分析表明，我们的新框架可以提高两个数据库的最新结果，ABIDE的分类准确率为70.4％，<code>ADNI的分类准确率为80.0％</code>。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/graph_convolutional_networks_Fig_1.png" width="250"><br>  <img src="/2019/08/13/AD-Papers-2/graph_convolutional_networks_Fig_2_3.png" width="250"><br></center><h5 id="Fast-predictive-simple-geodesic-regression"><a href="#Fast-predictive-simple-geodesic-regression" class="headerlink" title="Fast predictive simple geodesic regression"></a>Fast predictive simple geodesic regression</h5><blockquote><p>快速预测简单测地回归</p></blockquote><p><strong>ABSTRACT</strong></p><p>可变形图像配准和回归是医学图像分析中的重要任务。但是，它们的计算成本很高，尤其是在分析包含数千个图像的大型数据集时。因此，通常使用集群计算，使得这些方法依赖于这种计算基础设施。随着研究规模的增加，甚至需要更大的计算资源。这限制了可变形图像配准和回归在临床应用中的使用以及作为其他图像分析方法的组合算法。因此，我们建议<code>使用快速预测方法来执行图像配准</code>。特别是，我们采用这些快速配准预测来近似简化的测地回归模型来捕捉<code>纵向大脑变化</code>。得到的方法比基于标准优化的回归模型<code>快</code>几个数量级，因此有助于在单个图形处理单元（GPU）上进行大规模分析。我们从ADNI数据集评估我们对3D脑磁共振图像（MRI）的结果。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/geodesic_regression_Fig_1.png" width="450"><br>  <img src="/2019/08/13/AD-Papers-2/geodesic_regression_Fig_2.png" width="450"><br></center><h5 id="Landmark-based-deep-multi-instance-learning-for-brain-disease-diagnosis"><a href="#Landmark-based-deep-multi-instance-learning-for-brain-disease-diagnosis" class="headerlink" title="Landmark-based deep multi-instance learning for brain disease diagnosis"></a>Landmark-based deep multi-instance learning for brain disease diagnosis</h5><h4 id="Elsevier（Others）"><a href="#Elsevier（Others）" class="headerlink" title="Elsevier（Others）"></a>Elsevier（Others）</h4><h5 id="Multi-stream-multi-scale-deep-convolutional-networks-for-Alzheimer’s-disease-detection-using-MR-images"><a href="#Multi-stream-multi-scale-deep-convolutional-networks-for-Alzheimer’s-disease-detection-using-MR-images" class="headerlink" title="Multi-stream multi-scale deep convolutional networks for Alzheimer’s disease detection using MR images"></a>Multi-stream multi-scale deep convolutional networks for Alzheimer’s disease detection using MR images</h5><blockquote><p>使用MR图像进行阿尔茨海默病检测的多流多尺度深度卷积网络</p></blockquote><p><strong>ABSTRACT</strong></p><p>本文讨论了磁共振图像（MRI）检测阿尔茨海默病（AD）的问题。现有的AD检测方法依赖于全脑扫描的全局特征学习，同时取决于组织类型，不同组织区域中的AD相关特征，例如，灰质（GM），白质（WM）和脑脊液（CSF）表现出不同的特征。在本文中，我们提出了一种<code>基于分割组织区域的多尺度特征学习的深度学习方法</code>。提出了一种新的深度三维多尺度卷积网络方案，用于生成AD检测的<code>多分辨率特征</code>。所提出的方案采用几个并行的3D多尺度卷积网络，每个网络应用于单个组织区域（GM，WM和CSF），然后是特征融合。所提出的融合应用于两个单独的水平：第一级融合应用于相同组织区域内的不同尺度，第二级别应用于不同组织区域。为了进一步减小特征的尺寸并减轻过度拟合，在分类之前使用特征提升和降维方法XGBoost。所提出的深度学习方案已经在ADNI的中等开放数据集上进行了测试（来自337个受试者的1198次扫描），在随机分区数据集上具有出色的测试性能（最佳99.67％，平均98.29％），并且在主题分离的分区上具有良好的测试性能数据集（最佳94.74％，平均89.51％）。还包括与最先进方法的比较。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/Multi-stream_multi-scale_Fig_1.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/Multi-stream_multi-scale_Fig_3.png" width="500"><br></center><h5 id="Deep-learning-only-by-normal-brain-PET-identify-unheralded-brain-anomalies"><a href="#Deep-learning-only-by-normal-brain-PET-identify-unheralded-brain-anomalies" class="headerlink" title="Deep learning only by normal brain PET identify unheralded brain anomalies"></a>Deep learning only by normal brain PET identify unheralded brain anomalies</h5><blockquote><p>仅通过正常脑PET进行深度学习可识别<code>未发现的大脑异常</code></p></blockquote><p><strong>ABSTRACT</strong></p><p><strong>背景</strong>：最近的深度学习模型已经显示出诊断分类的显着准确性。然而，由于训练组和现实世界数据之间的差距，它们在临床应用中具有局限性。我们的目的是开发一种仅通过正常脑PET数据训练的模型，以<code>无监督</code>的方式识别各种疾病中的异常，作为临床常规的成像数据。<br><strong>方法</strong>：使用<code>变分自动编码器</code>(variational autoencoder)，一种无监督学习，<code>异常分数</code>(Abnormality Score)定义为给定脑图像与正常数据的距离。该模型应用于阿尔茨海默病（AD）和轻度认知障碍（MCI）的FDG PET数据和用于评估行为异常和癫痫发作的临床常规FDG PET数据。通过接收器操作特性（ROC）曲线的曲线下面积（AUC）测量准确度。我们调查了深度学习是否具有额外的好处，专家的视觉解释，以识别异常模式。<br><strong>结果</strong>：用于区分AD的ROC曲线的AUC为0.90。从基线到2年随访的认知评分变化与基线时的异常评分显着相关。用于区分患有各种疾病的患者的ROC曲线的AUC为0.74。专家的视觉解释得到了深度学习模型的帮助，以确定60％没有模型的情况下最初没有识别的异常模式。<br><strong>解释</strong>：我们建议仅通过正常数据训练的深度学习模型适用于识别脑部疾病的广泛异常，甚至是不常见的，提出其可能用于解释现实世界的临床数据。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/unheralded_brain_anomalies_Fig_1.png" width="500"><br></center><h5 id="Diagnosis-and-monitoring-of-Alzheimer’s-patients-using-classical-and-deep-learning-techniques"><a href="#Diagnosis-and-monitoring-of-Alzheimer’s-patients-using-classical-and-deep-learning-techniques" class="headerlink" title="Diagnosis and monitoring of Alzheimer’s patients using classical and deep learning techniques"></a>Diagnosis and monitoring of Alzheimer’s patients using classical and deep learning techniques</h5><blockquote><p>使用常规和深度学习技术诊断和监测阿尔茨海默病患者</p></blockquote><p><strong>ABSTRACT</strong></p><p>基于机器的分析和预测系统广泛用于诊断阿尔茨海默病（AD）。然而，现有技术的较低准确性和缺乏后诊断监测系统限制了此类研究的范围。本文提出了一种基于机器学习的AD样疾病诊断和监测方法。通过使用深度学习分析磁共振成像（MRI）扫描来完成AD样疾病诊断过程，然后是<code>活动监测框架</code>，以使用身体磨损的惯性传感器监测受试者的日常生活活动。活动监测为日常生活活动提供辅助框架，并根据活动水平评估患者的脆弱性。与众所周知的现有技术相比，AD诊断结果显示出高达82％的改善。此外，<code>对于日常生活活动进行分类，达到95％以上的准确度</code>，这在监测受试者的活动概况方面是非常令人鼓舞的。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/monitoring_Fig_2.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/monitoring_Fig_5_6.png" width="500"><br></center><h5 id="Multi-modal-deep-learning-model-for-auxiliary-diagnosis-of-Alzheimer’s-disease"><a href="#Multi-modal-deep-learning-model-for-auxiliary-diagnosis-of-Alzheimer’s-disease" class="headerlink" title="Multi-modal deep learning model for auxiliary diagnosis of Alzheimer’s disease"></a>Multi-modal deep learning model for auxiliary diagnosis of Alzheimer’s disease</h5><blockquote><p>多模态深度学习模型辅助诊断阿尔茨海默病</p></blockquote><p><strong>ABSTRACT</strong></p><p>阿尔茨海默病（AD）是最难治愈的疾病之一。阿尔茨海默病严重影响老年人及其家人的正常生活。轻度认知障碍（MCI）是正常衰老和阿尔茨海默病之间的过渡状态，MCI最有可能在以后转变为AD。 MCI经常被误诊为正常衰老的症状，这导致错过最佳治疗机会。因此，MCI的准确诊断对AD的早期诊断和治疗至关重要。本文提出了AD辅助诊断的深度学习模型，模拟临床医生的诊断过程。在诊断AD期间，临床医生通常会参考各种神经影像学的结果，以及神经心理学诊断的结果。在本文中，<code>多模态医学图像由两个独立的卷积神经网络训练</code>。然后通<code>过相关分析判断两个卷积神经网络的输出的一致性</code>。最后，多模态神经影像诊断的结果<code>与临床神经心理学诊断的结果相结合</code>。该模型同时提供了患者病理和心理的综合分析，提高了辅助诊断的准确性。诊断过程更接近临床医生的诊断过程，易于实施。 ADNI（阿尔茨海默病神经影像学计划）公共数据库的实验表明，该方法具有更好的性能，可以在AD辅助诊断中取得良好的诊断效率。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/Multi-modal_deep_learning_model_Fig_3.png" width="500"><br></center><h5 id="RNN-based-longitudinal-analysis-for-diagnosis-of-Alzheimer’s-disease"><a href="#RNN-based-longitudinal-analysis-for-diagnosis-of-Alzheimer’s-disease" class="headerlink" title="RNN-based longitudinal analysis for diagnosis of Alzheimer’s disease"></a>RNN-based longitudinal analysis for diagnosis of Alzheimer’s disease</h5><blockquote><p>基于RNN的纵向分析诊断阿尔茨海默病<br>Ruoxuan Cuia, Manhua Liua,b,∗, the Alzheimer’s Disease Neuroimaging Initiative<br>Received 8 July 2018; Received in revised form 30 September 2018; Accepted 21 January 2019</p></blockquote><p><strong>ABSTRACT</strong></p><p>阿尔茨海默病（AD）是一种不可逆的神经退行性疾病，伴有记忆和其他心理功能的进行性损害。磁共振图像（MRI）已被广泛用作脑的重要成像模式，用于AD诊断和监测疾病进展。顺序MRI的纵向分析对于模拟和测量疾病沿时间轴的进展非常重要，以便更准确地诊断。大多数现有方法使用MRI提取捕获脑的形态异常及其纵向变化的特征，然后设计分类器以区分不同的组。但是，这些方法有一些局限性。首先，由于特征提取和分类器模型是独立的，因此提取的特征可能无法捕获与AD相关的大脑异常的完整特征。其次，对于一些受试者，在某些时间点可能缺少纵向MR图像，这导致难以提取用于纵向分析的一致特征。在本文中，我们提出了一个基于卷积和递归神经网络相结合的分类框架，用于AD诊断中结构MR图像的纵向分析。首先，构造卷积神经网络（<code>CNN</code>）以学习用于分类任务的MR图像的<code>空间特征</code>。之后，在多个时间点的CNN输出上构建具有级联三个双向门控递归单元（BGRU）层的递归神经网络（<code>RNN</code>），用于提取AD分类的<code>纵向特征</code>。该方法不是独立地进行特征提取和分类器训练，而是共同学习空间和纵向特征以及疾病分类器，从而实现最佳性能。另外，所提出的方法可以使用RNN从不同时间点的成像数据对纵向分析进行建模。我们的方法用830名参与者的纵向T1加权MR图像进行评估，包括198名AD，403名轻度认知障碍（MCI）和229名来自阿尔茨海默病神经影像学倡议（ADNI）数据库的正常对照（NC）受试者。实验结果表明，该方法实现了<code>AD与NC的分类精度为91.33％，pMCI与sMCI的分类精度分别为71.71％</code>，证明了纵向MR图像分析具有良好的性能。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/RNN_Fig_1.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/RNN_Fig_2.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/RNN_Fig_3.png" width="500"><br></center><h5 id="A-hybrid-Convolutional-and-Recurrent-Neural-Network-for-Hippocampus-Analysis-in-Alzheimer’s-Disease"><a href="#A-hybrid-Convolutional-and-Recurrent-Neural-Network-for-Hippocampus-Analysis-in-Alzheimer’s-Disease" class="headerlink" title="A hybrid Convolutional and Recurrent Neural Network for Hippocampus Analysis in Alzheimer’s Disease"></a>A hybrid Convolutional and Recurrent Neural Network for Hippocampus Analysis in Alzheimer’s Disease</h5><blockquote><p>用于阿尔茨海默病海马分析的混合卷积和递归神经网络<br>Fan Lia, Manhua Liua,b,c,⁎, the Alzheimer’s Disease Neuroimaging Initiative<br>Received 1 February 2019; Received in revised form 22 April 2019; Accepted 15 May 2019</p></blockquote><p><strong>ABSTRACT</strong></p><p><strong>背景</strong>：海马是最早受神经退行性疾病影响的结构之一，如阿尔茨海默病（AD）和轻度认知障碍（MCI）。可以使用结构MR图像根据海马体积和形状评估海马萎缩。然而，海马面罩的形状和体积特征对于AD诊断具有有限的辨别信息。此外，这些特征的提取与分类模型无关，导致疾病诊断的次优性能。<br><strong>新方法</strong>：本文提出了一种<code>混合卷积和递归神经网络</code>，用于在AD中使用<code>结构MR图像</code>进行更详细的<code>海马体分析</code>。 DenseNets构建在内部和外部海马的分解图像块上，以学习强度和形状特征。循环神经网络（RNN）级联以结合左右海马的特征并学习疾病分类的高级特征。<br><strong>结果</strong>：我们提出的方法用807名受试者的基线MR图像进行评估，包括194名AD，397名MCI和216名来自阿尔茨海默病神经影像学倡议（ADNI）数据库的正常对照（NC）。实验表明，对于<code>AD与NC</code>，<code>MCI与NC</code>和<code>pMCI与sMCI</code>的分类，所提出的方法实现了AUC（ROC曲线下面积）分别为91.0％，75.8％和74.6％。<br><strong>与现有方法比较</strong>：该方法比体积和形状分析方法具有更好的性能。<br><strong>结论</strong>：结合DenseNets和双向门控复发单元（BGRU）进行海马分析和AD诊断，提出了一种混合卷积和复发神经网络。结果显示其优化的性能。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/hybrid_Fig_1.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/hybrid_Fig_4.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/hybrid_Fig_5.png" width="500"><br></center><h5 id="CBIR-system-using-Capsule-Networks-and-3D-CNN-for-Alzheimer’s-disease-diagnosis"><a href="#CBIR-system-using-Capsule-Networks-and-3D-CNN-for-Alzheimer’s-disease-diagnosis" class="headerlink" title="CBIR system using Capsule Networks and 3D CNN for Alzheimer’s disease diagnosis"></a>CBIR system using Capsule Networks and 3D CNN for Alzheimer’s disease diagnosis</h5><blockquote><p>使用胶囊网络和3D CNN进行阿尔茨海默病诊断的CBIR系统<br>K.R. Kruthikaa,∗, Rajeswarib, H.D. Maheshappab, Alzheimer’s Disease Neuroimaging Initiative1<br>Received 27 August 2018; Received in revised form 30 November 2018; Accepted 4 December 2018</p></blockquote><p><strong>ABSTRACT</strong></p><p>阿尔茨海默病（AD）是一种与记忆丧失相关的不可逆的大脑疾病，常见于老年人和老年人群。利用<code>基于内容的图像检索（CBIR）</code>的革命性计算机辅助诊断技术的实现已经在磁共振成像（MRI）中在相关图像检索和训练中创造了新的潜力，用于检测早期AD的进展。本文提出了一种<code>使用3D胶囊网络，3D卷积神经网络和预训练3D自动编码器技术的早期检测阿尔茨海默病的CBIR系统</code>。 3D-Capsule Networks（CapsNets）能够快速学习，即使对于小型数据集也能够有效处理强大的图像旋转和转换。据观察，与单独的Deep-CNN相比，使用3D-CapsNets和卷积神经网络（CNN）与3D自动编码器的集合方法提高了检测性能。发现使用所提出的模型的CBIR<code>在AD分类中高达98.42％的准确度</code>。此外，我们认为CapsNet似乎是一种很有前景的图像分类新技术，使用更强大的计算资源和精炼的CapsNet架构的进一步实验可能会产生更好的结果。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/CBIR_Fig_1.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/CBIR_Fig_2.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/CBIR_Fig_3.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/CBIR_Fig_4.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/CBIR_Fig_8.png" width="500"><br></center><h4 id="IEEE"><a href="#IEEE" class="headerlink" title="IEEE"></a>IEEE</h4><h5 id="Hippocampus-Analysis-by-Combination-of-3D-DenseNet-and-Shapes-for-Alzheimer’s-Disease-Diagnosis"><a href="#Hippocampus-Analysis-by-Combination-of-3D-DenseNet-and-Shapes-for-Alzheimer’s-Disease-Diagnosis" class="headerlink" title="Hippocampus Analysis by Combination of 3D DenseNet and Shapes for Alzheimer’s Disease Diagnosis"></a>Hippocampus Analysis by Combination of 3D DenseNet and Shapes for Alzheimer’s Disease Diagnosis</h5><blockquote><p>结合3D DenseNet和形状进行阿尔茨海默病诊断的海马分析<br>Ruoxuan Cui, Manhua Liu* and the Alzheimer’s Disease Neuroimaging Initiative</p></blockquote><p><strong>ABSTRACT</strong></p><p>摘要 - 海马是阿尔茨海默病（AD）和轻度认知功能障碍（MCI）中首批涉及的区域之一，这是AD的前驱期。海马萎缩是一种经过验证，易于获取且广泛使用的AD诊断生物标志物。大多数现有方法使用结构磁共振图像（MRI）计算海马体分析的形状和体积特征。然而，<code>与海马相邻的区域</code>可能与AD有关，并且海马区域的视觉特征对于疾病诊断是重要的。在本文中，我们提出了一种新的海马分析方法，通过3D密集连接卷积网络（<code>3D DenseNet</code>）和AD诊断的<code>形状分析</code>，并结合<code>海马的全局和局部特征</code>。所提出的方法可以利用局部视觉和全局形状特征来增强分类。所提出的方法<code>不需要组织分割和非线性配准</code>。我们的方法用来自811名受试者的T1加权结构MRI评估，包括192 AD，396 MCI（231稳定MCI和165进行性MCI），223阿尔茨海默病神经成像倡议（ADNI）数据库中的正常对照（NC）。实验结果表明，该方法对<code>AD诊断的分类准确率为92.29％，AUC（ROC曲线下面积）为96.95％</code>。结果对比表明，该方法比其他方法表现更好。</p><center><br>  <img src="/2019/08/13/AD-Papers-2/3D_DenseNet_and_Shapes_Fig_1.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/3D_DenseNet_and_Shapes_Fig_2.png" width="500"><br>  <img src="/2019/08/13/AD-Papers-2/3D_DenseNet_and_Shapes_Fig_3.png" width="500"><br></center><h5 id="Hierarchical-Fully-Convolutional-Network-for-Joint-Atrophy-Localization-and-Alzheimer’s-Disease-Diagnosis-using-Structural-MRI"><a href="#Hierarchical-Fully-Convolutional-Network-for-Joint-Atrophy-Localization-and-Alzheimer’s-Disease-Diagnosis-using-Structural-MRI" class="headerlink" title="Hierarchical Fully Convolutional Network for Joint Atrophy Localization and Alzheimer’s Disease Diagnosis using Structural MRI"></a>Hierarchical Fully Convolutional Network for Joint Atrophy Localization and Alzheimer’s Disease Diagnosis using Structural MRI</h5><p><strong>ABSTRACT</strong></p><h5 id="Joint-Classification-and-Regression-via-Deep-Multi-Task-Multi-Channel-Learning-for-Alzheimer’s-Disease-Diagnosis"><a href="#Joint-Classification-and-Regression-via-Deep-Multi-Task-Multi-Channel-Learning-for-Alzheimer’s-Disease-Diagnosis" class="headerlink" title="Joint Classification and Regression via Deep Multi-Task Multi-Channel Learning for Alzheimer’s Disease Diagnosis"></a>Joint Classification and Regression via Deep Multi-Task Multi-Channel Learning for Alzheimer’s Disease Diagnosis</h5><p><strong>ABSTRACT</strong></p><h5 id="Studying-the-Manifold-Structure-of-Alzheimer’s-Disease-A-Deep-Learning-Approach-Using-Convolutional-Autoencoders"><a href="#Studying-the-Manifold-Structure-of-Alzheimer’s-Disease-A-Deep-Learning-Approach-Using-Convolutional-Autoencoders" class="headerlink" title="Studying the Manifold Structure of Alzheimer’s Disease A Deep Learning Approach Using Convolutional Autoencoders"></a>Studying the Manifold Structure of Alzheimer’s Disease A Deep Learning Approach Using Convolutional Autoencoders</h5><p><strong>ABSTRACT</strong></p><h5 id="Deep-Learning-of-Static-and-Dynamic-Brain-Functional-Networks-for-Early-MCI-Detection"><a href="#Deep-Learning-of-Static-and-Dynamic-Brain-Functional-Networks-for-Early-MCI-Detection" class="headerlink" title="Deep Learning of Static and Dynamic Brain Functional Networks for Early MCI Detection"></a>Deep Learning of Static and Dynamic Brain Functional Networks for Early MCI Detection</h5><p><strong>ABSTRACT</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/Paper/ADNI/"/>
    
    
  </entry>
  
  <entry>
    <title>Multimodal-and-Multiscale-Deep-Neural-Networks-for-the-Early-Diagnosis-of-Alzheimer’s-Disease-using-structural-MR-and-FDG-PET-images</title>
    <link href="http://yoursite.com/2019/08/09/Multimodal-and-Multiscale-Deep-Neural-Networks-for-the-Early-Diagnosis-of-Alzheimer%E2%80%99s-Disease-using-structural-MR-and-FDG-PET-images/"/>
    <id>http://yoursite.com/2019/08/09/Multimodal-and-Multiscale-Deep-Neural-Networks-for-the-Early-Diagnosis-of-Alzheimer’s-Disease-using-structural-MR-and-FDG-PET-images/</id>
    <published>2019-08-09T14:10:37.000Z</published>
    <updated>2019-08-09T15:21:48.157Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><blockquote><p>使用结构MR和FDG-PET图像进行早期诊断阿尔茨海默病的多模态和多尺度深度神经网络</p></blockquote><h3 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a><strong>ABSTRACT</strong></h3><p>阿尔茨海默病（AD）是一种进行性神经退行性疾病。遗忘性轻度认知障碍（MCI）是转变为临床损伤之前的常见首发症状，其中个体变得不能独立地进行日常生活活动。虽然目前没有可用的治疗方法，但早期确定的诊断结果越早，干预的可能性就越早，甚至可能阻止进展为完全AD。从MRI获得的神经成像扫描和通过FDG-PET获得的代谢图像提供了对活脑的结构和功能（葡萄糖代谢）的体内观察。假设结合不同的图像模态可以更好地表征人脑的变化，从而更准确地早期诊断AD。在本文中，我们提出了一种新的框架，以区分正常对照（NC）受试者与AD病理对象（AD和NC，MCI受试者将来转换为AD）。我们利用多模态和多尺度深度神经网络的新方法被发现在转换后3年内对受试者的预测提供了85.68％的准确度。交叉验证实验证明，与现有已发表文献的结果相比，它具有更好的辨别能力。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><strong>Introduction</strong></h3><p>阿尔茨海默病（AD）是最常见的痴呆症，影响了65岁以上的9个人中的1个。阿尔茨海默氏病涉及进行性认知障碍，通常与早期记忆丧失有关，需要在晚期阶段为自我护理活动提供帮助。阿尔茨海默氏症通过前驱期进化，通常被称为轻度认知障碍（MCI）阶段，10-15％的MCI患者每年进展为AD。随着预期寿命的提高，估计全球约1.2％的人口将在2046年之前患上阿尔茨海默病，从而直接影响数百万人，并且通过对家人和照顾者的影响间接影响更多人。目前阿尔茨海默氏症的研究目标是对患有阿尔茨海默病的病人进行可靠的前驱鉴定，原因是早期干预可能会改变病程。临床诊断涉及严格的评估，以排除非阿尔茨海默病的认知能力下降的原因，但这受限于识别前驱AD的特异性。因此，我们认为有必要使用一种工具来可靠地检测和鉴别前驱的阿尔茨海默病。<br>过去努力了解AD病理学导致将神经影像学识别为前驱诊断的有希望的工具之一。涉及磁共振成像（MRI）和氟脱氧葡萄糖正电子发射断层扫描（FDG-PET）的神经影像是独特的成像模式，被认为是识别前驱AD患者的有用工具。 MRI提供结构细节，如纹理，厚度，密度和各种大脑区域的形状，而FDG-PET测量静息状态葡萄糖代谢，反映下面组织的功能活动6。 FDG-PET和MRI经常用于神经成像疾病的计算机辅助诊断中的神经成像技术。已经进行了相当大的努力来使用结构MRI或FDG-PET或与其他生物标记物的组合来开发用于前驱诊断阿尔茨海默病的自动计算机辅助工具。<br>深度神经网络已被广泛研究并证明其具有许多识别任务的最佳性能。深度神经网络在识别AD相关模式中的应用也引起了对前驱AD的应用的兴趣。通过应用深度神经网络来提取特征，例如堆叠自动编码器（SAE）或深度玻尔兹曼机器（DBM），这些方法优于其他流行的机器学习方法，例如支持向量机（SVM）和随机森林技术。然而，阻碍神经影像学中深度学习应用的主要障碍之一是它需要大量数据集来训练模型，而可用的图像数量限制在数百或数千，远小于数据样本的特征维数。 。为了克服这一挑战，一种流行的方法是将图像分割成补丁并从每个补丁中提取特征。然而，对输入数据进行下采样可能导致判别信息的丢失，这可能是先前方法未能达到该诊断任务的令人满意的准确度的潜在原因。用于图像识别的模式挖掘和特征提取的共同扩展是多尺度处理。通过提取不同分辨率或尺度的特征，多尺度特征可以更好地表征分类任务的图像，最近的研究表明它还可以提高深度神经网络的分类性能。<br>因此，我们提出了一种将多尺度和多模态处理与深度神经网络相结合的新方法，用于早期诊断AD。通过对1000多名受试者的交叉验证实验，我们证明了1）我们的网络可以从多尺度和多模态特征中学习隐藏模式，用于检测AD病理; 2）我们的方法优于以前关于潜在AD受试者的判别任务的方法; 3）我们的网络可以识别将在3年内转换为AD的受试者，准确率为85.68％，这是一个很有希望的结果。</p><h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a><strong>Methods</strong></h3><p>提出的框架有两个主要步骤：1）图像预处理：将MRI扫描和FDG-PET图像分成补丁，并从每个补丁中提取特征; 并且，2）分类：训练深度神经网络以学习区分AD病理学的模式，然后使用它来分类具有AD病理学的个体。</p><h4 id="Data"><a href="#Data" class="headerlink" title="Data"></a><strong>Data</strong></h4><p>用于制备本文的数据来自阿尔茨海默氏病神经影像学倡议（ADNI）数据库（adni.loni.usc.edu）。 ADNI于2003年作为公私合作伙伴关系启动，由首席研究员Michael W. Weiner博士领导。 ADNI的主要目标是测试是否可以将连续MRI，PET，其他生物标志物以及临床和神经心理学评估结合起来测量轻度认知障碍（MCI）和早期阿尔茨海默病（AD）的进展。<br>为了全面验证所提出的方法，强调在本研究中使用所有可用的ADNI受试者（N = 1242），其具有在制备该手稿时的T1加权MRI扫描和FDG-PET图像。这些受试者被分为5组：1）稳定的正常对照（sNC）：在基线时诊断为NC的360名受试者，并且在制备该手稿时保持相同; 2）稳定的MCI（sMCI）：在所有时间点（至少2年）诊断为MCI的409名受试者; 3）进行性NC（pNC）：18名受试者在基线时评估为NC，但在制备该手稿时已进展至可能的AD; 4）进行性MCI（pMCI）：217名受试者在基线时评估为MCI并且进展至可能的AD; 5）稳定的阿尔茨海默病（sAD）：238名受试者被诊断为所有可用时间点的AD。受试者的人口统计学和临床信息如表1所示。括号中的数字是第二行中男性和女性受试者的数量，而其余3行中的两个数字代表年龄，教育年份的最小值和最大值和MMSE（迷你精神状态检查）得分。值得一提的是，每个受试者可以在不同的时间点进行多次扫描。在该研究中总共有2402个FDG-PET扫描和2402个MRI图像。有关ADNI主题群组，图像采集协议程序和采集后预处理程序的详细说明，请访问<a href="http://www.adni-info.org。" target="_blank" rel="noopener">http://www.adni-info.org。</a></p><h4 id="Image-Processing"><a href="#Image-Processing" class="headerlink" title="Image Processing"></a><strong>Image Processing</strong></h4><p>与深度学习已证明有效的典型图像识别问题不同，我们的数据集相对较小。因此，直接使用这个较小的图像数据库来训练深度神经网络不太可能提供高分类精度。然而，与典型的图像识别任务相反，其中图像包含大的异质性，该数据库中的图像是以相似的姿势和比例获得的所有人脑图像，其在比较中显示相对小得多的异质性。因此，我们应用以下处理步骤来提取贴片特征，如图1所示：FreeSurfer 5.1用于将每个T1结构MRI图像分割成灰质和白质，然后将灰质细分为87个感兴趣的解剖区域（ROI） ）。 Freesurfer分割由专家神经解剖学家进行质量控制，并且所记录的任何错误都是手动校正的。然后，对于标准T1 MRI图像，执行基于空间坐标的体素方式k均值聚类，以基于其空间信息将每个ROI分割成块。在本研究中，贴片的大小预定为500,1000和2000个体素，分别产生1488,705和343个贴片。考虑到本研究中可用的数据样本数量有限，它旨在保留足够的详细信息以及避免过大的特征维度。随后，通过高维非刚性配准方法（LDDMM）将标准模板MRI的每个ROI登记到每个目标图像的相同ROI。然后将配准图应用于标准模板的贴片分割。这将模板分割转换为每个目标MRI空间，从而将目标图像细分为相同数量的补丁。值得一提的是，在变换之后，由于非刚性配准编码局部膨胀/收缩，不同图像中模板贴片的大小不一样，因此是用于表示给定结构脑的区域信息的特征之一扫描。然后，对于每个目标受试者，使用基于标准化互信息的FSL-FLIRT程序，使用FSL-FLIRT程序进行刚性变换，将受试者的FDG-PET图像共同配准到其颅骨剥离的T1 MRI扫描。自由度（DOF）设定为12，归一化相关用作成本函数。 FDG-PET图像的脑干区域中的平均强度是选择的参考，以标准化个体脑代谢图像中的体素强度，因为脑干区域最不可能受AD影响。每个贴片的平均强度用作形成特征向量以表示代谢活动的元素，并且每个贴片的体积用于表示脑萎缩。</p><h4 id="Multiscale-Deep-Neural-Network"><a href="#Multiscale-Deep-Neural-Network" class="headerlink" title="Multiscale Deep Neural Network"></a><strong>Multiscale Deep Neural Network</strong></h4><p>利用从MRI和FDG-PET图像中提取的特征，我们训练了多模态多尺度深度神经网络（MMDNN）来执行分类。如图2所示，网络由两部分组成。第一部分是6个独立的深度神经网络（DNN），对应于单个模态的每个尺度。第二部分是用于融合从这6个DNN中提取的特征的另一个DNN。该DNN的输入数据是从每个单个DNN获知的级联潜在表示。两部分的DNN共享相同的结构。对于每个DNN，每个隐藏层的节点数分别设置为3N，3N和100，其中N表示输入特征向量的维数。选择节点4的数量以探索来自第一层中的不同补片的特征之间的所有可能的隐藏相关性，并逐渐减少后续层中的特征的数量以避免过度拟合。我们分别训练每个DNN两个步骤，无监督预训练和监督微调。然后MMDNN的所有参数一起调整。</p><h3 id="Results-and-Discussion"><a href="#Results-and-Discussion" class="headerlink" title="Results and Discussion"></a><strong>Results and Discussion</strong></h3><h4 id="Experiments-Setup"><a href="#Experiments-Setup" class="headerlink" title="Experiments Setup"></a><strong>Experiments Setup</strong></h4><p>提出的深度神经网络是使用Tensorflow构建的，Tensorflow是Google提供的开源深度学习工具箱。首先，为了将判别能力与现有技术方法进行比较，应用10倍交叉验证实验对sMCI和pMCI受试者进行分类。然后我们用不同的训练集进行了三次实验，以测试pNC和pMCI的图像是否包含AD病理学。对于这些实验，将4种数据集：sNC，sAD，pNC和pMCI以3种不同方式分成两组：1）sNC和sAD的受试者被认为是组1，pNC和pMCI的受试者属于组2; 2）pMCI，sNC和sAD的受试者属于group1，pNC被认为是group2; 3）所有受试者均被视为组1。对于每个实验，我们对group1应用了10倍交叉验证。第1组的受试者随机分为10个子集，其中9组用于训练，其余组与用于测试的组2的受试者组合。如方法部分所述，随机选择10％的训练受试者作为早期停止的验证集，以防止过度拟合。对具有不同验证集的10个网络进行训练以对最终分类结果进行“投票”。注意到它不是图像而是我们分裂的对象，因此来自同一主题的不同时间点的图像将不会用于训练和测试。</p><h4 id="Compare-with-State-of-the-Art-Methods"><a href="#Compare-with-State-of-the-Art-Methods" class="headerlink" title="Compare with State-of-the-Art Methods"></a><strong>Compare with State-of-the-Art Methods</strong></h4><p>过去的研究人员一直在研究具有进行性认知衰退和稳定认知障碍的受试者的分类。 pMCI被认为是具有高AD风险的个体，而sMCI被认为是在这些研究中没有风险或AD风险低的个体。 为了评估我们的方法的性能，我们将pMCI与sMCI的分类准确性与使用相同数据模态的先前方法进行比较，即T1加权MRI和FDG-PET13,18,31,32。 所提出的网络在分类pMCI和sMCI个体方面优于最先进的方法，无论使用单模态还是多模式成像，如表2所示。值得一提的是，在Chen et.al32的研究中，他们 进行域转移学习以利用辅助域数据（AD / NC对象）来改进分类。 尽管如此，我们没有辅助知识的方法的准确性比他们的准确率高3.5％。</p><h4 id="AD-Pathology-Classification"><a href="#AD-Pathology-Classification" class="headerlink" title="AD Pathology Classification"></a><strong>AD Pathology Classification</strong></h4><p>sMCI受试者的一个问题是我们只知道他们在准备这份手稿时保持稳定，但他们未来仍可能进展为AD或其他精神疾病。虽然sMCI与pMCI实验通常用于测试近期研究中分类器的辨别能力，但sMCI受试者的分类结果可能不是很准确。因此，我们进行了第二个实验，包括仅用已知的阿尔茨海默病进展（pNC，pMCI和sAD）和正常对照（sNC）对个体进行分类。我们通过在训练阶段使用各种样本组合来研究分类器的性能。在非常基础的层面上，我们通过区分sAD和sNC来训练分类器，在下一级别将pMCI和sAD组合起来代表阿尔茨海默氏症组并训练他们将其与sNC组区分开来。在过去的水平，我们结合PNC，PMCI和悲哀地表示老年痴呆症的研究小组从SNC组来区分。深度神经网络提取的特征如图3所示。我们观察到分类器的准确性和灵敏度通过额外的pMCI和pNC训练逐步改善，而特异性降低，如表4所示。此外，分类器性能为与单独模态的性能相比，FDG-PET和结构测量的组合略微更好。有趣的是，结构成像测量的分类器性能不如FDG-PET测量。支持，FDG-PET，一种神经元活动的衡量标准，与结构图像相比，是识别前驱阿尔茨海默氏症的更好工具。</p><h4 id="Multiscale-Classification"><a href="#Multiscale-Classification" class="headerlink" title="Multiscale Classification"></a><strong>Multiscale Classification</strong></h4><p>表3中列出了用不同尺度提取的特征的分类精度。随着斑块尺寸的变化，我们无法识别出任何增加或减少分类性能的趋势。 因此，具有较高分辨率的特征不一定涵盖较低分辨率特征的辨别信息。 然而，融合多尺度特征与非透视特征相比产生了更高的准确性，表明网络能够捕获粗到细分辨率的判别信息。</p><h4 id="Early-Diagnosis"><a href="#Early-Diagnosis" class="headerlink" title="Early Diagnosis"></a><strong>Early Diagnosis</strong></h4><p>我们还研究了分类器识别疾病发作前患阿尔茨海默氏症高风险人群的能力。与仅用sAD训练的分类器相比，用阿尔茨海默氏病的轨迹（pNC，pMCI和sAD）的组合样本训练的分类器是优越的。由于使用pNC和pMCI对网络分类器进行了AD轨迹模式的训练，因此网络能够在识别具有AD风险的个体时获得特殊的分类性能，即分类器识别具有90.08％，85.61％和81.20的AD风险的个体。 ％，分别在疾病发作前约1,2和3年。过去的研究使用单峰或多模调查预测了AD发病。很少有研究使用PET作为单一模式或结合MRI，CSF或认知测量来预测AD发病。目前网络分析中3年预测的准确性优于引用研究中报告的准确性。使用结构MRI作为独立工具或除了其他临床变量之外的预测疾病发作的研究报告的准确度值低于使用PET获得的准确度值。</p><p>深度神经网络是通过对具有良好表征的对象的图像进行先验训练来准确识别物体的强大工具。因此，使用DNN工具进行精确分类的基本要求是在监督训练阶段提供大量图像（通常以百万计）和良好表征的对象。因此，在训练期间提供的对象的先验知识（阿尔茨海默氏症的特征）的折衷将限制后续分类的准确性。由于我们目前对AD发病机制的理解及其在FDG-PET和结构MRI图像中的精确特征是有限的，DNN在实现100％准确分类方面受到威胁。诊断AD的临床标准涉及一系列评估以提供接近精确的诊断。尽管进行了严格的评估，临床诊断的AD患者并非100％准确，因此FDG-PET和结构MRI特征可能与AD以外的其他疾病重叠，包括NC。因此，用不太精确表征的图像（FDG-PET和结构MRI）训练的DNN不能实现100％准确的分类。我们建议通过升级FDG-PET和结构成像方法或增加对AD特异性发病机制的理解来改进AD特征的表征，这将对DNN分类器工具在未来研究中的分类准确性产生积极影响。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h3><p>总之，我们提出了一种深度神经网络来识别有患阿尔茨海默病风险的个体。我们使用隐藏在不同分辨率和不同模态中的模式训练分类器，以区分具有阿尔茨海默氏症轨迹（pNC，pMCI和sAD）和没有认知缺陷（sNC）的受试者。我们的结果显示分类器使用交叉验证实验成功地将具有AD病理学的个体与sNC区分开来的能力具有82.93％的显着准确度。我们观察到，FDG-PET和结构MRI图像组合构建的网络分类器的性能优于单独使用结构MRI或FDG-PET构建的网络分类器。此外，发现用pNC，pMCI和sAD（阿尔茨海默氏病的轨迹）的组合样本训练的分类器产生最高的分类准确度。最后，我们在疾病发作前识别患有AD病理的个体的实验表明，疾病发作前3年的敏感性为85.68％。因此，所提出的深度神经网络分类器可以是将来用于AD病理学的早期预测的潜在工具。在该研究中pNC受试者的数量有限，导致pNC的准确性相对较低，因为将来积累的数据越多，我们期望在AD病理学的NC受试者的预测中具有更高的准确性。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/Paper/ADNI/"/>
    
    
  </entry>
  
  <entry>
    <title>AD-Papers-Abstract</title>
    <link href="http://yoursite.com/2019/07/29/AD-Papers-Abstract/"/>
    <id>http://yoursite.com/2019/07/29/AD-Papers-Abstract/</id>
    <published>2019-07-29T12:11:30.000Z</published>
    <updated>2019-07-30T07:38:09.729Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h4 id="Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease"><a href="#Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease" class="headerlink" title="Multiscale deep neural network based analysis of FDG-PET images for the early diagnosis of Alzheimer’s disease"></a>Multiscale deep neural network based analysis of FDG-PET images for the early diagnosis of Alzheimer’s disease</h4><blockquote><p> 基于多尺度深度神经网络的FDG-PET图像分析早期诊断阿尔茨海默病</p></blockquote><p>Alzheimer’s disease (AD) is one of the most common neurodegenerative diseases with a commonly seen prodromal mild cognitive impairment (MCI) phase where memory loss is the main complaint progressively worsening with behavior issues and poor self-care. However, not all individuals clinically diagnosed with MCI progress to AD. A fraction of subjects with MCI either progress to non-AD dementia or remain stable at the MCI stage without progressing to dementia. Although a curative treatment of AD is currently unavailable, it is extremely important to correctly identify the individuals in the MCI phase that will go on to develop AD so that they may benefit from a curative treatment when one becomes avail- able in the near future. At the same time, it would be highly desirable to also correctly identify those in the MCI phase that do not have AD pathology so they may be spared from unnecessary pharmocologic interventions that, at best, may provide them no benefit, and at worse, could further harm them with adverse side-effects. Additionally, it may be easier and simpler to identify the cause of the cognitive impairment in these non-AD cases, and hence proper identification of prodromal AD will be of benefit to these individuals as well.<br>Fluorodeoxy glucose positron emission tomography (FDG-PET) captures the metabolic activity of the brain, and this imaging modality has been reported to identify changes related to AD prior to the on- set of structural changes. Prior work on designing classifier using FDG-PET imaging has been promising. Since deep-learning has recently emerged as a powerful tool to mine features and use them for accu- rate labeling of the group membership of given images, we propose a novel deep-learning framework using FDG-PET metabolism imaging to identify subjects at the MCI stage with presymptomatic AD and discriminate them from other subjects with MCI (non-AD / non-progressive). Our multiscale deep neural network obtained 82.51% accuracy of classification just using measures from a single modality (FDG-PET metabolism data) outperforming other comparable FDG-PET classifiers published in the recent literature.</p><p>阿尔茨海默病（AD）是最常见的神经退行性疾病之一，具有常见的前驱性轻度认知障碍（MCI）期，其中记忆丧失是随着行为问题和自我护理不良而逐渐恶化的主要原因。然而，并非所有临床诊断为MCI的个体都进展为AD。一小部分患有MCI的受试者进展至非AD痴呆或在MCI阶段保持稳定而未进展至痴呆。虽然目前无法对AD进行治愈性治疗，但正确识别MCI阶段的个体将非常重要，这些个体将继续发展AD，以便在不久的将来可以获得治愈性治疗。同时，非常需要正确识别那些没有AD病理的MCI期患者，这样他们就可以免于不必要的药物干预，这些干预最多可能不会给他们带来任何好处，更糟糕的是，进一步伤害他们的副作用。另外，在这些非AD病例中识别认知障碍的原因可能更容易和更简单，因此正确鉴定前驱AD也将对这些个体有益。<br>氟脱氧葡萄糖正电子发射断层扫描（FDG-PET）捕获大脑的代谢活动，据报道这种成像方式可以在结构变化发生之前识别与AD相关的变化。使用FDG-PET成像设计分类器的先前工作一直很有前景。由于深度学习最近已成为挖掘特征的有力工具，并将其用于精确标记给定图像的群组成员，我们提出了一种新的深度学习框架，使用FDG-PET代谢成像来识别MCI的受试者患有症状前AD的阶段，并将其与其他MCI（非AD /非进展）受试者区分开来。我们的多尺度深度神经网络仅使用来自单一模态（FDG-PET代谢数据）的测量获得82.51％的分类准确度，优于最近文献中公布的其他可比较的FDG-PET分类器。</p><h4 id="Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer’s-Disease"><a href="#Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer’s-Disease" class="headerlink" title="Application of Machine Learning to Arterial Spin Labeling in Mild Cognitive Impairment and Alzheimer’s Disease"></a>Application of Machine Learning to Arterial Spin Labeling in Mild Cognitive Impairment and Alzheimer’s Disease</h4><blockquote><p>机器学习在动脉自旋标记治疗轻度认知障碍和阿尔茨海默病中的应用</p></blockquote><p><code>Purpose</code>: This current study investigates whether multivariate pattern recognition analysis of arterial spin labeling (ASL) perfusion maps can be used for classification and single-subject prediction of patients with Alzheimer’s disease (AD), mild cognitive impairment (MCI), and subjective cognitive decline (SCD), after using the W-score method to remove confounding effects of gender and age. Materials and Methods: The local institutional review board approved the study. Subjects provided written informed consent. 3.0-T pseudo-continuous ASL images were acquired from 100 probable AD patients, 60 MCI patients, of which 12 remained stable (MCIs), 12 converted to AD (MCIc), and 36 without follow-up, 100 SCD subjects, and 26 healthy controls. The three main groups (i.e. AD, MCI, SCD) were divided into a gender and age-matched training-set (N = 130) and independent prediction-set (N = 130). Standardized perfusion scores adjusted for age and gender (W-scores) were computed per voxel for each subject. Training of a Support Vector Machine (SVM) classifier used diagnostic status and perfusion maps. Discrimination maps were extracted and used for single-subject classification in the prediction-set. Prediction performance was assessed by means of a ROC analysis, generating an area under the curve (AUC) and sensitivity/specificity distribution.<br><code>Results</code>: Single-subject diagnosis in the prediction-set using the discrimination maps yielded excellent performance for AD vs. SCD (AUC .96, p &lt; .01), good performance for AD vs. MCI (AUC = 0.89, p &lt; .01), and poor performance for MCI vs. SCD (AUC = 0.63, p = .06). Application of the AD vs. SCD discrimination map for prediction of MCI subgroups resulted in good performance for MCIc vs. SCD (AUC = .84, p &lt; .01) and fair performance for MCIc vs. MCIs (AUC = .71, p &gt; .05).<br><code>Conclusion</code>: Using automated methods, age- and gender adjusted ASL perfusion maps can be used to classify and predict diagnoses of AD, MCI-converters, stable MCI patients and SCD subjects with good to excellent accuracy and AUC values.<br><code>目的</code>：本研究调查动脉自旋标记（ASL）灌注图的多变量模式识别分析是否可用于阿尔茨海默病（AD），轻度认知障碍（MCI）和主观认知能力下降患者的分类和单一主题预测（SCD），使用W-score方法消除性别和年龄的混杂影响。材料与方法：当地机构审查委员会批准了该研究。受试者提供书面知情同意书。从100例可能的AD患者，60例MCI患者中获得3.0-T假连续ASL图像，其中12例保持稳定（MCI），12例转换为AD（MCIc），36例未随访，100例SCD受试者和26例健康的控制。三个主要组（即AD，MCI，SCD）被分为性别和年龄匹配的训练集（N = 130）和独立预测集（N = 130）。对于每个受试者，针对每个体素计算针对年龄和性别（W分数）调整的标准化灌注分数。支持向量机（SVM）分类器的训练使用诊断状态和灌注图。提取歧视图并将其用于预测集中的单主题分类。通过ROC分析评估预测性能，产生曲线下面积（AUC）和灵敏度/特异性分布。<br><code>结果</code>：使用鉴别图在预测集中进行单一主题诊断，对AD与SCD（AUC .96，p &lt;.01）表现出优异的表现，AD与MCI的良好表现（AUC = 0.89，p &lt;.01 ），MCI与SCD的表现不佳（AUC = 0.63，p = .06）。应用AD与SCD鉴别图预测MCI亚组导致MCIc与SCD的良好表现（AUC = .84，p &lt;.01），MCIc与MCI的公平表现（AUC = .71，p&gt; .05）。<br><code>结论</code>：使用自动化方法，年龄和性别调整的ASL灌注图可用于分类和预测AD，MCI转换器，稳定MCI患者和SCD受试者的诊断，具有良好至极好的准确度和AUC值。</p><h4 id="Reproducible-evaluation-of-classification-methods-in-Alzheimer’s-disease-Framework-and-application-to-MRI-and-PET-data"><a href="#Reproducible-evaluation-of-classification-methods-in-Alzheimer’s-disease-Framework-and-application-to-MRI-and-PET-data" class="headerlink" title="Reproducible evaluation of classification methods in Alzheimer’s disease: Framework and application to MRI and PET data"></a>Reproducible evaluation of classification methods in Alzheimer’s disease: Framework and application to MRI and PET data</h4><blockquote><p>阿尔茨海默病分类方法的可重复评估：MRI和PET数据的框架和应用</p></blockquote><p>A large number of papers have introduced novel machine learning and feature extraction methods for automatic classification of Alzheimer’s disease (AD). However, while the vast majority of these works use the public dataset ADNI for evaluation, they are difficult to reproduce because different key components of the validation are often not readily available. These components include selected participants and input data, image preprocessing and cross-validation procedures. The performance of the different approaches is also difficult to compare objectively. In particular, it is often difficult to assess which part of the method (e.g. preprocessing, feature extraction or classification algorithms) provides a real improvement, if any. In the present paper, we propose a framework for reproducible and objective classification experiments in AD using three publicly available datasets (ADNI, AIBL and OASIS). The framework comprises: i) automatic conversion of the three datasets into a standard format (BIDS); ii) a modular set of preprocessing pipelines, feature extraction and classification methods, together with an evaluation framework, that provide a baseline for benchmarking the different components. We demonstrate the use of the framework for a large-scale evaluation on 1960 participants using T1 MRI and FDG PET data. In this evaluation, we assess the influence of different modalities, preprocessing, feature types (regional or voxel-based features), classifiers, training set sizes and datasets. Performances were in line with the state-of-the-art. FDG PET outperformed T1 MRI for all classification tasks. No difference in performance was found for the use of different atlases, image smoothing, partial volume correction of FDG PET images, or feature type. Linear SVM and L2- logistic regression resulted in similar performance and both outperformed random forests. The classification performance increased along with the number of subjects used for training. Classifiers trained on ADNI gener- alized well to AIBL and OASIS. All the code of the framework and the experiments is publicly available: general- purpose tools have been integrated into the Clinica software (<a href="http://www.clinica.run" target="_blank" rel="noopener">www.clinica.run</a>) and the paper-specific code is available at: <a href="https://gitlab.icm-institute.org/aramislab/AD-ML" target="_blank" rel="noopener">https://gitlab.icm-institute.org/aramislab/AD-ML</a>.</p><p>大量论文引入了用于阿尔茨海默病（AD）自动分类的新型机器学习和特征提取方法。然而，虽然绝大多数这些作品使用公共数据集ADNI进行评估，但它们很难再现，因为验证的不同关键组件通常不易获得。这些组件包括选定的参与者和输入数据，图像预处理和交叉验证程序。不同方法的表现也难以客观地比较。特别地，通常难以评估方法的哪个部分（例如，预处理，特征提取或分类算法）提供真正的改进（如果有的话）。在本文中，我们使用三个公开可用的数据集（ADNI，AIBL和OASIS）提出了AD中可重复和客观分类实验的框架。该框架包括：i）将三个数据集自动转换为标准格式（BIDS）; ii）模块化的预处理流水线，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准。我们使用T1 MRI和FDG PET数据证明该框架用于对1960名参与者进行大规模评估。在此评估中，我们评估不同模态，预处理，要素类型（基于区域或体素的特征），分类器，训练集大小和数据集的影响。表演符合最新技术水平。对于所有分类任务，FDG PET优于T1 MRI。使用不同的图册，图像平滑，FDG PET图像的部分体积校正或特征类型没有发现性能差异。线性SVM和L2逻辑回归导致相似的性能，并且都优于随机森林。分类性能随着用于训练的受试者数量而增加。在ADNI上训练的分类器很好地适用于AIBL和OASIS。框架和实验的所有代码都是公开的：通用工具已集成到Clinica软件（<a href="http://www.clinica.run）中，特定于纸张的代码可从以下网址获得：https://gitlab.icm-institute.ORG/aramislab/AD-ML。" target="_blank" rel="noopener">www.clinica.run）中，特定于纸张的代码可从以下网址获得：https://gitlab.icm-institute.ORG/aramislab/AD-ML。</a></p><h4 id="Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease"><a href="#Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="Multimodal Neuroimaging Feature Learning With Multimodal Stacked Deep Polynomial Networks for Diagnosis of Alzheimer’s Disease"></a>Multimodal Neuroimaging Feature Learning With Multimodal Stacked Deep Polynomial Networks for Diagnosis of Alzheimer’s Disease</h4><blockquote><p> 多模态神经影像学特征学习多模态叠加深度多项式网络诊断阿尔茨海默病</p></blockquote><p><code>Abstract</code>—The accurate diagnosis of Alzheimer’s disease (AD) and its early stage, i.e., mild cognitive impairment, is essential for timely treatment and possible delay of AD. Fu- sion of multimodal neuroimaging data, such as magnetic resonance imaging (MRI) and positron emission tomogra- phy (PET), has shown its effectiveness for AD diagnosis. The deep polynomial networks (DPN) is a recently proposed deep learning algorithm, which performs well on both large- scale and small-size datasets. In this study, a multimodal stacked DPN (MM-SDPN) algorithm, which MM-SDPN con- sists of two-stage SDPNs, is proposed to fuse and learn feature representation from multimodal neuroimaging data for AD diagnosis. Specifically speaking, two SDPNs are first used to learn high-level features of MRI and PET, respec- tively, which are then fed to another SDPN to fuse multi- modal neuroimaging information. The proposed MM-SDPN algorithm is applied to the ADNI dataset to conduct both binary classification and multiclass classification tasks. Ex- perimental results indicate that MM-SDPN is superior over the state-of-the-art multimodal feature-learning-based algo- rithms for AD diagnosis.<br><code>Index Terms</code>—Alzheimer’s disease, deep learning, deep polynomial networks, multimodal stacked deep polynomial networks, multimodal neuroimaging.</p><p><code>摘要</code> - 阿尔茨海默病（AD）及其早期的准确诊断，即轻度认知障碍，对于及时治疗和可能的AD延迟至关重要。多模态神经影像数据的融合，如磁共振成像（MRI）和正电子发射断层扫描（PET），已显示其对AD诊断的有效性。深度多项式网络（DPN）是最近提出的深度学习算法，其在大规模和小尺寸数据集上都表现良好。在这项研究中，提出了一种多模式堆叠DPN（MM-SDPN）算法，MM-SDPN由两级SDPN组成，用于融合和学习用于AD诊断的多模态神经成像数据的特征表示。具体而言，两个SDPN首先用于分别学习MRI和PET的高级特征，然后将其输入另一个SDPN以融合多模态神经影像信息。建议的MM-SDPN算法应用于ADNI数据集，以进行二进制分类和多类分类任务。实验结果表明，MM-SDPN优于最先进的基于多模态特征学习的算法用于AD诊断。<br><code>索引术语</code> - 阿尔茨海默病，深度学习，深度多项式网络，多模态叠加深度多项式网络，多模态神经成像。</p><h4 id="Multimodal-Neuroimaging-Feature-Learning-for-Multiclass-Diagnosis-of-Alzheimer’s-Disease"><a href="#Multimodal-Neuroimaging-Feature-Learning-for-Multiclass-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer’s Disease"></a>Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer’s Disease</h4><blockquote><p>多模式神经影像学特征学习用于阿尔茨海默病的多类诊断</p></blockquote><p><code>Abstract</code>—The accurate diagnosis of Alzheimer’s disease (AD) is essential for patient care and will be increasingly important as disease modifying agents become available, early in the course of the disease. Although studies have applied machine learning meth- ods for the computer-aided diagnosis of AD, a bottleneck in the diagnostic performance was shown in previous methods, due to the lacking of efficient strategies for representing neuroimaging biomarkers. In this study, we designed a novel diagnostic frame- work with deep learning architecture to aid the diagnosis of AD. This framework uses a zero-masking strategy for data fusion to ex- tract complementary information from multiple data modalities. Compared to the previous state-of-the-art workflows, our method is capable of fusing multimodal neuroimaging features in one setting and has the potential to require less labeled data. A performance gain was achieved in both binary classification and multiclass clas- sification of AD. The advantages and limitations of the proposed framework are discussed.<br><code>Index Terms</code>—Alzheimer’s disease (AD), classification, deep Learning, MRI, neuroimaging, positron emission tomography (PET).</p><p><code>摘要</code> - 阿尔茨海默病（AD）的准确诊断对于患者护理至关重要，并且随着疾病调节剂的出现在疾病早期将变得越来越重要。虽然研究已将机器学习方法应用于AD的计算机辅助诊断，但由于缺乏表示神经影像生物标记物的有效策略，因此在先前的方法中显示出诊断性能的瓶颈。在这项研究中，我们设计了一个新的诊断框架与深度学习架构，以帮助诊断AD。该框架使用零掩蔽策略进行数据融合，以从多种数据模式中提取补充信息。与之前最先进的工作流程相比，我们的方法能够在一个设置中融合多模态神经成像功能，并且可能需要较少标记的数据。 AD的二元分类和多类分类均实现了性能提升。讨论了拟议框架的优点和局限性。<br><code>索引术语</code> - 阿尔茨海默病（AD），分类，深度学习，MRI，神经影像学，正电子发射断层扫描（PET）。</p><h4 id="Joint-Classification-and-Regression-via-Deep-Multi-Task-Multi-Channel-Learning-for-Alzheimer’s-Disease-Diagnosis"><a href="#Joint-Classification-and-Regression-via-Deep-Multi-Task-Multi-Channel-Learning-for-Alzheimer’s-Disease-Diagnosis" class="headerlink" title="Joint Classification and Regression via Deep Multi-Task Multi-Channel Learning for Alzheimer’s Disease Diagnosis"></a><strong>Joint Classification and Regression via Deep Multi-Task Multi-Channel Learning for Alzheimer’s Disease Diagnosis</strong></h4><blockquote><p>多模式神经影像学特征学习用于阿尔茨海默病的多类诊断</p></blockquote><p><code>Abstract</code>—In the field of computer-aided Alzheimer’s disease (AD) diagnosis, jointly identifying brain diseases and predicting clinical scores using magnetic resonance (MR) imaging have attracted increasing attention since these two tasks are highly correlated. Most of existing joint learning approaches require hand-crafted feature representations for MR images. Since hand- crafted features of MRI and classification/regression models may not coordinate well with each other, conventional methods may lead to sub-optimal learning performance. Also, demographic information (e.g., age, gender, and education) of subjects may also be related to brain status, and thus can help improve the di- agnostic performance. However, conventional joint learning meth- ods seldom incorporate such demographic information into the learning models. To this end, we propose a deep multi-task multi-channel learning (DM2L) framework for simultaneous brain dis- ease classification and clinical score regression, using MR imaging data and demographic information of subjects. Specifically, we first identify the discriminative anatomical landmarks from MR images in a data-driven manner, and then extract multiple image patches around these detected landmarks. We then propose a deep multi-task multi-channel convolutional neural network for joint classification and regression. Our DM2L framework can not only automatically learn discriminative features for MR images, but also explicitly incorporate the demographic information of subjects into the learning process. We evaluate the proposed method on four large multi-center cohorts with 1, 984 subjects, and the experimental results demonstrate that DM2L is superior to several state-of-the-art joint learning methods in both the tasks of disease classification and clinical score regression.</p><p><code>摘要</code> - 在计算机辅助阿尔茨海默病（AD）诊断领域，联合识别脑疾病和使用磁共振（MR）成像<code>预测</code>临床评分已引起越来越多的关注，因为这两个任务高度相关。大多数现有的联合学习方法需要手工制作的MR图像特征表示。由于MRI和分类/回归模型的手工制作特征可能彼此不能很好地协调，因此传统方法可能导致次优的学习性能。此外，受试者的人口统计信息（例如，年龄，性别和教育）也可能与大脑状态有关，因此可以帮助改善诊断性能。然而，传统的联合学习方法很少将这些人口统计信息纳入学习模型。为此，我们提出了一个深层多任务多通道学习（$DM^2L$）框架，用于同时进行脑部疾病分类和临床评分回归，使用MR成像数据和受试者的人口统计信息。具体地，我们首先以数据驱动的方式从MR图像识别辨别解剖标志，然后围绕这些检测到的标志提取多个图像块。然后，我们提出了一种用于<code>联合分类和回归的深度多任务多通道卷积神经网络</code>。我们的$DM^2L$框架不仅可以自动学习MR图像的判别特征，还可以明确地将受试者的人口统计信息纳入学习过程。我们对具有1,984名受试者的四个大型多中心队列进行了评估，实验结果表明$DM^2L$在疾病分类和临床评分回归的任务中均优于几种最先进的联合学习方法。</p><h4 id="Automated-classification-of-Alzheimer’s-disease-and-mild-cognitive-impairment-using-a-single-MRI-and-deep-neural-networks"><a href="#Automated-classification-of-Alzheimer’s-disease-and-mild-cognitive-impairment-using-a-single-MRI-and-deep-neural-networks" class="headerlink" title="Automated classification of Alzheimer’s disease and mild cognitive impairment using a single MRI and deep neural networks"></a>Automated classification of Alzheimer’s disease and mild cognitive impairment using a single MRI and deep neural networks</h4><blockquote><p>使用单个MRI和深度神经网络自动分类阿尔茨海默病和轻度认知障碍</p></blockquote><p>We built and validated a deep learning algorithm predicting the individual diagnosis of Alzheimer’s disease (AD) and mild cognitive impairment who will convert to AD (c-MCI) based on a single cross-sectional brain structural MRI scan. Convolutional neural networks (CNNs) were applied on 3D T1-weighted images from ADNI and subjects recruited at our Institute (407 healthy controls [HC], 418 AD, 280 c-MCI, 533 stable MCI [s-MCI]). CNN performance was tested in distinguishing AD, c-MCI and s-MCI. High levels of accuracy were achieved in all the classifications, with the highest rates achieved in the AD vs HC classification tests using both the ADNI dataset only (99%) and the combined ADNI + non-ADNI dataset (98%). CNNs discriminated c-MCI from s-MCI patients with an accuracy up to 75% and no difference between ADNI and non-ADNI images. CNNs provide a powerful tool for the automatic individual patient diagnosis along the AD continuum. Our method performed well without any prior feature engineering and regardless the variability of imaging protocols and scanners, demonstrating that it is exploitable by not-trained operators and likely to be generalizable to unseen patient data. CNNs may accelerate the adoption of structural MRI in routine practice to help assessment and management of patients.</p><p>我们建立并验证了一种深度学习算法，该算法<code>预测阿尔茨海默病（AD）和轻度认知障碍的个体诊断</code>，他们将基于单个横断面脑结构MRI扫描转换为AD（c-MCI）。将卷积神经网络（CNN）应用于来自ADNI的3D T1加权图像和我们研究所招募的受试者（407健康对照[HC]，418 AD，280 c-MCI，533稳定MCI [s-MCI]）。测试CNN性能以区分AD，c-MCI和s-MCI。在所有分类中都实现了高水平的准确性，使用仅ADNI数据集（99％）和组合ADNI +非ADNI数据集（98％），在AD与HC分类测试中实现了最高的准确率。 CNN从s-MCI患者中区分c-MCI，准确度高达75％，ADNI和非ADNI图像之间没有差异。 CNN为AD连续体中的自动个体患者诊断提供了强大的工具。我们的方法在没有任何先前特征工程的情况下表现良好，并且无论成像协议和扫描仪的可变性如何，都表明它可以被未经过培训的操作员利用，并且可能被推广到看不见的患者数据。 CNN可以在常规实践中加速结构MRI的采用，以帮助评估和管理患者。</p><h4 id="Multimodal-and-Multiscale-Deep-Neural-Networks-for-the-Early-Diagnosis-of-Alzheimer’s-Disease-using-structural-MR-and-FDG-PET-images"><a href="#Multimodal-and-Multiscale-Deep-Neural-Networks-for-the-Early-Diagnosis-of-Alzheimer’s-Disease-using-structural-MR-and-FDG-PET-images" class="headerlink" title="Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images"></a>Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images</h4><blockquote><p>使用结构MR和FDG-PET图像进行早期诊断阿尔茨海默病的多模态和多尺度深度神经网络</p></blockquote><p>Alzheimer’s Disease (AD) is a progressive neurodegenerative disease. Amnestic mild cognitive impairment (MCI) is a common first symptom before the conversion to clinical impairment where the individual becomes unable to perform activities of daily living independently. Although there is currently no treatment available, the earlier a conclusive diagnosis is made, the earlier the potential for interventions to delay or perhaps even prevent progression to full-blown AD. Neuroimaging scans acquired from MRI and metabolism images obtained by FDG-PET provide in-vivo view into the structure and function (glucose metabolism) of the living brain. It is hypothesized that combining different image modalities could better characterize the change of human brain and result in a more accuracy early diagnosis of AD. In this paper, we proposed a novel framework to discriminate normal control(NC) subjects from subjects with AD pathology (AD and NC, MCI subjects convert to AD in future). Our novel approach utilizing a multimodal and multiscale deep neural network was found to deliver a 85.68% accuracy in the prediction of subjects within 3 years to conversion. Cross validation experiments proved that it has better discrimination ability compared with results in existing published literature.</p><p>阿尔茨海默病（AD）是一种进行性神经退行性疾病。遗忘性轻度认知障碍（MCI）是转变为临床损伤之前的常见首发症状，其中个体变得不能独立地进行日常生活活动。虽然目前没有可用的治疗方法，但早期确定的诊断结果越早，干预的可能性就越早，甚至可能阻止进展为完全AD。从MRI获得的神经成像扫描和通过FDG-PET获得的代谢图像提供了对活脑的结构和功能（葡萄糖代谢）的体内观察。假设结合不同的图像模态可以更好地表征人脑的变化，从而更准确地早期诊断AD。在本文中，我们提出了一种新的框架，<code>以区分正常对照（NC）受试者与AD病理对象</code>（AD和NC，MCI受试者将来转换为AD）。我们利用多模态和多尺度深度神经网络的新方法被发现<code>在转换后3年内</code>对受试者的预测提供了85.68％的准确度。交叉验证实验证明，与现有已发表文献的结果相比，它具有更好的辨别能力。</p><h4 id="Corpus-Callosum-Radiomics-Based-Classification-Model-in-Alzheimer’s-Disease-A-Case-Control-Study"><a href="#Corpus-Callosum-Radiomics-Based-Classification-Model-in-Alzheimer’s-Disease-A-Case-Control-Study" class="headerlink" title="Corpus Callosum Radiomics-Based Classification Model in Alzheimer’s Disease: A Case-Control Study"></a>Corpus Callosum Radiomics-Based Classification Model in Alzheimer’s Disease: A Case-Control Study</h4><blockquote><p>基于胼Call体基于放射学的阿尔茨海默病分类模型：病例对照研究</p></blockquote><p><strong>Background</strong>: Alzheimer’s disease (AD) is a progressive neurodegenerative disease that causes the decline of some cognitive impairments. The present study aimed to identify the corpus callosum (CC) radiomic features related to the diagnosis of AD and build and evaluate a classification model.<br><strong>Methods</strong>: Radiomics analysis was applied to the three-dimensional T1-weighted magnetization-prepared rapid gradient echo (MPRAGE) images of 78 patients with AD and 44 healthy controls (HC). The CC, in each subject, was segmented manually and 385 features were obtained after calculation. Then, the feature selection were carried out. The logistic regression model was constructed and evaluated according to identified features. Thus, the model can be used for distinguishing the AD from HC subjects.<br><strong>Results</strong>: Eleven features were selected from the three-dimensional T1-weighted MPRAGE images using the LASSO model, following which, the logistic regression model was constructed. The area under the receiver operating characteristic curve values (AUC), sensitivity, specificity, accuracy, precision, and positive and negative predictive values were 0.720, 0.792, 0.500, 0.684, 0.731, 0.731, and 0.583, respectively.<br><strong>Conclusion</strong>: The results demonstrated the potential of CC texture features as a biomarker for the diagnosis of AD. This is the first study showing that the radiomics model based on machine learning was a valuable method for the diagnosis of AD.</p><p><strong>背景</strong>：阿尔茨海默病（AD）是一种进行性神经退行性疾病，导致一些认知障碍的减少。本研究旨在确定与AD诊断相关的胼call体（CC）<code>放射学特征</code>，并建立和评估分类模型。<br><strong>方法</strong>：将自由基分析应用于78例AD患者和44例健康对照（HC）的三维T1加权磁化制备快速梯度回波（MPRAGE）图像。在每个受试者中，CC被手动分割，并且在计算后获得385个特征。然后，进行特征选择。根据已识别的特征构建并评估逻辑回归模型。因此，该模型可用于区分AD与HC受试者。<br><strong>结果</strong>：使用LASSO模型从三维T1加权MPRAGE图像中选择11个特征，然后构建逻辑回归模型。接收器操作特征曲线值（AUC），灵敏度，特异性，准确度，精确度以及阳性和阴性预测值下的面积分别为0.720,0.729,0.500,0.684,0.731,0.731和0.583。<br><strong>结论</strong>：结果证明了<code>CC纹理特征作为诊断AD的生物标志物的潜力</code>。这是第一项研究表明，基于机器学习的放射学模型是诊断AD的有效方法。</p><p><strong>Keywords</strong>: magnetic resonance imaging, Alzheimer’s disease, corpus callosum, radiomics, neuroimaging</p><p><strong>关键词</strong>：磁共振成像，阿尔茨海默病，胼call体，放射学，神经影像学</p><h4 id="An-Ensemble-Learning-System-for-a-4-Way-Classification-of-Alzheimer’s-Disease-and-Mild-Cognitive-Impairment"><a href="#An-Ensemble-Learning-System-for-a-4-Way-Classification-of-Alzheimer’s-Disease-and-Mild-Cognitive-Impairment" class="headerlink" title="An Ensemble Learning System for a 4-Way Classification of Alzheimer’s Disease and Mild Cognitive Impairment"></a>An Ensemble Learning System for a 4-Way Classification of Alzheimer’s Disease and Mild Cognitive Impairment</h4><blockquote><p>一种用于阿尔茨海默病和轻度认知障碍的四向分类的集成学习系统</p></blockquote><p>Discriminating Alzheimer’s disease (AD) from its prodromal form, mild cognitive impairment (MCI), is a significant clinical problem that may facilitate early diagnosis and intervention, in which a more challenging issue is to classify MCI subtypes, i.e., those who eventually convert to AD (cMCI) versus those who do not (MCI). To solve this difficult 4-way classification problem (AD, MCI, cMCI and healthy controls), a competition was hosted by Kaggle to invite the scientific community to apply their machine learning approaches on pre-processed sets of T1-weighted magnetic resonance images (MRI) data and the demographic information from the international Alzheimer’s disease neuroimaging initiative (ADNI) database. This paper summarizes our competition results. We first proposed a hierarchical process by turning the 4-way classification into five binary classification problems. A new feature selection technology based on relative importance was also proposed, aiming to identify a more informative and concise subset from 426 sMRI morphometric and 3 demographic features, to ensure each binary classifier to achieve its highest accuracy. As a result, about 2% of the original features were selected to build a new feature space, which can achieve the final four-way classification with a 54.38% accuracy on testing data through hierarchical grouping, higher than several alternative methods in comparison. More importantly, the selected discriminative features such as hippocampal volume, parahippocampal surface area, and medial orbitofrontal thickness, etc. as well as the MMSE score, are reasonable and consistent with those reported in AD/MCI deficits. In summary, the proposed method provides a new framework for multi-way classification using hierarchical grouping and precise feature selection.</p><p>辨别阿尔茨海默病（AD）从其前驱形式，轻度认知障碍（MCI），是一个重要的临床问题，可能有助于早期诊断和干预，其中一个更具挑战性的问题是分类MCI亚型，即最终转换为AD（cMCI）与不参与者（MCI）。为解决这一困难的<code>四向分类问题（AD，MCI，cMCI和NC）</code>，Kaggle主办了一项竞赛，邀请科学界将其机器学习方法应用于预处理的T1加权磁共振图像集（ MRI）数据和来自国际阿尔茨海默病神经影像学倡议（ADNI）数据库的人口统计信息。本文总结了我们的比赛结果。我们首先通过将4路分类转换为<code>五个二元分类问题</code>来提出分层过程。还提出了一种基于相对重要性的新特征选择技术，旨在从426个sMRI形态测量和3个人口统计特征中识别出更具信息性和简洁性的子集，以确保每个二元分类器达到其最高精度。因此，大约2％的原始特征被选择用于构建新的特征空间，这可以通过分层分组实现最终的四向分类，测试数据的准确率为54.38％，高于几种替代方法。更重要的是，选择的辨别特征，如海马体积，海马旁表面积和内侧眶额厚度等，以及MMSE评分，是合理的，与AD / MCI缺陷报道的一致。总之，所提出的方法提供了一种使用分层分组和精确特征选择的多路分类的新框架。</p><p><strong>Keywords</strong>: multi-class classification, feature selection, Alzheimer’s disease(AD), mild cognitive impairment (MCI), structural MRI, hierarchical classification, relative importance.</p><p><strong>关键字</strong>：多级分类，特征选择，阿尔茨海默病（AD），轻度认知障碍（MCI），结构MRI，等级分类，相对重要性</p><h4 id="alzheimer-Disease-and-Behavioral-Variant-Frontotemporal-Dementia-Automatic-Classification-Based-on-Cortical-Atrophy-for-Single-Subject-Diagnosis"><a href="#alzheimer-Disease-and-Behavioral-Variant-Frontotemporal-Dementia-Automatic-Classification-Based-on-Cortical-Atrophy-for-Single-Subject-Diagnosis" class="headerlink" title="alzheimer Disease and Behavioral Variant Frontotemporal Dementia: Automatic Classification Based on Cortical Atrophy for Single-Subject Diagnosis"></a>alzheimer Disease and Behavioral Variant Frontotemporal Dementia: Automatic Classification Based on Cortical Atrophy for Single-Subject Diagnosis</h4><blockquote><p>阿尔茨海默病和行为变异性额颞叶痴呆：基于皮层萎缩的单一主题诊断自动分类</p></blockquote><p><strong>Purpose:</strong>To investigate the diagnostic accuracy of an image-based classifier to distinguish between Alzheimer disease (AD) and behavioral variant frontotemporal dementia (bvFTD) in individual patients by using gray matter (GM) density maps computed from standard T1-weighted structural images obtained with multiple imagers and with indepen- dent training and prediction data.<br><strong>Materials and Methods:</strong>The local institutional review board approved the study. Eighty-four patients with AD, 51 patients with bvFTD, and 94 control subjects were divided into independent training (n = 115) and prediction (n = 114) sets with identical diagno- sis and imager type distributions. Training of a support vec- tor machine (SVM) classifier used diagnostic status and GM density maps and produced voxelwise discrimination maps. Discriminant function analysis was used to estimate suitabil- ity of the extracted weights for single-subject classification in the prediction set. Receiver operating characteristic (ROC) curves and area under the ROC curve (AUC) were calculated for image-based classifiers and neuropsychological z scores.<br><strong>Results:</strong>Training accuracy of the SVM was 85% for patients with AD versus control subjects, 72% for patients with bvFTD versus control subjects, and 79% for patients with AD ver- sus patients with bvFTD (P  .029). Single-subject diag- nosis in the prediction set when using the discrimination maps yielded accuracies of 88% for patients with AD ver- sus control subjects, 85% for patients with bvFTD versus control subjects, and 82% for patients with AD versus pa- tients with bvFTD, with a good to excellent AUC (range, 0.81–0.95; P  .001). Machine learning-based categori- zation of AD versus bvFTD based on GM density maps outperforms classification based on neuropsychological test results.<br><strong>Conclusion:</strong>The SVM can be used in single-subject discrimination and can help the clinician arrive at a diagnosis. The SVM can be used to distinguish disease-specific GM patterns in patients with AD and those with bvFTD as compared with normal aging by using common T1-weighted structural MR imaging.</p><p><strong>目的：</strong>通过使用从多个获得的标准T1加权结构图像计算的灰质（GM）密度图，研究基于图像的分类器在个体患者中区分阿尔茨海默病（<code>AD</code>）和行为变异额颞叶痴呆（<code>bvFTD</code>）的诊断准确性成像仪以及独立的训练和预测数据。<br><strong>材料和方法：</strong>当地机构审查委员会批准了该研究。将84名AD患者，51名bvFTD患者和94名对照受试者分为独立训练组（n = 115）和预测组（n = 114），具有相同的诊断和成像者类型分布。培训支持向量机（SVM）分类器使用诊断状态和GM密度图并生成体素识别图。判别函数分析用于估计预测集中单主题分类的提取权重的适用性。针对基于图像的分类器和神经心理学z分数计算接收器操作特征（ROC）曲线和ROC曲线下面积（AUC）。<br><strong>结果：</strong>AD患者与对照组相比，SVM的训练准确率为85％，bvFTD患者与对照组相比，72％，对于患有bvFTD的AD患者，则为79％（P &lt;.029）。使用鉴别图时预测集中的单一主体诊断对AD患者的对照组产生了88％的准确率，对于bvFTD患者与对照组相比，准确率为85％，AD患者与对照组相比，82％。 bvFTD患者，具有良好至优良的AUC（范围，0.81-0.95; P &lt;0.001）。基于机器学习的基于GM密度图的AD与bvFTD的分类优于基于神经心理学测试结果的分类。<br><strong>结论：</strong>SVM可用于单一主体辨别，可帮助临床医生做出诊断。通过使用常见的T1加权结构MR成像，SVM可用于区分AD患者和bvFTD患者的疾病特异性GM模式与正常衰老相比。</p><h4 id="ALZHEIMER’S-DISEASE-DIAGNOSTICS-BY-A-DEEPLY-SUPERVISED-ADAPTABLE-3D-CONVOLUTIONAL-NETWORK"><a href="#ALZHEIMER’S-DISEASE-DIAGNOSTICS-BY-A-DEEPLY-SUPERVISED-ADAPTABLE-3D-CONVOLUTIONAL-NETWORK" class="headerlink" title="ALZHEIMER’S DISEASE DIAGNOSTICS BY A DEEPLY SUPERVISED ADAPTABLE 3D CONVOLUTIONAL NETWORK"></a>ALZHEIMER’S DISEASE DIAGNOSTICS BY A DEEPLY SUPERVISED ADAPTABLE 3D CONVOLUTIONAL NETWORK</h4><blockquote><p>ALZHEIMER通过深度监控的适应性3D卷积网络诊断疾病</p></blockquote><p>Early diagnosis, playing an important role in preventing progress and treating the Alzheimer’s disease (AD), is based on classification of features extracted from brain images. The features have to accurately capture main AD-related varia- tions of anatomical brain structures, such as, e.g., ventricles size, hippocampus shape, cortical thickness, and brain vol- ume. This paper proposes to predict the AD with a deep 3D convolutional neural network (3D-CNN), which can learn generic features capturing AD biomarkers and adapt to dif- ferent domain datasets. The 3D-CNN is built upon a 3D convolutional autoencoder, which is pre-trained to capture anatomical shape variations in structural brain MRI scans. Fully connected upper layers of the 3D-CNN are then fine- tuned for each task-specific AD classification. Experiments on the ADNI MRI dataset with no skull-stripping preprocess- ing have shown our 3D-CNN outperforms several conven- tional classifiers by accuracy and robustness. Abilities of the 3D-CNN to generalize the features learnt and adapt to other domains have been validated on the CADDementia dataset.<br><strong>Index Terms</strong>— Alzheimer’s disease, deep learning, 3D convolutional neural network, autoencoder, brain MRI.</p><p>早期诊断，在预防进展和治疗阿尔茨海默病（AD）中发挥重要作用，是基于从脑图像中提取的特征的分类。这些特征必须准确地捕获解剖学大脑结构的主要AD相关变异，例如，心室大小，海马体形状，皮质厚度和脑容积。本文提出用深度3D卷积神经网络（3D-CNN）预测AD，其可以学习捕获AD生物标记物并适应不同域数据集的通用特征。 3D-CNN基于<code>3D卷积自动编码器</code>，其经过预先训练以捕获结构脑MRI扫描中的解剖学形状变化。然后，针对每个特定于任务的AD分类，对3D-CNN的完全连接的上层进行微调。在没有颅骨剥离预处理的ADNI MRI数据集上进行的实验表明，我们的3D-CNN在准确性和稳健性方面优于几种传统的分类器。已经在CADDementia数据集上验证了3D-CNN概括所学习的特征并适应其他领域的能力。<br><strong>索引术语</strong> - 阿尔茨海默病，深度学习，3D卷积神经网络，自动编码器，脑MRI。</p><h4 id="Early-Diagnosis-of-Alzheimer’s-Disease-Based-on-Resting-State-Brain-Networks-and-Deep-Learning"><a href="#Early-Diagnosis-of-Alzheimer’s-Disease-Based-on-Resting-State-Brain-Networks-and-Deep-Learning" class="headerlink" title="Early Diagnosis of Alzheimer’s Disease Based on Resting-State Brain Networks and Deep Learning"></a>Early Diagnosis of Alzheimer’s Disease Based on Resting-State Brain Networks and Deep Learning</h4><blockquote><p>基于静息状态脑网络和深度学习的阿尔茨海默病早期诊断</p></blockquote><p><strong>Abstract</strong>—Computerized healthcare has undergone rapid development thanks to the advances in medical imaging and machine learning technologies. Especially, recent progress on deep learning opens a new era for multimedia based clinical decision support.<br>In this paper, we use deep learning with brain network and clinical relevant text information to make early diagnosis of Alzheimer’s Disease (AD). The clinical relevant text information includes age, gender, and ApoE gene of the subject. The brain network is constructed by computing the functional connectivity of brain regions using resting-state functional magnetic resonance imaging (R-fMRI) data. A targeted autoencoder network is built to distinguish normal aging from mild cognitive impairment, an early stage of AD. The proposed method reveals discriminative brain network features effectively and provides a reliable classifier for AD detection. Compared to traditional classifiers based on R-fMRI time series data, about 31.21 percent improvement of the prediction accuracy is achieved by the proposed deep learning method, and the standard deviation reduces by 51.23 percent in the best case that means our prediction model is more stable and reliable compared to the traditional methods. Our work excavates deep learning’s advantages of classifying high-dimensional multimedia data in medical services, and could help predict and prevent AD at an early stage.</p><p><strong>摘要</strong> - 由于医学成像和机器学习技术的进步，计算机化医疗保健业得到了快速发展。特别是，最近深度学习的进展开启了基于多媒体的临床决策支持的新时代。<br>在本文中，我们使用脑网络和临床相关文本信息的深度学习来早期诊断阿尔茨海默病（AD）。临床相关文本信息包括受试者的年龄，性别和ApoE基因。通过使用静止状态功能磁共振成像（R-fMRI）数据计算脑区域的功能连接来构建脑网络。建立一个有针对性的<code>自动编码器网</code>络，以<code>区分正常老化与AD早期阶段的轻度认知障碍</code>。该方法有效地揭示了有区别的脑网络特征，为AD检测提供了可靠的分类器。与基于R-fMRI时间序列数据的传统分类器相比，所提出的深度学习方法提高了预测精度约31.21％，在最佳情况下标准差减少了51.23％，这意味着我们的预测模型更稳定与传统方法相比可靠。我们的工作挖掘了深度学习在医疗服务中对高维多媒体数据进行分类的优势，并有助于在早期阶段预测和预防AD。</p><p><strong>Index Terms</strong>—Brain network, deep learning, early diagnosis, Alzheimer’s disease</p><p><strong>索引术语</strong> - 脑网络，深度学习，早期诊断，阿尔茨海默病</p><h4 id="Auto-Detection-of-Alzheimer’s-Disease-Using-Deep-Convolutional-Neural-Networks"><a href="#Auto-Detection-of-Alzheimer’s-Disease-Using-Deep-Convolutional-Neural-Networks" class="headerlink" title="Auto-Detection of Alzheimer’s Disease Using Deep Convolutional Neural Networks"></a>Auto-Detection of Alzheimer’s Disease Using Deep Convolutional Neural Networks</h4><blockquote><p>利用深度卷积神经网络自动检测阿尔茨海默病</p></blockquote><p><code>Abstract</code>—Alzheimer’s disease(AD) is a kind of progressive neurodegenerative disease. One who is diagnosed as an Alzheimer’s disease patient may has many symptoms, such as deterioration of memory and language. Once those symptoms was noticed, they usually can survive 4 to 20 years. So far, Alzheimer’s disease has become the sixth leading cause of death, and it has become a worldwide health and social challenge. Traditional methods of diagnosing AD and mild cognitive impairment(MCI), mostly depend on capturing features from variable modalities of brain image data. It is a big challenge to pick out the MCI from normal controller (NC) and AD, especially for those who are lacking experience. In this article, we employ deep convolutional neural network (DCNN) to extract the most useful features of the structural magnetic resonance imaging (MRI). Firstly, the structural MRIs are pre-processed in a strict pipeline. Then, instead of parcellating regions of interest, we re-slice each volume, and put the resliced images into a DCNN directly. Finally, four stages of Alzheimer’s are identified, and the average accuracy is 94.5% for NC versus LMCI, 96.9% for NC versus AD, 97.2% for LMCI and AD, 97.81% for EMCI versus AD, 94.8% for LMCI versus EMCI. The results show that the DCNN outperforms existing methods.</p><p><code>摘要</code> - 阿尔茨海默病（AD）是一种进行性神经退行性疾病。被诊断为阿尔茨海默病患者的人可能有许多症状，例如记忆和语言的恶化。一旦发现这些症状，它们通常可以存活4到20年。到目前为止，阿尔茨海默病已成为第六大死亡原因，并已成为全球健康和社会的挑战。诊断AD和轻度认知障碍（MCI）的传统方法主要依赖于从脑图像数据的可变模态捕获特征。从NC和AD中挑选出MCI是一个很大的挑战，特别是那些缺乏经验的人。在本文中，我们采用深度卷积神经网络（DCNN）来提取结构磁共振成像（MRI）最有用的特征。首先，结构MRI在严格的管道中进行预处理。然后，我们不是分割感兴趣的区域，而是重新切片每个卷，并将重新分割的图像直接放入DCNN。最后，确定了阿尔茨海默氏症的四个阶段，<code>NC与LMCI</code>的平均准确度为94.5％，<code>NC与AD</code>的平均准确度为96.9％，<code>LMCI和AD</code>为97.2％，<code>EMCI与AD</code>相比为97.81％，<code>LMCI与EMCI</code>相比为94.8％。结果表明DCNN优于现有方法。</p><p><code>Keywords</code>-Deep Learning; Alzheimer’s Disease; MRI; Early Diagnose</p><p><code>关键词</code> - 深度学习;阿尔茨海默氏病; MRI;早期诊断</p><h4 id="A-Deep-CNN-based-Multi-class-Classification-of-Alzheimer’s-Disease-using-MRI"><a href="#A-Deep-CNN-based-Multi-class-Classification-of-Alzheimer’s-Disease-using-MRI" class="headerlink" title="A Deep CNN based Multi-class Classification of Alzheimer’s Disease using MRI"></a>A Deep CNN based Multi-class Classification of Alzheimer’s Disease using MRI</h4><h4 id="Classification-of-Alzheimer-Disease-on-Imaging-Modalities-with-Deep-CNNs-using-Cross-Modal-Transfer-Learning"><a href="#Classification-of-Alzheimer-Disease-on-Imaging-Modalities-with-Deep-CNNs-using-Cross-Modal-Transfer-Learning" class="headerlink" title="Classification of Alzheimer Disease on Imaging Modalities with Deep CNNs using Cross-Modal Transfer Learning"></a>Classification of Alzheimer Disease on Imaging Modalities with Deep CNNs using Cross-Modal Transfer Learning</h4><blockquote><p>利用跨模式转移学习分析阿尔茨海默病对深部CNN成像模式的影响</p></blockquote><p><code>Abstract</code>—A recent imaging modality Diffusion Tensor Imag- ing completes information used from Structural MRI in studies of Alzheimer disease. A large number of recent studies has explored pathologic staging of Alzheimer disease using the Mean Diffusivity maps extracted from the Diffusion Tensor Imaging modality. The Deep Neural Networks are seducing tools for classification of subjects’ imaging data in computer- aided diagnosis of Alzheimer’s disease. The major problem here is the lack of a publicly available large amount of training data in both modalities. The lack number of training data yields over-fitting phenomena. We propose a method of a cross- modal transfer learning: from Structural MRI to Diffusion Tensor Imaging modality. Models pre-trained on a structural MRI dataset with domain-depended data augmentation are used as initialization of network parameters to train on Mean Diffusivity data. The method shows a reduction of the over-fitting phenomena, improves learning performance, and thus increases the accuracy of prediction. Classifiers are then fused by a majority vote resulting in augmented scores of classification between Normal Control, Alzheimer Patients and Mild Cognitive Impairment subjects on a subset of ADNI dataset.</p><p><code>摘要</code> - 最近的成像模式Diffusion Tensor Imaging完成了结构MRI在阿尔茨海默病研究中使用的信息。最近的大量研究使用从扩散张量成像模式中提取的平均扩散系数图来探索阿尔茨海默病的病理分期。深度神经网络是用于在计算机辅助诊断阿尔茨海默病中对受试者成像数据进行分类的诱导工具。这里的主要问题是两种模式都缺乏公开提供的大量培训数据。缺乏训练数据会产生过度拟合现象。我们提出了一种跨模式转移学习的方法：从结构MRI到扩散张量成像模式。在具有依赖于域的数据增强的结构MRI数据集上预训练的模型被用作网络参数的初始化以训练平均扩散率数据。该方法显示了过拟合现象的减少，提高了学习性能，从而提高了预测的准确性。然后通过多数投票来对分类器进行融合，从而在ADNI数据集的子集上对正常对照，<code>阿尔茨海默病患者和轻度认知障碍受试者之间的分类</code>进行增强评分。</p><p><code>Keywords</code>-Multi-Modal ; Alzheimer’s Disease ; Hippocampus ; Mild Cognitive Impairment ; Convolutional Neural Networks ; Transfer Learning ; Deep Learning ; Medical Imaging.</p><p><code>关键词</code> - 多模态;阿尔茨海默氏病 ;海马;轻度认知障碍 ;卷积神经网络;转学习;深度学习;医学影像</p><h4 id="A-Deep-Learning-Pipeline-to-Classify-Different-Stages-of-Alzheimer’s-Disease-From-fMRI-Data"><a href="#A-Deep-Learning-Pipeline-to-Classify-Different-Stages-of-Alzheimer’s-Disease-From-fMRI-Data" class="headerlink" title="A Deep Learning Pipeline to Classify Different Stages of Alzheimer’s Disease From fMRI Data"></a>A Deep Learning Pipeline to Classify Different Stages of Alzheimer’s Disease From fMRI Data</h4><blockquote><p>从fMRI数据分类阿尔茨海默病不同阶段的深度学习流程</p></blockquote><p><code>Abstract</code>—Alzheimer’s disease (AD) is an irreversible, pro- gressive neurological disorder that causes memory and thinking skill loss. Many different methods and algorithms have been applied to extract patterns from neuroimaging data in order to distinguish different stages of Alzheimer’s disease (AD). However, the similarity of the brain patterns in older adults and in different stages makes the classification of different stages a challenge for researchers.<br>In this paper, convolutional neuronal network architecture AlexNet was applied to fMRI datasets to classify different stages of the disease. We classified five different stages of Alzheimer’s us- ing a deep learning algorithm. The method successfully classified normal healthy control (NC), significant memory concern (SMC), early mild cognitive impair (EMCI), late cognitive mild impair (LMCI), and Alzheimer’s disease (AD). The model was imple- mented using GPU high performance computing. Before applying any classification, the fMRI data were strictly preprocessed. Then, low to high level features were extracted and learned using the AlexNet model. Our experiments show significant improvement in classification. The average accuracy of the model was 97.63%. We then tested our model on test datasets to evaluate the accuracy of the model per class, obtaining an accuracy of 94.97% for AD, 95.64% for EMCI, 95.89% for LMCI, 98.34% for NC, and 94.55% for SMC.</p><p><code>摘要</code> - 阿尔茨海默病（AD）是一种不可逆转的进行性神经系统疾病，可引起记忆和思维技能的丧失。已经应用许多不同的方法和算法从神经成像数据中提取模式以区分阿尔茨海默病（AD）的不同阶段。然而，老年人和不同阶段的大脑模式的相似性使得不同阶段的分类成为研究人员的挑战。<br>在本文中，卷积神经网络结构AlexNet被应用于fMRI数据集，以分类疾病的不同阶段。我们使用深度学习算法<code>对阿尔茨海默氏症的五个不同阶段进行了分类</code>。该方法成功地分类了正常健康对照（NC），显着记忆关注（SMC），早期轻度认知障碍（EMCI），晚期认知轻度损伤（LMCI）和阿尔茨海默氏病（AD）。该模型使用GPU高性能计算实现。在应用任何分类之前，fMRI数据是严格预处理的。然后，使用AlexNet模型提取和学习低到高级别的特征。我们的实验显示分类显着改善。该模型的平均准确度为97.63％。然后，我们在测试数据集上测试我们的模型，以评估每类模型的准确性，AD的准确度为94.97％，EMCI为95.64％，LMCI为95.89％，NC为98.34％，SMC为94.55％。</p><h4 id="Brain-MRI-analysis-for-Alzheimer’s-disease-diagnosis-using-an-ensemble-system-of-deep-convolutional-neural-networks"><a href="#Brain-MRI-analysis-for-Alzheimer’s-disease-diagnosis-using-an-ensemble-system-of-deep-convolutional-neural-networks" class="headerlink" title="Brain MRI analysis for Alzheimer’s disease diagnosis using an ensemble system of deep convolutional neural networks"></a>Brain MRI analysis for Alzheimer’s disease diagnosis using an ensemble system of deep convolutional neural networks</h4><blockquote><p>使用深度卷积神经网络的集合系统进行阿尔茨海默病诊断的脑MRI分析</p></blockquote><p><code>Abstract</code>：Alzheimer’s disease is an incurable, progressive neurologicalbrain disorder. Earlier detection of Alzheimer’s disease can help with proper treatment and prevent brain tissue damage. Several statistical and machine learning models have been exploited by researchers for Alzheimer’s disease diagnosis. Analyzing magnetic resonance imaging (MRI) is a common practice for Alzheimer’s disease diagnosis in clinical research. Detection of Alzheimer’s disease is exacting due to the similarity in Alzheimer’s disease MRI data and standard healthy MRI data of older people. Recently, advanced deep learning techniques have successfully demonstrated human-level performance in numerous fields including medical image analysis. We propose a deep convolutional neural network for Alzheimer’s disease diagnosis using brainMRI data analysis. While most of the existing approaches perform binary classification, our model can iden- tify different stages of Alzheimer’s disease and obtains superior performance for early-stage diagnosis. We conducted ample experiments to demonstrate that our proposed model outperformed comparative baselines on the Open Access Series of Imaging Studies dataset.<br><code>Keywords</code>: Neurological disorder, Alzheimer’s disease, Deep learning, Convolutional neural network, MRI, Brain imaging</p><p><code>摘要</code>：阿尔茨海默病是一种无法治愈的进行性神经系统疾病。早期发现阿尔茨海默病有助于正确治疗和预防脑组织损伤。研究人员已经开发了几种统计学和机器学习模型用于阿尔茨海默病的诊断。分析磁共振成像（MRI）是临床研究中阿尔茨海默病诊断的常见做法。由于阿尔茨海默病MRI数据与老年人的标准健康MRI数据相似，阿尔茨海默病的检测非常严格。最近，先进的深度学习技术成功地证明了包括医学图像分析在内的众多领域的人类水平表现。我们使用brainMRI数据分析提出了一种用于阿尔茨海默病诊断的深度卷积神经网络。虽然大多数现有方法都进行二元分类，但我们的模型可以识别阿尔茨海默病的不同阶段，并在早期诊断中获得优异的表现。我们进行了大量实验，以证明我们提出的模型在开放获取系列成像研究数据集上的表现优于比较基线。<br><code>关键词</code>：神经障碍，阿尔茨海默病，深度学习，卷积神经网络，MRI，脑成像</p><h4 id="Combining-Convolutional-and-Recurrent-Neural-Networks-for-Alzheimer’s-Disease-Diagnosis-Using-PET-Images"><a href="#Combining-Convolutional-and-Recurrent-Neural-Networks-for-Alzheimer’s-Disease-Diagnosis-Using-PET-Images" class="headerlink" title="Combining Convolutional and Recurrent Neural Networks for Alzheimer’s Disease Diagnosis Using PET Images"></a>Combining Convolutional and Recurrent Neural Networks for Alzheimer’s Disease Diagnosis Using PET Images</h4><blockquote><p>结合卷积和回归神经网络用于PET图像的阿尔茨海默病诊断</p></blockquote><p><code>Abstract</code>—Alzheimer’s disease (AD) is a progressive and irreversible brain degenerative disorder which often happens in people aged more than 65 years old. Accurate and early diagnosis of AD is vital for the patient care and development of future treatment. Positrons Emission Tomography (PET) is a functional molecular imaging modality, which proves to be a powerful tool to help understand the brain changes related to AD. Most existing methods extract the handcraft features from images, and then train a classifier to distinguish AD from other groups. The success of these computer-aided diagnosis methods highly depends on the image preprocessing, including rigid registration and segmentation. Motivated by the success of deep learning in image classification, this paper proposes a new classification framework based on combination of 2D convolutional neural networks (CNN) and recurrent neural networks (RNN), which learns the features of 3D PET images by decomposing the 3D image into a sequence of 2D slices. In this framework, the hierarchical 2D CNNs are built to capture the intra-slice features while the gated recurrent unit (GRU) of RNN is used to extract the inter-slice features for final classification. No rigid image registration and segmentation are required for PET images. Our method is evaluated on the baseline PET images from 339 subjects including 93 AD patients, 146 mild cognitive impairments (MCI) and 100 normal controls (NC) from Alzheimer’s Disease Neuroimaging Initiative (ADNI) database. Experimental results show that the proposed method achieves an AUC of 95.28% for classification of AD vs. NC and 83.90% for classification of MCI vs. NC, respectively, demonstrating the promising classification performance.</p><p><code>摘要</code> - 阿尔茨海默病（AD）是一种进行性和不可逆转的脑退行性疾病，常见于65岁以上的人群。 AD的准确和早期诊断对于患者护理和未来治疗的发展至关重要。正电子发射断层扫描（PET）是一种功能性分子成像模式，被证明是一种有助于理解与AD相关的大脑变化的有力工具。大多数现有方法从图像中提取手工艺特征，然后训练分类器以区分AD与其他组。这些计算机辅助诊断方法的成功很大程度上取决于图像预处理，包括刚性配准和分割。在深度学习图像分类成功的推动下，本文提出了<code>一种基于二维卷积神经网络（CNN）和递归神经网络（RNN）相结合的新分类框架</code>，通过分解三维图像来学习三维PET图像的特征。进入一系列2D切片。在该框架中，构建分层2D CNN以捕获片内特征，同时使用RNN的门控重复单元（GRU）来提取片间特征以用于最终分类。 <code>PET图像不需要严格的图像配准和分割</code>。我们的方法评估来自339名受试者的基线PET图像，包括93名AD患者，146名轻度认知障碍（MCI）和100名来自阿尔茨海默病神经影像学倡议（ADNI）数据库的正常对照（NC）。实验结果表明，该方法对AD与NC分类的AUC分别为95.28％，对MCI与NC的分类分别为83.90％，表明分类性能良好。</p><p><code>Keywords</code>—Alzheimer’s disease; Convolutional neural network ; Recurrent neural network; Deep learning; Positron emission tomography; Image classification</p><p><code>关键词</code> - 阿尔茨海默病;卷积神经网络;递归神经网络;深度学习;正电子发射断层扫描;图像分类</p><h4 id="A-Deep-Neural-Network-Approach-For-Early-Diagnosis-of-Mild-Cognitive-Impairment-Using-Multiple-Features"><a href="#A-Deep-Neural-Network-Approach-For-Early-Diagnosis-of-Mild-Cognitive-Impairment-Using-Multiple-Features" class="headerlink" title="A Deep Neural Network Approach For Early Diagnosis of Mild Cognitive Impairment Using Multiple Features"></a>A Deep Neural Network Approach For Early Diagnosis of Mild Cognitive Impairment Using Multiple Features</h4><h4 id="Visual-Explanations-From-Deep-3D-Convolutional-Neural-Networks-for-Alzheimer’s-Disease-Classification"><a href="#Visual-Explanations-From-Deep-3D-Convolutional-Neural-Networks-for-Alzheimer’s-Disease-Classification" class="headerlink" title="Visual Explanations From Deep 3D Convolutional Neural Networks for Alzheimer’s Disease Classification"></a>Visual Explanations From Deep 3D Convolutional Neural Networks for Alzheimer’s Disease Classification</h4><blockquote><p>深度3D卷积神经网络对阿尔茨海默病分类的视觉解释</p></blockquote><p><code>Abstract</code> — We develop three efficient approaches for generating visual explanations from 3D convolutional neural networks (3D- CNNs) for Alzheimer’s disease classification. One approach conducts sensitivity analysis on hierarchical 3D image segmentation, and the other two visualize network activations on a spatial map. Visual checks and a quantitative localization benchmark indicate that all approaches identify important brain parts for Alzheimer’s disease diagnosis. Comparative analysis show that the sensitivity analysis based approach has difficulty handling loosely distributed cerebral cortex, and approaches based on visualization of activations are constrained by the resolution of the convo- lutional layer. The complementarity of these methods improves the understanding of 3D-CNNs in Alzheimer’s disease classification from different perspectives.</p><p><code>摘要</code> — 我们开发了三种有效的方法，用于从三维卷积神经网络（3D-CNN）生成<code>视觉解释</code>，用于阿尔茨海默病的分类。 一种方法对分层3D图像分割进行灵敏度分析，另外两种方法在空间地图上可视化网络激活。 视觉检查和定量定位基准测试表明，所有方法都可以识别阿尔茨海默病诊断的重要大脑部分。 对比分析表明，基于灵敏度分析的方法难以处理松散分布的大脑皮层，基于激活可视化的方法受到旋转层分辨率的限制。 这些方法的互补性从不同角度提高了对阿尔茨海默病分类中3D-CNN的理解。</p><h4 id="Visualizing-Convolutional-Networks-for-MRI-based-Diagnosis-of-Alzheimer’s-Disease"><a href="#Visualizing-Convolutional-Networks-for-MRI-based-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="Visualizing Convolutional Networks for MRI-based Diagnosis of Alzheimer’s Disease"></a>Visualizing Convolutional Networks for MRI-based Diagnosis of Alzheimer’s Disease</h4><blockquote><p>可视化卷积网络用于基于MRI的阿尔茨海默病诊断</p></blockquote><p>Abstract. Visualizing and interpreting convolutional neural networks (CNNs) is an important task to increase trust in automatic medical decision making systems. In this study, we train a 3D CNN to detect Alzheimer’s disease based on structural MRI scans of the brain. Then, we apply four different gradient-based and occlusion-based visualization methods that explain the network’s classification decisions by highlight- ing relevant areas in the input image. We compare the methods qualita- tively and quantitatively. We find that all four methods focus on brain regions known to be involved in Alzheimer’s disease, such as inferior and middle temporal gyrus. While the occlusion-based methods focus more on specific regions, the gradient-based methods pick up distributed rel- evance patterns. Additionally, we find that the distribution of relevance varies across patients, with some having a stronger focus on the temporal lobe, whereas for others more cortical areas are relevant. In summary, we show that applying different visualization methods is important to understand the decisions of a CNN, a step that is crucial to increase clinical impact and trust in computer-based decision support systems.<br>Keywords: Alzheimer · Visualization · MRI · Deep Learning · CNN · 3D · Brain</p><p><code>摘要</code>. <code>可视化</code>和解释卷积神经网络（CNN）是增加对自动医疗决策系统的信任的重要任务。在这项研究中，我们根据大脑的结构MRI扫描训练3D CNN来检测阿尔茨海默病。然后，我们应用四种不同的基于梯度和基于遮挡的可视化方法，通过突出显示输入图像中的相关区域来解释网络的分类决策。我们定性和定量地比较这些方法。我们发现所有四种方法都集中在已知与阿尔茨海默病有关的大脑区域，例如下颞中回和下颞中回。虽然基于遮挡的方法更侧重于特定区域，但基于梯度的方法可以获取分布式相关模式。此外，我们发现相关性的分布因患者而异，其中一些人更关注颞叶，而另一些则更多的皮质区域是相关的。总之，我们表明应用不同的可视化方法对于理解CNN的决策非常重要，这对于增加临床影响和对基于计算机的决策支持系统的信任至关重要。<br>关键词：阿尔茨海默病·可视化·MRI·深度学习·CNN·3D·脑</p><h4 id="Shearlet-based-Stacked-Convolutional-Network-for-Multiclass-Diagnosis-of-Alzheimer’s-Disease-using-the-Florbetapir-PET-Amyloid-Imaging-Data"><a href="#Shearlet-based-Stacked-Convolutional-Network-for-Multiclass-Diagnosis-of-Alzheimer’s-Disease-using-the-Florbetapir-PET-Amyloid-Imaging-Data" class="headerlink" title="Shearlet based Stacked Convolutional Network for Multiclass Diagnosis of Alzheimer’s Disease using the Florbetapir PET Amyloid Imaging Data"></a>Shearlet based Stacked Convolutional Network for Multiclass Diagnosis of Alzheimer’s Disease using the Florbetapir PET Amyloid Imaging Data</h4><blockquote><p>基于Shearlet的堆叠卷积网络用于使用Florbetapir PET淀粉样蛋白成像数据进行阿尔茨海默病的多类诊断</p></blockquote><p><code>Abstract</code>—Although there is no cure for Alzheimer’s disease (AD), an accurate early diagnosis is essential for health and social care, and will be of great significance when the course of the disease could be reversed through treatment options. Florbetapir positron emission tomography (18F-AV-45 PET) is proven to be the most powerful imaging technique to investigate the deposition of amyloid plaques, one of the potential hallmarks of AD, signify- ing the onset of AD before it changes the brains structure. In this paper, we propose a novel classification algorithm to discriminate the patients having AD, early mild cognitive impairment (MCI), late MCI, and normal control in 18F-AV-45 PET using shearlet based deep convolutional neural network (CNN). It is known that the conventional CNNs involve convolution and pooling layers, which in fact produce the smoothed representation of data, and this results in losing detailed information. In view of this fact, the conventional CNN is integrated with shearlet transform incorporating the multiresolution details of the data. Once the model is pretrained to transform the input data into a better stacked representation, the resulting final layer is passed to softmax classifier, which returns the probabilities of each class. Through experimental results, it is shown that the performance of the proposed classification framework is superior to that of the traditional CNN in Alzheimer’s disease neuroimaging initiative (ADNI) database in terms of classification accuracy. As a result, it has the potential to distinguish the different stages of AD progression with less clinical prior information.<br><code>Index Terms</code>—Alzheimer’s disease (AD), Florbetapir positron emission tomography (18F-AV-45 PET) amyloid imaging, Shearlet transform (ST), Convolutional neural network (CNN), Softmax, Deep learning.</p><p><code>摘要</code> - 虽然阿尔茨海默病（AD）无法治愈，但准确的早期诊断对于健康和社会护理至关重要，并且当通过治疗方案可以逆转疾病病程时具有重要意义。 Florbetapir正电子发射断层扫描（18F-AV-45 PET）被证明是最有效的成像技术，用于研究淀粉样斑块的沉积，淀粉样斑块是AD的潜在标志之一，在改变大脑结构之前表示AD的发作 。在本文中，我们提出了一种新的分类算法，使用基于剪切的深度卷积神经网络（CNN）来区分患有<code>AD，EMCI，LMCI和18F-AV-45 PET中的正常对照的患者</code>。众所周知，传统的CNN涉及卷积和汇集层，这实际上产生了<code>数据的平滑表示，并且这导致丢失详细信息</code>。鉴于这一事实，<code>传统的CNN与剪切变换相结合</code>，并结合了数据的多分辨率细节。一旦模型被预训练以将输入数据转换为更好的堆叠表示，则将得到的最终层传递给softmax分类器，其返回每个类的概率。通过实验结果表明，在分类准确性方面，所提出的分类框架的性能优于传统CNN在阿尔茨海默病神经影像学计划（ADNI）数据库中的表现。因此，它有可能用较少的临床先验信息区分AD进展的不同阶段。<br><code>索引术语</code> - 阿尔茨海默病（AD），Florbetapir正电子发射断层扫描（18F-AV-45 PET）淀粉样蛋白成像，Shearlet变换（ST），卷积神经网络（CNN），Softmax，深度学习。</p><h4 id="Alzheimer’s-disease-Classification-from-Brain-MRI-based-on-transfer-learning-from-CNN"><a href="#Alzheimer’s-disease-Classification-from-Brain-MRI-based-on-transfer-learning-from-CNN" class="headerlink" title="Alzheimer’s disease Classification from Brain MRI based on transfer learning from CNN"></a>Alzheimer’s disease Classification from Brain MRI based on transfer learning from CNN</h4><blockquote><p>阿尔茨海默病从脑MRI分类基于CNN转移学习</p></blockquote><p><code>Abstract</code>— Various Convolutional Neural Network (CNN) architecture has been proposed for image classification and Object recognition. For the image based classification, it is a complex task for CNN to deal with hundreds of MRI Image slices, each of almost identical nature in a single patient. So, classifying a number of patients as an AD, MCI or NC based on 3D MRI becomes vague technique using 2D CNN architecture. Hence, to address this issue, we have simplified the idea of classifying patients on basis of 3D MRI but acknowledging the 2D features generated from the CNN framework. We present our idea regarding how to obtain 2D features from MRI and transform it to be applicable to classify using machine learning algorithm. Our experiment shows the result of classifying 3 class subjects patients. We employed scratched trained CNN or pretrained Alexnet CNN as generic feature extractor of 2D image which dimensions were reduced using PCA+TSNE, and finally classifying using simple Machine learning algorithm like KNN, Navies Bayes Classifier. Although the result is not so impressive but it definitely shows that this can be better than scratch trained CNN softmax classification based on probability score. The generated feature can be well manipulated and refined for better accuracy, sensitivity, and specificity.<br><code>Keywords</code>—CNN, MRI, generic feature, PCA, TSNE, Classifier</p><p><code>摘要</code> - 已经提出了各种卷积神经网络（CNN）架构用于图像分类和对象识别。对于基于图像的分类，CNN处理数百个MRI图像切片是一项复杂的任务，每个切片在单个患者中具有几乎相同的性质。因此，基于3D MRI将许多患者分类为AD，MCI或NC变为使用2D CNN架构的模糊技术。因此，为了解决这个问题，我们简化了基于3D MRI对患者进行分类的想法，但承认了CNN框架产生的2D特征。我们提出了如何从MRI获取2D特征并将其转换为适用于使用机器学习算法进行分类的想法。我们的实验显示了对3名受试者患者进行分类的结果。我们采用划痕训练的CNN或预训练的Alexnet CNN作为2D图像的通用特征提取器，使用PCA + TSNE减小尺寸，最后使用简单的机器学习算法（如KNN，Navies Bayes分类器）进行分类。虽然<code>结果并不那么令人印象深刻</code>，但它肯定表明这可能比基于概率得分的刮刮训练的CNN softmax分类更好。生成的特征可以很好地操作和细化，以获得更好的准确性，灵敏度和特异性。<br><code>关键词</code> -  CNN，MRI，通用特征，PCA，TSNE，分类器</p><h4 id="A-Novel-Multimodal-MRI-Analysis-for-Alzheimer’s-Disease-Based-on-Convolutional-Neural-Network"><a href="#A-Novel-Multimodal-MRI-Analysis-for-Alzheimer’s-Disease-Based-on-Convolutional-Neural-Network" class="headerlink" title="A Novel Multimodal MRI Analysis for Alzheimer’s Disease Based on Convolutional Neural Network"></a>A Novel Multimodal MRI Analysis for Alzheimer’s Disease Based on Convolutional Neural Network</h4><blockquote><p> 基于卷积神经网络的阿尔茨海默病多模式MRI分析</p></blockquote><p>Abstract—Recent years, Alzheimer’s disease (AD) has be- come a significant threat to human health while the accurate screening and diagnosis of AD remain a tough problem. Multimodal Magnetic resonance imaging (MRI) can help to identify the variation of brain function and structure in a non-invasive way. Deep learning, especially the convolutional neural networks (CNN), can be utilized to automatically detect appropriate features for classification, which is well adapted for computer-aided AD screening and identification. This paper proposed a multimodal MRI analytical method based on CNN, which is also suitable for single type MRI data analysis. First, the human brain network connectivity matrix were extracted from multimodal MRI data, used as the input data for CNN. Then a novel CNN framework was proposed to process the network matrix and classify AD, amnestic mild cognitive impairment (aMCI) patients and normal controls (NC). The advantage of this method lies in that we combined multi- modal MRI information through CNN convolution kernel, and achieved a higher classification accuracy. In our experiments, the comprehensive classification accuracy of AD, aMCI patients and NC was as high as 92.06% when using multimodal MRI data as input, which is effective enough to provide a reference for multimodal MRI data analysis.</p><p>摘要 - 近年来，阿尔茨海默病（AD）已经成为人类健康的重大威胁，而AD的准确筛查和诊断仍然是一个棘手的问题。多模式磁共振成像（MRI）可以帮助以非侵入性方式识别脑功能和结构的变化。深度学习，尤其是卷积神经网络（CNN），可用于自动检测适当的分类特征，这非常适合于计算机辅助AD筛选和识别。本文提出了一种基于CNN的多模态MRI分析方法，该方法也适用于单一类型的MRI数据分析。首先，从多模式MRI数据中提取人脑网络连接矩阵，用作CNN的输入数据。然后提出了一种新的CNN框架来处理网络矩阵并对AD，遗忘型轻度认知障碍（aMCI）患者和正常对照（NC）进行分类。该方法的优点在于我们通过CNN卷积核将多模态MRI信息结合起来，实现了更高的分类精度。在我们的实验中，当使用多模式MRI数据作为输入时，<code>AD，aMCI患者和NC的综合分类准确率高达92.06％</code>，这足以为多模式MRI数据分析提供参考。</p><h4 id="DISCRIMINATIVE-ANALYSIS-OF-THE-HUMAN-CORTEX-USING-SPHERICAL-CNNS-A-STUDY-ON-ALZHEIMER’S-DISEASE-DIAGNOSIS"><a href="#DISCRIMINATIVE-ANALYSIS-OF-THE-HUMAN-CORTEX-USING-SPHERICAL-CNNS-A-STUDY-ON-ALZHEIMER’S-DISEASE-DIAGNOSIS" class="headerlink" title="DISCRIMINATIVE ANALYSIS OF THE HUMAN CORTEX USING SPHERICAL CNNS - A STUDY ON ALZHEIMER’S DISEASE DIAGNOSIS"></a>DISCRIMINATIVE ANALYSIS OF THE HUMAN CORTEX USING SPHERICAL CNNS - A STUDY ON ALZHEIMER’S DISEASE DIAGNOSIS</h4><blockquote><p>球形CNNS对人体皮质的判别分析 - 阿尔茨海默病的诊断研究</p></blockquote><p><code>ABSTRACT</code> — In neuroimaging studies, the human cortex is commonly mod- eled as a sphere to preserve the topological structure of the cortical surface. Cortical neuroimaging measures hence can be modeled in spherical representation. In this work, we ex- plore analyzing the human cortex using spherical CNNs in an Alzheimer’s disease (AD) classification task using corti- cal morphometric measures derived from structural MRI. Our results show superior performance in classifying AD versus cognitively normal and in predicting MCI progression within two years, using structural MRI information only. This work demonstrates for the first time the potential of the spherical CNNs framework in the discriminative analysis of the human cortex and could be extended to other modalities and other neurological diseases.<br><code>Index Terms</code>— Spherical CNNs, cortex, Alzheimer’s disease, structural MRI</p><p><code>摘要</code> — 在神经影像学研究中，人体皮层通常被建模为球体，以保持皮质表面的拓扑结构。 因此，皮质神经成像测量可以以球形表示来建模。 在这项工作中，我们使用来自结构MRI的皮质形态测量指标，在阿尔茨海默病（AD）分类任务中使用球形CNN分析人体皮层。 我们的结果显示，在仅使用结构MRI信息的情况下，在分类AD与认知正常以及预测MCI在两年内的进展方面表现优异。 这项工作首次证明了球形CNN框架在人类皮层的辨别分析中的潜力，并可能扩展到其他形式和其他神经系统疾病。<br><code>索引术语</code> - 球形CNN，皮层，阿尔茨海默病，结构MRI</p><h4 id="3D-Inception-based-CNN-with-sMRI-and-MD-DTI-data-fusion-for-Alzheimer’s-Disease-diagnostics"><a href="#3D-Inception-based-CNN-with-sMRI-and-MD-DTI-data-fusion-for-Alzheimer’s-Disease-diagnostics" class="headerlink" title="3D Inception-based CNN with sMRI and MD-DTI data fusion for Alzheimer’s Disease diagnostics"></a>3D Inception-based CNN with sMRI and MD-DTI data fusion for Alzheimer’s Disease diagnostics</h4><blockquote><p>3D基于初始的CNN，具有用于阿尔茨海默病诊断的sMRI和MD-DTI数据融合</p></blockquote><p><code>Abstract</code>: In the last decade, computer-aided early diagnostics of Alzheimers Disease (AD) and its prodromal form, Mild Cognitive Impair- ment (MCI), has been the subject of extensive research. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. Furthermore, fusion of imaging modalities in a supervised machine learning framework has shown promising direction of research.<br>In this paper we first review major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer’s Disease diagnostics. Then we propose our own design of a 3D Inception-based Convolutional Neural Network (CNN) for Alzheimer’s Disease diagnostics. The network is designed with an emphasis on the interior resource utilization and uses sMRI and DTI modalities fusion on hippocampal ROI. The comparison with the conventional AlexNet-based network using data from the Alzheimers Disease Neuroimaging Initiative (ADNI) dataset (<a href="http://adni.loni.usc.edu" target="_blank" rel="noopener">http://adni.loni.usc.edu</a>) demonstrates significantly better performance of the proposed 3D Inception-based CNN.<br><code>Keywords</code>: MedicalImaging,AlzheimersDisease,MildCognitiveImpairment,MachineLearning,Deeplearning,Convolutional Neural Networks, Data Fusion.</p><p><code>摘要</code>：在过去十年中，计算机辅助阿尔茨海默病（AD）的早期诊断及其前驱形式，轻度认知障碍（MCI），已成为广泛研究的主题。一些最近的研究已经显示使用结构和功能磁共振成像（sMRI，fMRI），正电子发射断层扫描（PET）和扩散张量成像（DTI）模式的AD和MCI测定中的有希望的结果。此外，在监督机器学习框架中融合成像模式已经显示出有前途的研究方向。<br>在本文中，我们首先回顾自动分类方法的主要趋势，例如基于特征提取的方法以及应用于阿尔茨海默病诊断领域的医学图像分析中的深度学习方法。然后我们提出了我们自己设计的基于3D初始的卷积神经网络（CNN）的阿尔茨海默病诊断。该网络的设计重点是内部资源的利用，并使用sMRI和DTI模式融合海马ROI。使用来自Alzheimers Disease Neuroimaging Initiative（ADNI）数据集（<a href="http://adni.loni.usc.edu）的数据与传统的基于AlexNet的网络的比较证明了所提出的基于3D" target="_blank" rel="noopener">http://adni.loni.usc.edu）的数据与传统的基于AlexNet的网络的比较证明了所提出的基于3D</a> Inception的CNN的显着更好的性能。<br><code>关键词</code>：医学成像，老年痴呆症，轻度认知障碍，机器学习，去学习，卷积神经网络，数据融合。</p><h4 id="3D-CNN-based-classification-using-sMRI-and-MD-DTI-images-for-Alzheimer-disease-studies"><a href="#3D-CNN-based-classification-using-sMRI-and-MD-DTI-images-for-Alzheimer-disease-studies" class="headerlink" title="3D CNN-based classification using sMRI and MD-DTI images for Alzheimer disease studies"></a>3D CNN-based classification using sMRI and MD-DTI images for Alzheimer disease studies</h4><blockquote><p> 基于3D CNN的分类使用sMRI和MD-DTI图像进行阿尔茨海默病研究</p></blockquote><p><code>Abstract</code>: Computer-aided early diagnosis of Alzheimers Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. Furthermore, fusion of imaging modalities in a supervised machine learning framework has shown promising direction of research.<br>In this paper we first review major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer’s Disease diagnostics. Then we propose our own algorithm for Alzheimer’s Disease diagnostics based on a convolutional neural network and sMRI and DTI modalities fusion on hippocampal ROI using data from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (<a href="http://adni" target="_blank" rel="noopener">http://adni</a>. loni.usc.edu). Comparison with a single modality approach shows promising results. We also propose our own method of data augmentation for balancing classes of different size and analyze the impact of the ROI size on the classification results as well.<br><code>Keywords</code>: MedicalImaging,AlzheimersDisease,MildCognitiveImpairment,MachineLearning,Deeplearning,Convolutional Neural Networks, Image Fusion.</p><p><code>摘要</code>：计算机辅助早期诊断阿尔茨海默病（AD）及其前驱形式，轻度认知障碍（MCI），近年来已成为广泛研究的主题。一些最近的研究已经显示使用结构和功能磁共振成像（sMRI，fMRI），正电子发射断层扫描（PET）和扩散张量成像（DTI）模式的AD和MCI测定中的有希望的结果。此外，在监督机器学习框架中融合成像模式已经显示出有前途的研究方向。<br>在本文中，我们首先回顾自动分类方法的主要趋势，例如基于特征提取的方法以及应用于阿尔茨海默病诊断领域的医学图像分析中的深度学习方法。然后，我们使用来自阿尔茨海默病神经影像学倡议（ADNI）数据库（<a href="http://adni.loni.usc.edu）的数据，基于卷积神经网络和海马ROI上的sMRI和DTI模态融合，提出我们自己的阿尔茨海默病诊断算法。" target="_blank" rel="noopener">http://adni.loni.usc.edu）的数据，基于卷积神经网络和海马ROI上的sMRI和DTI模态融合，提出我们自己的阿尔茨海默病诊断算法。</a> 。与单一模态方法的比较显示出有希望的结果。我们还提出了自己的数据增强方法，用于平衡不同大小的类，并分析ROI大小对分类结果的影响。<br><code>关键词</code>：医学成像，老年痴呆症，轻度认知障碍，机器学习，深度学习，卷积神经网络，图像融合。</p><h4 id="MRI-to-FDG-PET-Cross-Modal-Synthesis-Using-3D-U-Net-For-Multi-Modal-Alzheimer’s-Classification"><a href="#MRI-to-FDG-PET-Cross-Modal-Synthesis-Using-3D-U-Net-For-Multi-Modal-Alzheimer’s-Classification" class="headerlink" title="MRI to FDG-PET: Cross-Modal Synthesis Using 3D U-Net For Multi-Modal Alzheimer’s Classification"></a>MRI to FDG-PET: Cross-Modal Synthesis Using 3D U-Net For Multi-Modal Alzheimer’s Classification</h4><blockquote><p>MRI到FDG-PET：使用3D U-Net进行多模态阿尔茨海默病分类的交叉模态综合</p></blockquote><p><code>Abstract</code>. Recent studies suggest that combined analysis of Magnetic resonance imaging (MRI) that measures brain atrophy and positron emission tomography (PET) that quantifies hypo-metabolism provides improved accuracy in diagnosing Alzheimer’s disease. However, such tech- niques are limited by the availability of corresponding scans of each modality. Current work focuses on a cross-modal approach to estimate FDG-PET scans for the given MR scans using a 3D U-Net architecture. The use of the complete MR image instead of a local patch based ap- proach helps in capturing non-local and non-linear correlations between MRI and PET modalities. The quality of the estimated PET scans is measured using quantitative metrics such as MAE, PSNR and SSIM. The efficacy of the proposed method is evaluated in the context of Alzheimer’s disease classification. The accuracy using only MRI is 70.18% while joint classification using synthesized PET and MRI is 74.43% with a p-value of 0.06. The significant improvement in diagnosis demonstrates the utility of the synthesized PET scans for multi-modal analysis.</p><p><code>摘要</code>.最近的研究表明，测量脑萎缩的磁共振成像（MRI）和量化低代谢的正电子发射断层扫描（PET）的联合分析提高了诊断阿尔茨海默病的准确性。然而，这些技术受到每种模态的相应扫描的可用性的限制。目前的工作重点是使用<code>3D U-Net架构</code>估算给定MR扫描的FDG-PET扫描的跨模态方法。使用完整的MR图像而不是基于局部补丁的方法有助于捕获MRI和PET模态之间的非局部和非线性相关性。使用诸如MAE，PSNR和SSIM的定量指标来测量估计的PET扫描的质量。在阿尔茨海默病分类的背景下评估所提出的方法的功效。仅使用MRI的准确率为70.18％，而使用合成PET和MRI的联合分类为74.43％，p值为0.06。诊断的显着改进证明了合成的PET扫描用于多模态分析的效用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/Paper/ADNI/"/>
    
    
  </entry>
  
  <entry>
    <title>AD-dataset</title>
    <link href="http://yoursite.com/2019/07/26/AD-dataset/"/>
    <id>http://yoursite.com/2019/07/26/AD-dataset/</id>
    <published>2019-07-26T02:52:57.000Z</published>
    <updated>2019-07-29T03:44:09.544Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p>pet数据集</p><ul><li><p>维度有误</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- em.183.npy</span><br><span class="line">- lm.114.npy</span><br><span class="line">- lm.139.npy</span><br><span class="line">- lm.201.npy</span><br><span class="line">- lm.204.npy</span><br><span class="line">- lm.235.npy</span><br><span class="line">- lm.248.npy</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/ADNI/"/>
    
    
      <category term="dataset" scheme="http://yoursite.com/tags/dataset/"/>
    
  </entry>
  
  <entry>
    <title>Git指令</title>
    <link href="http://yoursite.com/2019/07/24/Git%E6%8C%87%E4%BB%A4/"/>
    <id>http://yoursite.com/2019/07/24/Git指令/</id>
    <published>2019-07-24T08:09:23.000Z</published>
    <updated>2019-07-28T11:33:11.093Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="新建分支"><a href="#新建分支" class="headerlink" title="新建分支"></a>新建分支</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/24/Git指令/create_new_branch.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="查看分支"><a href="#查看分支" class="headerlink" title="查看分支"></a>查看分支</h3><ul><li>远程+本地分支：<code>git branch -a</code></li><li>远程分支：<code>git branch -r</code></li></ul><h3 id="切换分支"><a href="#切换分支" class="headerlink" title="切换分支"></a>切换分支</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git co dev/zj/605</span><br></pre></td></tr></table></figure><h3 id="查看文件差异"><a href="#查看文件差异" class="headerlink" title="查看文件差异"></a>查看文件差异</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git diff xxx.py</span><br></pre></td></tr></table></figure><h3 id="rebase"><a href="#rebase" class="headerlink" title="rebase"></a>rebase</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SH-platform</title>
    <link href="http://yoursite.com/2019/07/24/SH-platform/"/>
    <id>http://yoursite.com/2019/07/24/SH-platform/</id>
    <published>2019-07-24T02:59:41.000Z</published>
    <updated>2019-08-05T07:54:46.756Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="功能实现"><a href="#功能实现" class="headerlink" title="功能实现"></a>功能实现</h2><h3 id="热力图叠加"><a href="#热力图叠加" class="headerlink" title="热力图叠加"></a>热力图叠加</h3><h4 id="数组转换为图片"><a href="#数组转换为图片" class="headerlink" title="数组转换为图片"></a>数组转换为图片</h4><ul><li><p><strong>Recommended: (without padding)</strong> </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imsave(savepath, img_array. cmap=<span class="string">'rainbow'</span>)  <span class="comment"># cmap -- colormap</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Others without padding:</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line">scipy.misc.imsave(savepath, img_array) <span class="comment"># Use ``imageio.imwrite`` instead</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line">imageio.imwirte(savepath, img_array)  <span class="comment"># Lossy conversion from float64 to uint8</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line">img = Image.fromarray(img_array.astype(<span class="string">'uint8'</span>)).convert(<span class="string">"RGB"</span>)</span><br><span class="line">img.save(savepath, cmap=<span class="string">'rainbow'</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Others with padding</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure()</span><br><span class="line">plt.axis(<span class="string">'off'</span>)  <span class="comment"># 仅不可见，但依旧存在</span></span><br><span class="line">plt.imshow(img_array, cmap=<span class="string">'bone'</span>)</span><br><span class="line">plt.savefig(savepath, bbox_inched=<span class="string">'tight'</span>)</span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/jifaley/article/details/79687000" target="_blank" rel="noopener">plt.savefig() 输出图片去除旁边的空白区域</a> 未测试</p></li></ul><h4 id="背景透明化"><a href="#背景透明化" class="headerlink" title="背景透明化"></a>背景透明化</h4><p>参考：<a href="https://blog.csdn.net/qq_40878431/article/details/82941982" target="_blank" rel="noopener"> Python PIL.Image之修改图片背景为透明</a></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以第一个像素为准，相同色改为透明</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transparent_back</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img.convert(<span class="string">'RGBA'</span>)</span><br><span class="line">    L, H = img.size</span><br><span class="line">    color_0 = img.getpixel((<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(H):</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">            dot = (l,h)</span><br><span class="line">            color_1 = img.getpixel(dot)</span><br><span class="line">            <span class="keyword">if</span> color_1 == color_0:</span><br><span class="line">                color_1 = color_1[:<span class="number">-1</span>] + (<span class="number">0</span>,)</span><br><span class="line">                img.putpixel(dot,color_1)</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    img=Image.open(<span class="string">'round.png'</span>)</span><br><span class="line">    img=transparent_back(img)</span><br><span class="line">    img.save(<span class="string">'round2.png'</span>)</span><br></pre></td></tr></table></figure><h4 id="图像叠加"><a href="#图像叠加" class="headerlink" title="图像叠加"></a>图像叠加</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plt_overlay</span><span class="params">(origin_array, heatmap, save_path=<span class="string">'output'</span>, title=<span class="string">'overlay'</span>)</span>:</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.imshow(origin_array, cmap=<span class="string">'bone'</span>)</span><br><span class="line">    plt.imshow(heatmap, cmap=<span class="string">'rainbow'</span>, alpha=<span class="number">0.3</span>)</span><br><span class="line">    plt.savefig(os.path.join(save_path, <span class="string">'&#123;&#125;.png'</span>.format(title)), bbox_inched=<span class="string">'tight'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h4 id="扩展链接"><a href="#扩展链接" class="headerlink" title="扩展链接"></a>扩展链接</h4><ul><li><p><a href="https://blog.csdn.net/JNingWei/article/details/78241973" target="_blank" rel="noopener">opencv: 图片 设置 透明度 并 叠加(cv2.addWeighted)</a></p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">cv2.addWeighted(src1, alpha, src2, beta, gamma[, dst[, dtype]]) → dst.</span><br></pre></td></tr></table></figure><p>其中，<code>alpha</code> 为 <code>src1</code> 透明度，<code>beta</code> 为 <code>src2</code> 透明度.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 底板图案</span></span><br><span class="line">bottom_pic = <span class="string">'elegent.jpg'</span></span><br><span class="line"><span class="comment"># 上层图案</span></span><br><span class="line">top_pic = <span class="string">'lena.jpg'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">bottom = cv2.imread(bottom_pic)</span><br><span class="line">top = cv2.imread(top_pic)</span><br><span class="line"><span class="comment"># 权重越大，透明度越低</span></span><br><span class="line">overlapping = cv2.addWeighted(bottom, <span class="number">0.8</span>, top, <span class="number">0.2</span>, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 保存叠加后的图片</span></span><br><span class="line">cv2.imwrite(<span class="string">'overlap(8:2).jpg'</span>, overlapping)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://blog.csdn.net/m0_37606112/article/details/78632896" target="_blank" rel="noopener">opencv3.3.1+python3.6.3图像上添加背景透明logo</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line">cv2.namedWindow("logo")  #定义一个窗口</span><br><span class="line">frame=cv2.imread('girl.png') #捕获图像1</span><br><span class="line">logo=cv2.imread('logo.jpg')</span><br><span class="line">logo_gray=cv2.cvtColor(logo,cv2.COLOR_BGR2GRAY)</span><br><span class="line">rows,cols,channels = logo.shape</span><br><span class="line"></span><br><span class="line">dx,dy=120,150  # roi初始坐标</span><br><span class="line">roi=frame[dx:dx+cols,dy:dy+rows]</span><br><span class="line">for i in range(cols):</span><br><span class="line">    for j in range(rows):</span><br><span class="line">        if (logo[i,j][0]+logo[i,j][1]+logo[i,j][2])&lt;=20:roi[i,j]=roi[i,j]</span><br><span class="line">        else:roi[i,j]=logo[i,j]</span><br><span class="line"><span class="meta">#</span><span class="bash">roi=cv2.addWeighted(logo,0.5,roi,0.5,1)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">roi=cv2.add(logo,roi)</span></span><br><span class="line">frame[dx:dx+cols,dy:dy+rows]=roi</span><br><span class="line"></span><br><span class="line">cv2.imshow("logo",frame)#显示轮廓</span><br><span class="line"></span><br><span class="line">cv2.waitKey(0)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure></li></ul><h3 id="模型参数量计算"><a href="#模型参数量计算" class="headerlink" title="模型参数量计算"></a>模型参数量计算</h3><h4 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">param_statistics</span><span class="params">(params)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    参考：https://blog.csdn.net/qq_18293213/article/details/79047742</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    total_statistics = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, param_i <span class="keyword">in</span> enumerate(params):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'OrderedDict'</span> <span class="keyword">in</span> str(type(params)):</span><br><span class="line">            param = params[param_i]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            (name, param) = param_i</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"第&#123;&#125;层的结构：&#123;&#125;"</span> .format(i, str(list(param.size()))))</span><br><span class="line"></span><br><span class="line">        layer_statistics = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> param.size():</span><br><span class="line">            layer_statistics *= j</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> param.size():</span><br><span class="line">            print(<span class="string">"该层参数和："</span> + str(layer_statistics))</span><br><span class="line">            total_statistics = total_statistics + layer_statistics</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"总参数数量和："</span> + str(total_statistics))</span><br><span class="line">    <span class="keyword">return</span> total_statistics</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. torch model</span></span><br><span class="line">model = CNNModel()</span><br><span class="line">load_model_path = <span class="string">'cnn_model_Epoch_ 32_batch_size_24.pkl'</span></span><br><span class="line">model.load_state_dict(torch.load(load_model_path))</span><br><span class="line">model_params = list(model.named_parameters())  <span class="comment"># list  </span></span><br><span class="line">total_statistics = param_statistics(model_params)</span><br><span class="line">print(total_statistics)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. torch model_path</span></span><br><span class="line">load_model_path = <span class="string">'cnn_model_Epoch_ 32_batch_size_24.pkl'</span></span><br><span class="line">pretrained_dict = torch.load(load_model_path)  <span class="comment"># OrderedDict</span></span><br><span class="line">total_statistics = param_statistics(pretrained_dict) </span><br><span class="line">print(total_statistics)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">参考：https://blog.csdn.net/appleml/article/details/81000301</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h4 id="keras"><a href="#keras" class="headerlink" title="keras"></a>keras</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. keras model</span></span><br><span class="line">model = Resnet((<span class="number">64</span>, <span class="number">104</span>, <span class="number">80</span>, <span class="number">1</span>)).get_model()</span><br><span class="line">total_statistics = model.count_params()  </span><br><span class="line">print(total_statistics)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. keras model path</span></span><br><span class="line">model_path = load_model(<span class="string">'resnet.acc.batchsize2.h5'</span>)</span><br><span class="line">total_statistics = model.count_params(load_model(model_path))</span><br><span class="line">print(total_statistics)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">参考：https://codeday.me/bug/20190417/984038.html</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h3 id="多模态配准结果可视化"><a href="#多模态配准结果可视化" class="headerlink" title="多模态配准结果可视化"></a>多模态配准结果可视化</h3><p><a href="https://blog.csdn.net/loveliuzz/article/details/73648505" target="_blank" rel="noopener">opencv中伪彩色applyColorMap函数（C++ / Python）</a></p><p><a href="https://blog.csdn.net/zh_jessica/article/details/77992578" target="_blank" rel="noopener">Python-OpenCV 图像叠加or图像混合加权（cv2.addWeighted）</a></p><p><a href="https://blog.csdn.net/qq_41895190/article/details/82905657" target="_blank" rel="noopener">OpenCV-Python图片叠加与融合，cv2.add与cv2.addWeighted的区别</a></p><p><a href="http://benjamintan.io/blog/2018/05/24/making-transparent-backgrounds-with-numpy-and-opencv-in-python/" target="_blank" rel="noopener">Making Transparent Backgrounds with Numpy and OpenCV in Python</a></p><p><strong>ToRead</strong>:</p><p><a href="https://stackoverflow.com/questions/54082300/how-to-create-a-transparent-mask-in-opencv-python" target="_blank" rel="noopener"><a href="https://stackoverflow.com/questions/54082300/how-to-create-a-transparent-mask-in-opencv-python" target="_blank" rel="noopener">How to create a transparent mask in opencv-python</a></a></p><p><strong>TODO</strong>:</p><ul><li>好好了解PIL、cv2</li></ul><h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p><a href="https://huilansame.github.io/huilansame.github.io/archivers/python-unittest" target="_blank" rel="noopener">Python必会的单元测试框架 —— unittest</a></p><p><strong>test fixture之setUp() tearDown()</strong></p><p>如果想要在所有case执行之前准备一次环境，并在所有case执行结束之后再清理环境，我们可以用 <code>setUpClass()</code> 与 <code>tearDownClass()</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestMathFunc</span><span class="params">(unittest.TestCase)</span>:</span></span><br><span class="line">    <span class="string">"""Test mathfuc.py"""</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setUpClass</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">"This setUpClass() method only called once."</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tearDownClass</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">"This tearDownClass() method only called once too."</span></span><br></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">This setUpClass() method only called once.</span><br><span class="line">add</span><br><span class="line">...</span><br><span class="line">multi</span><br><span class="line">This tearDownClass() method only called once too.</span><br></pre></td></tr></table></figure><p>扩展阅读：<a href="https://docs.python.org/zh-cn/3/library/unittest.html#module-unittest" target="_blank" rel="noopener"><code>unittest</code> — 单元测试框架</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Multimodal-Neuroimaging-Feature-Learning-for-Multiclass-Diagnosis-of-Alzheimer’s-Disease-翻译</title>
    <link href="http://yoursite.com/2019/07/19/Multimodal-Neuroimaging-Feature-Learning-for-Multiclass-Diagnosis-of-Alzheimer%E2%80%99s-Disease-%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/07/19/Multimodal-Neuroimaging-Feature-Learning-for-Multiclass-Diagnosis-of-Alzheimer’s-Disease-翻译/</id>
    <published>2019-07-19T05:15:52.000Z</published>
    <updated>2019-07-19T05:27:40.406Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p><a href="https://scholar.google.com/scholar?q=Multimodal+Neuroimaging+Feature+Learning+for+Multiclass+Diagnosis+of+Alzheimer%E2%80%99s+Disease&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart" target="_blank" rel="noopener">《Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer’s Disease》</a>  多模式神经影像学特征学习用于阿尔茨海默病的多类诊断</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><strong><em>Abstract</em></strong></h2><p>阿尔茨海默病（AD）的准确诊断对于患者护理是必不可少的，并且随着疾病调节剂在疾病早期可用而变得越来越重要。虽然研究已将机器学习方法应用于AD的计算机辅助诊断，但由于缺乏表示神经影像生物标记物的有效策略，因此在先前的方法中显示出诊断性能的瓶颈。在这项研究中，我们设计了一个新的诊断框架与深度学习架构，以帮助诊断AD。该框架使用零掩蔽策略进行数据融合，以从多种数据模式中提取补充信息。与之前最先进的工作流程相比，我们的方法能够在一个设置中融合多模态神经成像功能，并且可能需要较少标记的数据。 AD的二元分类和多类分类均实现了性能提升。讨论了拟议框架的优点和局限性。</p><p>Index Terms—Alzheimer’s disease (AD), classification, deep Learning, MRI, neuroimaging, positron emission tomography (PET).</p><h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><p>ALZHEIMER’S疾病（AD）是一种退行性脑紊乱，其特征是进行性痴呆，其特征在于特定神经细胞的退化，神经炎斑块的存在和神经原纤维缠结[1]。记忆和其他认知功能的下降是通常的早期综合症。由于社会年龄的增长，AD将成为未来几十年的全球负担。据报道，2006年全世界共有2660万例AD病例，其中约56％的病例处于早期阶段。预计到2050年，AD患者人数将增长四倍，达到1.068亿[2]。 AD的精确诊断被认为是一项困难的临床任务，其特异性不足，因为当意识受损时，不能对精神状态进行评估。另一个困难是由于其他非AD痴呆综合征的混乱引起的。轻度认知障碍（MCI）是AD的一个前驱阶段，最近引起了研究人员的注意，因为它对临床试验很有用。尽管MCI并未显着干扰日常活动，但已经不断证明MCI患者存在AD进展的高风险[3]。为了预测MCI的转变风险，可以将MCI受试者进一步分类为MCI转换器（cMCI）和MCI非转换器（ncMCI）。检测早期阶段以及整个AD进展范围是至关重要的;因此，在发生不可逆的脑损伤之前，允许患者控制危险因素，例如单纯收缩期高血压[4]，[5]。神经影像学技术，如磁共振成像（MRI）[6]  -  [11]和正电子发射断层扫描（PET）[12]  -  [18]，已被广泛用于AD的评估，以及许多其他非成像生物标志物[ 6]，[19]，[20]。<br>已经提出机器学习方法来帮助AD的诊断。预先计算的医学描述符被广泛用于表示生物医学图像。近似测量，例如体积[21]和葡萄糖的脑代谢率（CMRGlc）[22]，通常是根据分割的3-D脑区域（ROI）计算出来的，并用于AD分类。支持向量机（SVM）[23]，贝叶斯方法[24]，或其他方法[25]，[26]。但是，此类工作流程存在一些限制。基于这些传统机器学习者的方法通常在二元分类中很好地工作，例如从正常对照（NC）受试者中分类AD受试者，但是很难将它们扩展到多类[27]。因此，虽然AD的诊断应该自然地建模为多类分类问题，但它通常被简化为一组二元分类任务[23,28]，将AD或MCI受试者与NC受试者区分开来。另一个限制是嵌入临床先验知识。 Liu等人最近提出了一种基于图切割算法的方法。 [10]。该工作流程调整了图形切割算法，其参数对应于AD的不同阶段之间的关系。虽然这种定制往往会产生有希望的分类结果，但工作流程可能对数据集的变化很敏感，并且很难扩展到大规模。 AD诊断的另一个挑战是以无监督的方法表示原始生物标志物。一些框架以监督的方式降低每种生物标记物的维度，然后融合特征形态以形成新的特征空间[29]  -  [31]。这种工作流程在很大程度上取决于难以实现的标记样品的数量。分离维数减少和数据融合也可能导致丢失补充信息。</p><p>我们相信，通过设计新的框架来有效地表示多种生物标记并有效地表征AD的多个阶段，可以优化先前的工作流程。具有浅结构和仿射数据变换的传统特征工程工作流通常简单地导致特征重复或维度选择。正如许多最近的研究所示，通过解开输入中的复杂模式，深度数据表示可以比多类分类中的浅层架构更有效[32]  -  [36]。深度学习架构通过多层特征表示逐步提取高级特征[37]。由于特征空间的连续变换，高级特征在分类问题中往往更加可分。<br>使用MR的Brosch和Tam报告说，多层学习结构能够有效地捕获与人口统计学和疾病信息相关的大脑区域的形状变化，例如心室大小[38]。在Suk等人提出的框架中。 [39]，对每种图像模态训练一个堆叠自动编码器（SAE）设置;然后，学习的高级特征进一步与多核支持向量机（MKSVM）融合。在这样的工作流程中，无论其他模态如何，都可以学习单模态高级特征，这可能会忽略特征学习中不同模态之间的协同作用。<br>在这项研究中，我们提出了一种新的多层AD诊断框架，其中嵌入了深度学习架构，其受益于多模态神经影像学特征之间的协同作用。该框架由SAE和soft-max逻辑回归器构成。自动编码器以无人监督的方式表示数据，可以扩展为在实践中使用未标记的数据。当提供多模态神经影像图像数据时，所提出的框架能够进行数据融合。遵循去噪自动编码器的概念[40]，我们将双掩模策略应用于双峰深度学习任务，以提取不同图像模态之间的协同作用。通过随机隐藏训练集的一种形态，神经网络的隐藏层倾向于能够通过推断多模态特征之间的相关性来重构丢失的模态与损坏的输入。通过深度学习架构中嵌入的soft-max回归，我们的框架能够将AD患者分为四个AD阶段。<br>本文的其余部分安排如下。我们在第二部分介绍了拟议的学习框架和培训策略。本研究的实验和结果见第III节。我们在第四节和第五节讨论了该文件的拟议框架和结论。</p><h2 id="II-METHODOLOGY"><a href="#II-METHODOLOGY" class="headerlink" title="II. METHODOLOGY"></a>II. METHODOLOGY</h2><p>所提出的框架的流程如图1所示。在该研究中，MR和PET数据被用作两种输入神经成像模态。 首先对所有收集的脑图像进行预处理并将其分割成83个功能ROI，并从每个ROI计算一组描述符。 数据集分为训练集和测试集。 我们仅对训练样本执行弹性网[9]，[41]，[42]，以选择特征参数的判别子集。 然后使用训练数据集中的所选特征子集训练由若干自动编码器组成的多层神经网络。 网络的每一层都通过非线性变换获得了前一层的更高层次的抽象[43]  -  [45]。 softmax层添加在SAE的顶部以进行分类。 然后用标记的测试样品评估训练的网络。</p><h3 id="A-Data-Acquisition-and-Feature-Extraction"><a href="#A-Data-Acquisition-and-Feature-Extraction" class="headerlink" title="A. Data Acquisition and Feature Extraction"></a><em>A. Data Acquisition and Feature Extraction</em></h3><p>本研究中使用的神经影像学数据来自阿尔茨海默病神经影像学倡议（ADNI）数据库1 [46]。该数据库于2003年由国家老龄化研究所，国家生物医学成像和生物工程研究所，食品和药物管理局，私营制药公司和非营利组织发起，作为一个为期五年的公共合作伙伴关系。 ADNI项目的主要目的是研究结合多种生物标记物（如MRI，PET和CSF数据以及神经心理学评估）预测MCI和早期AD的进展的效果。大约200个正常实例和400个MCI实例被跟踪了三年; 200名AD患者在两年内随访。确定敏感的生物标志物对AD的进展也可能有助于临床医生发现新的治疗方法，以及其他可能的生物医学探索。<br>我们从ADNI获得了两个数据集。对于仅具有MR图像的数据集，从ADNI库中回收了816个年龄和性别匹配的受试者，并从每个受试者获得T1加权的MR图像。我们排除了20名具有多次转换或逆转的受试者以及数据不完整的21名MCI受试者。我们将从第一次扫描起0.5至3年转换为AD的MCI受试者标记为cMCI，否则将MCI受试者标记为ncMCI。正常受试者和AD患者被标记为NC和AD [10]。所有原始MR图像均按照ADNI MR图像协议进行校正，并使用图像配准工具包[48]非线性地注册到ICBM_152模板[47]。由于无法容忍的失真，仅排除了17张图像。最后，758名MR受试者被保留用于在该研究中进行的实验，包括180名AD受试者，160名cMCI受试者，214名ncMCI受试者和204名正常老化对照受试者。<br>对于具有多模式数据融合的数据集，从基线群组中选择331个年龄和性别匹配的受试者，包括77个NC-，102个ncMCI-，67个cMCI-，85个AD受试者，其具有MR和PET数据。每个实例都与T1加权体积和FDG-PET图像相关联。使用前面描述的MR图像的类似工作流程对所有3-D图像进行预处理。使用FSL FLIRT将PET图像与相应的MR图像对齐[49]。<br>对于每个注册的三维图像，使用增强配准方法的多特征传播在模板空间中映射了83个脑区[50]。从MR图像中提取灰质体积，与[9]和[10]相同。对于PET图像，我们提取了与[22]和[51]相同的区域平均CMRGlc特征。然后，我们将特征归一化为介于0和1之间，以通过移位负值和重新缩放来支持S形解码器。</p><h3 id="B-Learning-Framework"><a href="#B-Learning-Framework" class="headerlink" title="B. Learning Framework"></a><em>B. Learning Framework</em></h3><p><strong>1) Pretraining SAEs</strong></p><p><strong>2) Multimodal Data Fusion:</strong>：当多个图像时，模态用于模型训练，需要模态融合方法来发现不同模态之间的协同作用。共享表示可以通过联合训练自动编码器和连接的MR和PET输入来获得。第一个共享隐藏层用于模拟不同数据模态之间的相关性。然而，简单的特征连接策略通常导致隐藏的神经元仅由单一模态激活，因为MR和PET的相关性是高度非线性的。灵感来自Ngiam等人。 [54]，我们将预训练方法应用于一定比例的损坏输入，这些输入仅提供一种模态，遵循深度建筑训练的去噪概念。通过用0替换这些输入，随机隐藏其中一种形式;其余的训练样本都有两种形式。训练第一自动编码器的隐藏层以重建来自与隐藏模态混合的输入的所有原始输入。原始输入和破坏的输入独立地传播到神经网络的较高层，以使用相同的神经网络获得清洁表示和噪声表示。然后逐步训练每个较高层以从传播的噪声表示重建清洁的高级表示。因此，一些隐藏的神经元有望推断出不同神经影像学模式之间的相关性。</p><p><strong>3) Fine-Tuning for AD Classification</strong></p><p>对于AD诊断，我们将任务建模为包含四个预定义标签的四级分类问题：NC，cMCI，ncMCI和AD。 虽然无监督网络学到的特征也可以转移到传统分类器，但是，软性逻辑回归使我们能够通过微调联合优化整个网络。<br>由无监督网络提取的特征通过softmax回归[55]输入到输出层。 softmax层使用不同的激活函数，其可能具有与先前层中应用的非线性不同的非线性。</p><p>…</p><h3 id="C-Feature-Examination"><a href="#C-Feature-Examination" class="headerlink" title="C. Feature Examination"></a><em>C. Feature Examination</em></h3><h2 id="III-EXPERIMENTS-AND-RESULTS"><a href="#III-EXPERIMENTS-AND-RESULTS" class="headerlink" title="III. EXPERIMENTS AND RESULTS"></a>III. EXPERIMENTS AND RESULTS</h2><h3 id="A-Visualization-of-High-Level-Biomarkers"><a href="#A-Visualization-of-High-Level-Biomarkers" class="headerlink" title="A. Visualization of High-Level Biomarkers"></a><em>A. Visualization of High-Level Biomarkers</em></h3><p>利用第II-C节中描述的特征检查方法，我们计算了每个脑ROI的稳定性得分，并将稳定性得分映射到NC对象的掩蔽的3-D MR图像（83个ROI），如图3所示。 各种投资回报率之间的区别清晰可见。 较暗的区域往往比较轻的ROI对AD和MCI的进展更敏感，因为从这些ROI中提取的特征往往同样有益于所有隐藏的神经元。 光区域未被表示为完全无关紧要，但携带较少的预测信息。</p><h3 id="B-Performance-Evaluation"><a href="#B-Performance-Evaluation" class="headerlink" title="B. Performance Evaluation"></a><em>B. Performance Evaluation</em></h3><p>我们将提出的框架与使用单核SVM和MKSVM的广泛应用的方法进行了比较[23]，[28]。为了评估所提出的数据融合方法，我们将零掩模方法与[39]中提出的架构进行了比较，该架构独立地训练两个SAE，然后在每个SAE经过微调后将高级特征与MKSVM融合。如第II-A部分所述，使用从MR图像和PET图像中提取的相同特征评估所有实验。<br>拟议的框架在MATLAB 2013a上实施。基于SVM的实验使用LIBSVM [58]进行。 MKSVM是通过使用预先计算的内核并使用相对权重融合多个内核来实现的。<br>通过使用十倍交叉验证进行评估。在包括多种模态的实验中，我们将性能与仅单模态数据，MR或PET以及具有两种模态的数据融合方法进行了比较。为避免“幸运试验”，我们从每个班级中随机抽取训练和测试实例，以确保它们与原始数据集具有相似的分布。对整个网络进行了培训，并使用90％的数据进行了微调，然后在每个验证试验中对其余样本进行了测试。在每个验证试验中使用对数域中的近似搜索选择所有比较方法的超参数以获得最佳执行模型[59]。在所有基于神经网络的实验中使用了两个隐藏层，因为添加额外的隐藏层未显示AD分类的进一步改进。假设两个非线性变换可以理想地表示AD分类的神经影像学特征是合理的。根据每个折叠中的分类性能，在30和200之间选择隐藏层中的神经元数量。在每个神经网络中，隐藏层共享相同数量的隐藏神经元[60]。使用训练样本训练MKSVM。按照[23]中的工作流程，通过步长为0.1的粗网格搜索选择MKSVM中每个内核的相对权重。在使用MKSVM融合两个SAE网络的实验中，每个SAE首先进行预训练并用训练数据进行微调，然后，从每个网络获得的高级特征与MKSVM融合，并采用前面所述的程序。<br>1）MR实验（758名受试者）：我们首先用758个3-D MR图像评估了所提出的框架。由于仅呈现了一种模态，因此在SVM和所提出的方法中都没有使用模态融合策略。<br>表I中显示了二元分类（NC与AD和NC与MCI）的性能。前两列是各个类的精确度，以下三列是整体性能，包括准确度，灵敏度和特异性。所提出的方法（SAE）在通过引导总体准确性（82.59％）和总体灵敏度（86.83％）对来自NC受试者的AD受试者进行分类方面优于SVM。这两种方法的总体特征非常接近（78.89％和77.78％）。在从MCI分类NC的所有整体性能测量中，所提出的方法优于SVM。所提出的方法在对NC受试者进行分类时实现了高出5％的精确度。</p><p>表II中显示了多类分类的性能。前四列是各个班级的精确度，以下三列是整体表现。所提出的方法在三个类别中比SVM执行更好的精确度（NC为52.40％，cMCI为38.71％，AD为46.89％）。所提出的方法导致总体准确度（46.30％）和总体特异性（77.78％）。 SVM实现了更高的灵敏度（75.00％）。总之，当仅呈现MR数据时，我们提出的方法在二元和多类AD分类问题中的大多数性能测量中优于最先进的基于SVM的方法。<br>2）MR和PET实验（331名受试者）：共有331名受试者同时获得MR和PET数据。我们首先仅使用MR图像（SVM-MR，SAE-MR）或PET图像（SVM-PET，SAE-PET）评估SVM的性能和所提出的基于SAE的方法。多核SVM的融合模式的性能显示为MKSVM。对于深度学习方法，我们将提出的零屏蔽训练策略（SAE-ZEROMASK）与简单特征连接（SAE-CONCAT）进行了比较。<br>二元分类性能显示在表III中。可以观察到，两种模态的实验（MKSVM，SAE-CONCAT和SAE-ZEROMASK）比在二元分类任务中仅具有单一模态的实验产生更好的性能。 SAE-CONCAT的总体准确度略高于MKSVM（90.15％ -  90.11％和77.65％ -  76.88％）。可以观察到，当使用所提出的SAE-ZEROMASK方法时，与SAE-CONCAT相比，所有测量中的性能都得到了提高。与ZERO-MASK相比，MKSVM在分类NC和AD方面的特异性略高。尽管SVM-MR在MCI上的精度略高（83.92％），但可以认为这种性能可能是由于决策失衡（NC上只有67％）。在所有方法中，SAE-ZEROMASK在NC和MCI之间的分类中取得了最均衡的表现（NC为81.95％，AD为83.88％），当MCI占据数据集的很大一部分时，这是相对困难的（246中有169个） ）。所提出的只有一个神经网络的数据融合方法SAE-ZEROMASK与2SAE-MKSVM实现了相当的性能，2SAE-MKSVM融合了来自两个独立训练网络的两个高级特征矩阵。 2SAE-MKSVM的准确率并不明显高于简单特征串联（77.90％至77.65％），因为在实验中观察到MKSVM为特征融合添加的仅保留了单模网络实现的更高精度。一些验证试验。<br>多类分类的性能如表IV所示。提议的框架与输入损坏（SAE-ZEROMASK）导致整体准确性和特异性（53.79％和86.98％）。基于深度学习的方法（SAE-CONCAT和SAE-ZEROMASK）引领NC，cMCI和AD的精确度。 cMCI的精确度受到cMCI实例数量的限制（331个中的67个），并受其兄弟类ncMCI（102个实例）的影响。对于ncMCI，SAE-ZEROMASK和MKSVM实现的精确度非常接近。与简单的特征串联（SAE-CONCAT）相比，SAE-ZEROMASK将整体精度提高了约5％。 SAE-ZEROMASK在整体准确性和特异性方面也优于其他数据融合选项2SAE-MKSVM。基于SVM的方法往往具有更好的灵敏度。</p><h2 id="IV-DISCUSSION"><a href="#IV-DISCUSSION" class="headerlink" title="IV. DISCUSSION"></a>IV. DISCUSSION</h2><h3 id="A-Model-Designing-and-Training"><a href="#A-Model-Designing-and-Training" class="headerlink" title="A. Model Designing and Training"></a><em>A. Model Designing and Training</em></h3><p>研究表明，学习具有原始数据的多重非线性表示的体系结构将产生有意义的分类特征[56]，[61]  -  [63]。为了在AD受试者中进行准确诊断，我们研究了神经影像生物标志物的多层表征在AD分类中的应用。我们的研究结果表明，多层结构可用于区分MR和PET受试者沿着AD进展的频谱，其准确度高于传统的浅层结构。分类的性能主要受益于学习架构的深度（来自复杂性理论的概念），其可以被示为特征空间的非线性变换序列。在微调期间，神经成像特征空间被扭曲和折叠以最小化训练数据上的分类损失。因此，在几层变换之后，不可分割的样本将在学习的高级特征空间中变得可分离。与传统方法相比，所提出的框架在提取基于神经影像学的ROI生物标记物之间的复杂相关性以及不同的特征形态方面更为强大。使用多层结构进行AD诊断的另一个动机是重复使用高级特征进行半监督学习[64]。除了监督数据融合或降维[29]之外，所提出的工作流程可以很容易地扩展到使用未标记的神经影像数据。<br>我们将不同的数据模态与所提出的零掩模融合策略结合起来，通过随机隐藏的一种模态传播噪声信号。训练自动编码器以利用损坏的输入信号重建原始输入信号。我们还试图避免在不同的数据模式上训练单独的神经网络，因为这可能会在特征学习期间忽略补充信息。具有一种隐藏模态的训练对象倾向于迫使一些神经元对MR和PET输入敏感，这使得零掩模融合网络不同，因为它具有两个独立的特征学习网络。值得注意的是，2SAE + MKSVM在NC和AD的二元分类中也实现了91.4％的总体分类准确度和91.67％的更高特异性。这可能表明，当不同特征聚类之间存在相对较大的边界时，两个特征融合方法之间的二元决策边界可能相似。观察所涉及的不可转换和可转换MCI主题的实验结果，我们假设当在嘈杂训练集中包括更微小的差异和更多异常值时，所提出的零掩模方法可能具有更多优点。</p><p>我们应用特征工程管道来提取MR和PET图像的初始ROI测量结果作为输入，而不是使用原始图像补丁进行医学特征学习.AD相关患者的三维医学图像之间的差异往往是微妙的，方差往往很大。从这个角度来看，网络决策系统的隐藏神经元也可以被解释为诊断规则的自动编码推断[65]。我们的实验表明，在使用ROI预计算特征时，无监督网络在预训练中实现了两个隐藏层的最佳性能。这意味着与使用原始图像作为输入的学习任务相比，当使用近似测量的成像特征时，实际上需要相对较浅的架构[38]。在我们的实验中，所有隐藏层中具有相同数量神经元的网络通常表现得更好。我们发现过度完成的歧管或低维流形都产生了AD分类的有效特征。根据不同的训练集选择隐藏神经元的数量。<br>使用弹性网的特征选择增强了所有检查方法的性能。它有助于控制由噪声和冗余特征参数引起的过度拟合。值得注意的是，大多数选定的特征参数都是由弹性网一致选择的。具有较少选择的特征参数的验证试验倾向于具有较高的泛化误差，这可能是由于训练集中包含的偏差异常值。<br>虽然提取的特征可以被其他一些传统的分类器使用，例如SVM，但我们将输出层与softmax回归连接到无监督网络。由于与其他层使用的非线性不同，softmax回归对应于多项式对数输出变量。结果，它能够在几个AD阶段之间对样本进行分类;它还简化了训练的微调阶段，因为softmax层可以与隐藏层共同优化。我们还研究了将微调功能转移到除嵌入式softmax回归器之外的流行分类器的框架设计。有趣的是，我们将深度学习网络学到的相同高级功能作为输入，所有被调查的分类器都倾向于做出高度一致的决策。</p><h3 id="B-Limitations-and-Future-Work"><a href="#B-Limitations-and-Future-Work" class="headerlink" title="B. Limitations and Future Work"></a><em>B. Limitations and Future Work</em></h3><p>考虑到可用神经成像数据的数量有限，我们假设可以使用可能具有较小方差的更多训练样本进一步提取不同生物标记之间的协同作用。所提出的数据融合策略遵循训练自动编码器的去噪方式，理论上增加了特征学习的难度，但控制了过度拟合。尽管四类AD分类的预测概率分布在决策系统中可能更具实际用途，但是在多类分类框架应用于临床应用之前，我们应该改进用可用数据集实现的性能。 。我们比较我们的方法的所有方法都倾向于过度拟合，但在训练集上具有高精度并且在测试集上具有低精度。由于具有神经网络（2SAE-MKSVM和SAE-ZEROMASK）的多模式学习架构是参数模型，我们假设当有更大的数据集时，它们可能有可能在多类AD诊断上获得更好的诊断准确性。这将允许以较低的方差更好地提取与主题无关的特征。<br>五，结论<br>我们提出了一种新的嵌入式深度学习AD诊断框架。该框架可以区分AD进展的四个阶段，并且需要较少的临床先验知识。由于无监督的特征表示嵌入在此工作流程中，因此在实践中有可能扩展到更多未标记的特征工程数据。在无人监督的预训练阶段，我们使用SAE来获得高级特征。当使用多种神经影像学模型时，我们应用零掩蔽策略来提取去噪方式之后不同模态之间的协同作用。在无监督的特征工程之后，使用softmax回归。我们使用了一种可视化高级脑生物标志物的新方法来分析提取的高级特征。<br>在第二阶段和第四阶段之间对AD分类进行了评估。基于MR和PET ADNI数据存储库，我们的框架优于最先进的基于SVM的方法和其他深度学习框架。因此，我们认为，所提出的方法可以成为代表多模态神经成像生物标志物的有力手段。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease-翻译</title>
    <link href="http://yoursite.com/2019/07/19/Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer%E2%80%99s-Disease-%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/07/19/Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease-翻译/</id>
    <published>2019-07-19T05:02:49.000Z</published>
    <updated>2019-07-29T04:01:24.616Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p><a href="https://scholar.google.com/scholar?q=Multimodal+Neuroimaging+Feature+Learning+With+Multimodal+Stacked+Deep+Polynomial+Networks+for+Diagnosis+of+Alzheimer&#39;s+Disease&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart" target="_blank" rel="noopener">Multimodal Neuroimaging Feature Learning With Multimodal Stacked Deep Polynomial Networks for Diagnosis of Alzheimer’s Disease</a>  多模态神经影像学特征学习多模态叠加深度多项式网络诊断阿尔茨海默病</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>阿尔茨海默病（AD）及其早期阶段（即轻度认知障碍）的准确诊断对于及时治疗和可能的AD延迟至关重要。多模态神经影像数据的融合，如磁共振成像（MRI）和正电子发射断层扫描（PET），已显示其对AD诊断的有效性。深度多项式网络（DPN）是最近提出的深度学习算法，其在大规模和小尺寸数据集上都表现良好。在这项研究中，提出了一种多模式堆叠DPN（MM-SDPN）算法，MM-SDPN由两级SDPN组成，用于融合和学习用于AD诊断的多模态神经成像数据的特征表示。具体而言，两个SDPN首先用于分别学习MRI和PET的高级特征，然后将其输入另一个SDPN以融合多模态神经影像信息。建议的MM-SDPN算法应用于ADNI数据集，以进行二进制分类和多类分类任务。实验结果表明，MM-SDPN优于最先进的基于多模态特征学习的算法用于AD诊断。</p><p>Index Terms—Alzheimer’s disease, deep learning, deep polynomial networks, multimodal stacked deep polynomial networks, multimodal neuroimaging.</p><h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><p>ALZHEIMER’S DISEASE（AD）是最常见的进行性神经退行性脑疾病之一，导致全世界老年人逐渐，不可逆转的记忆丧失和其他认知功能[1]。预计到2050年世界范围内AD患者人数将增加到1亿[2]。由于AD患病率的急剧增加，AD的准确诊断及其早期即轻度认知障碍（MCI）对于及时治疗和可能的AD延迟至关重要。<br>在过去的几十年中，神经成像技术，如磁共振成像（MRI），功能磁共振成像（fMRI）和正电子发射断层扫描（PET），已经深入推进了神经科学研究和临床应用[3]，[4]。可识别的成像生物标记物已被有效地用于AD的诊断或预后，这是由于它们通过神经成像可视化和定量测量脑结构和功能信息的优点[5]  -  [7]。<br>近年来，基于神经影像学的AD计算机辅助诊断（CAD）引起了人们的广泛关注[4]，[8]  -  [10]。此外，由于不同的神经影像学模型，如MRI和PET，可以提供不同的和互补的信息，以提高AD的诊断性能[4]，[11]，[12]，基于多模态神经影像学的AD分类具有引起了相当多的关注[4]，[13]  -  [18]。<br>CAD中机器学习算法的性能通常取决于数据表示（或特征），因此特征提取成为分类框架中的关键步骤[10]，[19]。用于神经成像数据的最常用的特征提取方法可以大致分为四类[8]，[20]：1）基于体素的方法，简单直接从体素强度中提取特征; 2）基于顶点的方法，其特征在皮质表面的顶点水平处定义; 3）基于感兴趣区域（ROI）的方法，从预定义的大脑区域提取特征; 4）基于补丁的方法，从本地补丁中学习新的特征表示。基于体素和顶点的特征通常具有非常高的维度，因此降低维数对于实现更紧凑和有效的特征是重要的。基于ROI的功能被广泛使用，因为它们不仅具有相对较低的特征维度，而且还覆盖整个大脑。然而，从ROI中提取的特征有些粗糙，并且不能反映脑疾病中涉及的微小或微妙变化。基于补丁的特征是从整个大脑中学习的，并且可以有效地捕获与疾病相关的病理。结果，这些特征通常可以获得更好的分类结果[20]  -  [24]。基于补丁的特征的方式本质上是一种特征重新表示的过程，用于学习，然后利用学习算法生成新的特征空间，其中深度学习（DL）已经实现了最先进的性能 [20]，[24]。</p><p>DL在各种应用中取得了巨大的成功，因为它是由Hinton等人首次引入的。在2006年[25]，[26]。与传统的浅层结构学习架构相比，DL开发了一种分层的分层架构，以产生高水平和更有效的数据表示[19]，[27]，[28]。近年来，DL在医学成像领域获得了良好的声誉，例如医学图像分类，检测和分割[29]  -  [35]。<br>在神经成像数据的情况下，DL可以有效地发现潜在或隐藏的表示，因此它已成功应用于AD和其他脑部疾病的诊断。 Suk等人。使用多模式深度限制玻尔兹曼机（RBM）来学习多模神经影像数据中的巨大3D斑块的特征，用于AD / MCI诊断[20]。古普塔等人。使用堆叠自动编码器（SAE）从自然图像中学习一组基础，然后应用卷积网络以获得更有效的AD分类特征表示[23]。 Brosch等人。通过深度置信网络（DBN）算法学习了一个低维数量的脑容量，以检测与AD的人口统计学和疾病参数相关的变异模式[36]。谢尔盖等人。还使用RBM和DBN来学习MRI和fMRI用于精神分裂症诊断的特征[34]。 Ithaput等。开发了一种随机去噪自动编码器算法，用于学习AD临床试验的高维神经影像学特征[37]。刘等人。还提出了一种基于SAE的多模态神经成像特征学习算法，用于学习基于ROI的AD诊断特征的特征表示[38]。 Payan和Montana结合稀疏自动编码器和卷积神经网络来学习AD预测局部斑块的特征表示[24]。 Suk和Shen将SAE与多任务特征选择和多核学习（MKL）算法结合起来，以学习用于AD分类的多模态神经影像数据和脑脊液（CSF）特征的ROI特征的潜在特征表示[39]。他们还提出了一种深层体系结构，以分层方式选择具有稀疏多任务学习功能的AD诊断功能[40]。李等人。开发了一种基于丢失技术的稳健多任务深度学习框架，以改善ADI / MCI诊断的ROI特征的表示[41]。陈等人。提出从纵向T1 MRI图像中提取特征，然后应用堆叠去噪稀疏自动编码器（SDSAE）融合这些特征以进行AD分期分析[42]。<br>另一方面，深度多项式网络（DPN）是一种新开发的有监督DL算法，其中每个节点计算其输入的线性或二次函数，因此学习的预测变量是输入空间上的多项式函数[43]。它不依赖于复杂的启发式，并且易于实现。与DBN算法相比，DPN在一些常用的大规模图像数据集上也取得了竞争性能[43]。值得注意的是，DPN还可以紧凑地表示有限样本数据集上的任何函数，因为其算法结构最初是针对小数据集开发的[43]。</p><p>由于神经影像数据集通常具有较小的标记样本[10]，作为小数据集的适当特征表示算法，受监督的DPN将潜在地从用于AD诊断的小神经影像数据中学习优越的特征表示。另一方面，特征提取的逐层堆叠通常在DL [19]中产生更好的表示，例如DBN和SAE [25]，[44]，这促使开发堆叠DPN（SDPN）算法以学习更高级别的特征表示。此外，已经证明，可以同时学习和融合多模态神经影像数据的多模式DL算法优于用于AD分类的单模态DL算法[20]，[38]，[39]，[41]。因此，值得研究多模堆叠DPN算法。<br>在这项工作中，提出了由上述因素驱动的多模式堆叠DPN（MM-SDPN）算法，然后将其应用于基于多模态神经成像的AD诊断。 MM-SDPN可以有效地融合和学习多模态神经影像学中的特征表示。<br>本文的其余部分安排如下。第二节提供了有关原始DPN和提议的MM-SDPN算法的介绍。所进行的实验和结果在第III节中给出，以评估所提出的MM-SDPN算法的性能。讨论和结论分别在第IV和V节中介绍。</p><h2 id="II-METHODS"><a href="#II-METHODS" class="headerlink" title="II. METHODS"></a>II. METHODS</h2><h3 id="A-Deep-Polynomial-Networks-Algorithm"><a href="#A-Deep-Polynomial-Networks-Algorithm" class="headerlink" title="A. Deep Polynomial Networks Algorithm"></a>A. Deep Polynomial Networks Algorithm</h3><h3 id="B-Stacked-Deep-Polynomial-Networks"><a href="#B-Stacked-Deep-Polynomial-Networks" class="headerlink" title="B. Stacked Deep Polynomial Networks"></a>B. Stacked Deep Polynomial Networks</h3><h3 id="C-Multimodal-Stacked-DPN"><a href="#C-Multimodal-Stacked-DPN" class="headerlink" title="C. Multimodal Stacked DPN"></a>C. Multimodal Stacked DPN</h3><p>为了通过SDPN融合和学习来自多媒体数据的特征表示，一个简单的解决方案是连接不同模态的所有向量。然而，这种简单的连接策略在一定程度上忽略了多种模态的多样性，并且不能很好地探索互补性质并且代表多种模态之间的高度非线性相关性。因此，我们提出了一种基于两级SDPN的MM-SDPN算法，如图4所示。<br>在第一阶段，每个神经影像数据将被馈送到其相应的SDPN模块，以学习高级特征表示。每种特定模态的高级特征反映了它自己的属性，但不同模态之间没有相关信息。然后，所有学习的特征在第二阶段被馈送到新的SDPN模块，以便与所有模态相关联。因此，最终学习的高级特征既包含每种模态的内在属性，也包含所有模态之间的相关性。因此，SDPN学到的特征更具有辨别力和鲁棒性。<br>在这项工作中，我们采用MRI和PET作为两种神经影像模式，这些模式常用于基于多模态神经影像学的AD分类[13]  -  [18]，[38]。值得注意的是，MM-SDPN中的融合策略与[38]中基于多模SAE的算法中的融合策略不同，以发现MRI和PET之间的协同作用。在[38]中，刘等人。采用预训练方法和一定比例的损坏输入，只有一种模态。特别是，他们通过将这些输入设置为0来随机隐藏一种模态，然后使用两种模态呈现其余训练样本。第一级AE中的隐藏层经过训练，可以重新组合输入中隐藏模态的所有原始输入。原始输入和损坏的输入都独立地传播到更高的网络层以实现清洁表示。在我们的MM-SDPN算法中，由于DPN在每个网络层中执行前馈监督学习而没有精细转向，因此难以执行与[38]中相同的学习策略来推断MRI和PET之间的相关性。因此，通过联合训练第二阶段SDPN与在第一阶段中学习的级联MRI和PET特征来学习共享表示。它类似于[42]中使用的简单融合方法。</p><h2 id="III-EXPERIMENTS-AND-RESULTS"><a href="#III-EXPERIMENTS-AND-RESULTS" class="headerlink" title="III. EXPERIMENTS AND RESULTS"></a>III. EXPERIMENTS AND RESULTS</h2><h3 id="A-Neuroimaging-Data-Preprocessing"><a href="#A-Neuroimaging-Data-Preprocessing" class="headerlink" title="A. Neuroimaging Data Preprocessing"></a>A. Neuroimaging Data Preprocessing</h3><p>为了评估提出的MM-SDPN算法的性能，这里使用来自阿尔茨海默病神经影像学倡议（ADNI）数据库的多模态神经影像学数据（<a href="http://www.loni.ucla.edu/ADNI）[45]。根据参考文献[14]，我们使用了相同的MRI和PET图像来自51名AD患者，99名MCI患者（43名MCI转换器（MCI-C），进展至AD，以及56名MCI非转换器（MCI-NC），谁没有在18个月内进展到AD，和52个正常对照（NC）具有相同的预处理和特征生成方法。" target="_blank" rel="noopener">www.loni.ucla.edu/ADNI）[45]。根据参考文献[14]，我们使用了相同的MRI和PET图像来自51名AD患者，99名MCI患者（43名MCI转换器（MCI-C），进展至AD，以及56名MCI非转换器（MCI-NC），谁没有在18个月内进展到AD，和52个正常对照（NC）具有相同的预处理和特征生成方法。</a><br>具体而言，预处理首先在MRI图像上进行，包括前连合（AC） - 后连合（PC）校正，N3算法强度不均匀[46]，以及由小脑提取的颅骨剥离和去除小脑。王等人。 [47]，[48]。然后通过FSL包中的FAST算法将MR图像分割成三种不同的组织，即灰质，白质和脑脊液[49]。在通过HAMMER算法[50]注册后，每个MR图像被分成93个ROI，基于模板，Kabani等人使用93个手动标记的ROI。 [51]。然后计算灰质组织的体积作为每个ROI的特征，产生93个特征。然后通过刚性配准将每个PET图像与其对应的MRI图像对准。将相同ROI的平均强度计算为PET图像的特征。因此，分别从MRI和PET图像中提取93个特征。</p><h3 id="B-Performance-Evaluation"><a href="#B-Performance-Evaluation" class="headerlink" title="B. Performance Evaluation"></a>B. Performance Evaluation</h3><p>执行四个分类任务，即AD与NC，MCI与NC，MCI-C与MCI-NC，以及AD与MCI-C对比MCI-NC对NC。<br>首先将提出的MM-SDPN算法与以下原始DPN和SDPN算法进行比较：（1）原始DPN分别从原始MRI和PET特征（称为DPN-3 MRI和DPN）的3层网络学习的特征-3-PET）; （2）原始DPN分别从原始MRI和PET特征（称为DPN-6-MRI和DPN-6-PET）获得的6层网络学习的特征; （3）SDPN分别从原始MRI和PET特征中学习的特征（称为SDPN-MRI和SDPN-PET）; （4）SDPN从连锁的MRI和PET特征（称为SDPN-MRI-PET）中学到的特征; （5）MM-SDPN从原始MRI和PET特征中学到的特征。<br>我们还将提出的MM-SDPN算法与用于AD分类的九种最先进的基于多模态学习的算法进行了比较，如表I所示。值得注意的是ADNI的MRI和PET子集以及提取的ROI特征。在我们的工作中，与参考文献[14]，[18]，[39]，[40]和[41]中使用的数据和特征相同。<br>在这项工作中，SDPN由2级基本DPN堆叠，每个基本DPN由3层网络组成。值得注意的是，上述SDPN也用于MM-SDPN。原始DPN算法主要有两个参数，即网络层数和每层隐藏节点数。在设置3层和6层DPN之后，我们通过对训练数据集进行贪婪搜索得到DPN-3算法中的数字隐藏节点，然后将它们作为所有其他基于DPN的算法的初始参数。进一步微调。<br>在这项工作中，SDPN由2级基本DPN堆叠，每个基本DPN由3层网络组成。值得注意的是，上述SDPN也用于MM-SDPN。原始DPN算法主要有两个参数，即网络层数和每层隐藏节点数。在设置3层和6层DPN之后，我们通过对训练数据集进行贪婪搜索得到DPN-3算法中的数字隐藏节点，然后将它们作为所有其他基于DPN的算法的初始参数。进一步微调。<br>两种分类器，即嵌入式线性分类器（LC）和线性支持向量机（SVM）[53]，被用来更全面地评估基于DPN的特征的性能。 SVM使用LIBSVM工具箱[54]执行。从单模式MRI或PET提取的原始ROI特征仅由SVM分类器处理。<br>对所有算法执行10倍交叉验证策略，并且该过程独立重复5次，以避免在交叉验证中随机分区数据集引入的采样偏差。选择分类准确度（ACC），灵敏度（SEN）和特异性（SPE）作为评估指标。分类结果由所有50个结果的平均值±SD（标准偏差）的格式给出。此外，接收器操作特性（ROC）曲线和ROC曲线下面积值（AUC）也用于SVM分类器。</p><h3 id="C-Results-on-AD-vs-NC"><a href="#C-Results-on-AD-vs-NC" class="headerlink" title="C. Results on AD vs. NC"></a>C. Results on AD vs. NC</h3><p>在AD和NC的分类中，表II显示了使用SVM分类器的不同特征学习算法的分类结果。可以发现MM-SDPN算法达到最佳性能，平均分类精度为97.13±4.44％，灵敏度为95.93±7.84％，特异性为98.53±5.05％，因为它成功地融合了MRI和PET信息。另一方面，尽管DPN-6（具有6层网络的DPN）在基于单一模态成像的AD分类的MRI和PET数据上优于DPN-3，但SDPN仍然比DPN-6略好，这表明其有效性由于堆叠技术的SDPN。<br>图5显示了不同算法的ROC曲线及其相应的AUC值。 MM-SDPN（0.972）的AUC值优于其他值。<br>如表III所示，当应用线性分类器时，不同的算法具有与表II中相似的趋势，这表明它不是分类器，而SPDN在最终分类性能中起关键作用。所提出的MM-SDPN算法再次获得最佳分类精度96.93±4.53％，灵敏度95.02±8.56％，特异性98.37±5.66％。 SDPN算法也优于原始DPN。在这里，DPN-6对于MRI和PET数据仍然优于DPN-3，但是SDPN比DPN-6获得了更好的结果。<br>表IV给出了所提出的MM-SDPN和其他最先进的基于多模态学习的算法的比较结果。 MM-SDPN在所有评估指标上都优于所有其他具有SVM和线性分类器的算法。与其他非SDPN多模态学习算法相比，具有SVM的MM-SDPN在准确度，灵敏度和特异性方面分别提高了至少1.23％，1.22％和2.20％，这表明了所提出的MM-SDPN算法的有效性。融合并学习MRI和PET数据的特征表示，用于AD分类。值得注意的是，提出的MM-SDPN算法优于[14]，[18]，[39]，[40]和[41]中的这些算法，在具有相同ROI的ADNI的相同MRI和PET子集上。特征。</p><h3 id="D-Results-on-MCI-vs-NC"><a href="#D-Results-on-MCI-vs-NC" class="headerlink" title="D. Results on MCI vs. NC"></a>D. Results on MCI vs. NC</h3><p>表V和VI分别显示了使用SVM和线性分类器对MCI与NC分类的不同基于DPN的算法的结果。可以看出，MM-SDPN仍然优于SVM和线性分类器的所有其他方法。最佳分类准确度，灵敏度和特异度分别为87.24±4.52％，97.91±4.17％和67.04±9.29％，SVM分别为86.99±4.82％，94.24±6.16％和71.32±9.93％。线性分类器。尽管DPN-6优于DPN-3，但SDPN在基于单模态成像的MRI和PET数据上的MCI分类方面优于DPN。<br>图6显示了不同算法的ROC曲线及其相应的AUC值，MM-SDPN达到了第二次性能，ROC值为0.901。<br>值得注意的是，在表V和VI中，由于样本不平衡问题，MCI患者的样本是NC受试者的两倍，因此灵敏度远高于MCI对NC的特异性。另一方面，对于MCI与NC，PET的结果优于MRI，这可以通过相对生物标志物的时间排序的理论模型来解释[55]。具体而言，FDG-PET代谢测量的变化先于MCI患者MRI脑结构的变化[55]。<br>如表VII所示，MM-SDPN再次优于比较的基于多模态学习的算法。与其他非SDPN算法相比，具有SVM的MM-SDPN分别在准确度和灵敏度方面提高了1.57％和2.54％。只有[38]中SAE的特异性非常高，但准确度和灵敏度都很低。</p><h3 id="E-Results-on-MCI-C-vs-MCI-NC"><a href="#E-Results-on-MCI-C-vs-MCI-NC" class="headerlink" title="E. Results on MCI-C vs. MCI-NC"></a>E. Results on MCI-C vs. MCI-NC</h3><p>众所周知，MCI-NC的MCI-C分类通常很困难，如先前的工作[18]，[20]，[41]所示。表VIII和IX给出了具有不同分类器的MCI-C对MCI-NC的不同基于DPN的算法的结果。可以观察到MM-SDPN再次获得最佳结果，其SVM分类器的平均分类准确度，灵敏度和特异性分别为78.88±4.38％，68.04±9.99％和86.81±9.12％，76.52±5.99线性分类器分别为％，62.50±10.65％和86.27±7.49％。对于这项困难的任务，SDPN对于单一模态神经成像数据的性能要比DPN-6和DPN-3好得多。<br>在图7中示出了不同算法的ROC曲线及其相应的AUC值，并且MM-SDPN再次实现了AUC值为0.801的最佳性能。<br>从表X可以看出，对于如此困难的分类任务，所提出的MM-SDPN仍然优于所有其他非SDPN多模态特征学习算法，提高了准确度和灵敏度至少2.96％和1.99％，分别用SVM分类器。基于深度RBM的算法实现了最佳特异性，但灵敏度较低。</p><h3 id="F-Results-on-AD-vs-MCI-C-vs-MCI-NC-vs-NC"><a href="#F-Results-on-AD-vs-MCI-C-vs-MCI-NC-vs-NC" class="headerlink" title="F. Results on AD vs. MCI-C vs. MCI-NC vs. NC"></a>F. Results on AD vs. MCI-C vs. MCI-NC vs. NC</h3><p>AD与MCI-C与MCI-NC与NC的多类分类是一项非常艰巨的任务。 表XI和XII的结果与其他二元分类的结果相似，表明MM-SDPN的分类精度为57.00±3.65％，灵敏度为53.65±4.04％，SVM的特异性为85.05±1.39％。 线性分类器的相应结果为55.34±4.57％，52.49±6.12％和84.18±1.99％。 同样，SDPN仍然优于DPN。<br>此外，如表XIII所示，与最先进的基于SAE的算法[38]相比，我们提出的具有SVM分类器的MM-SDPN算法实现了分类精度提高3.21％和灵敏度提高1.51％。</p><h2 id="IV-DISCUSSION"><a href="#IV-DISCUSSION" class="headerlink" title="IV. DISCUSSION"></a>IV. DISCUSSION</h2><p>在这项工作中，我们提出了一种MM-SDPN算法，可以有效地学习基于多模态神经影像学的AD诊断的特征。 ADNI数据集上的四组实验结果表明，与最先进的基于多模态学习的算法相比，所提出的MM-SDPN算法实现了最佳性能。特别，<br>在我们的研究中，原始ROI特征是低级特征，不能以良好的差异化方式表示AD的属性。当应用DPN来学习ROI特征的特征时，提供了更复杂的表示，因此DPN已经实现了显着的改进。在多次堆叠基本DPN块之后，获得更高级别的表示。因此，对于基于单模态神经成像的AD分类，SDPN比原始DPN具有更好的性能。值得注意的是，尽管随着隐层的增加，6层DPN优于3层DPN，但是根据神经网络中的通用逼近定理，太深的网络将增加计算复杂度，而近似的精度没有明显增加。工作。另一方面，结果还表明，具有两个3层DPN的SDPN优于6层DPN，因为第二层基本DPN中第一层的基础建立在更高级别的特征上，即串联特征第一级基本DPN，这个基础将在学习二级DPN后生成更有效和更高级别的功能。此外，与具有更深网络的DPN相比，SDPN更容易调整参数以实现相同的性能。<br>由于多模态神经影像数据的融合可以有效地有利于AD诊断的分类性能，因此提出了一种MM-SDPN算法，该算法由两阶段SDPN组成。两个SDPN分别应用于MRI和PET的ROI特征，以获得第一阶段中每种模态的摘要。然后将两个学习的特征集中并馈送到新的SDPN以学习融合特征，其包括每种模态的固有特性以及MRI和PET之间的相关性。与直接将单个SDPN应用于MRI和PET的集中ROI特征的方式相比，第一阶段DPN学习的高级特征将有益于并提高MM-SDPN中第二阶段SDPN的学习性能，因此， MM-SDPN具有更有效的学习和融合多模态神经影像数据的能力。<br>在这项研究中，两个分类器，即SVM和线性分类器，用于评估SDPN和MM-SDPN的性能。两个分类器都给出了类似的结果，这表明AD分类的良好性能更多地取决于学习的特征而不是分类器。因此，MM-SDPN真正有效地学习了一个好的特征表示。<br>DPN的三个主要特性有助于MM-SDPN的优异性能如下。 （1）神经影像数据集通常具有小的标记样本。当将DPN应用于如此小的数据集时，其构建的网络具有小节点，这保证了训练有素的深度网络和小样本[42]。 （2）在DPN网络中，第一个k层的节点构成了由k次多项式获得的所有值的基础。因此，DPN网络可能具有较大的偏差，但往往不会过度拟合（即低方差），即使对于较深的网络，偏差也会随着方差的增加而逐渐减小。因此，原则上，可以通过控制偏差 - 方差权衡来抑制过度拟合[43]。此外，DPN中的中间层连接非常稀疏，即中间层中的每个节点仅限于连接到少数其他节点，而不是前一层中的所有节点，这可以防止过度拟合[43]。因此，DPN的算法结构使其适用于小型数据集。 （3）由于神经影像数据通常仅提供有限的标记地面实况样本，并且先验标签信息有利于小数据的分类任务，因此监督DPN比不受控制的DL算法更适合于小神经成像数据集。此外，如工作中所示，所提出的MM-SDPN优于[38]和[39]中基于精细转向的监督SAE算法。</p><p>DPN是一种新的DL算法，其理论和算法的改进仍然很少。因此，在未来的工作中，我们不仅要进一步改进DPN算法，还要更加注重分析DPN的框架，特别是DPN与其他DL算法的区别。另一方面，本工作中提出的MM-SDPN算法显示了其对小数据集的有效性。事实上，DPN已经在[43]中从大规模数据中学习特征方面获得了良好的声誉。由于在连续的基本DPN之间没有前向和后向反馈，因此SDPN和MM-SDPN相对简单且快速。因此，它们也有望用于大规模数据。在未来的工作中，我们计划应用MM-SDPN直接从局部MRI和PET片段学习特征表示，这与[20]中的方法类似。此外，未来还将研究半监督MM-SDPN，因为获取有助于提高表征学习性能的未标记医学图像相对容易。由于MKL成功应用于多模态神经影像学数据，我们将尝试将MKL与MM-SDPN结合用于AD分类。来自第二阶段SDPN的学习特征和来自第一阶段SDPN中的MRI和PET的各个学习特征可以被视为MKL的多视图数据。因此，CSF功能也可以嵌入到MKL中，这有可能通过基于MM-SDPN和MKL的框架提高AD分类的性能。</p><h2 id="V-CONCLUSION"><a href="#V-CONCLUSION" class="headerlink" title="V. CONCLUSION"></a>V. CONCLUSION</h2><p>在这项工作中，提出了一种MM-SDPN算法。 它由两阶段SDPN组成，可以有效地学习和融合多模态数据，用于诊断阿尔茨海默病。 MM-SDPN实现了最先进的表现，用于对AD进展的两个阶段和四个阶段进行分类。 因此，所提出的MM-SDPN不仅可以作为多模神经成像数据的强大表示算法，还可以用于其他医学数据。</p><h3 id="ACKNOWLEDGMENT"><a href="#ACKNOWLEDGMENT" class="headerlink" title="ACKNOWLEDGMENT"></a>ACKNOWLEDGMENT</h3><p>作者要感谢阿尔茨海默氏病神经影像倡议为本文提供数据。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/Paper/ADNI/"/>
    
    
      <category term="translation" scheme="http://yoursite.com/tags/translation/"/>
    
  </entry>
  
  <entry>
    <title>Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译</title>
    <link href="http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/07/19/Reproducible-evaluation-of-classification-methods-in-Alzheimer-disease-Framework-and-application-to-MRI-and-PET-data-翻译/</id>
    <published>2019-07-19T03:39:20.000Z</published>
    <updated>2019-07-29T04:00:23.997Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p><a href="https://arxiv.org/abs/1808.06452" target="_blank" rel="noopener">《Reproducible evaluation of classification methods in Alzheimer’s disease: Framework and application to MRI and PET data》</a>  阿尔茨海默病分类方法的可行性评估：MRI和PET数据的框架和应用</p><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>大量论文引入了新颖的机器学习和自动特征提取方法<br>阿尔茨海默病（AD）的分类。然而，虽然绝大多数这些作品使用公共数据集ADNI进行评估，但它们很难再现，因为验证的不同关键组件通常不易获得。这些组件包括选定的参与者和输入数据，图像预处理和交叉验证程序。不同方法的表现也难以客观地比较。特别地，通常难以评估方法的哪个部分（例如，预处理，特征提取或分类算法）提供真正的改进（如果有的话）。在本文中，我们使用三个公开可用的数据集（ADNI，AIBL和OASIS）提出了AD中可重复和客观分类实验的框架。该框架包括：i）将三个数据集自动转换为标准格式（BIDS）; ii）模块化的预处理流水线，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准。我们使用T1 MRI和FDG PET数据证明该框架用于对1960名参与者进行大规模评估。在此评估中，我们评估不同模态，预处理，要素类型（基于区域或体素的特征），分类器，训练集大小和数据集的影响。表演符合最新技术水平。对于所有分类任务，FDG PET优于T1 MRI。使用不同的图册，图像平滑，FDG PET图像的部分体积校正或特征类型没有发现性能差异。线性SVM和L2逻辑回归导致相似的性能，并且都优于随机森林。分类性能随着用于训练的受试者数量而增加。在ADNI上训练的分类器很好地适用于AIBL和OASIS。框架和实验的所有代码都是公开的：通用工具已集成到Clinica软件（<a href="http://www.clinica.run）中，特定于纸张的代码可从以下网址获得：https：//gitlab.icm-institute" target="_blank" rel="noopener">www.clinica.run）中，特定于纸张的代码可从以下网址获得：https：//gitlab.icm-institute</a> .ORG / aramislab / AD-ML。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>阿尔茨海默病（AD）影响全世界超过2000万人。早期识别AD对于充分护理患者和测试新疗法非常重要。神经影像学提供了识别AD的有用信息（Ewers等，2011）：由于灰质丢失引起的萎缩，解剖学磁共振成像（MRI），18F-氟脱氧葡萄糖正电子发射断层扫描（FDG PET）的低代谢，淀粉样蛋白的积累 - β蛋白与淀粉样蛋白PET成像。然后，主要关注的是分析这些标志物以在早期识别AD。特别是，机器学习方法有助于通过学习神经影像数据的判别模式来帮助识别AD患者。<br>已经提出了大量的机器学习方法来分类和预测AD阶段（参见（Falahati等人，2014; Haller等人，2011; Rathore等人，2017）以进行评论）。他们中的一些使用单一成像模式（通常是解剖学MRI）（Cuingnet等人，2011; Fan等人，2008; Klo€ppel等人，2008; Liu等人，2012; Tong等人。 （2014）和其他人提出结合多种方式（MRI和PET图像，流体生物标记物）（Gray等，2013; Jie等，2015; Teipel等，2015; Young等，2013; Yun et al。，2015; Zhang et al。，2011）。验证和比较这些方法需要随着时间的推移跟踪大量患者。大量已发表的作品使用了公开的阿尔茨海默病神经影像学倡议（ADNI）数据集。然而，他们的结果之间的客观比较几乎是不可能的，因为它们在以下方面不同：i）患者的子集（选择标准的规范不清楚）; ii）图像预处理管道（因此不清楚优越性能是来自分类还是预处理）; iii）特征提取和选择; iv）机器学习算法; v）交叉验证程序和vi）报告的评估指标。由于这些差异，很难得出哪些方法表现最佳，甚至给定模态是否提供有用的附加信息。结果，这些作品的实际影响仍然非常有限。此外，绝大多数这些作品使用ADNI数据集（ADNI1用于早期论文，最常见的是ADNI1，ADNI-GO和ADNI2的组合用于更近期的工作）。因此，即使存在其他公开可用的数据集，例如澳大利亚成像生物标记物和生活方式研究（AIBL）以及开放获取系列成像研究（OASIS），也很少对其他数据集的泛化进行评估。<br>比较论文（Cuingnet et al。，2011; Sabuncu et al。，2015）和挑战（Allen et al。，2016; Bron et al。，2015）通过允许基准测试，成为客观评估机器学习方法的重要一步在同一数据集上使用相同的预处理方法。然而，这些研究提供了方法的“静态”评估。评估数据集在研究时以其当前状态使用，而新患者不断包括在ADNI等研究中。同样，它们仅限于研究时使用的分类和预处理方法。因此很难用新方法补充它们。<br>在本文中，我们提出了一个可重复评估AD中机器学习算法的框架，并展示了它在从三个公开可用的数据集中获得的PET和MRI数据的分类中的用途：ADNI，AIBL和OASIS。具体来说，我们的贡献是三方面的：</p><p>i）管理公开数据集的框架及其与新主题的持续更新，特别是全脑自动转换为脑成像数据结构4（BIDS）格式的工具（Gorgolewski等，2016）;<br>ii）一组模块化的预处理管道，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准;<br>iii）对来自三个公开可用的神经成像数据集（ADNI，AIBL和OASIS）的T1 MRI和PET数据进行大规模评估。</p><p>我们使用这个框架来证明从三个数据集（ADNI，AIBL和OASIS）获得的T1 MRI和PET数据进行自动分类。 我们评估各种成分对分类性能的影响：模态（T1 MRI或PET），特征类型（体素或区域特征），预处理，诊断标准（标准NINCDS / ADRDA标准或淀粉样蛋白精制标准），分类算法。 首先在ADNI，AIBL和OASIS数据集上独立进行实验，并通过将对ADNI训练的分类器应用于AIBL和OASIS数据来评估结果的推广。<br>框架和实验的所有代码都是公开的：通用工具已经集成到Clinica5（Routier等，2018），这是一个开源软件平台，我们开发用于处理来自神经影像学研究的数据，以及 特定于纸张的代码可从以下网址获得：https：//gitlab.icm-institute.org/aramislab/AD-ML。</p><h2 id="2-Materials"><a href="#2-Materials" class="headerlink" title="2. Materials"></a>2. Materials</h2><h3 id="2-1-Datasets"><a href="#2-1-Datasets" class="headerlink" title="2.1. Datasets"></a>2.1. Datasets</h3><p>用于制备本文的部分数据来自阿尔茨海默病神经影像学倡议数据库（adni.loni.usc.edu）。 ADNI于2003年作为公私合作伙伴关系启动，由首席研究员Michael W. Weiner博士领导。 ADNI的主要目标是测试是否可以将连续MRI，PET，其他生物标志物以及临床和神经心理学评估结合起来，以测量轻度认知障碍（MCI）和早期AD的进展。在研究的三个阶段（ADNI1，ADNI GO和ADNI2），北美地区招募了超过1650名参与者。大约400名参与者被诊断患有AD，900名患有MCI，350名被诊断为对照受试者。使用三个主要标准对受试者进行分类（Petersen等，2010）。正常受试者没有记忆投诉，而患有MCI和AD的受试者都必须进行投诉。 CN和MCI受试者的迷你精神状态检查（MMSE）评分在24到30之间（包括在内），AD受试者在20到26之间（包括在内）。 CN受试者的临床痴呆评分（CDR）评分为0，MCI受试者为0.5，强制要求记忆盒评分为0.5或更高，AD受试者为0.5或1.其他标准可见于（Petersen等，2010）。<br>我们还使用了AIBL研究组收集的数据。与ADNI类似，澳大利亚影像，生物标志物和生活方式老龄化旗舰研究旨在发现哪些生物标志物，认知特征以及健康和生活方式因素决定了AD的发展。 AIBL已招募1100名参与者并收集了超过4。5年的纵向数据：211名AD患者，133名MCI患者和768名可比较的健康对照。之前已经报道了AIBL研究方法（Ellis等，2010,2009）。简而言之，MCI诊断是根据基于（Winblad等人，2004）标准的方案和NINCDS-ADRDA标准的AD诊断（McKhann等人，1984）进行的。请注意，大约一半被诊断为健康对照的受试者报告存在记忆（Ellis等，2010,2009）。<br>最后，我们使用了开放获取系列成像研究项目的数据，该项目的目的是使科学界免费获得大脑的MRI数据集。我们专注于“年轻，中年，非痴呆和痴呆老年人的横断面MRI数据”（Marcus等，2007），其中包括416名年龄在18到96.100之间的受试者的横断面收集。 60岁以上的受试者已经临床诊断为非常轻度至中度的AD。用于评估诊断的标准是CDR评分。 CDR大于0的所有参与者被诊断为可能的AD。请注意，OASIS中没有MCI主题。</p><h3 id="2-2-Participants"><a href="#2-2-Participants" class="headerlink" title="2.2. Participants"></a>2.2. Participants</h3><h4 id="2-2-1-ADNI"><a href="#2-2-1-ADNI" class="headerlink" title="2.2.1. ADNI"></a>2.2.1. ADNI</h4><p>从ADNI数据集创建了三个子集：ADNIT1w，ADNICLASS和ADNICLASS，Aß。 ADNIT1w包括所有参与者（N = 1628），在基线时T1加权（T1w）MR图像可用。 ADNICLASS包括1159名参与者，他们在基线时可获得具有已知有效分辨率的T1w MR图像和FDG PET扫描。 ADNICLASS，Aß是ADNICLASS的一个子集，包括已知的淀粉样蛋白状态的918名参与者，这些参与者分别使用1.47和1.10作为截止值从PiB或AV45 PET扫描确定（Landau等，2013）。 对于每个ADNI子集，考虑了五个诊断组：</p><ul><li>CN：在基线时被诊断为CN的受试者;</li><li>AD：在基线时被诊断为AD的受试者;</li><li>MCI：在基线时被诊断为MCI，EMCI或LMCI的受试者;</li><li>pMCI：在基线时被诊断为MCI，EMCI或LMCI的受试者在至少36个月内进行随访，并在第一次就诊和36个月就诊之间进展至AD;</li><li>sMCI：在基线时被诊断为MCI，EMCI或LMCI的受试者在至少36个月内被跟踪并且未进展至AD<br>他们的第一次访问和36个月的访问之间。</li></ul><p>当然，pMCI和sMCI小组的所有参与者也都在<br>MCI集团。 请注意，反过来是错误的，因为一些MCI受试者没有转换为AD，但没有足够长时间地说明他们是sMCI还是pMCI。 我们没有考虑具有显着记忆问题（SMC）的受试者，因为此类别仅存在于ADNI 2中。<br>表1-3总结了参与者组成的人口统计学，MMSE和全球CDR分数 $ADNI_{T1w}$, $ADNI_{CLASS}$ and $ADNI_{CLASS, Aß}$.</p><h4 id="2-2-2-AIBL"><a href="#2-2-2-AIBL" class="headerlink" title="2.2.2. AIBL"></a>2.2.2. AIBL</h4><p>在这项工作中考虑的AIBL数据集由608名参与者组成，他们在基线时可获得T1加权MR图像。 用于创建诊断组的标准与用于ADNI的标准相同。 表4总结了AIBL参与者的人口统计学，MMSE和全球CDR得分。</p><h4 id="2-2-3-OASIS"><a href="#2-2-3-OASIS" class="headerlink" title="2.2.3. OASIS"></a>2.2.3. OASIS</h4><p>在这项工作中考虑的OASIS数据集由193名年龄在61岁或以上的参与者组成（参与者被诊断为AD的最低年龄）。 表5总结了OASIS参与者的人口统计学，MMSE和全球CDR分数。</p><h3 id="2-3-Imaging-data"><a href="#2-3-Imaging-data" class="headerlink" title="2.3. Imaging data"></a>2.3. Imaging data</h3><h4 id="2-3-1-ADNI"><a href="#2-3-1-ADNI" class="headerlink" title="2.3.1. ADNI"></a>2.3.1. ADNI</h4><p><strong>2.3.1.1. T1加权MRI.</strong> 3D T1w图像的采集协议可以在ADNI 1的（Jack等人，2008）和ADNI GO / 2的（Jack等人，2010a）中找到。 图像可以在获取时或在经历多个预处理校正步骤之后下载，其中包括校正由于梯度非线性（gradwarp）引起的图像几何失真，校正在执行RF传输时发生的图像强度不均匀性 具有更均匀的体线圈，同时接收使用不均匀的头部线圈（B1不均匀性），并且由于3T处的波或介电效应或1.5的残余强度不均匀性导致的强度不均匀性降低 T扫描（N3）（Jack等，2010a，2008）。</p><p><strong>2.3.1.2. PET.</strong> ADNI FDG PET方案包括动态采集6个5分钟帧（ADNI 1）或4个5分钟帧（ADNI GO / 2），注射后30-60分钟（Jagust等，2015,2010）。 可以下载预处理的不同阶段的图像（帧平均，空间对齐，插值到标准体素大小，以及平滑到8 mm全宽半高的公共分辨率）。 即使没有在实验中使用，也可以获得用于ADNI 1的11C-Pittsburgh化合物B（PIB）和用于ADNI 1 / GO / 2的18F-Florbetapir，也称为AV45，以成像脑中淀粉样蛋白的沉积。。 该方案包括在注射后50至70分钟动态采集4个5分钟的帧（Jagust等，2015,2010）。 至于FDG PET，可以下载预处理的不同阶段的图像。</p><h4 id="2-3-2-AIBL"><a href="#2-3-2-AIBL" class="headerlink" title="2.3.2. AIBL"></a>2.3.2. AIBL</h4><p>用于AIBL受试者的T1w MR图像使用ADNI 3D T1w序列采集，with 1 ✕ 1 mm in-plane resolution，切片厚度为1.2mm，TR/TE/TI=2300/2.98/900，翻转角度为9 和视野240鉁 256和160片（Ellis等，2010）。 尽管他们没有在实验中使用，但也获得了Florbetapir，PiB和Flutemeta-mol PET数据。</p><h4 id="2-3-3-OASIS"><a href="#2-3-3-OASIS" class="headerlink" title="2.3.3. OASIS"></a>2.3.3. OASIS</h4><p>对于每个OASIS主题，三个或四个T1w图像，with 1 ✕ 1 mm in-plane resolution ，切片厚度为1.25 mm，TR/TE/TI = 9.7/4.0/20，翻转角10，视野256 在单次成像会话中，在1.5T扫描仪上获得了鉁256和128个切片（Marcus等，2007）。 对于每个受试者，还可以下载重新采样到1mm各向同性体素的运动校正的共同配准图像的平均值，以及空间标准化图像。</p><h2 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3. Methods"></a>3. Methods</h2><p>我们开发了一套统一的工具，用于数据管理，图像预处理，特征提取，分类和评估。 这些工具已集成到我们开发的Clinica6（Routier等，2018），这是一个开源软件平台。 转换工具允许在新主题可用时轻松更新数据集。 不同的组件以模块化方式设计：使用Nipype处理管道（Gorgolewski等，2011），以及使用scikit-learn7库的分类和评估工具（Pedregosa等，2011）。 这允许开发和测试其他方法作为给定步骤的替代，并且客观测量每个组分对结果的影响。 提供了一个简单的命令行界面，代码也可以用作Python库。</p><h3 id="3-1-Converting-datasets-to-a-standardized-data-structure"><a href="#3-1-Converting-datasets-to-a-standardized-data-structure" class="headerlink" title="3.1. Converting datasets to a standardized data structure"></a>3.1. Converting datasets to a standardized data structure</h3><p>尽管公共数据集非常有价值，但这些研究的一个重要困难在于临床和成像数据的组织。例如，ADNI和AIBL成像数据在下载状态下不依赖于社区标准来进行数据组织和缺乏明确的结构。对于参与者的给定访问存在多个图像采集，并且补充图像信息包含在许多csv文件中，使得对数据库和主题选择的探索非常复杂。为了组织数据，我们选择了BIDS格式（Gorgolewski等，2016），这是一种能够存储多种神经影像模型的社区标准。 BIDS基于文件层次结构而不是数据库管理系统，可以在任何环境中轻松部署。非常重要的是，我们提供的代码可以在下载到BIDS组织版本时自动执行数据转换，用于所有使用的数据集：ADNI，AIBL和OASIS。这允许其他组直接再现，而不必重新分配数据集，这在ADNI和AIBL的情况下是不允许的。我们还根据所需的成像模式，随访持续时间和诊断提供了用于选择受试者的工具，这使得可以使用具有尽可能多的研究对象的相同组。最后，我们提出了一个BIDS启发的标准化结构，用于实验的所有输出。</p><h4 id="3-1-1-Conversion-of-the-ADNI-dataset-to-BIDS"><a href="#3-1-1-Conversion-of-the-ADNI-dataset-to-BIDS" class="headerlink" title="3.1.1. Conversion of the ADNI dataset to BIDS"></a>3.1.1. Conversion of the ADNI dataset to BIDS</h4><p>ADNI到BIDS转换器要求用户下载所有ADNI研究数据（csv格式的表格数据）和感兴趣的成像数据。请注意，下载的文件必须与下载文件完全一致。以下步骤由自动转换器执行（无需用户干预）。要将成像数据转换为BIDS，首先从ADNIMERGE电子表格中获取其会话的主题列表。该列表针对每种感兴趣的模态与可用的扫描列表进行比较，如模态特定的csv文件（例如MRILIST.csv）所提供的。如果针对特定的一对主题会话获取模态，并且几个扫描和/或预处理的图像可用，则仅转换一个。关于T1扫描，当几个可用于单个会话时，选择首选扫描（如MAYOADIRL_MRI_IMAGEQC_12_08_15.csv中所标识）。如果未指定首选扫描，则选择更高质量的扫描（如MRIQUALI-TY.csv中所定义）。如果未找到质量控制，则我们选择第一次扫描。当可用时选择Gradwarp和B1不均匀校正图像，因为这些校正可以在临床环境中进行，否则选择原始图像。对于ADNI 1，1.5T图像是优选的，因为它们可用于更多患者。关于FDG PET扫描，选择在时间帧上共同登记和平均的图像。扫描失败的质量控制（如果在PETQC.csv中指定）将被丢弃。请注意，AV45 PET扫描也会被转换，但不会在实验中使用。一旦选择了感兴趣的图像并识别出图像文件的路径，就可以将成像数据转换为BIDS。当采用dicom格式时，首先使用dcm2niix工具将图像转换为nifti，或者在dcm2nii工具出现错误的情况下（Li et al。，2016）。通过为每个主题创建子文件夹来生成BIDS文件夹结构。在每个主题子文件夹中创建会话文件夹，并在每个会话子文件夹内创建一个模态文件夹。最后，将nifti中的每个图像复制到相应的文件夹并重命名以遵循BIDS规范。临床数据也转换为BIDS。不随时间变化的数据，例如受试者的性别，教育水平或基线诊断，可从ADNIMERGE电子表格中获取，并收集在位于BIDS文件夹层次结构顶部的participant.tsv文件中。依赖于会话的数据，例如临床分数，从特定的csv文件（例如MMSE.csv）获得，并收集在每个参与者子文件夹中的<subjectid> _session.tsv文件中。转换的临床数据在电子表格（clin-ical_specifications_adni.xlsx）中定义，该电子表格可与转换器的代码一起使用。如果用户想要转换其他临床数据，则可以轻松修改该文件。</subjectid></p><h4 id="3-1-2-Conversion-of-the-AIBL-dataset-to-BIDS"><a href="#3-1-2-Conversion-of-the-AIBL-dataset-to-BIDS" class="headerlink" title="3.1.2. Conversion of the AIBL dataset to BIDS"></a>3.1.2. Conversion of the AIBL dataset to BIDS</h4><p>AIBL到BIDS转换器要求用户下载AIBL非成像数据（csv格式的表格数据）和感兴趣的成像数据。将成像数据转换为BIDS依赖于特定于模态的csv文件，这些文件提供可用的扫描列表。对于每个AIBL参与者，每个会话可用的唯一T1w MR图像被转换。请注意，即使它们未用于此项工作，我们也会转换Florbetapir，PiB和Flutemetamol PET图像（每个会话只有一个图像可用于每个会话）。一旦选择了感兴趣的图像并识别出图像文件的路径，就按照上一节中描述的相同步骤将成像数据转换为BIDS。临床数据的转换依赖于在成像数据转换之后和包含非成像数据的csv文件之后获得的受试者和会话的列表。不随时间变化的数据收集在位于BIDS文件夹层次结构顶部的participant.tsv文件中，而会话相关数据收集在每个参与者子文件夹中的<subjectid> _session.tsv文件中。对于ADNI转换器，转换的临床数据在电子表格（clinical_specifications.xlsx）中定义，该电子表格可与转换器的代码一起使用，用户可以修改该代码。</subjectid></p><h4 id="3-1-3-Conversion-of-the-OASIS-dataset-to-BIDS"><a href="#3-1-3-Conversion-of-the-OASIS-dataset-to-BIDS" class="headerlink" title="3.1.3. Conversion of the OASIS dataset to BIDS"></a>3.1.3. Conversion of the OASIS dataset to BIDS</h4><p>OASIS到BIDS转换器要求用户下载OASIS-1成像数据和相关的csv文件。 要将成像数据转换为BIDS，可从下载的文件夹中获取主题列表。 对于每个受试者，在可用的多个T1w MR图像中，我们选择重新采样为1mm各向同性体素的运动校正的共同配准的单个图像的平均值，位于SUBJ_111子文件夹中。 确定图像文件的路径后，使用FreeSurfer的mri_convert工具（Fischl，2012）将Analyze格式的图像转换为nifti，创建BIDS文件夹层次结构，并将图像复制到相应的文件夹 并重命名。 使用成像数据转换后获得的受试者列表和包含非成像数据的csv文件转换临床数据，如上一节所述。</p><h3 id="3-2-Preprocessing-pipelines"><a href="#3-2-Preprocessing-pipelines" class="headerlink" title="3.2. Preprocessing pipelines"></a>3.2. Preprocessing pipelines</h3><p>开发了两条管道来预处理解剖学T1w MRI和PET图像。 这些管道具有基于Nipype的模块化结构，允许用户容易地连接和/或替换组件，并且依赖于使用公开可用的标准图像处理工具的完善的程序。 这些管道在Clinica中以名称t1-volume- *和pet-volume提供。</p><h4 id="3-2-1-Preprocessing-of-T1-weighted-MR-images"><a href="#3-2-1-Preprocessing-of-T1-weighted-MR-images" class="headerlink" title="3.2.1. Preprocessing of T1-weighted MR images"></a>3.2.1. Preprocessing of T1-weighted MR images</h4><p>对于解剖学T1w MRI，预处理管道基于SPM128。首先，统一分割程序（Ashburner和Friston，2005）用于同时执行输入图像的组织分割，偏差校正和空间归一化。接下来，使用DARTEL创建一个组模板，DARTEL是一种用于微分图像配准的算法（Ashburner，2007），来自受试者在原生空间的组织概率图，通常是GM，WM和CSF组织，在之前获得步。这里，不仅获得了组模板，还获得了每个主体的原生空间到DARTEL模板空间的变形字段。最后，应用DARTEL到MNI方法（Ashburner，2007），提供将原生空间图像注册到MNI空间：对于给定主题，其进入DARTEL模板的流场与DARTEL模板到MNI的转换相结合。空间，并将得到的变换应用于对象的不同组织图。结果，所有图像都在共同的空间中，提供跨主体的体素方式对应。</p><h4 id="3-2-2-Preprocessing-of-PET-images"><a href="#3-2-2-Preprocessing-of-PET-images" class="headerlink" title="3.2.2. Preprocessing of PET images"></a>3.2.2. Preprocessing of PET images</h4><p>PET预处理管道依赖于SPM12和PETPVC9工具进行部分体积校正（PVC）（Thomas等，2016）。我们假设每个PET图像具有使用上述管道预处理的相应T1w图像。第一步是使用SPM的Co寄存器方法将PET图像配准到原生空间中的相应T1w图像（Friston等，1995）。可以使用来自原生空间中的T1w的不同组织图作为输入区域来执行具有基于区域体素的（RBV）方法（Thomas等人，2011）的可选PVC步骤。然后，使用与对应的T1w相同的变换将PET图像登记到MNI空间中（使用DARTEL到MNI方法）。然后根据参考区域（FDG PET的侵蚀脑桥）对MNI空间中的PET图像进行强度归一化，并且我们获得标准化的摄取值比（SUVR）图。最后，我们使用二进制掩码来掩蔽非脑区域，所述二进制掩模是通过对MNI空间中的受试者的GM，WM和CSF组织概率图的总和进行阈值处理而得到的。由此产生的蒙版SUVR图像也位于一个公共空间中，并提供跨主体的体素相关性。</p><h3 id="3-3-Feature-extraction"><a href="#3-3-Feature-extraction" class="headerlink" title="3.3. Feature extraction"></a>3.3. Feature extraction</h3><p>从成像数据中提取了两种类型的特征：体素和区域特征。 在预处理之后，T1w MRI和FDG PET图像都在MNI空间中。 对于每个图像，第一类特征简单地对应于大脑中的所有体素。 从T1w MR图像获得的信号是灰质密度，从FDG PET图像获得的信号是SUVR。<br>区域特征对应于在不同地图集中获得的一组感兴趣区域（ROI）中计算的平均信号（灰质密度或SUVR），也在MNI空间中。 选择的五个地图集包含皮质和皮质下区域，并覆盖受AD影响的大脑区域。 它们描述如下：</p><ul><li>AAL2（Tzourio-Mazoyer等，2002）是基于单个受试者的解剖图谱。它是AAL的更新版本，可能是神经影像学文献中使用最广泛的分割图。它是在MNI空间中对空间标准化的单一主体高分辨率T1体积使用手动追踪建立的（Holmes等，1998）。它由覆盖整个皮层的120个区域以及主要的皮质下结构组成。</li><li>AICHA（Joliot等，2015）是基于多个主题的功能性地图集。它是使用由281名健康受试者的静息状态fMRI数据计算的组级功能连接概况的分组构建的。它由345个区域组成，覆盖整体皮质以及主要的皮质下结构。</li><li>Hammers（Gousias等，2008; Hammers等，2003）是一个基于多个科目的解剖图谱。它是使用30名健康受试者的解剖MRI进行手动追踪而建立的。然后将个体受试者分组登记到MNI空间以生成概率图谱以及最大概率图。后者用于目前的工作。它由覆盖整个皮层的69个区域以及主要的皮质下结构组成。</li><li>LPBA40（Shattuck等，2008）是基于多个受试者的解剖图谱。它是使用40名健康受试者的解剖MRI进行手动追踪而建立的。然后将各个主题分组登记到MNI空间以生成最大概率图。它由覆盖整个皮层的56个区域以及主要的皮质下结构组成。</li><li>Neuromorphometrics10是基于多个科目的解剖图谱。它是使用30名健康受试者的解剖MRI进行手动追踪而建立的。然后将各个主题分组登记到MNI空间以生成最大概率图。它由覆盖整个皮层的140个区域以及主要的皮质下结构组成。数据可用于“MIC-CAI 2012大挑战和多图谱标签研讨会”。</li></ul><p>LBPA40，Hammers和Neuro-morphometrics地图集之间的主要区别在于解剖学分割的细节程度（即区域的数量）。</p><h3 id="3-4-Classification-models"><a href="#3-4-Classification-models" class="headerlink" title="3.4. Classification models"></a>3.4. Classification models</h3><p>我们考虑了三种不同的分类器：线性SVM，具有L2正则化的逻辑回归和随机森林，所有这些都可以在Clinica中获得。线性SVM与体素和区域特征一起使用，因为其计算复杂性仅取决于使用其双重形式时的主体数量。另一方面，使用L2正则化和随机森林模型的逻辑回归仅用于基于区域的分析，因为它们的复杂性取决于特征的数量，这对于包含大约100万个体素的图像变得不可行。我们使用了scikit-learn库的实现（Pedregosa等，2011）。<br>对于执行的每个任务，我们获得描述给定特征对当前分类任务的重要性的特征权重。这些权重存储为分类输出的一部分，重建分类器的信息也是如此，如找到的最佳参数。对于每个分类，我们可以获得具有跨脑体素或区域的权重表示的图像。</p><h4 id="3-4-1-Linear-SVM"><a href="#3-4-1-Linear-SVM" class="headerlink" title="3.4.1. Linear SVM"></a>3.4.1. Linear SVM</h4><p>第一种方法是线性SVM。 为了减少计算负荷，使用线性核k对每对图像（xi，xj）（使用区域或体素特征）预先计算Gram矩阵K =（k（xi，xj））i，j 提供科目。 该Gram矩阵用作通用SVM的输入。 我们选择优化误差项的惩罚参数C. SVM的一个优点是，当使用预先计算的Gram矩阵（双SVM）时，计算时间取决于主题的数量，而不取决于特征的数量。 鉴于其简单性，线性SVM可用作比较不同方法的性能的基线。</p><h4 id="3-4-2-Logistic-regression-with-L2-regularization"><a href="#3-4-2-Logistic-regression-with-L2-regularization" class="headerlink" title="3.4.2. Logistic regression with L2 regularization"></a>3.4.2. Logistic regression with L2 regularization</h4><p>第二种方法是使用L2正则化的逻辑回归（通常用于减少过度拟合）。 对于线性SVM，我们优化了误差项的惩罚参数C. 具有L2正则化的逻辑回归直接优化每个特征的权重，并且特征的数量影响训练时间。 这就是我们仅将其用于区域特征的原因。</p><h4 id="3-4-3-Random-forest"><a href="#3-4-3-Random-forest" class="headerlink" title="3.4.3. Random forest"></a>3.4.3. Random forest</h4><p>使用的第三个分类器是随机森林。 与线性SVM和逻辑回归不同，随机森林是一种集合方法，它适合数据集的各个子样本上的许多决策树。 组合估计器可防止过度拟合并提高预测精度。 基于scikit-learn库（Pedregosa等，2011）提供的实现，可以优化大量参数。 在评估哪个影响较大的初步实验后，我们选择了以下两个超参数来优化：i）森林中的树木数量; ii）寻找最佳分割时要考虑的特征数量。 由于计算成本高，随机森林仅用于区域特征而不是体素特征。</p><h3 id="3-5-Evaluation-strategy"><a href="#3-5-Evaluation-strategy" class="headerlink" title="3.5. Evaluation strategy"></a>3.5. Evaluation strategy</h3><h4 id="3-5-1-Cross-validation"><a href="#3-5-1-Cross-validation" class="headerlink" title="3.5.1. Cross-validation"></a>3.5.1. Cross-validation</h4><p>分类性能的评估主要遵循最近的指南（Varoquaux等，2017）。进行交叉验证（CV），维持列车组（用于拟合模型）和测试集（用于评估性能）的独立性的经典策略。 CV过程包括两个嵌套循环：评估分类性能的外循环和用于优化模型超参数的内循环（C用于SVM和L2逻辑回归，树的数量和随机森林的分割特征） 。应当注意，在优化超参数时，使用CV的内环对于避免向上偏置性能是重要的。这一步并不总是在文献中得到适当的执行（Querbes等，2009; Wolz等，2011），导致过度乐观的结果，如（Eskildsen等，2013; Maggi- pinto等。 ，2017）。<br>我们在Clinica实施了三种不同的外部CV方法：k-fold，重复k-fold和重复随机分裂（所有这些分层），使用基于scikit-learn的工具（Pedregosa等，2011）。方法的选择取决于手头的计算资源。但是，只要有可能，建议使用具有大量重复次数的重复随机分组，以获得更稳定的性能估计。因此，我们用于每个实验250次迭代的随机分裂。我们报告评估指标的完整分布以及平均值和经验标准偏差，如（Raamana和Strother，2017）使用神经预测（Raamana，2017）。然而应该指出的是，交叉验证没有无偏差的方差估计（Bengio和Grandvalet，2004; Nadeau和Bengio，2003），经验方差在很大程度上低估了真实的方差。在解释经验方差值时应牢记这一点。此外，我们选择不对不同分类器的性能进行统计测试。这是一个复杂的问题，没有通用的解决方案。在许多出版物中，使用了交叉验证结果的标准t检验。然而，如Nadeau和Bengio（2003）所示，这种方法过于宽松，不应该应用。已经提出了更好的表现方法，例如保守的Z或校正的重采样t检验（Nadeau和Bengio，2003）。但是，必须谨慎使用此类方法，因为它们的行为取决于数据和交叉验证设置。因此，我们选择避免在本文中使用统计检验，以免误导读者。相反，我们报告了指标的完整分布。<br>对于超参数优化，我们实现了内部k折叠。对于每个分割，选择具有最高平衡精度的模型，然后将这些选定的模型在分割中平均以获得模型平均的利润，这应该具有稳定效果。在本文中，对于内环，用k 1/4 10进行实验。</p><h4 id="3-5-2-Metrics"><a href="#3-5-2-Metrics" class="headerlink" title="3.5.2. Metrics"></a>3.5.2. Metrics</h4><p>作为分类的输出，我们报告平衡准确度，ROC曲线下面积（AUC），准确度，灵敏度，特异性，以及每个受试者的预测类别，因此用户可以使用此信息计算其他所需指标。、</p><h3 id="3-6-Classification-experiments"><a href="#3-6-Classification-experiments" class="headerlink" title="3.6. Classification experiments"></a>3.6. Classification experiments</h3><p>在表6中详细列出了由数据可用性驱动的每个数据集的分析中考虑的不同分类任务。有关组成分的详细信息可在表2-5中找到。一般而言，我们执行临床诊断分类任务，或MCI受试者进化的“预测”任务。请注意，由于sMCI和pMCI类别中的参与者数量较少，因此未对AIBL执行涉及从MCI进展到AD的任务。然而，当更多进化的MCI受试者在AIBL中公开可用时，该框架将允许非常容易地进行这些实验。<br>根据特征的类型，测试了几个具有不同参数的分类器的性能。对于体素特征，唯一的分类器是线性SVM。使用高斯核对图像应用四种不同的平滑水平，从无平滑到半高全宽12mm（FWHM）。对于基于区域的分类实验，测试了三种分类器：线性SVM，逻辑回归和随机森林。使用五个地图集提取特征：AAL2，AICHA，Hammers，LPBA40和Neuromorphometrics。表7总结了这些信息。<br>对于所研究的数据集，可获得不同的成像模式：虽然T1W MRI和FDG PET图像均可用于ADNI参与者，但只有T1w MRI可用于AIBL和OASIS参与者。对于所考虑的每种模态，使用表7中详述的不同参数提取体素和区域特征。在该工作中测试的所有分类实验总结在表8中。如果没有另外说明，则从图像中提取FDG PET特征。没有经过PVC的。</p><h2 id="4-Results"><a href="#4-Results" class="headerlink" title="4. Results"></a>4. Results</h2><p>在这里，我们提供了一些我们认为最有价值的结果。 所有实验（包括其他任务，预处理参数，特征或分类器）的完整结果可在补充材料以及包含所有代码和实验的存储库中找到（<a href="https://gitlab.icm-institute.org/aramislab" target="_blank" rel="noopener">https://gitlab.icm-institute.org/aramislab</a> / AD- ML）。 在以下小节中，我们使用平衡准确度作为性能指标来呈现结果，但结果中提供了所有其他指标。</p><h3 id="4-1-Influence-of-the-atlas"><a href="#4-1-Influence-of-the-atlas" class="headerlink" title="4.1. Influence of the atlas"></a>4.1. Influence of the atlas</h3><p>为了评估选择地图集对分类精度的影响并潜在地确定首选地图集，选择了使用区域特征的线性SVM分类器。使用五种不同的图谱提取来自ADNI参与者的T1w MRI和FDG PET图像的特征：AAL2，AICHA，Hammers，LPBA40和Neuro-morphometrics。研究了三个分类任务：CN与AD，CN与pMCI和sMCI与pMCI。<br>如图1所示，没有特定的图集为所有任务提供最高的分类准确度。例如，Neuromorphometrics和AICHA在T1w和FDG PET图像上为CN与AD提供了更好的结果，对于T1w提供了LBPA40，而AAL2在两种成像模式下为CN与pMCI和sMCI与pMCI提供了最高的平衡准确度。对AIBL受试者进行了相同的分析（仅限T1w MR图像），同样地，没有任何图集在任务中始终比其他图谱更好地执行。对于以下基于区域的实验，选择AAL2图谱作为参考图谱，因为它具有良好的分类准确度，并广泛用于神经影像学界。同样，存储库中提供了所有其他结果。</p><h3 id="4-2-Influence-of-the-smoothing"><a href="#4-2-Influence-of-the-smoothing" class="headerlink" title="4.2. Influence of the smoothing"></a>4.2. Influence of the smoothing</h3><p>使用具有4mm，8mm和12mm的FWHM的高斯核不对T1w MRI和FDG PET图像进行平滑或平滑。为了确定不同平滑度对分类精度的影响，选择了使用体素特征的线性SVM分类器。研究了三个分类任务：CN与AD，CN与pMCI和sMCI与pMCI。图2中的结果表明，对于大多数分类任务，平衡精度在很大程度上不随光滑内核大小而变化。当从T1w MR图像中提取特征时，观察到CN与pMCI和sMCI与pMCI任务的唯一变化：平衡精度随着核尺寸略微增加。使用来自AIBL数据集的T1w MR图像进行相同的分析。平均精度也随着核尺寸略有增加，但平衡精度的标准偏差大于ADNI。由于平滑程度对分类性能没有明显影响，我们选择提供与基于体素的分类相关的后续结果，参考平滑度为4 mm。</p><h3 id="4-3-Influence-of-the-type-of-features"><a href="#4-3-Influence-of-the-type-of-features" class="headerlink" title="4.3. Influence of the type of features"></a>4.3. Influence of the type of features</h3><p>我们将使用线性SVM分类器将体素特征的平衡精度与参考平滑（4 mm FWHM的高斯核）与参考图谱（AAL2）的区域特征所获得的平衡精度进行比较。 这些特征是从ADNI参与者的T1w MRI和FDG PET图像中提取的。 评估了与以前相同的三个分类任务。<br>表9中显示的结果未显示使用体素或区域特征获得的平均平衡精度之间的显着差异。 在AIBL数据集的情况下，基于区域的分类的平衡准确度更高（对于AD与CN：基于体素的0.79 [0.059]，基于区域的0.86 [0.042]），但我们可以观察到相应的标准 偏差很高。</p><h3 id="4-4-Influence-of-the-classification-method"><a href="#4-4-Influence-of-the-classification-method" class="headerlink" title="4.4. Influence of the classification method"></a>4.4. Influence of the classification method</h3><p>使用三种不同的分类器进行基于区域的实验，以评估取决于所选分类器的平衡精度是否存在变化。 使用来自T1w MRI的参考AAL2图谱和ADNI参与者的FDG PET图像提取区域特征。 执行了三个先前定义的分类任务。<br>图3中显示的结果显示，具有L2正则化模型的线性SVM和逻辑回归导致类似的平衡精度，始终高于用随机森林获得的所有任务和测试的成像模态。</p><h3 id="4-5-Influence-of-the-partial-volume-correction-of-PET-images"><a href="#4-5-Influence-of-the-partial-volume-correction-of-PET-images" class="headerlink" title="4.5. Influence of the partial volume correction of PET images"></a>4.5. Influence of the partial volume correction of PET images</h3><p>使用线性SVM分类器进行基于区域和体素的分析，以评估用于部分体积效应的校正PET图像是否对分类准确性有影响。 具有和不具有PVC的ADNI参与者的FDG PET图像用于这些实验。<br>图4中显示的结果表明，使用和不使用PVC获得的平衡精度之间几乎没有差异。 使用体素功能时，无论是否存在PVC，平均平衡精度几乎相同。 使用区域特征时，当FDG PET图像未针对部分体积效应进行校正时，平均平衡精度的增加非常小。</p><h3 id="4-6-Influence-of-the-magnetic-field-strength"><a href="#4-6-Influence-of-the-magnetic-field-strength" class="headerlink" title="4.6. Influence of the magnetic field strength"></a>4.6. Influence of the magnetic field strength</h3><p>ADNI1参与者的大多数T1w扫描是在1.5T扫描仪上获得的，而3T扫描仪用于获取ADNIGO / 2参与者的MR图像。 为了评估场强的差异是否对分类性能产生影响，我们分别计算了1.5T和3T扫描的受试者的平衡准确度。 结果显示在表10中。我们观察到，无论实验如何，与1.5T扫描子集相比，3T扫描子集的平衡精度总是更高，这并不奇怪，因为3T图像应该具有更好的信号 噪声比。</p><h3 id="4-7-Influence-of-class-imbalance"><a href="#4-7-Influence-of-class-imbalance" class="headerlink" title="4.7. Influence of class imbalance"></a>4.7. Influence of class imbalance</h3><p>我们执行的任务是使用不平衡的类完成的。这种类别不平衡的范围从非常温和（对于ADNI的CN是CN的1.2倍）到中等（比pMCI多1.7倍的CN和比ADNI的pMCI多2倍的sMCI）到非常强（在AIBL中比AD多6.1倍的CN）。<br>我们的目的是评估这种阶级不平衡是否会影响绩效。为此目的，我们随机抽样亚组并进行实验，其中237 CN对237 AD，167 pMCI vs 167 CN和167 pMCI vs 167 pMCI用于ADNI和72 CN和72 AD用于AIBL。我们确保平衡子集的人口统计学和临床特征与原始子集没有差异。结果如图5所示。对于ADNI，其表现与完整人群的表现相似。对于AIBL，基于体素的功能的平衡组的性能要高得多。因此，似乎非常强的阶级不平衡（如AIBL中比例为6比1的情况）导致较低的性能，但适度的阶级不平衡（ADNI中高达2比1）得到充分处理。</p><h3 id="4-8-Influence-of-the-dataset"><a href="#4-8-Influence-of-the-dataset" class="headerlink" title="4.8. Influence of the dataset"></a>4.8. Influence of the dataset</h3><p>我们还想知道数据集的结果是否一致，因此我们比较了从ADNI，AIBL和OASIS获得的分类性能，用于区分控制对象与阿尔茨海默病患者的任务。从T1w MR图像中提取体素（4mm平滑）和区域（AAL2图谱）特征并与线性SVM分类器一起使用。我们测试了两种配置：在同一数据集上训练和测试分类器，在ADNI上训练分类器并在AIBL和OASIS上进行测试。<br>结果显示在表11中。在ADNI和AIBL上获得的性能是可比的并且远高于在OASIS上获得的性能。在ADNI培训和AIBL或OASIS测试时，平衡准确度至少与AIBL或OASIS上的训练和测试时一样高，这表明在ADNI上训练的分类器能够很好地推广到其他数据集。特别是，ADNI培训大大提高了OASIS的分类性能。我们的目的是评估这是否是由于ADNI中的大量受试者。为此目的，我们进行了相同的实验，但每个数据集的参与者子集大小相同。我们从每个数据集中随机抽取70名AD患者和70名CN参与者的人群，确保子群体的人口统计学和临床特征与原始人群没有差异。从表11中可以看出，使用该子集，基于体素的改进消失了，但仍然保留了区域特征。</p><h3 id="4-9-Influence-of-the-training-dataset-size"><a href="#4-9-Influence-of-the-training-dataset-size" class="headerlink" title="4.9. Influence of the training dataset size"></a>4.9. Influence of the training dataset size</h3><p>计算学习曲线以评估线性SVM分类器的性能如何根据训练数据集的大小而变化。仅使用ADNI参与者，我们测试了四种情景：从T1w MRI和FDG PET图像中提取的体素和区域特征。作为交叉验证，运行250次迭代，其中数据集被随机分成测试数据集（30％的样本）和训练数据集（70％的样本）。对于CN和AD，用于训练和测试每项不同任务的最大受试者数量为362，CN与pMCI为313，sMCI与pMCI为355。对于每次运行，使用10％到所有训练集（从7％到高达70％的样本）在相同的测试集上训练和评估10个分类器，每个样本使用的样本数量增加10％步。因此，用于培训的参与者数量从CN到20到197，sMCI为24到239，pMCI为12到117，AD为17到166。我们可以从图6中的学习曲线中观察到，正如预期的那样，平衡精度随着训练样本的数量而增加。<br>当使用通过组合来自ADNI和AIBL（由72个CN受试者和72个AD受试者组成的平衡子集）和来自ADNI，AIBL和OASIS的参与者获得的较大数据集时，还计算CN与AD任务的学习曲线。结果显示在图7中。我们观察到，对于相同数量的受试者，组合ADNI和AIBL或仅使用ADNI导致类似的平衡准确度。对于区域特征，与仅使用ADNI相比，ADNI和AIBL组合时的性能略高，但差异主要在标准偏差范围内。当结合ADNI和AIBL时，更多的受试者被用于训练时，平衡的准确性会略微增加。然而，当结合ADNI，AIBL和OASIS时，无论是多少科目，性能都比仅使用ADNI或组合ADNI和AIBL时更差。这可能是由于ADNI和AIBL遵循相同的诊断和采集协议，这与OASIS不同。</p><h3 id="4-10-Influence-of-the-diagnostic-criteria"><a href="#4-10-Influence-of-the-diagnostic-criteria" class="headerlink" title="4.10. Influence of the diagnostic criteria"></a>4.10. Influence of the diagnostic criteria</h3><p>我们通过使用关于每个受试者的淀粉样蛋白状态的信息（如果可用）来改进先前使用的诊断标准来定义新的分类任务。 从图8中可以看出，当将这些任务的性能与不使用淀粉样蛋白状态的相关任务进行比较时，对于所有新定义的任务，平均平衡准确度更高或至少相同。 我们必须注意，尽管所有受试者都不知道淀粉样蛋白状态，但尽管计数较少，但仍达到了这一性能。</p><h3 id="4-11-Computation-time"><a href="#4-11-Computation-time" class="headerlink" title="4.11. Computation time"></a>4.11. Computation time</h3><p>总的来说，我们使用SVM分类器进行了279次实验，使用逻辑回归分类器进行了155次实验，使用随机森林分类器进行了26次实验（有关任务和参数的详细信息，请参阅表6-8）。 使用具有72个核心（Xeon E5-2699 @ 2.30 GHz）和256 GB RAM的机器，运行434个SVMþ逻辑回归实验需要6天，运行26个随机森林实验需要8天。</p><h2 id="5-Discussion"><a href="#5-Discussion" class="headerlink" title="5. Discussion"></a>5. Discussion</h2><p>我们提出了一个开源框架，用于可重复评估AD分类方法，其中包含以下组件：i）转换器将三个公开可用的数据集标准化为BIDS; ii）用于T1w MRI和PET的标准化预处理和特征提取管道; iii）标准分类算法; iv）遵循最近的最佳实践的交叉验证程序。我们展示了它用于评估三个公共数据集上的不同成像模态，预处理选项，特征和分类器。<br>在这项工作中，我们首先致力于在AD中评估机器学习方法：i）更可重复; ii）更客观。再现性是基于相同数据和实验程序再现结果的能力。已经在不同领域提出了提高再现性的呼吁，包括神经成像（Poldrack等，2017）和机器学习（Ke等，2017）。再现性与复制不同，复制是在独立数据上确认结果的能力。可重复研究的关键要素包括：数据共享，使用社区标准存储数据，全自动数据处理，代码共享。我们的工作有助于通过不同方面提高AD ML研究的可重复性。第一个组件是将三个公共数据集全自动转换为社区标准BIDS。实际上，ADNI和AIBL无法重新分配。通过这些工具，我们希望能够轻松地基于这些数据集重现实验，而无需重新分发它们。特别是，与简单地公开使用的主题列表相比，我们为用户节省了大量时间。对于复杂的多模态数据集（例如ADNI（具有大量不完整数据，给定模态的多个实例和复杂元数据）尤其如此。第二个关键组件是公开可用的代码，用于预处理，特征提取和分类。这些贡献收集在Clinica11中，这是一个免费提供的临床神经科学研究软件平台。除了提高可重复性之外，我们希望这些工具还能使研究人员的工作变得更加容易。<br>我们也希望为更客观的评估做出贡献。对新方法（分类算法，特征提取方法或其他方法）的客观评估需要在不改变其他组件的情况下测试该特定组件。我们的框架包括用于T1加权MRI和FDG PET数据的预处理和特征提取的标准方法，以及标准分类工具。这些构成了一套基线方法，可以很容易地比较新方法。然后，研究新方法的研究人员可以直接用自己的解决方案替换管道的给定部分（例如特征提取，分类），并评估该特定新组件相对于所提供的基线方法的附加值。我们还提出了严格验证的工具，主要基于最近的指南（Varoquaux等，2017），并基于标准软件scikit-learn（Pedregosa等，2011）实施。其中包括：i）大量重复随机分割，以广泛评估性能的变化; ii）报告准确度和标准偏差的完整分布，而不仅仅是平均准确度; iii）用于超参数调整的足够嵌套CV。</p><p>然后，我们基于T1 MRI和FDG PET数据证明了该框架在不同分类任务中的应用。通过这一点，我们的目标是提供一个基准性能，可以比较先进的机器学习和特征提取方法。这些基线性能符合最先进的结果，这些结果已在（Arbabshirani等，2017; Falahati等，2014; Rathore）中进行了总结。<br>et al。，2017），其中CN与AD的分类准确度通常为80％至95％，sMCI与pMCI的分类准确度为60％至80％。例如，使用线性SVM，区域特征（AAL2）和FDG PET数据，我们报告CN与AD相比为88％，CN与pMCI为80％，sMCI与pMCI为73％。<br>ADNI中使用的诊断标准来自NINCDS-ADRDA（McKhann等，1984），其仅依赖于患者的症状和认知状态。然而，AD的明确诊断只能在尸检时进行，并且在很大比例的病例中发现临床诊断是错误的（Knopman等，2001）。在过去的十年中，AD的诊断取得了实质性进展。特别是，有人建议不仅要依靠临床和认知评估，还要依赖成像和脑脊液（CSF）生物标志物。这导致了新的诊断标准。尽管黄金标准仍然是尸检，但这导致在患者生命期间更准确地诊断AD。特别是，IWG（Dubois等，2007），IWG-2（Dubois等，2014）和NIA-AA（Albert等，2011）提出了β-淀粉样蛋白和/或tau蛋白的存在。 ; Jack等人，2011; McKhann等人，2011; Sperling等人，2011）。在这项工作中，我们评估了使用淀粉样蛋白精制诊断组是否改善了表现。从每个参与者的淀粉样蛋白PET扫描（PiB或AV45）确定淀粉样蛋白状态。我们发现了<br>使用淀粉样蛋白精制诊断的分类总是比使用NINCDS-ADRDA诊断的相关任务更好或至少类似，即使训练集包含更少的个体。<br>与T1w MRI相比，FDG PET的分类在任务，特征和分类方法上的表现始终更好。一些研究支持我们的发现（Dukart等，2011,2011; Gray等，2013; Ota等，2015; Young等，2013），而其他研究没有发现性能上的差异（Hinrichs等。 ，2009; Zhang等，2011; Zhu等，2014）。鉴于我们的研究样本量较大且严格的评估设计，我们认为FDG PET与MRI相比具有优越的性能，这是一个很好的发现。这可能是由于在萎缩之前可以在疾病过程中更早地检测到代谢减退（Jack等，2010b）。</p><p>多种参数和选项用于AD机器学习研究中的预处理和特征提取。它们对分类性能的影响尚不清楚，并且构成了分类方法可比性的问题。我们评估了图谱选择，平滑程度，PET图像校正部分体积效应以及特征类型（区域或体素）的影响。我们没有发现这些不同组分中的每一种对性能的系统影响。一些研究发现地图集对分类性能的影响（Ota等，2015; 2014）。然而，该研究中的受试者数量很少。在（Chu等人，2012）中，与使用所有体素相比，使用少量ROI的组合时发现了3％的改善。在我们的研究中，使用了更多的受试者和严格的验证过程。<br>我们比较了三种广泛使用的分类方法：SVM，Logistic回归与L2正则化和随机森林。我们的主要发现是后者的表现不佳。这可能是由包含相对同质值的大脑成像数据的性质引起的，并且应该显示体素或大脑区域的依赖性。数据的这些特征可以解释为什么试图找到平滑特征组合的技术（例如使用L2正则化的特征）更适合于单一模态分类问题。另一方面，当组合来自不同模态的特征（例如图像，临床数据和认知评分）时，随机森林或其他集合方法可能是有用的，如（Moradi等，2015;Sørensen等，2018）所述。在比较几种标准分类算法（如SVM，LDA或Naive Bayes）的其他论文中（Aguilar等，2013; Cabral等，2015; Sabuncu等，2015），结果未显示方法之间的差异。</p><p>我们还评估了类别不平衡的影响，在我们的数据集中，这种影响范围从非常温和（AD比AD对ADNI多1.2倍）到中度（比pMCI多1.7倍的CN和比ADNI的pMCI多2倍的sMCI）到非常强（在AIBL中，CN比AD多6.1倍。在基于体素的特征的情况下，我们发现非常强的类不平衡（如AIBL中比例为6比1的情况）导致性能降低但是适度的类不平衡（ADNI中高达2比1）得到妥善处理。另一方面，阶级不平衡对区域特征没有影响。这突出表明，当存在非常强的类不平衡和使用非常高维度的特征时，使用平衡组进行训练可能是有益的。<br>我们评估了各种成分对分类性能的影响：模态（T1w MRI与PET），特征类型，寰椎的选择，PVC，平滑，分类器。其他研究评估了其他成分的影响：不同类型的解剖学特征，包括体积，皮质厚度和其他表面特征（Go？mez-Sancho等，2018; Schwarz等，2016; Westman等， 2013），特征选择技术（Tohka等，2016），颅内体积归一化（Voevodskaya等，2014; Westman等，2013）。此外（Tohka等，2016），将LASSO和弹性网与SVM进行了比较，发现前一种方法提供了更高的性能。使用我们的框架也可以评估这些不同组件的影响。在本文中，我们将框架的应用限制为由于以下原因而选择的一组组件。基于体素和区域特征都包括在内，因为它们被广泛使用。另一方面，由于其计算成本，不包括基于Freesurfer的皮质测量。 PVC是一种非常常见的PET数据预处理方法。平滑被广泛用于神经影像学社区中基于体素的分析，并且评估其影响似乎是有用的。然而，在这样的选择中总是存在一些任意性，用框架研究其他组件会很有趣。<br>在这项工作中，我们使用了预定义的功能（在区域或体素级别）。应该提到的另一类方法是直接从数据中学习特征的方法。基于补丁的方法旨在自动学习主体和训练集之间的非局部相似性（Coup？et al。，2015,2012）。此外，深度学习方法可以自动学习多个尺度的相关特征，并且最近在AD的自动分类中变得流行（B€ackstro€et al。，2018; Liu et al。，2018; Lu et al。，2018; Suk等，2017）。两种类型的方法都产生了有希望的结果（例如pMCI与sMCI的比例为73％至83％）。此外，各种工作已经提出使用不同类型的数据驱动特征选择（例如单变量统计检验，多变量方法）（Chu等人，2012; Tohka等人，2016; Vemuri等人，2008）和维数降低（例如主成分分析，流形学习）（Beheshti等，2015; Guerrero等，2014; Liu等，2015; Salvatore等，2015）。这些方法具有改善性能的潜力，但需要使用严格的交叉验证程序进行验证（Eskildsen等，2013; Maggipinto等，2017）。可以使用我们的框架评估所有这些方法的附加值。这超出了本文的范围，留待将来工作。</p><p>使用多个数据集对于评估在不同条件下获得的不同群体的表现是否稳健非常重要。第一个组件包括在不同的数据集上执行相同的实验。我们发现ADNI和AIBL数据集的分类结果相似，但OASIS的分类结果要低得多。 OASIS的较低性能可能是由于诊断标准较不严格（在<br>OASIS，CDR&gt; 0的所有参与者被认为是AD）。了解分类器在一个数据集上进行训练并在另一个数据集上进行测试时，它也是有价值的。对ADNI数据进行过培训的分类器很好地适用于AIBL和OASIS。有趣的是，对于OASIS而言，与在OASIS培训时相比，ADNI培训时的表现大大增加。这种改善可能源于几个因素：更大的训练集大小，更高的图像质量或更严格的诊断标准。当使用相同大小的子集时，基于体素的特征所获得的改进消失，这表明增加的训练集大小很重要，特别是在使用非常高的维度特征时。另一方面，对于区域特征，与OASIS子集的训练相比，ADNI子集的训练改善了性能，表明其他因素（图像质量，更严格的诊断标准）有助于改善。一般来说，我们可以说分类器能够在不同的数据集中进行推广，如Dukart等，2013; Sabuncu等，2015中所述，特别是如果它们是使用具有严格诊断标准的大型多中心数据集获得的，就像ADNI的情况一样。<br>不出所料，增加训练集的规模可以提高分类表现。在其他研究中也发现了取决于训练集大小的结果的改进（Abdulkadir等，2011; Chu等，2012; Franke等，2010）。可以注意到，当组合多个数据集时，性能也随着训练集大小而增加。但是，当将OASIS与ADNI和AIBL结合使用时，性能低于仅使用AIBL和ADNI时的性能。这与OASIS的性能系统性低于ADNI和AIBL的性能一致。同样，这可能是由于诊断标准在OASIS中不太严格。有趣的是，根据目前可用的样本数量，尚未达到结果停止改善的程度。分类器的性能取决于为训练提供的图像数量所施加的限制，这意味着需要更多数据才能找到分类器的最佳性能。这些结果强调了对更多可公开获得的数据集的需求，该领域目前的大部分研究都依赖于这些数据集。</p><h2 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6. Conclusions"></a>6. Conclusions</h2><p>我们的可重复分类实验框架旨在解决基于机器学习的AD分类领域当前面临的问题，例如结果的可比性和可重复性。 它在T1w MRI和FDG PET数据中的应用允许广泛评估成像模态，预处理选项，特征和算法对性能的影响。 这些结果提供了基准性能，可以与其他方法进行比较。 我们希望框架和实验结果对AD领域的研究人员都有用。</p><h3 id="Acknowledgments"><a href="#Acknowledgments" class="headerlink" title="Acknowledgments"></a>Acknowledgments</h3><p>导致这些结果的研究得到了“Investissements d’avenir”ANR-10-IAIHU-06（Agence Natio- nale de la Recherche-10-IA Agence Institut Hospitalo-Universitaire-6）计划的资助，ANR-11- IDEX-004（Agence Nationale de la Recherche-11- Initiative d’Excellence-004，项目LearnPETMR编号SU-16-R-EMR-16），来自欧盟H2020计划（项目EuroPOND，拨款号666992，项目HBP SGA1授权号720270），来自ICM大脑理论计划（项目DYNAMO），来自欧洲研究理事会（Durrleman博士项目LEASP，授权号678304），来自NSF / NIH / ANR联合计划“计算机协作研究”神经科学“（项目HIPLAY7，授权号ANR-16-NEUC- 0001-01）和来自巴塞罗那协会公共部门的”Contrat d’Interface Local“计划（致Col-liot博士）（AP-HP） 。注：根据REA拨款协议no，从欧洲联盟第七框架计划（FP7 / 2007-2013）的人民计划（Marie Curie Actions）获得资金。 PCOFUND-GA-2013-609102，通过由法国校园协调的PRESTIGE计划。<br>该项目的数据收集和共享由Alz-资助 Heimer’s疾病神经影像学倡议（ADNI）（美国国立卫生研究院资助U01 AG024904）和DOD ADNI（国防部奖项编号W81XWH-12-2-0012）。 ADNI由国家老龄化研究所，国家生物医学成像和生物工程研究所资助，并通过以下方面的慷慨捐助：AbbVie，Alzheimer’s Association;阿尔茨海默氏症的药物发现基金会; Araclon Biotech; BioClinica，Inc。;生物遗传; Bristol-Myers Squibb Company; CereSpir，Inc。; Cogstate; Eisai Inc。; Elan Pharmaceuticals，Inc。;礼来公司;欧蒙; F. Hoffmann-La Roche Ltd及其附属公司Genentech，Inc。; Fujirebio公司; GE Healthcare; IXICO有限公司; Janssen Alzheimer Immunotherapy Research＆Development，LLC。;强生药业研发有限责任公司; Lumosity; Lundbeck公司; Merck＆Co.，Inc。; Meso Scale Diagnostics，LLC。; NeuroRx研究; Neurotrack技术;诺华制药公司;辉瑞公司; Piramal成像;施维雅;武田制药公司;和过渡治疗学。加拿大卫生研究院正在提供资金支持加拿大的ADNI临床站点。国家卫生研究院基金会（<a href="http://www.fnih.org）为私营部门的捐助提供了便利。受助组织是北加州研究和教育研究所，该研究由南加州大学阿尔茨海默氏症治疗研究所协调。" target="_blank" rel="noopener">www.fnih.org）为私营部门的捐助提供了便利。受助组织是北加州研究和教育研究所，该研究由南加州大学阿尔茨海默氏症治疗研究所协调。</a> ADNI数据由南加州大学的神经成像实验室传播。<br>OASIS项目得到以下资助：P50 AG05681，P01 AG03991，R01 AG021910，P20 MH071616和U24 RR021382。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/Paper/ADNI/"/>
    
    
      <category term="translation" scheme="http://yoursite.com/tags/translation/"/>
    
  </entry>
  
  <entry>
    <title>Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer-Disease-翻译</title>
    <link href="http://yoursite.com/2019/07/18/Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer-Disease-%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/07/18/Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer-Disease-翻译/</id>
    <published>2019-07-18T05:49:56.000Z</published>
    <updated>2019-07-29T03:45:29.633Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p><a href="https://pubs.rsna.org/doi/full/10.1148/radiol.2016152703" target="_blank" rel="noopener">《Application of Machine Learning to Arterial Spin Labeling in Mild Cognitive Impairment and Alzheimer Disease》</a></p><p>机器学习在动脉自旋标记治疗轻度认知障碍和阿尔茨海默病中的应用</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h3 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h3><p>调查动脉自旋标记（ASL）灌注图的多变量模式识别分析是否可用于阿尔茨海默病（AD）和轻度认知障碍（MCI）患者的分类和单一主题预测以及主观认知能力下降（SCD）的受试者 使用W分数法去除性别和年龄的混杂影响。</p><h3 id="Materials-and-Methods"><a href="#Materials-and-Methods" class="headerlink" title="Materials and Methods"></a>Materials and Methods</h3><p>在具有可能AD的100名患者中获得了伪连续3.0-T ASL图像; 60例MCI患者，其中12例保持稳定，12例转为AD诊断，36例无随访; 100名SCD患者; 和26名健康对照受试者。 AD，MCI和SCD组分为性别和年龄匹配训练集（n = 130）和独立预测集（n = 130）。 对于每个参与者，针对每个体素计算针对年龄和性别（W分数）调整的标准化灌注分数。 使用诊断状态和灌注图执行支持向量机分类器的训练。 提取识别图并将其用于预测集中的单主题分类。 通过接受者操作特征（ROC）分析评估预测性能，以产生ROC曲线下面积（AUC）和灵敏度和特异性分布。</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>通过使用鉴别图在预测集中的单一主题诊断产生AD与SCD的优异表现（AUC，0.96; P &lt;.01），AD与MCI（AUC，0.89; P &lt;.01）的良好表现，以及差 MCI与SCD的表现（AUC，0.63; P = .06）。 应用AD与SCD鉴别图预测MCI亚组导致MCI诊断转为AD的患者与SCD患者相比具有良好的表现（AUC，0.84; P &lt;.01），MCI诊断患者的公平表现转为AD 与那些稳定的MCI（AUC，0.71; P&gt; .05）。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>使用自动化方法，年龄和性别调整的ASL灌注图可用于分类和预测AD的诊断，MCI向AD的转换，稳定的MCI和SCD，具有良好至极好的准确度和AUC值。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>阿尔茨海默病（AD）是最常见的痴呆类型（1），是65岁及以上人群的第五大死因（2）。结构磁共振（MR）成像在诊断患者与对照受试者的AD时具有高准确度，特别是在疾病的晚期阶段（3,4）。然而，通过使用结构MR成像识别AD的早期诊断准确性和早期AD诊断患者的预后价值仍存在问题（5）。</p><p>在Jack等人的动态2010生物标志物模型中，功能性AD相关的脑变化发生在结构变化之前（6）。与对照组（7,8）相比，AD患者使用专用磁共振成像测量了全脑低灌注，最突出的是顶叶结构，如后扣带皮层，前躯和下顶叶（7 -9）。与AD患者相比，轻度认知障碍（MCI）患者表现出相似但不太明显的低灌注模式（10），AD患者和对照受试者之间的定量脑血流值介于中间（11）。与对照组相比，MCI患者的枕叶和颞叶也有低灌注（11）。</p><p>动脉自旋标记（ASL）MR成像是一种非侵入性，快速且日益广泛的量化脑血流量的方法;与正电子发射断层扫描（PET）（12-14）相比，ASL代表了测量脑灌注的潜在替代方式，其可以促进痴呆治疗中的常规临床应用。用ASL测量的AD相关灌注变化与用PET测量的葡萄糖代谢改变强烈相关（9,15,16）。这一系列研究结果表明，ASL是早期诊断AD的有前景的替代功能生物标志物。</p><p>除了本文提到的与灌注相关的诊断参数之外，将多变量模式识别软件应用于结构MR成像数据已经在AD中产生了高诊断准确度（17）。与海马体积的视觉评估相比（18,19），自动分类导致MCI向AD转换的预测具有高准确性（20），这表明自动分类包括神经退行性病理过程的更多特征（18）。</p><p>支持向量机（SVM）代表二元机器学习多变量方法，可以训练这些方法在留一法交叉验证框架中对单个图像进行分类（17）。与单变量方法相比，这种多变量方法的优点包括增加统计功效和单一主题检查适用性，能够处理大量依赖的体素数据，更准确地类似于全局脑功能（21）。具体来说，在SVM的设置中，W得分方法是一种统计工具，可以减少二元测试中混杂因素的影响（22,23）。由于自动化的基于图像的分类器尚未在ASL的设置中进行诊断使用，因此开发用于单一主题分类的这种工具在临床上是相关的并且有助于筛选目的。</p><p>在目前的研究中，我们研究了ASL灌注图的多变量模式识别分析是否可用于AD和MCI患者的分类和单一主题预测以及使用W分数法去除混杂后主观认知能力下降（SCD）的受试者性别和年龄的影响。</p><h2 id="Materials-and-Methods-1"><a href="#Materials-and-Methods-1" class="headerlink" title="Materials and Methods"></a>Materials and Methods</h2><p>主要作者对该研究负全部责任，并对数据和所有分析以及患者同意书具有完全的访问权和权利。 所有作者都同意所有条件。</p><h3 id="Participants"><a href="#Participants" class="headerlink" title="Participants"></a>Participants</h3><p>当地机构审查委员会批准了这项研究。所有受试者均提供书面知情同意该研究回顾性地包括来自VU大学医学中心痴呆队列的阿尔茨海默病中心的311名参与者，他们在2010年10月至2012年11月期间接受了ASL MR成像（24）。排除标准是占位过程（n = 7），创伤后偏差（n = 6），大血管出血或梗塞（n = 4），癫痫（n = 4）或精神疾病（n = 5）的适应症， AD的非典型临床表现（n = 3）（PS，具有27年的经验），或脑提取或ASL获取失败（n = 22）。临床诊断是通过多学科团队在标准痴呆筛查的基础上达成的共识建立的，包括病史检查，身体和神经系统检查，筛查实验室检查，神经心理学测试和脑MR成像。尽可能获得脑脊液。 AD患者符合国家老龄化 - 阿尔茨海默氏症协会可能AD的标准（25）。在2012年之前，MCI诊断基于Petersen及其同事定义的标准（26）; 2012年之后，MCI诊断基于国家老龄化协会 - 阿尔茨海默氏症协会标准（25）。如果参与者不符合AD或MCI标准，则被视为SCD受试者。健康对照受试者的临床结果正常，没有认知能力下降（25）。这些标准导致包括100名患有可能的AD的患者，60名患有MCI的患者，100名患有SCD的患者和26名健康对照受试者。教育水平按七分制评定（27）。</p><p>诊断组内的参与者被随机分配到训练组或预测组（每组，n = 130; 50名AD患者，30名MCI患者和50名SCD患者），年龄和性别均衡分布（图1;表1,2）。</p><h3 id="Data-Acquisition"><a href="#Data-Acquisition" class="headerlink" title="Data Acquisition"></a>Data Acquisition</h3><p>使用3.0通道全身MR系统（Signa HDxt; GE Medical Systems，Milwaukee，Wis）通过使用八通道头部线圈收集成像数据。结构成像涉及使用矢状三维T1加权序列（反转恢复快速损坏梯度回波;重复时间[毫秒] /回波时间[毫秒]，7.8 / 3.0;反转时间，450毫秒;翻转角， 12°;和体素尺寸，1×0.9×0.9 mm）。伪连续ASL灌注图像（具有背景抑制的三维快速自旋回波采集;标记时间，1.5秒;标记后延迟，2.0秒;重复时间，4.8秒;回波时间，9毫秒;八臂的螺旋读数×512个样本;在减去标记后，使用单室模型（28）计算36×5.0mm的轴向截面; 3.2×3.2mm的面内分辨率;重建的像素尺寸为1.7×1.7mm;以及获取时间，4分钟）来自控制图像的图像。获得近似中等加权的图像，以通过使用具有相同参数的饱和度恢复采集来缩放每个参与者的灌注图像。</p><h3 id="Preprocessing-of-MR-Imaging-Data"><a href="#Preprocessing-of-MR-Imaging-Data" class="headerlink" title="Preprocessing of MR Imaging Data"></a>Preprocessing of MR Imaging Data</h3><p>针对三个方向上的梯度非线性校正T1加权和伪连续ASL图像。 使用脑软件库的功能磁共振成像或FSL软件（牛津大学，牛津，英国）进行进一步分析（29）。 T1加权图像的预处理包括去除非脑组织，对蒙特利尔神经病学研究所（MNI）空间进行归一化，以及使用部分体积估计进行组织分割。 ASL图像线性地记录到灰质密度图并映射到MNI标准空间，然后是高斯平滑，具有6mm全宽半最大值，并且以3mm各向同性分辨率重新采样（A.M.W.，具有15年的经验）。</p><h3 id="W-Score-Maps"><a href="#W-Score-Maps" class="headerlink" title="W Score Maps"></a><em>W</em> Score Maps</h3><p>因为女性的脑血流量高于男性（7），并且随着年龄的增长逐渐减少（30），这些混杂因素在二元分类之前通过使用W分数法（22,23）和脚本github.com/amwink被删除。 /bias/blob/master/scripts/bash/compute_w.sh。它在参考ASL图像的一般线性模型分析中计算每个混淆器对脑灌注的体素效应（图2a）。体素截距（β0），性别和年龄相关的回归系数（分别为β1和β2）和残差（ε）用于计算混淆校正的归一化统计量如下：[（测量灌注） - （预测）灌注）] /残差标准差。测量的灌注是预处理的ASL强度，预测的灌注是分别由个体的年龄和性别参数加权的体素效应的总和（图2b）。与Z分数一样，阴性W分数表示灌注较低，阳性分数表明灌注高于参考预期，考虑到个体的年龄和性别。</p><h3 id="SVM-Multivariate-Pattern-Recognition-in-the-Training-Set"><a href="#SVM-Multivariate-Pattern-Recognition-in-the-Training-Set" class="headerlink" title="SVM: Multivariate Pattern Recognition in the Training Set"></a>SVM: Multivariate Pattern Recognition in the Training Set</h3><p>在Matlab（Mathworks，Natick，Mass）中实现的神经成像工具箱（21）的模式识别为神经学图像提供了多变量模式分析。线性SVM生成多维超平面，以最佳方式分离标记组中的数据（监督学习）。对于二维矢量，这将是直线（31），但在我们的情况下，超平面具有所包括的体素的数量的维度。表示该超平面（21）的法向矢量的辨别图将每个体素的相对权重存储到分类中。</p><p>SVM接受了一次性交叉验证框架的培训，以区分AD患者，MCI患者和SCD患者。我们通过比较两个患者组的W得分图与具有SCD的受试者的W得分图来评估分类器的诊断价值。通过区分AD患者和患有MCI的患者的W评分图来评估分类器对疾病进展的敏感性。最后，使用转化为AD的MCI诊断患者和具有稳定MCI的患者的W评分图进行探索性分类训练，以研究分类器是否显示预后值。</p><p>分类准确性反映了算法的预测能力，因此具有直接的诊断相关性。因此，通过计算准确度，灵敏度，特异性和接受者操作特征（ROC）曲线来评估分类器性能，从该曲线计算ROC曲线下面积（AUC）。置换测试用于导出精度的P值（100个排列）（21）。</p><p>首先根据整个大脑计算训练精度。随后，使用阿尔茨海默氏病特异性感兴趣区域（ROI）的面罩来最大化训练精度。 ROI基于文献（1,7,8,10,11,16）和阈值组平均灌注图。它们包括顶叶，海马，枕叶及其组合。通过使用MNI结构和Harvard-Oxford皮质下结构图谱（29）（L.E.C.，F.H。和A.M.W.，分别具有1,1和15年的经验）创建掩模。</p><h3 id="SVM-Prediction-in-New-Participants"><a href="#SVM-Prediction-in-New-Participants" class="headerlink" title="SVM: Prediction in New Participants"></a>SVM: Prediction in New Participants</h3><p>在独立样本中复制结果既支持内部有效性又支持分类器的普遍性（32）。辨别图用于预测预测集灌注图的标签。辨别图显示预定义ROI中体素的辨别力，但不应解释为统计测试。相反，它们提供决策边界法向量的空间表示 - 即，区分组之间的每个体素的权重。正值（红色和黄色）表示对于更严重的条件具有预测值的体素，而负值（深蓝色和浅蓝色）表示对于不太严重的条件具有预测值的体素。单个受试者灌注图乘以辨别图，由训练偏差调整。使用个体积分乘积得分来定义类别，其可以通过使用ROC分析（L.E.C.，F.H。和A.M.W.）以简单的阈值来预测。</p><h3 id="Statistical-Analysis"><a href="#Statistical-Analysis" class="headerlink" title="Statistical Analysis"></a>Statistical Analysis</h3><p>在SPSS（版本20; SPSS，Chicago，Ill）中进行预测结果的统计分析。 通过ROC分析计算预测的特异性，灵敏度和AUC。 通过使用单因素方差分析评估组间连续测量的差异，使用事后Bonferroni校正进行多重比较。 进行χ2检验以评估参与者性别的频率分布（L.E.C.）。</p><h2 id="Results-1"><a href="#Results-1" class="headerlink" title="Results"></a>Results</h2><h3 id="Participant-Characteristics"><a href="#Participant-Characteristics" class="headerlink" title="Participant Characteristics"></a>Participant Characteristics</h3><p>在六个训练组和预测组之间未观察到显着性别（P = .74）或年龄（P = .68）差异。 在诊断组中，在训练组和预测组之间未观察到MMSE评分的显着差异（P&gt; 0.99，表1,2）。</p><p>在MCI亚组和12名匹配的SCD受试者之间未发现显着的患者性别（P&gt; .99）或年龄（P = .96）差异。 具有SCD的受试者具有比两个MCI亚组中的患者更高的MMSE得分（P &lt;.01），而MCI亚组在MMSE得分中没有差异（P = .47，表3）。</p><h3 id="Training-of-the-Classifiers"><a href="#Training-of-the-Classifiers" class="headerlink" title="Training of the Classifiers"></a>Training of the Classifiers</h3><p>训练结果概述见表4.对于AD与SCD，使用全脑ASLW评分图的训练准确率为87.0％（敏感性为84.0％，特异性为90.0％），AUC为0.94（P = .01）。 当训练限于顶叶和海马ROI时，获得进一步改善至89.0％（84.0％敏感性; 94.0％特异性; AUC，0.93; P = .01）。 AD与MCI全脑分析相比，训练准确率为78.8％（敏感性为84.0％，特异性为70.0％），AUC为0.84（P = .01）。 限制对顶叶和枕叶ROI的分析将准确性提高至83.8％（灵敏度83.3％;特异性84.0％; AUC，0.88; P = .01）。 最后，MCI与SCD全脑分析的准确度为57.5％（灵敏度为40.0％，特异性为68.0％），AUC为0.49（P = .42），在基于ROI的分析中未得到改善。 由此产生的具有最高精度的训练的鉴别图如图3所示。</p><h3 id="Predictions-Assessment-of-Generalizability"><a href="#Predictions-Assessment-of-Generalizability" class="headerlink" title="Predictions: Assessment of Generalizability"></a>Predictions: Assessment of Generalizability</h3><p>结果总结在表5中。在患有AD的患者中使用辨别权重与患有SCD的受试者使得能够在90.0％的个体中正确预测（94.0％敏感性和86.0％特异性）。 ROC曲线表现出优异的性能（AUC，0.96; 95％置信区间：0.92,1; P &lt;.001）。 AD患者与MCI患者使用鉴别权重可以在82.0％的个体中进行正确预测（84.0％的敏感性和80.0％的特异性）。 ROC曲线显示出良好的性能（AUC，0.89; 95％置信区间：0.81,0.97; P &lt;.001）。 在MCI患者中使用歧视权重与患有SCD的患者在仅60.0％的个体中能够正确预测（60.0％敏感性和60.0％特异性）。 ROC曲线表现出差的表现（AUC，0.63; 95％置信区间：0.50,0.76; P = .06;图4）。</p><h3 id="Exploratory-Analyses-Classifying-MCI-Subgroups"><a href="#Exploratory-Analyses-Classifying-MCI-Subgroups" class="headerlink" title="Exploratory Analyses: Classifying MCI Subgroups"></a>Exploratory Analyses: Classifying MCI Subgroups</h3><p>对于MCI诊断转为AD的患者与SCD患者相比，全脑训练的准确率为83.8％（灵敏度为66.7％，特异性为100％），AUC为0.90（P = .01）。 当训练限于后扣带皮层和海马ROI时，获得了进一步改善至87.5％（75.0％灵敏度; 100％特异性; AUC，0.92; P = .01）。 对于MCI诊断转为AD的患者与稳定MCI患者相比，全脑训练的准确率为70.8％（敏感性为66.7％，特异性为75.0％），AUC为0.77（P = .05）。 当训练局限于海马ROI时，获得了进一步改善至83.3％（83.3％的敏感性; 83.3％的特异性; AUC，0.77; P = .01）（表4）。 得到的鉴别图如图5所示。</p><p>由于小群组，没有匹配数据可用于基于转换为AD或稳定MCI的MCI诊断创建独立预测集。 然而，对于那些MCI诊断转化为AD的患者与使用SCD的患者相比，使用AD与SCD训练鉴别权重导致79.0％的个体的正确预测（灵敏度为83.0％，特异性为75.0％）。 ROC曲线表现出良好的性能（AUC，0.84; 95％置信区间：0.68,1; P &lt;.01）。 在MCI诊断中使用相同的鉴别权重转换为AD与稳定的MCI相比，在71.0％的个体中得到了正确的预测（灵敏度为67.0％，特异性为75.0％）。 ROC曲线显示出良好的性能（AUC，0.71; 95％置信区间：0.49,0.93; P = .08;表5;图6）。</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>在目前的研究中，灌注图的自动分类使我们能够以高精度区分具有不同AD阶段的患者。另外，鉴别权重可以用于独立数据集中的单主题诊断预测，具有良好至极好的准确度和AUC值。与放射科医师使用的传统评估策略（18,19）以及先前的研究相比，我们的训练和预测准确度相似，其中模式识别软件应用于全脑结构MR成像数据（17,18,20,33）。此外，本文提出的分类器具有良好的敏感性和特异性。这表明灌注差异与AD的诊断相关。</p><p>与先前的ASL研究（7-11,16）一致，通过使用AD特异性ROI而不是全脑ROI观察SVM训练的最高准确度。对于AD与SCD，组合的顶叶和海马ROI产生最高的准确度。然而，我们没有发现通过仅使用前躯体或后扣带皮层作为ROI来增加准确性，表明尽管这些区域显示最明显的低灌注（7-9,16），但是当不服用时，最佳分化的重要信息会丢失。整个顶叶考虑在内。可能的解释是默认模式网络的参与，已经证明在AD患者中活动较少并且包括几个顶叶区域（4）。</p><p>当应用顶叶和枕叶ROI时，我们实现了AD与MCI的最大训练准确度。尽管顶叶的使用与先前的ASL结果一致（10,11），但枕叶的应用是矛盾的，因为在MCI患者中观察到枕骨低灌注与对照组相比，但未与AD患者进行比较（ 11,13）。大多数AD样本由早发性AD患者组成，与晚发AD患者相比，总体和枕部低灌注和代谢减退更为明显（34,35），可以解释这种差异。</p><p>MCI患者与患有SCD的患者的自动分类没有产生高准确度，这在以前的工作基础上是出乎意料的（8-10）。 MCI组内缺乏同质性可能导致这种低精度，因为它禁止SVM找到模式。实际上，MCI亚组的训练改善了分类，与之前使用结构MR成像的研究相比，准确度略高（20,33）。该观察结果证明了分类器患者之间的异质性问题（33）。但是，通过使用大型训练数据集可以避免这个问题。由于MCI亚组的训练由小样本组成（n = 12），因此需要使用更大的样本复制我们的结果以支持MCI转换器的自动化方法的预后价值。</p><p>通过SVM分类培训，使用了一次性交叉验证框架;因此，相同的数据被重新用于学习和分类，这可能产生有偏见的结果。与该领域的大多数先前研究不同，在我们的研究中，我们使用独立的数据集进行训练和预测以评估普遍性。此外，我们的研究通过使用更大的样本量，减少疾病异质性问题和提高分类准确性，扩展了结构MR成像的先前结果。然而，在日常实践中真正临床应用SVM所需的数据量明显更高（33）。因此，一项大型多中心研究具有重要的临床意义。</p><p>我们的结果受到以下事实的限制：我们的AD样本中相对较高比例的患者包括早发性AD患者。进一步的研究涉及使用老年受试者的样本将使我们能够研究晚发性AD患者的适用性。其次，我们使用SCD受试者代替健康对照受试者进行分类。然而，患有SCD的受试者在记忆诊所和医院中遇到，因此用这种方法很好地代表了操作环境。</p><p>我们的结果支持自动分类可以促进并可能改善诊断的方式，特别是在没有经验的（神经）放射科医师的中心。此外，考虑到AD的高患病率，自动分类可能适用于筛查目的（1）。</p><p>总之，使用自动化方法，年龄和性别调整的ASL灌注图可用于分类和预测AD的诊断，MCI诊断转换为AD，稳定MCI和SCD，具有良好至极好的准确度和AUC值。</p><h2 id="Advances-in-Knowledge"><a href="#Advances-in-Knowledge" class="headerlink" title="Advances in Knowledge"></a>Advances in Knowledge</h2><ul><li><p>可以训练自动化机器学习方法，根据动脉自旋标记（ASL）图像区分主观认知能力下降的受试者，轻度认知障碍患者和阿尔茨海默病（AD）患者，具有较高的分类训练准确度（范围） ，83.8％-89.0％; P &lt;.01）。</p></li><li><p>基于这些训练的分类器可用于预测具有高诊断准确度的单个受试者的诊断（接收器操作特征曲线范围下的面积，0.89-0.96; P &lt;.001）。</p></li></ul><h2 id="Implications-for-Patient-Care"><a href="#Implications-for-Patient-Care" class="headerlink" title="Implications for Patient Care"></a>Implications for Patient Care</h2><ul><li><p>用于高精度（&gt; 82％）检测AD患者的三维假连续ASL图像的自动分类可支持基于图像的诊断，尤其是在没有经验（神经）放射科医师的中心。</p></li><li><p>三维假连续ASL图像的自动分类可用于AD筛查目的，而不会影响诊断准确性。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/Paper/ADNI/"/>
    
    
      <category term="translation" scheme="http://yoursite.com/tags/translation/"/>
    
  </entry>
  
  <entry>
    <title>Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease-翻译</title>
    <link href="http://yoursite.com/2019/07/16/Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer%E2%80%99s-disease%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/2019/07/16/Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease翻译/</id>
    <published>2019-07-16T09:15:25.000Z</published>
    <updated>2019-08-12T07:31:37.981Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h2><p>阿尔茨海默病（AD）是最常见的神经变性疾病之一，具有常见的前驱性轻度认知障碍（MCI）阶段，其中记忆丧失是主要抱怨，随着行为问题和自我保健不足逐渐恶化。然而，并非所有临床诊断患有MCI的个体都进展为AD。一部分患有MCI的受试者要么进展到非AD痴呆症，要么在MCI阶段保持稳定而不进展为痴呆症。尽管目前还没有治愈性AD的治疗方法，但正确识别MCI期患者继续发展AD的个体是非常重要的，以便在不久的将来可以获得治愈性治疗。同时，也非常希望能够正确识别那些没有AD病理的MCI阶段的患者，这样他们就可以免于不必要的药理学干预，这些干预最多可能不会给他们带来好处，更糟糕的是，不利的副作用会进一步伤害他们。此外，在这些非AD病例中识别认知障碍的原因可能更容易和更简单，因此正确识别前驱AD也会对这些个体有益。氟脱氧葡萄糖正电子发射断层扫描（FDG-PET）捕获大脑的代谢活动，并且这种成像模式已经被报道在发生结构变化之前识别与AD有关的变化。使用FDG-PET成像设计分类器的先前工作一直很有前景。由于深度学习最近已成为一种强大的工具来挖掘特征并将其用于准确标记给定图像的群体成员，我们提出了一种新型深度学习框架，使用FDG-PET代谢成像来识别MCI阶段的受试者症状前AD并将其与其他MCI患者（非AD /非进行性）区分开来。我们的多尺度深度神经网络获得82.51％的分类准确度，仅使用来自单一模态（FDG-PET代谢数据）的测量值超过了最近文献中发表的其他类似FDG-PET分类器。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>阿尔茨海默病（AD）占所有类型痴呆症的约50-75％，影响了65岁以上的9人中的1人（Kawas，2003; Alzheimer’s and Association，2011）。其特征是进行性认知衰退，如记忆缺失，注意力和执行功能。虽然目前还没有治疗AD的治疗方法，但有几种有希望的药物化合物处于发展的后期阶段，预计很快就会有一个突破性的治疗方法。迄今为止被推测为导致疾病改善治疗缺乏成功的原因之一是无法正确识别轻度认知障碍（MCI）阶段的个体，这些个体将从那些发展AD的人发展为AD。由于其他原因导致MCI症状。当对那些MCI症状不是由于AD导致明显有希望的治疗被认为是失败的那些人施用时，AD的潜在有希望的治疗可能不会显示益处。在MCI阶段识别前驱AD患者的另一个原因是疾病早期的干预可能有助于延缓发病和/或降低面临完全AD的风险，而疾病过程中的干预措施可能减缓疾病，但在已经发生之后不能逆转病理引起的神经元损失。因此，在MCI阶段诊断为症状前AD是一项极其重要的任务，当治愈性治疗可用时，这将变得更加重要和紧迫。对于那些MCI是由AD以外的原因引起的人来说，这种诊断也很有价值，因为这些其他原因可能更容易和更容易识别和管理。<br>氟脱氧葡萄糖正电子发射断层扫描（FDG-PET）提供了大脑代谢活动的定量测量（Mosconi等，2010）。 AD中的脑区域代谢异常被认为发生在结构性脑变化发生之前（Jack等人，2010; Jagust等人，2006）。此外，区域代谢异常被认为是AD患者功能和认知能力下降的基础（Landau等，2011; Kawachi等，2006）。因此，FDG-PET被认为是AD症状前诊断的潜在工具，具有可接受的敏感性和准确性（Cheng等，2015; Davatzikos等，2011; Albert等，2011; Chen等。 ，2010）。以前的研究试图找出患有症状前AD的受试者，但取得了有限的成功（Cheng等，2015; Davatzikos等，2011; Suk等，2014）。尽管付出了相当大的努力，但是先前开发用于识别渐进式MCI的自动化工具的尝试导致精度有限（低于80％），如表2所示（Suk等人，2014; Young等人，2013; Zhu等人。 ，2014; Lange等，2016; Cheng等，2015）。</p><p>深度神经网络最近已经成熟，并且在为识别任务提出的机器学习方法中提供了一些最佳性能（He et al。，2016; Krizhevsky et al。，2012）。深度学习网络也被应用于最近识别AD相关的进展模式。例如，刘等人。训练了一个Stacked Autoencoder来学习隐藏表示，然后用softmax输出层进行分类（Liu et al。，2014），Suk et al。结合深Boltzmann机器和支持向量机来识别AD患者（Suk等，2014），Ortiz等。使用深度学习结构的集合来投票分类（Ortiz等，2016）和Payan等。应用3D卷积神经网络进行NC，MCI和AD科目的多类分类（Payan和Monana，2015）。此外，多尺度处理是物体识别模式挖掘的自然延伸（Zhang et al。，2007; Lowe，2004）。通过将图像下采样到不同的尺寸并在不同的分辨率下提取特征，这种方法也被证明可以提高深度神经网络的分类性能（Tang和Mohamed，2012）。<br>我们还要注意的是，先前发布的在AD中使用FDG-PET图像的非深度学习分类方法通常使用小的测试数据集，包括有限的ADNI数据库子集，或者通过少于一百个图像呈现它们的验证，于最近的回顾结果可见（Rice和Bisdas，2017）。最近的另一篇Cochrane评价报告还列出了总共14项研究（在所有这些研究中总共分析了421名受试者），其中每项研究仅对数十名受试者进行了研究（Smailagic等，2015a）。相比之下，本文的标志之一是我们对几乎所有可用的ADNI受试者（N = 1051）进行分析，这些受试者在研究中准备本手稿时同时具有结构MRI和FDG-PET图像。最近的评论（Rice和Bisdas，2017; Smailagic等，2015b）进行了以展示为基础的研究，展示了他们在较小数字（大约几十到几百）的结果。因此，我们的论文对迄今为止为该项工作发布的最大数据集提出了最全面的方法验证。其次，我们的论文将是第一个利用深度学习开发多尺度FDG-PET分类器的论文。这些观察结果支持我们提交的新颖性和全面性。</p><p>总的来说，该论文的主要贡献：（1）我们提出了一种新的多尺度深度神经网络框架，以学习AD病理学代谢变化的模式，作为正常对照（NC）代谢模式的判别; （2）我们发现，通过从NC和AD个体转移样本，深层结构可以在早期诊断任务中获得更好的判别能力; 3）我们证明了具有不同验证设置的集合多个分类器可以使所提出的方法更加稳定和稳健，并提高其分类性能。此外，我们对我们的方法进行了全面验证，分析了1051名受试者采取的代谢措施，这些受试者的质量控制要求严格，包括专家手动编辑所有分段以确保准确性。到目前为止，我们的研究可能是第一个利用如此大量FDG-PET图像的研究，因此，这些结果表明了普遍性的良好潜力。</p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p>我们的框架可以分为两个主要步骤：（1）图像预处理和（2）使用深度神经网络进行分类。 在图像处理步骤中，我们执行T1 MR图像的分割，包括将较大的感兴趣区域（ROI）细分为较小尺寸区域的粗到细方法，以及FDG-PET的共同配准 具有相同受试者的T1 MR图像的图像在这些局部区域中提取代谢测量。 在分类步骤中，我们利用在多尺度特征上训练的深度神经网络来学习AD病理学的模式，并使用显示MCI个体分类表现的大群体进行实验。</p><h3 id="Materials"><a href="#Materials" class="headerlink" title="Materials"></a><em>Materials</em></h3><p>用于制备本文的数据来自公众可获得的阿尔茨海默病神经影像学倡议（ADNI）数据库（<a href="http://adni.loni.usc.edu" target="_blank" rel="noopener">http://adni.loni.usc.edu</a>）。 ADNI于2003年作为公私合作伙伴关系启动，由首席研究员Michael W. Weiner博士领导。 ADNI的主要目标是测试是否可以将连续MRI，PET，其他生物标志物以及临床和神经心理学评估结合起来，以测量轻度认知障碍（MCI）和早期阿尔茨海默病（AD）的进展。<br>在正在进行的纵向ADNI研究中招募的1051名受试者的FDG-PET图像和结构MRI被下载并包含在本文中。受试者分为四类，即（1）正常对照（NC），（2）稳定轻度认知障碍（sMCI）类，（3）进行性轻度认知障碍（pMCI）类，和（4）临床诊断的那些与阿尔茨海默病（AD）。 NC组（N = 304）是没有任何认知投诉的受试者，在整个ADNI研究中仍然保持NC。 sMCI组（N = 409）是在基线时诊断为MCI症状的受试者，并且在制备该手稿时的整个ADNI研究中继续保持为MCI（中位随访时间为3年）。 pMCI组（N = 112）的受试者在基线时被诊断为MCI并且进展至可能的AD，中位时间转换为1年，因此这些图像被前瞻性地标记以指示他们将来进展至AD。可能的AD组（N = 226）受试者临床诊断为在基线时具有AD，并且随后在整个ADNI研究中保持AD的临床诊断。具有诊断变化的受试者（例如NC进展至MCI，或MCI恢复至NC）被排除在拟议研究之外。表1中描述了这些受试者的人口统计学和临床信息。在第二行中，括号中的数字是男性和女性受试者的数量，而其余三行代表年龄，教育年份和MMSE（Mini-精神状态考试）得分。每行中从左到右的两个数字是所有受试者的平均值和标准偏差。<a href="http://www.adni-info.org" target="_blank" rel="noopener">http://www.adni-info.org</a>上描述了ADNI受试者群组，NC，MCI和AD的临床诊断，图像采集协议程序和采集后预处理程序的详细描述。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/16/Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease翻译/Table-1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="Image-processing"><a href="#Image-processing" class="headerlink" title="Image processing"></a><em>Image processing</em></h3><p>最近的研究表明，深度学习的方法在解决许多<code>图像识别问题</code>方面是有效的，但大多数已发表的方法都归功于对大量数据样本（大约数百万）的训练。对于医学成像研究而言，这是一个相当大的障碍，例如，使用T1加权和/或FDG-PET成像等方式登记和扫描的受试者数量限制在最多几千人。因此，在这种较小的数据库上直接训练深度神经网络系统最有可能提供低于标准的分类精度和性能。然而，与典型的图像识别任务相比，图像可能具有高度的变异性和多样性，医学成像问题在更大程度上被标准化（相同的组织，例如，大脑），并且通常遵循严格标准化的协议，从而显示出不同受试者的成像异质性。这<code>有利于减少充分训练所需的样本数量</code>。事实上，我们在本文中的实验表明，随着ADNI提供的图像数量（1000个数量级），这种训练是实用的，可以带来出色的性能。</p><h3 id="ROI-segmentation"><a href="#ROI-segmentation" class="headerlink" title="ROI segmentation"></a><em>ROI segmentation</em></h3><p>使用免费提供的具有默认参数设置的<code>FreeSurfer 5.3软件包</code>将每个T1结构MRI图像分割成<code>灰质和白质</code>（Fischl等，1999）。 Freesurfer进一步<code>将灰质细分为85个皮质和皮层下感兴趣解剖区域（ROI）</code>。尽管在处理ADNI等数据库中出现的各种解剖学配置时存在巨大的稳定性，但有些情况下Freesurfer输出可能包含错误，例如由于不正确的大脑掩模，皮质，皮层下或白质细胞包裹引起的错误。 - 大约10％的案例中的错误。由于此类错误可能会在测量中注入未知的可变性，因此我们通过训练有素且专业的神经病理学家对每个脑图像的每个FreeSurfer分段进行<code>手动质量评估</code>。此外，通过<code>手动编辑校正</code>脑膜，白质，皮质或皮质下分割中的任何错误，并重新运行Freesurfer，直到T1 MR图像分割变得准确，然后包括在分析中。</p><h3 id="Patch-parcellation"><a href="#Patch-parcellation" class="headerlink" title="Patch parcellation"></a><em>Patch parcellation</em></h3><p>由Freesurfer提供的T1 MR图像分割中的每个ROI内的体素进一步细分为不同大小的较小区域，在此表示为“补丁”。由于每个ROI（例如枕叶皮层）可能非常大，因此在ROI中提取新陈代谢的平均值可能会导致由于AD导致的信号变化的灵敏度降低，因为在大量体素上进行平均。然而，如果没有先验知道可能发生变化的位置以及它们的定位程度，最佳方法是多尺度方法所建议的方法，其表明信号是从较小的子区域以多个尺度提取的（精细的）尺度）到较大的子区域（粗尺度），并且这些子区域可以同时用于检测由于疾病或病症引起的变化。因此，我们选择这些补丁的大小在每个ROI中包含500,1000和2000个体素。将每个ROI细分为自身以及许多不同大小的较小单元导致整个大脑的1488,705和343个总数补丁，分别针对每个规模。确定补丁的大小以保持足够的详细信息以及避免过大的特征维度，考虑到本研究中只有有限数量的数据样本。用于此细分的技术是先前发布的技术，其中每个ROI可通过k均值聚类算法使用其空间坐标进行聚类（Raamana等，2015）。该子细分执行一次，在所选模板中，代表性表面可视化显示在图1的左图中。由于ROI具有不同的大小，将每个ROI细分为预定的固定数量的补丁将导致他们的细分大小不同。因此，为了解决这个问题，我们首先预先定义每个补丁（500,1000,2000）中的体素数量，然后根据ROI大小计算每个ROI需要多少个聚类（k值）。这种方法具有以下优点：针对整个大脑中的不同ROI提供均匀的贴片尺寸密度（ROI中的贴片/ ROI中的体素），导致大脑中每个ROI的相同尺度的信号聚集。在模板T1 MR图像FreeSurfer分割进一步细分为多尺度贴片后，该模板MRI被注册到每个目标MRI空间，通过高维度高度精确的非刚性配准方法逐个贴片进行二元分割图像配准。 （LDDMM（Beg等，2005））被认为是领先的注册工具之一。利用分段模板和目标ROI之间的计算的注册图，所选模板的逐片分割向前传播到每个目标，其中这些通过在这些补丁内平均来用于FDG-PET信号聚合。</p><h3 id="Coregistration"><a href="#Coregistration" class="headerlink" title="Coregistration"></a><em>Coregistration</em></h3><p>然后使用<code>FSL-FLIRT程序</code>（Jenkinson等，2002）基于归一化的互信息共同配准每个目标的FDG-PET图像和颅骨剥离的MRI扫描。 使用归一化相关作为成本函数，并将自由度（DOF）设置为12.目视检查每个共同配准的准确性，并且记录并校正共同配准中的任何错误。 然后利用估计的模态间配准图将目标MRI的斑块分割转移到目标FDG-PET域。</p><h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a><em>Normalization</em></h3><p>提取每个贴片中FDG-PET图像的平均强度作为代谢特征。 之前已经注意到，从不同个体获取的FDG-PET图像的强度可能具有不同的“基线”值，使得在没有标准化的个体之间进行比较以校正该基线的不同是不一致的。 遵循该领域的标准方法，脑干是用于内部正常化的区域，因为它被认为最不可能受到AD的影响。 因此计算脑干区域中的平均强度并用于划分每个受试者的脑中所有其他ROI的代谢测量值。</p><h3 id="Visualization-of-metabolism-measures"><a href="#Visualization-of-metabolism-measures" class="headerlink" title="Visualization of metabolism measures"></a><em>Visualization of metabolism measures</em></h3><p>用补片大小作为2000个体素提取的整个数据集的代谢特征在图1中显示为右图。该可视化中的每一列是一个主题，并且每行代表在主题上测量的相同补丁。 在列入NC，sMCI，pMCI和AD的临床诊断类别后，显示列（个体）。 这是所有1051个受试者同时采集的新陈代谢测量分布的精确可视化，并用于突出每个受试者的每个区域的一致代谢模式。 此外，由于任何预处理步骤中的潜在错误，任何个体的代谢测量中的任何异常值都可以在视觉上被检测到，因此这也用于对整个数据库中的测量进行视觉质量控制。</p><h3 id="Multiscale-Deep-Neural-Network"><a href="#Multiscale-Deep-Neural-Network" class="headerlink" title="Multiscale Deep Neural Network"></a><em>Multiscale Deep Neural Network</em></h3><p>在图像处理之后，我们开发了一种多尺度深度神经网络（MDNN），它将多尺度贴片代谢特征作为输入，如图2所示。对于每个主体，有三个不同的输入特征向量，分别为1488,705和343用分别由500,1000和2000个体素组成的贴片提取的尺寸。 MDNN由四个深度神经网络（DNN）组成，第一阶段有三个，第二阶段有一个。 MDNN的培训分为三个步骤。首先，第一阶段的三个DNN独立地训练，具有在不同尺度下提取的代谢特征。然后将三个DNN的输出连接为输入向量，以在第二阶段训练DNN。最后，将MDNN的所有参数与作为输入的三种代谢特征一起调整。如果我们使用N来表示输入要素维度，则对于每个单个DNN，前两个隐藏层的大小设置为3N和3 / 4N，对于最后一个层，隐藏单位的数量设置为100为DNN在第一阶段，在第二阶段DNN为50。选择单元的数量以最大化在第一层中探索跨越不同贴片的非常广泛的可能隐藏相关性的机会，并逐渐减少后续层中的特征的数量以避免过度拟合。</p><h3 id="Network-training"><a href="#Network-training" class="headerlink" title="Network training"></a><em>Network training</em></h3><p>对于每个单独的DNN，训练包含两个步骤，无监督预训练和监督微调，如图3所示。<br>无人监督的预训练。对于每个DNN，我们将其预先训练为堆叠自动编码器（SAE）。 Autoencoder是一种人工神经网络，用于学习输入数据的生成模型。它学习隐藏层（编码器）中的潜在表示，并重建输出层（解码器）中的数据。如果网络由多个编码器层组成，后跟几个解码器层，则它将成为堆叠自动编码器。对于输入数据x，隐藏层中的潜在表示可以显示为：y = s（W x + b），其中W是权重矩阵，b是偏差项，s是我们使用的激活函数在这项工作中纠正了线性函数max（0，x）。重建通过类似的变换发生：z = s（W’y + b’），其中我们用绑定权重W’= W T约束它。平方误差1/2 || x  -  z || 2用于计算反向传播的重建误差。如图2所示，对于每种特征，我们使用了三层SAE。我们不是一起训练所有隐藏层，而是使用贪婪的分层训练（Bengio et al。，2007），它一次训练一个隐藏层。</p><p>•监督微调。 在使用贪婪的逐层预训练初始化参数之后，我们仅保留三个编码器层并为每个DNN添加softmax输出层。 网络被监督微调为多层感知器（MLP）与标准的主题标签。 交叉熵用作梯度体面反向传播的损失函数，其定义为：</p><p>其中N是输入样本的数量，j表示样本类，xi，yi是特征向量和第i个样本的标签，h表示网络函数。</p><p>整个MDNN训练的成本函数与方程式相同。 （1）除了使用不同的输入特征向量。 所有训练步骤都采用相同的反向传播方法。 使用小批量梯度下降最小化成本函数（Bengio，2012）; 将训练集随机分成几个小批量或子集，每批50个样本。 在每次迭代中，仅使用这些小批量中的一个用于最小化。 在将所有样本用于训练之后，我们重新排序训练集并将其再次划分，使得每个不同回波中的批次将不具有相同的样本。</p><h3 id="Reducing-chances-of-over-fitting"><a href="#Reducing-chances-of-over-fitting" class="headerlink" title="Reducing chances of over-fitting"></a><em>Reducing chances of over-fitting</em></h3><p>深度神经网络可能过度拟合给定的样本，因此，在没有任何正则化的情况下训练它将导致小的训练误差和可能的大的泛化误差。我们采用了两种技术来防止过度拟合：（1）辍学（Srivastava等，2014），以及（2）提前停止。<br>•辍学策略。在深度神经网络的训练中，辍学是防止过度拟合的流行策略。通过在每个隐藏层之后插入一个dropout图层，我们可以选择保留为下一层的单位百分比。对于训练，在每次迭代中随机丢弃一半隐藏单元，而保留另一半隐藏单元以将特征提供给下一层。为了测试，保留所有单元以对受试者进行分类。通过避免训练每个训练样本上的所有隐藏单元，这种正则化技术不仅防止了对训练数据的复杂协同适应，而且减少了计算量并提高了训练速度。<br>•早期停止策略。早期停止是我们应用于防止过度安装的另一种技术。当使用梯度下降等迭代方法训练深层架构时，改善网络与训练集的匹配将在开始时提高性能，但在某一点上，它将开始增加泛化误差，同时减少训练误差。为了防止这种情况，早期停止通常用于在过度拟合之前提供适当迭代次数的指导。在这项工作中，我们将训练数据随机分成训练集和验证集，网络参数仅通过训练集的数据进行优化，而验证集仅用于确定早期停止时间点：网络的迭代验证集的泛化误差最小。值得一提的是，训练集，验证集和测试数据是截然不同的;网络的性能仅在测试集上进行评估。</p><h3 id="Instance-transfer-learning"><a href="#Instance-transfer-learning" class="headerlink" title="Instance-transfer learning"></a><em>Instance-transfer learning</em></h3><p>转移学习是缺乏数据样本时最常用的解决分类问题的方法之一（Pan and Yang，2010）。通过将与目标组相关的其他源组的实例引入到训练中，它扩大了训练集并用于提高分类准确性。通常需要优化算法来找到最相关的实例或重新加权来自其他组的实例（Zhang et al。，2014）。然而，在这项工作中，那些进展为AD的MCI可能处于潜在AD病理的早期阶段。此外，如图1的右图所示，在该pMCI组中发现了类似的代谢模式，如图1的右图所示。因此，AD组可以直接用于扩大pMCI训练集。考虑到这两个群体的相似性和相关性。另一方面，可以将NC个体添加到sMCI组以增加可能不具有AD病理学的个体库。交叉验证实验证明了我们使用这种方法背后的基本原理，即使用转移实例训练的网络显示出比没有特别在sMCI中使用转移实例的网络与用于检测前驱AD的pMCI分类任务相比更好的判别能力。在MCI集团中。</p><h3 id="Ensemble-classifiers"><a href="#Ensemble-classifiers" class="headerlink" title="Ensemble classifiers"></a><em>Ensemble classifiers</em></h3><p>由于本研究中数据样本数量有限，特别是对于pMCI受试者，我们只能随机选择一小组数据进行验证，以保留足够的数据用于培训和测试。如此少量的验证样本可能无法代表整个数据集，并可能使网络偏向于在验证集上过度拟合。为了设计包含更好的鲁棒性，稳定性和普遍性的算法，选择了一个集合分类器框架。在这个框架中，我们培训了10个独立的网络，而不是培训一个网络，以便对最终的分类结果进行“投票”。从数据集中随机选择训练数据后，进一步分为10组。在训练阶段，9组用于训练网络，另一组用于验证。这些步骤重复10次，每组用于验证，从而产生10个不同的网络。在测试阶段，将测试数据馈送到每个网络，并生成属于每个类的样本的概率。对于每个样本，我们总结了来自10个网络的概率以确定分类结果。实验表明，集合多个网络使分类器更加鲁棒和稳定，并在统计上提高了分类精度。</p><h3 id="Experimental-setup"><a href="#Experimental-setup" class="headerlink" title="Experimental setup"></a><em>Experimental setup</em></h3><p>建议的网络是使用开源深度学习工具箱Tensorflow构建的（Abadi等，2015）。三个二元分类实验，（i）NC与AD，（ii）sMCI与pMCI和（iii）sMCI与pMCI以及来自NC和AD的转移学习，用于测试所提出的网络架构。进行十次交叉验证以测试每个训练网络的分类结果的普遍性。简言之，将可用数据集随机分成10个子集，其中9个用于训练，其余用于测试。为了防止过度拟合，选择训练数据的子集作为验证集并用于提供停止网络优化的指导（内部分类器优化循环）。如上所述，该训练过程重复十次，以训练十个不同的网络以“投票”以获得最终的分类结果。学习率分别设定为10-1和10-4，用于预训练和微调。使用准确度，灵敏度和特异性的标准测量来评估所提出的分类器的性能。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>表2中显示了交叉验证实验的平均准确度，灵敏度和特异性以及当前可用的现有技术方法的结果。 此外，我们还将结果与使用支持向量机（SVM）构建的标准分类器（Chang和Lin，2011）和使用主成分分析（PCA）的维数降低进行了比较（Wold等，1987） ）在相同的多尺度功能上。 选择PCA特征向量的数量以保持95％的方差。 径向基函数用于SVM，因为它在分类任务中具有优越的性能。 表2的最后三行来自sMCI vs pMCI实验，其中实例从NC和AD组转移用于训练。</p><h3 id="Multiscale-classification"><a href="#Multiscale-classification" class="headerlink" title="Multiscale classification"></a><em>Multiscale classification</em></h3><p>分类实验是通过使用包括500,1000和2000个体素的不同贴片尺寸以粗到精的方法在几个尺度上提取特征来进行的（表3），从而在整体中产生1488,705和343个贴片。 大脑，分别。 正如所料，分类结果对不同分类实验中不同的斑块大小敏感，表明感兴趣的信号位于不同的空间尺度。 这些结果列于表3中。</p><h3 id="Ensemble-classifier-design"><a href="#Ensemble-classifier-design" class="headerlink" title="Ensemble classifier design"></a><em>Ensemble classifier design</em></h3><p>如方法部分所述，此处提供的分类结果来自分类器集合而不是单个分类器。在这个集合中，训练多个网络，然后对最终的分类结果进行“投票”。具有或不具有集合分类器的分类性能如图4所示。从左到右的三个图分别代表NC与AD，sMCI与pMCI和sMCI与pMCI的分类实验，其中分别来自NC和AD组的转移实例。 y轴绘制分类精度，x轴代表不同的实验。在x轴上，’ensemble’表示来自集合分类器的结果，数字1到10表示没有集合分类器的实验。在每个非整体实验中，我们还进行了10次交叉验证实验，但是我们不是训练10个网络对最终结果进行“投票”，而是随机选择验证集并训练单个分类器。图4中显示的结果显示了交叉验证实验的平均值和标准偏差。</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><h3 id="Comparison-with-state-of-the-art-methods"><a href="#Comparison-with-state-of-the-art-methods" class="headerlink" title="Comparison with state-of-the-art methods"></a><em>Comparison with state-of-the-art methods</em></h3><p>我们将我们研究中获得的结果与标准SVM和PCA + SVM方法以及Suk等人最近提出的最新方法进行了比较。 （2014），Young等。 （2013），朱等人。 （2014），Lange等。 （2016）和Cheng等人。 （2015）关于NC与AD的分类，以及sMCI与pMCI（在MCI阶段检测前驱AD）。虽然不同研究中使用的数据并不相同，但它们都来自ADNI数据库，因此它们共享一个类似的获取和预处理程序，允许进行比较。<br>对于使用FDG-PET代谢测量的NC组与AD组的分类，我们提出的方法，精确度为93.58％，比之前公布的最佳方法高出1.28％（Zhu et al。，2014 ）。此外，该结果还与之前使用多模式研究（FDG-PET +结构MRI）报道的95.35％的准确度相当（Suk等，2014）。为了将MCI阶段的个体区分为sMCI或pMCI组，我们提出的方法（没有转移学习）的表现明显好于（比现有的最先进方法高出至少10％）。此外，与将FDG-PET测量结果与其他方式相结合的多模式方法相比，使用单一模态（代谢FDG-PET）的方法对于sMCI与pMCI任务相比仍然高5.63％（Young et al。， 2013; Suk等，2014）。<br>使用在训练阶段从NC和AD受试者转移的实例，我们的方法在sMCI与pMCI分类任务中的准确性被发现为82.51％，这不仅比先前的最佳结果高出约10.91％（Cheng等人。，2015）使用单一模态，但也比以前的多模态方法高3.11％。即使只使用单一模态而没有NC和AD科目，我们的方法的准确性比Cheng等人的方法更高（2.15％）。 （2015）使用多种形式与NC和AD科目的转学习。对于使用相同多尺度特征的SVM和PCA + SVM实验，基于PCA的特征选择提高了SVM的分类性能。但与提出的深度神经网络相比，PCA + SVM的准确度降低了6％，这表明在这种情况下MDNN性能更优越。<br>总之，我们的实验表明，与过去的研究相比，我们提出的基于深度学习的方法显示sMCI和pMCI之间的分类准确性更高，无论是使用单一模式还是多模式研究（Suk等，2014）。无论从NC与AD实验获得的转移学习的应用如何，我们的准确性结果也是优越的（Cheng等，2015）。这些结果表明，提出的方法的优越性表明FDG-PET模式作为一项独立调查的潜力，无论转移学习的使用（Cheng et al.，2015; Suk et al.，2014）用于区分pMCI个体与在MCI个人中。</p><h3 id="Multiscale-classification-1"><a href="#Multiscale-classification-1" class="headerlink" title="Multiscale classification"></a><em>Multiscale classification</em></h3><p>在AD与NC分类实验中发现，随着贴片尺寸的增加，精度普遍降低，表明AD的变化可能局限于较小的尺度，并且在使用较大尺寸的贴片时趋于平均。我们没有观察到其他分类实验的类似模式（sMCI vs pMCI和sMCI vs pMCI使用转移学习）。因此，发现使用较小尺寸的斑块来提取新陈代谢特征并不总能恢复较高的辨别力信息，并且可能sMCI与pMCI情况下的信号甚至更精细地定位，并且可能需要在每个ROI中使用甚至更小尺寸的补丁。然而，与单一尺度特征相比，使用所有多尺度特征的组合分类性能（包括从每个单一尺度特征获得的判别信息）产生了更高的精度结果。这表明在连锁多尺度特征上训练的网络（图2）仍然能够学习本文中使用的从小到大的补丁大小的隐藏模式。未来的工作可能包括在系统优化框架中学习每个分类实验中信号存在的规模，以找到最佳分类性能的最佳尺寸特征。</p><h3 id="Ensemble-classifier"><a href="#Ensemble-classifier" class="headerlink" title="Ensemble classifier"></a><em>Ensemble classifier</em></h3><p>图4中的结果表明，通过训练和验证集的不同划分，各个分类器的分类精度（中间图像的第二和第四个条）可能有多达8％的差异。 ，表明每个单一分类器的性能不稳定，普遍性降低。 在所有三个实验中，集合分类器的准确性高于单个分类器的平均值，这表明具有不同训练和验证集的分集的集合分类器可以产生更稳健和稳定的分类器，因此可以改善分类 性能更好的普遍性。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>在本文中，我们使用多尺度贴片FDG-PET深度学习功能提出了一种新的AD早期诊断框架。所提出的框架利用转移学习方法和集合分类器策略来改善深度神经网络在区分sMCI和pMCI主体的任务中的性能。在1051名受试者的FDG-PET图像的大型数据库上进行的实验提供了支持三种断言的证据。 （1）所提出的方法，使用仅来自单一FDG-PET模态的特征，能够胜过在sMCI和pMCI分类任务中采用多模态特征的现有方法。 （2）所提出的网络可以从多尺度特征中学习判别模式，以提供具有更好判别性能的更健壮的分类器。 （3）使用不同验证集的集合多个分类器可以使网络更加健壮和稳定，并在统计上提高其分类性能。<br>对于未来的工作，将所提出的框架扩展到包含来自多种模态的信息是自然的，假设所得到的深度神经网络将从多个模态数据中学习更多信息，从而进一步改进所获得的分类准确度。尽管sMCI vs pMCI实验常用于验证近期研究（包括我们的研究）中方法的鉴别能力，但我们只能知道那些sMCI受试者在研究进展中时保持稳定并且可以转化为AD或其他神经退行性疾病。在将来。因此，临床诊断的sMCI受试者的基本事实可能不完全准确，因此可能在分类中引入噪声/偏倚。幸运的是，随着更多数据的收集，分类系统将更好地捕获这些和其他噪声和可变性来源，我们提出的深度学习集合分类器可能非常适合这种情况。</p><h2 id="Acknowledgments"><a href="#Acknowledgments" class="headerlink" title="Acknowledgments"></a>Acknowledgments</h2><p>这项工作得到了国家科学工程研究委员会（NSERC），加拿大卫生研究院（CIHR），迈克尔史密斯健康研究基金会（MSFHR），加拿大大脑，太平洋阿尔茨海默氏症研究基金会（PARF）的支持。该项目的数据收集和共享由阿尔茨海默氏病神经影像学倡议（ADNI）（国家卫生研究院资助U01 AG024904）和DOD ADNI（国防部颁发号W81XWH-12-2-0012）资助。 ADNI由国家老龄化研究所，国家生物医学成像和生物工程研究所资助，并通过以下方面的大量贡献：AbbVie，阿尔茨海默氏症协会;阿尔茨海默氏症药物发现基金会; Araclon Biotech; BioClinica，Inc。;生物遗传; Bristol-Myers Squibb Company; CereSpir，Inc。; Cogstate; Eisai Inc。; Elan Pharmaceuticals，Inc。;礼来公司;欧蒙; F. Hoffmann-La Roche Ltd及其附属公司Genentech，Inc。; Fujirebio公司; GE Healthcare; IXICO有限公司; Janssen Alzheimer Immunotherapy Research＆Development，LLC。;强生药业研发有限责任公司; Lumosity; Lundbeck公司; Merck＆Co.，Inc。; Meso Scale Diag- nostics，LLC。; NeuroRx研究; Neurotrack技术;诺华制药公司;辉瑞公司; Piramal成像;施维雅;武田制药公司;和过渡治疗学。加拿大卫生研究院正在提供资金支持加拿大的ADNI临床站点。国家卫生研究院基金会（<a href="https://fnih.org" target="_blank" rel="noopener">https://fnih.org</a>）为私营部门的贡献提供了便利。受助组织是北加利福尼亚州研究和教育研究所，该研究由南加州大学阿尔茨海默氏症治疗研究所协调。 ADNI数据由南加州大学的神经成像实验室传播。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a><strong>References</strong></h2><p>…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/Paper/ADNI/"/>
    
    
      <category term="translation" scheme="http://yoursite.com/tags/translation/"/>
    
  </entry>
  
  <entry>
    <title>FSL-Introduction</title>
    <link href="http://yoursite.com/2019/07/15/FSL-Introduction/"/>
    <id>http://yoursite.com/2019/07/15/FSL-Introduction/</id>
    <published>2019-07-15T03:43:56.000Z</published>
    <updated>2019-09-07T13:52:23.938Z</updated>
    
    <content type="html"><![CDATA[<p>【摘要】FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.</p><a id="more"></a><h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><p><a href="http://learning-archive.org/wp-content/uploads/2017/09/Ubuntu%E4%B8%8B%E5%AE%89%E8%A3%85AFNI%E5%92%8CFSL.pdf" target="_blank" rel="noopener">Ubuntu下安装AFNI和FSL</a>  、<a href="http://neuro.debian.net/pkgs/fsl-complete.html" target="_blank" rel="noopener">NeuroDebian-fsl-complete</a></p><p><strong>Notice: </strong> <code>source .profile</code></p><p>Mac: <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation" target="_blank" rel="noopener">FslInstallation</a>\<a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation/MacOsX" target="_blank" rel="noopener">MacOsX</a>、 <a href="https://fsl.fmrib.ox.ac.uk/fsldownloads/fslinstaller.py" target="_blank" rel="noopener">Download FSL 6.0.1</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--- FSL Installer - Version 3.0.16 ---</span><br><span class="line">[Warning] Some operations of the installer require administative rights,</span><br><span class="line">    for example installing into the default folder of /usr/local.</span><br><span class="line">    If your account is an 'Administrator' (you have 'sudo' rights)</span><br><span class="line">    then you will be prompted for your administrator password</span><br><span class="line">    when necessary.</span><br><span class="line">When asked a question, the default answer is given in square brackets.</span><br><span class="line">Hit the Enter key to accept this default answer.</span><br><span class="line">Where would you like the FSL install to be (including the FSL folder name)? [/usr/local/fsl]:</span><br><span class="line">Downloading...</span><br></pre></td></tr></table></figure><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><blockquote><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslOverview" target="_blank" rel="noopener">FslOverview</a> 、 <a href="https://en.wikibooks.org/wiki/Neuroimaging_Data_Processing/FSL" target="_blank" rel="noopener">Neuroimaging Data Processing/FSL</a></p></blockquote><p> Currently FSL <code>only</code> accepts the input files in <code>NIFTI</code> format, and the DICOM files need to be converted to NIFTI after acquisition and before processing. </p><h4 id="Functional-MRI-功能导向"><a href="#Functional-MRI-功能导向" class="headerlink" title="Functional MRI 功能导向"></a>Functional MRI 功能导向</h4><ul><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FEAT" target="_blank" rel="noopener">FEAT</a> - model-based analysis, e.g., for task FMRI  基于模型的分析，例如，用于FMRI任务</p><p>FEAT基于一般线性建模（GLM, general linear modelling）。 它允许描述实验设计; 然后创建一个适合数据的模型，告诉你大脑响应刺激的位置。 在FEAT中，用于第一级（时间序列）数据的GLM方法称为FILM。 FILM使用稳健且准确的、时间序列自相关的非参数估计来预先计算每个体素的时间序列; 与未预先白化的方法相比，这提高了估计效率。</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MELODIC" target="_blank" rel="noopener">MELODIC</a> - model-free ICA-based analysis, e.g., for resting FMRI  基于ICA的无模型分析，例如，用于静息FMRI</p><p>MELODIC是无模型分析，它使用独立分量分析（ICA）将单个或多个4D数据集分解为不同的空间和时间分量。 MELODIC可以选择不同的激活和人工组件，而无需指定任何明确的时间序列模型。</p><p>使用概率独立分量分析（PICA, Probabilistic Independent Component Analysis）的无模型FMRI分析。 由于相关的“噪声模型”，MELODIC可自动估计数据中感兴趣的噪声和信号源的数量，并且能够将重要性（“p值”）分配给输出空间图。 MELODIC还可以使用Tensor-ICA分析多个受试者或sessions evening。</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FABBER" target="_blank" rel="noopener">FABBER</a> - dual-echo (ASL/BOLD) perfusion FMRI  双回波（ASL / BOLD）灌注FMRI</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BASIL" target="_blank" rel="noopener">BASIL</a> - quantitative resting perfusion analysis from perfusion ASL FMRI  灌注ASL FMRI定量静息灌注分析</p></li></ul><h4 id="Structural-MRI-结构导向"><a href="#Structural-MRI-结构导向" class="headerlink" title="Structural MRI 结构导向"></a>Structural MRI 结构导向</h4><ul><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/fsl_anat" target="_blank" rel="noopener">fsl_anat</a> - general anatomical pipeline script that calls most of the below tools to provide a “one-stop” comprehensive integrated structural analysis  常规解剖管道脚本，调用大多数工具，提供“一站式”综合结构分析</li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BET" target="_blank" rel="noopener">BET</a> - brain extraction 脑区域提取</li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FAST" target="_blank" rel="noopener">FAST</a> - tissue-type segmentation 脑组织分割</li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FIRST" target="_blank" rel="noopener">FIRST</a> - segmentation of subcortical structures  皮质下结构的分割</li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT" target="_blank" rel="noopener">FLIRT</a> &amp; <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FNIRT" target="_blank" rel="noopener">FNIRT</a> - linear and nonlinear registration  线性&amp;&amp;非线性校准  </li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSLVBM" target="_blank" rel="noopener">FSLVBM</a> - voxelwise analysis of multi-subject grey-matter density 多受试者灰质密度的体素分析</li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/SIENA" target="_blank" rel="noopener">SIENA &amp; SIENAX</a> - longitudinal and cross-sectional analysis of structural changes  结构变化的纵向()和横截面分析</li></ul><h4 id="Diffusion-MRI-面向DTI的功能"><a href="#Diffusion-MRI-面向DTI的功能" class="headerlink" title="Diffusion MRI 面向DTI的功能"></a>Diffusion MRI 面向DTI的功能</h4><ul><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT" target="_blank" rel="noopener">FDT</a> - diffusion MRI preprocessing, tensor fitting and tractography  弥散MRI预处理，张量拟合和纤维束成像</li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/TBSS" target="_blank" rel="noopener">TBSS</a> - voxelwise analysis of multi-subject diffusion MRI datasets  多受试者弥散MRI数据集的体素分析 </li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/eddy" target="_blank" rel="noopener">EDDY</a> - improved eddy-current and head motion correction for diffusion MRI datasets  弥散MRI数据集的涡流改善和头部运动校正</li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/topup" target="_blank" rel="noopener">TOPUP</a> - improved distortion correction for diffusion MRI datasets  弥散MRI数据集的失真校正改善</li></ul><h4 id="Perfusion-MRI-模拟器"><a href="#Perfusion-MRI-模拟器" class="headerlink" title="Perfusion MRI 模拟器"></a>Perfusion MRI 模拟器</h4><blockquote><p>用于生成逼真的模拟MRI和FMRI图像或时间序列</p></blockquote><ul><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BASIL" target="_blank" rel="noopener">BASIL</a> - perfusion quantification using arterial spin labelling  使用动脉自旋标记进行灌注量化</li></ul><h4 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h4><ul><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Randomise" target="_blank" rel="noopener">Randomise</a> - permutation-based inference, including TFCE  基于排列的推理，包括TFCE</li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/PALM" target="_blank" rel="noopener">PALM</a> - permutation-based inference, including multivariate and handling of surface data  基于排列的推理，包括多变量和表面数据的处理</li><li><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/DualRegression" target="_blank" rel="noopener">Dual Regression</a> - Generating subject-specific maps from a group-ICA  从ICA组生成特定于受试者的映射</li></ul><h4 id="Other-tools"><a href="#Other-tools" class="headerlink" title="Other tools"></a>Other tools</h4><ul><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSLeyes" target="_blank" rel="noopener">FSLeyes</a> - 3D/4D image, timeseries and surface viewer  3D / 4D图像，时间序列和表面查看器</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Nets" target="_blank" rel="noopener">Nets</a> - various tools relating to network/connectivity analysis  与网络/连通性分析有关的各种工具</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDR" target="_blank" rel="noopener">FDR</a> - false discovery rate thresholding  发现率的错误阈值</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Mm" target="_blank" rel="noopener">Mm</a> - mixture-model thresholding  混合模型阈值</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Cluster" target="_blank" rel="noopener">Cluster</a> - various cluster-based analyses including GRF-based cluster inference 各种基于群集（cluster）的分析，包括基于GRF的群集推理</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Fslutils" target="_blank" rel="noopener">Fslutils</a> - miscellaneous command-line programs (including fslmaths and fslstats)  其他命令行程序（包括fslmaths和fslstats）</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Atlases" target="_blank" rel="noopener">Atlases</a> - various atlases included with FSL  FSL附带的各种地图集(atlases)</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/SUSAN" target="_blank" rel="noopener">SUSAN</a> - nonlinear image noise reduction  非线性图像降噪</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FUGUE" target="_blank" rel="noopener">FUGUE</a> - fieldmap-based EPI distortion correction  基于场图(fieldmap)的EPI失真校正</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MCFLIRT" target="_blank" rel="noopener">MCFLIRT</a> - head-motion correction  头部运动校正</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLOBS" target="_blank" rel="noopener">FLOBS</a> - flexible haemodynamic basis functions  灵活的血液动力学基础功能</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Miscvis" target="_blank" rel="noopener">Miscvis</a> - programs for making summary pictures  制作摘要图片的程序</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/lesion_filling" target="_blank" rel="noopener">lesion_filling</a> - tool for filling lesion masks with non-lesion intensities  用于填充具有非病变强度的病变掩膜的工具</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/POSSUM" target="_blank" rel="noopener">POSSUM</a> - FMRI/MRI image simulator  FMRI / MRI图像模拟器</p><p>（用于理解MRI的面向物理的模拟扫描仪）包括用于脉冲序列生成，信号生成，噪声添加和图像重建的工具.</p></li><li><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Atlasquery" target="_blank" rel="noopener">Atlasquery</a> - tool for reporting atlas labels for masks or single coordinates  用于报告掩膜或单个坐标的图集（atlas）标签</p></li></ul><p><a href="http://fsl.fmrib.ox.ac.uk/fslcourse/" target="_blank" rel="noopener">FSL_Course</a></p><ul><li><a href="http://fsl.fmrib.ox.ac.uk/fslcourse/lectures/practicals/registration/index.html" target="_blank" rel="noopener"><strong>Brain Extraction, Registration, Motion Correction and EPI Distortion</strong></a></li><li><a href="http://fsl.fmrib.ox.ac.uk/fslcourse/lectures/practicals/seg_struc/index.html" target="_blank" rel="noopener"><strong>Segmentation &amp; Structural Statistics</strong>  (FAST, FIRST, SIENA, FSL-VBM)</a></li></ul><hr><h3 id="使用教程"><a href="#使用教程" class="headerlink" title="使用教程"></a>使用教程</h3><h4 id="核磁数据处理-FSL系列之bet命令"><a href="#核磁数据处理-FSL系列之bet命令" class="headerlink" title="核磁数据处理: FSL系列之bet命令"></a><a href="https://blog.csdn.net/happyhorizion/article/details/79630973" target="_blank" rel="noopener">核磁数据处理: FSL系列之bet命令</a></h4><blockquote><p>提取脑结构，即剔除颅骨</p></blockquote><h5 id="BET-Usage"><a href="#BET-Usage" class="headerlink" title="BET Usage"></a>BET Usage</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bet &lt;input&gt; &lt;output&gt; [options]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bet &lt;input_nifti_file&gt; &lt;output_nifti_file&gt; -f 0.5 -g 0</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/share/fsl/5.0/bin/bet /home/captain/Desktop/FSL/flirt/sample-001 /home/captain/Desktop/FSL/flirt/sample-001_brain -f 0.5 -g 0</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">options:</span><br><span class="line">-m</span><br><span class="line">-f 可以指定图像密度阈值, 默认是0.5</span><br><span class="line">-g (the gradient threshold)修正边缘没有去除的痕迹</span><br><span class="line">-R  效果优于 -m -f 0.5</span><br></pre></td></tr></table></figure><center class="half"><br>    <img src="/2019/07/15/FSL-Introduction/sample-001.png" width="200"><br>    <img src="/2019/07/15/FSL-Introduction/bet/sample-001_brain.png" width="200"><br></center><h4 id="核磁数据处理-FSL系列之脑组织分割—FAST"><a href="#核磁数据处理-FSL系列之脑组织分割—FAST" class="headerlink" title="核磁数据处理: FSL系列之脑组织分割—FAST"></a><a href="https://blog.csdn.net/happyhorizion/article/details/79573488" target="_blank" rel="noopener">核磁数据处理: FSL系列之脑组织分割—FAST</a></h4><blockquote><p>White matter、Gray matter、CSF</p></blockquote><p>FAST (FMRIB’s Automated Segmentation Tool)采用的算法是隐马尔科夫随机场模型,以及相关的EM算法(Expectation-Maximization).</p><h5 id="FAST-Usage"><a href="#FAST-Usage" class="headerlink" title="FAST Usage"></a>FAST Usage</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fast [options] [input_nifti_file] [output_nifti_file]</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[options]:</span><br><span class="line">-t &lt;n&gt; or --type=&lt;n&gt; : type of image (n=1 for T1, n=2 for T2, n=3 for PD)</span><br><span class="line">-n &lt;n&gt; or --class=&lt;n&gt; : number of tissue-type classes</span><br><span class="line">-H &lt;v&gt; or --Hyper=&lt;v&gt; : MRF beta value for main segmentation phase (increasing this gives spatially smoother segmentations) 主分割阶段的MRF beta值（增加此值可以提供空间更平滑的分段）</span><br><span class="line">-I &lt;n&gt; : Loop iterations during initial bias-field removal phase 在初始偏置场移除阶段期间循环迭代</span><br><span class="line">-l &lt;n&gt; | -l &lt;m&gt; or --lowpass=&lt;m&gt;: Bias field smoothing : n is iterations, m is FWHM in mm - rough guide is m ≅ 2 √n 偏置场平滑</span><br><span class="line">-o &lt;base&gt; or --out=&lt;base&gt; : basename for outputs</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> Partial volume maps  (Default)</span><br><span class="line">/usr/share/fsl/5.0/bin/fast -t 1 -n 3 -H 0.1 -I 4 -l 20.0 -o /home/captain/Desktop/7_30_fsl/sample-001_brain_fast /home/captain/Desktop/7_30_fsl/sample-001_brain</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> output the files as below:</span><br><span class="line">sample-001_brain_fast_mixeltype.nii.gz</span><br><span class="line">sample-001_brain_fast_pve_0.nii.gz</span><br><span class="line">sample-001_brain_fast_pve_1.nii.gz</span><br><span class="line">sample-001_brain_fast_pve_2.nii.gz</span><br><span class="line">sample-001_brain_fast_pveseg.nii.gz</span><br><span class="line">sample-001_brain_fast_seg.nii.gz</span><br></pre></td></tr></table></figure><center class="half"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_mixeltype.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_pve_0.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_pve_1.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_pve_2.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_pveseg.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_seg.png" width="100"><br></center><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> pure</span><br><span class="line">/usr/share/fsl/5.0/bin/fast -t 1 -n 3 -H 0.1 -I 4 -l 20.0 --nopve -o /home/captain/Desktop/7_30_fsl/sample-001_brain_fast /home/captain/Desktop/7_30_fsl/sample-001_brain</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> output the files as below:</span><br><span class="line">sample-001_brain_fast_seg.nii.gz</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> Binary segmentation: Also output one image per class</span><br><span class="line">/usr/share/fsl/5.0/bin/fast -t 1 -n 3 -H 0.1 -I 4 -l 20.0 -g --nopve -o /home/captain/Desktop/7_30_fsl/sample-001_brain_fast /home/captain/Desktop/7_30_fsl/sample-001_brain</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> output the files as below: (White matter、Gray matter、CSF)</span><br><span class="line">sample-001_brain_fast_seg.nii.gz</span><br><span class="line">sample-001_brain_fast_seg_0.nii.gz</span><br><span class="line">sample-001_brain_fast_seg_1.nii.gz</span><br><span class="line">sample-001_brain_fast_seg_2.nii.gz</span><br></pre></td></tr></table></figure><center class="half"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_seg.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_seg_0.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_seg_1.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_seg_2.png" width="100"><br></center><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> Restored input</span><br><span class="line">/usr/share/fsl/5.0/bin/fast -t 1 -n 3 -H 0.1 -I 4 -l 20.0 --nopve -B -o /home/captain/Desktop/7_30_fsl/sample-001_brain_fast /home/captain/Desktop/7_30_fsl/sample-001_brain</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> output the files as below:</span><br><span class="line">sample-001_brain_fast_restore.nii.gz</span><br><span class="line">sample-001_brain_fast_seg.nii.gz</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> Estimated Bias field</span><br><span class="line">/usr/share/fsl/5.0/bin/fast -t 1 -n 3 -H 0.1 -I 4 -l 20.0 --nopve -b -o /home/captain/Desktop/7_30_fsl/sample-001_brain_fast /home/captain/Desktop/7_30_fsl/sample-001_brain</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> output the files as below:</span><br><span class="line">sample-001_brain_fast_bias.nii.gz</span><br><span class="line">sample-001_brain_fast_seg.nii.gz</span><br></pre></td></tr></table></figure><center class="half"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_bias.png" width="150"><br>    <img src="/2019/07/15/FSL-Introduction/fast/sample-001_brain_fast_seg.png" width="150"><br></center><h4 id="皮下组织分割-—-FIRST"><a href="#皮下组织分割-—-FIRST" class="headerlink" title="皮下组织分割 — FIRST"></a><a href="https://blog.csdn.net/baidu_36669549/article/details/69753022" target="_blank" rel="noopener">皮下组织分割 — FIRST</a></h4><blockquote><p>ex. 海马体分割</p></blockquote><h5 id="FIRST-Usage"><a href="#FIRST-Usage" class="headerlink" title="FIRST Usage"></a>FIRST Usage</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fsl5.0-first -i &lt;input_nii&gt; -l &lt;input_mat&gt; -m /usr/share/fsl/data/first/models_336_bin/L_Hipp_bin.bmv -k &lt;output_L_path&gt;</span><br></pre></td></tr></table></figure><p>其中FLIRT依赖包：<a href="http://neuro.debian.net/install_pkg.html?p=fsl-atlases" target="_blank" rel="noopener">fsl-atlases</a> </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> fsl5.0-first -i '/home/captain/Desktop/FSL/first/sample-001_brain_flirt.nii.gz' -l '/home/captain/Desktop/FSL/first/sample-001_brain_flirt.mat' -m /usr/share/fsl/data/first/models_336_bin/L_Hipp_bin.bmv -k /home/captain/Desktop/FSL/first/L_Hipp</span><br><span class="line">create shapeModel </span><br><span class="line">done creating shapeModel </span><br><span class="line">0.00128597 -0.948526 0.229461 </span><br><span class="line">-0.0119911 -0.222583 -0.867114 </span><br><span class="line">0.895755 -0.0140377 0.0013288 </span><br><span class="line">NEw done imodes transform</span><br><span class="line">mode  0.972959</span><br><span class="line">D </span><br><span class="line">336 2196 2196 336</span><br><span class="line">open file /home/captain/Desktop/FSL/first/L_Hipp.vtk to save.</span><br><span class="line">succesfully opened file /home/captain/Desktop/FSL/first/L_Hipp.vtk to save.</span><br></pre></td></tr></table></figure><center class="half"><br>    <img src="/2019/07/15/FSL-Introduction/flirt/sample-001_brain_flirt.png" width="150"><br>    <img src="/2019/07/15/FSL-Introduction/first/L_Hipp.png" width="150"><br></center><h4 id="MRI数据处理-FSL线性配准flirt命令"><a href="#MRI数据处理-FSL线性配准flirt命令" class="headerlink" title="MRI数据处理: FSL线性配准flirt命令"></a><a href="https://blog.csdn.net/happyhorizion/article/details/79645421" target="_blank" rel="noopener">MRI数据处理: FSL线性配准flirt命令</a></h4><blockquote><p>仿射变换</p></blockquote><h5 id="FLIRT-Usage"><a href="#FLIRT-Usage" class="headerlink" title="FLIRT Usage"></a>FLIRT Usage</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flirt -in &lt;invol&gt; -ref /usr/share/fsl/5.0/data/standard/MNI152_T1_2mm_brain -out &lt;outvol&gt; -omat &lt;invol2refvol.mat&gt; -bins 256 -cost corratio -searchrx -90 90 -searchry -90 90 -searchrz -90 90 -dof 12  -interp trilinear</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/share/fsl/5.0/bin/flirt -in /home/captain/Desktop/FSL/flirt/sample-001_brain.nii.gz -ref /usr/share/fsl/5.0/data/standard/MNI152_T1_2mm_brain -out /home/captain/Desktop/FSL/flirt/sample-001_brain_flirt.nii.gz -omat /home/captain/Desktop/FSL/flirt/sample-001_brain_flirt.mat -bins 256 -cost corratio -searchrx -90 90 -searchry -90 90 -searchrz -90 90 -dof 12  -interp trilinear</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-in 指定的输入图像;</span><br><span class="line">-ref 指定的参考模板; ex. MNI152_T1_2mm_brain.nii.gz</span><br><span class="line">-omat 指定仿射变换(将输入图像配准到ref模板上用的)4*4的仿射矩阵; </span><br><span class="line">-out 指定的是完成仿射变换后的输出；</span><br><span class="line">-dof 自由度(degrees of freedom)</span><br></pre></td></tr></table></figure><center class="half"><br>    <img src="/2019/07/15/FSL-Introduction/bet/sample-001_brain.png" width="150"><br>  <img src="/2019/07/15/FSL-Introduction/flirt/sample-001_brain_flirt.png" width="150"><br></center><h4 id="MRI数据处理-FSL非线性配准命令fnirt"><a href="#MRI数据处理-FSL非线性配准命令fnirt" class="headerlink" title="MRI数据处理: FSL非线性配准命令fnirt"></a><a href="https://blog.csdn.net/happyhorizion/article/details/79665363" target="_blank" rel="noopener">MRI数据处理: FSL非线性配准命令fnirt</a></h4><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FNIRT/UserGuide" target="_blank" rel="noopener"><strong>FNIRT-UserGuide</strong></a> 内含<a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FNIRT/UserGuide#Tools_for_handling_warps" target="_blank" rel="noopener">Tools for handling warps</a></p><blockquote><p>为了<code>排除个体差异</code>, 进而使得个体间脑组织的对比得以实施, 常需要将不同的大脑配准到<code>公共的模板/空间</code>. 这样做是为了后续用TBSS等统计方法进行组间分析(compare fractional anisotropy or tissue composition). 配准算法根据适用的问题不同分为<code>线性</code>和<code>非线性</code>两种, flirt采用了线性配准算法, 所谓的线性配准就是采用坐标旋转\平移\缩放\剪切将两个图像匹配到一起.但是线性配准也有局限性,很多时候仅仅用线性配准是不够的.</p></blockquote><p>操作流程：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.bet获得去除颅骨的`my_betted_structural.nii.gz`文件（作为flirt指令的输入）；</span><br><span class="line">2.flirt 获得`my_affine_transf.mat`仿射变换的矩阵参数（作为fnirt指令的aff参数）；</span><br><span class="line">3.fnirt 获得`my_nonlinear_transf.nii.gz` 非线性变换输出(作为applywarp指令中的warp参数)，以及my_betted_structural_to_MNI152_T1_2mm.log；</span><br><span class="line">4.applywarp 获得`my_warped_structural.nii.gz` 完成 struct-&gt;MNI152 映射</span><br></pre></td></tr></table></figure><h5 id="FNIRT-Usage"><a href="#FNIRT-Usage" class="headerlink" title="FNIRT Usage"></a>FNIRT Usage</h5><p>For the following set of <code>&quot;standard tasks&quot;</code> the commands below are likely to work for you, provided that your data are of reasonable quality.</p><ul><li><strong>Registering T1-structural to MNI152</strong></li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> bet my_structural my_betted_structural  # output: my_betted_structural.nii.gz</span><br><span class="line"><span class="meta">$</span> flirt -ref $&#123;FSLDIR&#125;/data/standard/MNI152_T1_2mm_brain -in my_betted_structural -omat my_affine_transf.mat  # output: my_affine_transf.mat</span><br><span class="line"><span class="meta">$</span> fnirt --in=my_structural --aff=my_affine_transf.mat --cout=my_nonlinear_transf --config=T1_2_MNI152_2mm  # output: my_structural_to_MNI152_T1_2mm.log &amp;&amp; my_nonlinear_transf.nii.gz (about 3min)  # 非线性变换的输出</span><br><span class="line"><span class="meta">$</span> applywarp --ref=$&#123;FSLDIR&#125;/data/standard/MNI152_T1_2mm --in=my_structural --warp=my_nonlinear_transf --out=my_warped_structural # output: my_warped_structural.nii.gz  # 完成 struct-&gt;MNI152 映射</span><br></pre></td></tr></table></figure><center class="half"><br>      <img src="/2019/07/15/FSL-Introduction/fnirt/my_structural.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fnirt/my_betted_structural.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fnirt/my_nonlinear_transf_1.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fnirt/my_nonlinear_transf_2.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fnirt/my_nonlinear_transf_3.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fnirt/my_warped_structural.png" width="100"><br></center><p>When you specify <code>--config=my_file</code>, ‘i.e.’ without explicit path or extension, <code>fnirt</code> will search for <code>./my_file</code>, <code>./my_file.cnf</code>, <code>${FSLDIR}/etc/flirtsch/my_fil</code>e and <code>${FSLDIR}/etc/flirtsch/my_file.cnf</code> in that order and use the first one that is found.</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> bet my_structural my_betted_structural  # output: my_betted_structural.nii.gz</span><br><span class="line"><span class="meta">$</span> flirt -ref $&#123;FSLDIR&#125;/data/standard/MNI152_T1_2mm_brain -in my_betted_structural -omat my_affine_transf.mat  # output: my_affine_transf.mat</span><br><span class="line"><span class="meta">$</span> fnirt --in=my_betted_structural --aff=my_affine_transf.mat --cout=my_betted_nonlinear_transf --config=T1_2_MNI152_2mm  # output: my_betted_structural_to_MNI152_T1_2mm.log &amp;&amp; my_betted_nonlinear_transf.nii.gz (about 3min)</span><br><span class="line"><span class="meta">$</span> applywarp --ref=$&#123;FSLDIR&#125;/data/standard/MNI152_T1_2mm --in=my_betted_structural --warp=my_betted_nonlinear_transf --out=my_warped_betted_structural # output: my_warped_betted_structural.nii.gz</span><br></pre></td></tr></table></figure><center class="half"><br>    <img src="/2019/07/15/FSL-Introduction/fnirt/my_structural.png" width="100"><br>      <img src="/2019/07/15/FSL-Introduction/fnirt/my_betted_structural.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fnirt/my_betted_nonlinear_transf_1.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fnirt/my_betted_nonlinear_transf_2.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fnirt/my_betted_nonlinear_transf_3.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/fnirt/my_warped_betted_structural.png" width="100"><br></center><ul><li><strong>Registering functional data (or any non-T1 image data) to MNI152 (via structural scan)</strong></li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> bet my_structural my_betted_structural</span><br><span class="line"><span class="meta">$</span> flirt -ref my_betted_structural -in my_functional -dof 6 -omat func2struct.mat</span><br><span class="line"><span class="meta">$</span> flirt -ref $&#123;FSLDIR&#125;/data/standard/MNI152_T1_2mm_brain -in my_betted_structural -omat my_affine_transf.mat</span><br><span class="line"><span class="meta">$</span> fnirt --in=my_structural --aff=my_affine_transf.mat --cout=my_nonlinear_transf --config=T1_2_MNI152_2mm</span><br><span class="line"><span class="meta">$</span> applywarp --ref=$&#123;FSLDIR&#125;/data/standard/MNI152_T1_2mm --in=my_functional --warp=my_nonlinear_transf --premat=func2struct.mat --out=my_warped_functional</span><br></pre></td></tr></table></figure><p>Only add the <code>func2struct.mat</code>的求取及应用过程</p><ul><li><strong>Registering FA-image to FMRIB58</strong></li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> flirt -ref $&#123;FSLDIR&#125;/data/standard/FMRIB58_FA_1mm_brain -in my_FA -omat my_affine_transf.mat</span><br><span class="line"><span class="meta">$</span> fnirt --in=my_FA --aff=my_affine_transf.mat --cout=my_nonlinear_transf --config=FA_2_FMRIB58_1mm</span><br><span class="line"><span class="meta">$</span> applywarp --ref=$&#123;FSLDIR&#125;/data/standard/FMRIB58_FA_1mm_brain --in=my_FA --warp=my_nonlinear_transf --out=my_warped_FA</span><br></pre></td></tr></table></figure><p><strong>List of parameters</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> Parameters that specify input files</span><br><span class="line">--in=filename : Name of file with images you want to register. E.g. my_brain.nii</span><br><span class="line">--ref=filname : Name of a file that defines the target space. E.g. MNI152_T1_2mm.nii.</span><br><span class="line">--config=config_file : Name of text-file with parameter settings. If you read nothing else, read this. </span><br><span class="line">--aff=mat_fname : Name of text-file with affine starting guess.`Typically the output from flirt.`</span><br><span class="line"><span class="meta">#</span> Parameters specifying names of output-files</span><br><span class="line">--cout=filename : Name of output-file containing the `coefficients`(相关系数) that determine the warp-field.</span><br><span class="line"><span class="meta">#</span> Parameters that are are specified "once and for all"</span><br><span class="line">--warpres=xres,yres,zres : Resolution of warps (in mm). E.g. 10,10,10</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--premat=filename : Specifies an affine transform that should be applied to the data prior to the non-linear warping. 指定应在非线性变形之前应用于数据的仿射变换；</span><br></pre></td></tr></table></figure><p><strong>理解</strong></p><ul><li><p>Step 0: Registering your functional data to the structural scan doing <em>e.g</em>. （FLIRT的作用）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> flirt -ref struct.nii -in func.nii -omat func2struct.mat -dof 6</span><br></pre></td></tr></table></figure><p>Where the text-file func2struct.mat now contains the <code>rigid</code>(刚性) body transform that maps <code>func.nii</code>onto <code>struct.nii</code>.   <code>FLIRT，即刚性校准</code> </p></li><li><p>Step 1: Get an intial affine transform mapping struct.nii onto the MNI152 template.</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> bet struct.nii betted_struct.nii </span><br><span class="line"><span class="meta">$</span> flirt -ref MNI152_T1_2mm_brain.nii -in betted_struct.nii -omat aff_struct2mni.mat</span><br></pre></td></tr></table></figure></li><li><p>Step 2: Use that as initial guess for fnirt</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> fnirt --ref=MNI152_T1_2mm.nii --in=struct.nii --aff=aff_struct2mni.mat ... --cout=warp_struct2mni.nii</span><br></pre></td></tr></table></figure></li><li><p>Step 3: And then we use that to resample the functional scan into the MNI152 space.</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> applywarp --ref=MNI152_T1_2mm.nii --in=func.nii --out=funcInMNI.nii --warp=warp_struct2mni.nii --premat=func2struct.mat</span><br></pre></td></tr></table></figure><p>We are now feeding applywarp information both about the <code>struct-&gt;MNI152</code> mapping and about the <code>func-&gt;struct</code> mapping allowing it to map <code>from func to MNI152</code> in a single step.</p></li></ul><p><strong>Questions：</strong></p><ul><li><p>what’s the difference between <code>func.nii</code> and <code>struct.nii</code> ?</p><ul><li><p>根据官方样例数据集猜测:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func.nii — PET (64, 64, 45); 3*3*3 Millmeters; Series Length:380   aMRI?</span><br><span class="line">struct.nii — MRI (192, 256, 192); 1.016*1.016*1 Millmeters; Series Length:1  fMRI?</span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>$ fnirt --in=my_structural --aff=my_affine_transf.mat --cout=my_nonlinear_transf --config=T1_2_MNI152_2mm</code> why <code>--in=my_structural</code> not <code>--in=my_betted_structural</code></p></li></ul><h4 id="磁共振影像分析之-基于FSL的VBM分析-1-2"><a href="#磁共振影像分析之-基于FSL的VBM分析-1-2" class="headerlink" title="磁共振影像分析之: 基于FSL的VBM分析(1) (2)"></a>磁共振影像分析之: 基于FSL的VBM分析<a href="https://blog.csdn.net/happyhorizion/article/details/79886554" target="_blank" rel="noopener">(1)</a> <a href="https://blog.csdn.net/happyhorizion/article/details/80529235" target="_blank" rel="noopener">(2)</a></h4><blockquote><p>VBM (voxel-based morphometry )将每个人的大脑都形成一个<code>模板</code>, 避免了人与人之间较大的解剖学差异, 并将结果经过平滑处理, 使得每个体素都代表它和邻居的平均值, 然后<code>对不同个体间进行体素水平的比较</code>.</p></blockquote><p>VBM可以在体素水平上定量检测出脑组织的密度和体积,反映不同群体或者个体局部脑区的脑组织成分与特征的差异. 目前, VBM已经广泛应用与描绘大脑结构的细微变化. </p><p>VBM的预处理流程可以大致总结为:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1) 从原始图像产生平均模板，也就是subject-specific模板 </span><br><span class="line">2) 配准：将所有的原始图像通过仿射变换配准到1）生成的模板上 </span><br><span class="line">3) 组织分割：灰质，白质和脑脊液 --- fast模块</span><br><span class="line">4) 保持体积调制(modulation)</span><br><span class="line">5) 平滑处理, 用各向同性的高斯核 </span><br><span class="line">6) 统计分析</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 以正确的格式准备T1图像，并使用randomise manual创建design.mat和design.con</span><br><span class="line">2. fslvbm_1_bet  - 对所有T1图像进行脑提取 ➜ *_brain.nii.gz</span><br><span class="line">3. fslvbm_2_template  - 创建特定于研究的对称灰质模板 ➜ template_4D_GM.nii.gz</span><br><span class="line">4. fslvbm_3_proc  - 将所有灰质图像注册到模板，使用不同的内核大小调整和平滑它们，</span><br><span class="line">   最后运行初始GLM分析进行定性评估（即统计分析组间差异） ➜ GM_mod_merg.nii.gz</span><br><span class="line">5. randomize -使用置换（permutation）测试进行体素GLM分析 ➜ *_tfce_corrp_tstat1.nii.gz</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 以正确的格式准备T1图像，并使用randomise manual创建design.mat和design.con</span><br><span class="line">2. fslvbm_1_bet  - 对所有T1图像进行脑提取</span><br><span class="line">   [option] 默认使用参数-b；当图像中出现大量脖颈部分时，使用参数-n</span><br><span class="line"> [output] BET -- *_brain.nii.gz</span><br><span class="line">3. fslvbm_2_template  - 创建特定于研究的对称灰质模板</span><br><span class="line"> [option] 基于灰质映射到ICBM-152模板的仿射校准生成模板时，使用参数-a；</span><br><span class="line">          基于非线性校准生成模板时，使用参数-n</span><br><span class="line"> [output] struc/</span><br><span class="line">          FAST -- Partial volume maps  (Default); *_GM.nii.gz</span><br><span class="line">   FLIRT -- *_GM_to_T.mat、*_GM_to_T.nii.gz;</span><br><span class="line">            *_GM_to_T_init.mat；*_GM_to_template_GM_init.log</span><br><span class="line">   FNIRT -- *_GM_to_T_init.nii.gz；</span><br><span class="line">         *_GM_to_T_init_warp.msf、*_GM_to_T_init_warp.nii.gz</span><br><span class="line">      -- template_GM_init、template_GM ➜ template_GM_flipped ➜ 'template_4D_GM' ？？(view 源码)       </span><br><span class="line">4. fslvbm_3_proc  - 将所有灰质图像注册到模板，使用不同的内核大小调整和平滑它们，</span><br><span class="line">   最后运行初始GLM分析进行定性评估（即统计分析组间差异） # how to apply...</span><br><span class="line">   [output] stats/</span><br><span class="line">     fslvbm3b*;`GM_mask`(二值化); GM_merge; template_GM(有颅骨阴影)</span><br><span class="line">       GM_mod_merg,</span><br><span class="line">       GM_mod_merg_s2, GM_mod_merg_s2_tstat1, # tstats映射帮助确定用于randomise的平滑值  how...</span><br><span class="line">      'GM_mod_merg_s3', GM_mod_merg_s3_tstat1,</span><br><span class="line">            GM_mod_merg_s4, GM_mod_merg_s4_tstat1 # _s*,数值越大，图像越模糊</span><br><span class="line">    [inter] struc/</span><br><span class="line">          *_GM_to_template_GM.log、*_GM_to_template_GM.mat、*_GM_to_template_GM.nii.gz; </span><br><span class="line">            *_GM_to_template_GM_mod.nii.gz；</span><br><span class="line">            *_GM_to_template_GM_warp.msf、*_GM_to_template_GM_warp.nii.gz；</span><br><span class="line">            *_JAC_nl.nii.gz.</span><br><span class="line"></span><br><span class="line">5. randomise -使用置换（permutation）测试进行体素GLM分析  # how to apply...</span><br><span class="line"> [output] stats/'GM_mod_merg_s3_tfce_corrp_tstat1.nii'</span><br></pre></td></tr></table></figure><center class="half"><br>    <img src="/2019/07/15/FSL-Introduction/vbm/vbm_data.png" height="200"><br>      <img src="/2019/07/15/FSL-Introduction/vbm/vbm_data_struc.png" height="200"><br>    <img src="/2019/07/15/FSL-Introduction/vbm/template_GM.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/vbm/GM_mod_merg_s3.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/vbm/GM_mod_merg_s3_tstat1.png" width="100"><br>    <img src="/2019/07/15/FSL-Introduction/vbm/GM_mod_merg_s3_tfce_corrp_tstat1.png" width="100"><br></center><h5 id="FSLVBM-Usage"><a href="#FSLVBM-Usage" class="headerlink" title="FSLVBM Usage"></a>FSLVBM Usage</h5><p><strong>A - Prepare your data for the FSL-VBM study</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ for g in con_1623.nii.gz con_2304.nii.gz con_2878.nii.gz con_3456.nii.gz con_3641.nii.gz con_3642.nii.gz con_3668.nii.gz con_3670.nii.gz pat_1433.nii.gz pat_1650.nii.gz pat_1767.nii.gz pat_2042.nii.gz pat_2280.nii.gz pat_2632.nii.gz pat_2662.nii.gz pat_2996.nii.gz; do</span><br><span class="line">&gt; echo $g &gt;&gt; template_list</span><br><span class="line">&gt; done</span><br></pre></td></tr></table></figure><p>生成template_list文件⤴️</p><p>It’s a good idea to consider your cross-subject statistical model <strong>before</strong> you run the FSL-VBM analysis. So you should at this point create your <code>design.mat</code> and <code>design.con</code> in your FSL-VBM directory; see the <a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Randomise" target="_blank" rel="noopener">randomise manual</a>.⤵️</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">design_ttest &lt;output design name&gt; &lt;size of first group&gt; &lt;size of second group&gt;</span><br></pre></td></tr></table></figure><p>比如现在对照组con_*有10项数据  病患组有pat_* 8项数据，使用<code>design_ttest2 design 10 8</code>指令，即会生成design.mat、desgin.con文件</p><ul><li><p>design.mat</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/NumWaves 2</span><br><span class="line">/NumPoints 18</span><br><span class="line">/PPheights 1 1</span><br><span class="line">/Matrix</span><br><span class="line">1 0</span><br><span class="line">1 0</span><br><span class="line">1 0</span><br><span class="line">1 0</span><br><span class="line">1 0</span><br><span class="line">1 0</span><br><span class="line">1 0</span><br><span class="line">1 0</span><br><span class="line">1 0</span><br><span class="line">1 0</span><br><span class="line">0 1</span><br><span class="line">0 1</span><br><span class="line">0 1</span><br><span class="line">0 1</span><br><span class="line">0 1</span><br><span class="line">0 1</span><br><span class="line">0 1</span><br><span class="line">0 1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/NumWaves2</span><br><span class="line">/NumPoints18</span><br><span class="line">/PPheights1.000000e+001.000000e+00</span><br><span class="line"></span><br><span class="line">/Matrix</span><br><span class="line">1.000000e+000.000000e+00</span><br><span class="line">1.000000e+000.000000e+00</span><br><span class="line">1.000000e+000.000000e+00</span><br><span class="line">1.000000e+000.000000e+00</span><br><span class="line">1.000000e+000.000000e+00</span><br><span class="line">1.000000e+000.000000e+00</span><br><span class="line">1.000000e+000.000000e+00</span><br><span class="line">1.000000e+000.000000e+00</span><br><span class="line">1.000000e+000.000000e+00</span><br><span class="line">1.000000e+000.000000e+00</span><br><span class="line">0.000000e+001.000000e+00</span><br><span class="line">0.000000e+001.000000e+00</span><br><span class="line">0.000000e+001.000000e+00</span><br><span class="line">0.000000e+001.000000e+00</span><br><span class="line">0.000000e+001.000000e+00</span><br><span class="line">0.000000e+001.000000e+00</span><br><span class="line">0.000000e+001.000000e+00</span><br><span class="line">0.000000e+001.000000e+00</span><br></pre></td></tr></table></figure></li><li><p>desgin.con</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/NumWaves 2</span><br><span class="line">/NumContrasts 2</span><br><span class="line">/PPheights 1 1</span><br><span class="line">/Matrix</span><br><span class="line">1 -1</span><br><span class="line">-1 1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/ContrastName1con &gt; pat</span><br><span class="line">/NumWaves2</span><br><span class="line">/NumContrasts1</span><br><span class="line">/PPheights1.000000e+00</span><br><span class="line">/RequiredEffect2.819</span><br><span class="line"></span><br><span class="line">/Matrix</span><br><span class="line">1.000000e+00 -1.000000e+00</span><br></pre></td></tr></table></figure></li></ul><p><strong>B - Extracting brain information: fslvbm_1_bet</strong></p><blockquote><p>脑组织提取</p></blockquote><ul><li><p>get <code>default</code> BET behaviour</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fslvbm_1_bet -b</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>if your images include a lot of neck (which most of the time confounds混淆 the BET preprocessing).</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fslvbm_1_bet -N</span><br></pre></td></tr></table></figure></li></ul><p><strong>C - Creating the template: fslvbm_2_template</strong></p><blockquote><p>fast &amp;&amp; flirt &amp;&amp; fnirt  ➜ 创建特定于研究的对称灰质模板</p></blockquote><ul><li><p>create a template based on an <code>affine registration</code> of GM images to the GM ICBM-152 template (After flirt)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fslvbm_2_template -a</span><br></pre></td></tr></table></figure></li><li><p>create a template based on a <code>non-linear registration</code> (After fnirt)</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fslvbm_2_template -n</span><br></pre></td></tr></table></figure></li></ul><p>  Once this is completed, <strong>CHECK</strong> the <code>&quot;template_GM_4D&quot;</code> image in <code>struc</code> with the movie loop in fslview.</p><p><strong>D - Processing the native GM images: fslvbm_3_proc</strong></p><blockquote><p>将所有灰质图像注册到模板，使用不同的内核大小调整和平滑它们 ➜ 运行初始GLM分析进行定性评估（统计分析组间差异）</p></blockquote><ul><li><p>By default fslvbm_3_proc concatenates the images in alphabetical order (following the names that they started with); make sure this matches the subject ordering assumed in your design.mat model.</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fslvbm_3_proc</span><br></pre></td></tr></table></figure><p>Please do not forget the final <strong>CHECK</strong> of the 4D image of modulated registered GM images <code>&quot;GM_mod_merg&quot;</code> using the movie loop in fslview.</p><p><code>$FSLDIR/bin/fslvbm_3_proc. shell</code>脚本中, 采用cat命令配合EOF符号生成了另两个脚本: fslvbm3a 和fslvbm3b . 值得一提的是, 这里采用<code>fsl_sub</code>命令调用并行计算进行计算加速. 但是这个命令是基于<code>Sun grid cluster</code>的, 所以对于普通的台式机, 或者工作站, 建议将这一步去掉, <code>直接运行fslvbm3a或者fslvbm3b</code>.</p></li></ul><p><strong>E - Obtaining and displaying your FSL-VBM results</strong></p><blockquote><p>randomise 使用置换（permutation）测试进行体素GLM分析</p></blockquote><ul><li><p><strong>E1 - Running randomise and displaying TFCE-based thresholding results</strong></p><p>1.为基于TFCE的分析选择最合适的平滑（例如，sigma = 3mm）。 如果要应用与已应用的平滑不同的平滑，可以使用以下方法（例如，sigma = 3.5mm）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fslmaths GM_mod_merg -s 3.5 GM_mod_merg_s3.5</span><br></pre></td></tr></table></figure><p>2.选择了最合适的平滑（例如sigma = 3mm）后，运行randomise，例如：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">randomise -i GM_mod_merg_s3 -m GM_mask -o fslvbm -d design.mat -t design.con -T -n 5000</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">randomise -i &lt;4D_input_data&gt; -o &lt;output_rootname&gt; -d design.mat -t design.con -m &lt;mask_image&gt; -n 500 -D -T</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">参数说明：</span><br><span class="line">-D 如果没有对设计矩阵中的均值进行建模，需要附加此参数，在执行randomise前demean数值</span><br><span class="line">-T 即使用TFCE（无阈值集群增强，threshold-free cluster Enhancement）</span><br><span class="line">-V 设置方差平滑的sigma数值</span><br></pre></td></tr></table></figure><p>终端推荐指令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">randomise -i GM_mod_merg_s3 -o GM_mod_merg_s3 -m GM_mask -d design.mat -t design.con -n 5000 -T -V</span><br></pre></td></tr></table></figure><p>3.可以在FSLView中查看（1-p）校正的p值图像：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fslview $FSLDIR/data/standard/MNI152_T1_2mm fslvbm_tfce_corrp_tstat1 -l Red-Yellow -b 0.949,1</span><br></pre></td></tr></table></figure></li><li><p><strong>E2 - Running randomise and displaying cluster-based thresholding results</strong></p><blockquote><p>不推荐此方式</p></blockquote><p>1.一旦为基于集群的校正选择了最合适的平滑（例如sigma = 3mm）和阈值（例如t&gt; 2.3），然后将它们输入完整的randomise运行，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">randomise -i GM_mod_merg_s3 -m GM_mask -o fslvbm -d design.mat -t design.con -c 2.3 -n 5000</span><br></pre></td></tr></table></figure><p>2.将“_clustere_corrp_”图像（校正的p值映射）设置为0.95，以仅保留重要的集群并使用它来屏蔽相应的tstats映射：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fslmaths fslvbm_clustere_corrp_tstat1 -thr 0.95 -bin mask_pcorrected</span><br><span class="line">fslmaths fslvbm_tstat1 -mas mask_pcorrected fslvbm_tstat1_corrected</span><br></pre></td></tr></table></figure><p>3.fslview展示·<code>template_GM</code>或MNI152模板，例如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fslview $FSLDIR/data/standard/MNI152_T1_2mm fslvbm_tstat1_corrected -l Red-Yellow -b 2.3,4</span><br></pre></td></tr></table></figure></li></ul><h4 id="SIENA-amp-SIENAX"><a href="#SIENA-amp-SIENAX" class="headerlink" title="SIENA &amp; SIENAX"></a><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/SIENA" target="_blank" rel="noopener">SIENA &amp; SIENAX</a></h4><h5 id="SIENA-Two-Time-Point-Estimation"><a href="#SIENA-Two-Time-Point-Estimation" class="headerlink" title="SIENA: Two-Time-Point Estimation"></a>SIENA: Two-Time-Point Estimation</h5><p>SIENA是针对大脑变化的单时间点（“横截面”）和双时间点（“纵向”）分析的包，特别是萎缩（脑组织的体积损失）的估计。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">"Siena估计"在`不同时间点拍摄同一受试者的两个输入图像之间的脑容量变化百分比` (PBVC, percentage brain volume change) .它调用一系列FSL程序从两个图像中剥离非脑组织，记录两个大脑（在注册期间头骨用于保持缩放常数的约束下）并分析两次之间的大脑变化点。还可以以允许多主体体素统计测试的方式将体素萎缩测量投射到标准空间中。 FSL5提供了心室分析的扩展。</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">SIENA。用SIENA [Smith 2001，Smith 2002]（FSL [Smith 2004]的一部分）`估计两时间点百分比脑容量变化`。 SIENA首先从两个时间点的全头输入数据中提取脑部和颅骨图像[Smith 2002b]。然后将两个大脑图像彼此对齐[Jenkinson 2001，Jenkinson 2002]（使用颅骨图像来约束配准缩放）;两个脑图像都被重新采样到两者之间的空间。接下来，进行组织类型分割[Zhang 2001]以找到脑/非脑边缘点，然后在这些边缘点估计垂直边缘位移（在两个时间点之间）。最后，将平均边缘位移转换为两个时间点之间脑容量变化百分比的（全局）估计。</span><br></pre></td></tr></table></figure><center class="half"><br>    <img src="/2019/07/15/FSL-Introduction/siena/siena.png" height="233"><br></center><h5 id="SIENA-Usage"><a href="#SIENA-Usage" class="headerlink" title="SIENA Usage"></a>SIENA Usage</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">siena &lt;input1&gt; &lt;input2&gt;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-o &lt;output-dir&gt; : 设置输出目录（默认输出为&lt;input1&gt; _to_ &lt;input2&gt; _siena）</span><br><span class="line">-d : debug（不删除中间文件）</span><br><span class="line">-B "bet options" :如果您想更改下注默认值，请在使用-B标志后将BET选项置于双引号内。</span><br><span class="line">                  例如，要增加大脑估计的大小，请使用：-B"-f 0.3" (-f &lt;threshold&gt;)</span><br><span class="line">-2 : 两类分割（不分别分割灰色和白色物质） - 如果灰度/白色对比度较差，请使用此功能</span><br><span class="line">-t2: 告诉FAST输入图像是T2加权的而不是T1</span><br><span class="line">-m : 使用`标准空间`掩膜（如果证明难以从BET获得可靠的大脑分割，例如，如果眼睛很难分割出来）</span><br><span class="line">     - 注册到标准空间以使用预定义标准空间脑部掩膜</span><br><span class="line">-t &lt;t&gt;: 在MNI152 / Talairach空间中从t(mm)向上忽略 </span><br><span class="line">        - 如果你需要忽略头部的顶部（例如，如果某些受试者缺少顶部并且你需要在受试者间保持一致）</span><br><span class="line">-b &lt;b&gt;: 在MNI152 / Talairach空间中向下忽略b(mm); b应该是-ve</span><br><span class="line">-S "siena_diff options" : 如果要向siena_diff程序发送选项（估计两个对齐图像之间的变化），</span><br><span class="line">                          请在-S标志之后将这些选项放在双引号中。</span><br><span class="line">                          例如，要告诉siena_diff以更多的迭代次数运行FAST分段，请使用-S"-s -i 20"</span><br><span class="line">-V                    : 执行心室分析VIENA</span><br><span class="line">-v &lt;mask image&gt;       : 可选的用户提供的心室掩膜(默认为$FSLDIR/bin/MNI152_T1_2mm_VentricleMask)</span><br></pre></td></tr></table></figure><h5 id="SIENA-Step"><a href="#SIENA-Step" class="headerlink" title="SIENA Step"></a>SIENA Step</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.bet on the two input images(提取脑组织) ➜ 提取的大脑，二元脑掩模和颅骨图像</span><br><span class="line"></span><br><span class="line">2.siena_flirt(对两个脑图像配准)：pairreg → 将最终变换解构为两个中间变换，这两个变换将两个脑图像放入两者之间的空间中(均匀化)，这样它们都会遭受相同数量的插值相关模糊 </span><br><span class="line">➜ 显示配准质量的多切片gif图片(其中一个转换图像作为背景，另一个转换图像的边缘叠加为红色)</span><br><span class="line"></span><br><span class="line">3.0 siena_cal(为了略微提高siena_diff程序的准确性，执行此自校准脚本)：在其中一个输入图像上相对于其自身的缩放版本运行siena_diff，其中缩放是预先确定的</span><br><span class="line"></span><br><span class="line">3.1 siena_diff(对配准的脑部图像进行变化分析)：将组织分割fast应用于第一个脑图像。在报告为脑和非脑（包括内部脑 - 脑脊液边界）之间的边界的所有点处，计算脑表面在两个时间点之间移动的距离。</span><br><span class="line">基于两个1D向量的子体素相关（匹配）来计算脑边缘的这种运动（垂直于局部边缘）;这些是从3D图像中获取的，表面点两侧的固定距离，并垂直于它，并且在相关之前进行区分，允许两个原始图像中的一些变化。计算平均垂直表面运动并转换为PBVC。</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">为了在`平均垂直边缘运动`和`PBVC`之间进行这种转换，有必要假设真实脑表面区域，估计边缘点数量和真实脑容量之间存在某种关系。</span><br><span class="line">可以针对一般图像估计该数量，但是将根据切片厚度，图像序列类型等而变化，从而在最终PBVC中导致小的缩放误差。</span><br><span class="line">为了纠正这个问题，应用自校准，其中siena调用siena_cal。该脚本在其中一个输入图像上相对于其自身的缩放版本运行siena_diff，其中缩放是预先确定的（因此已知）。</span><br><span class="line">因此，`最终的PBVC是预先已知的`，并且可以将估计值与此进行比较以获得当前图像的`校正因子`。这是针对输入图像和平均值进行的，以给出要输入siena_diff的校正因子。</span><br></pre></td></tr></table></figure><h5 id="SIENA-Output"><a href="#SIENA-Output" class="headerlink" title="SIENA Output"></a>SIENA Output</h5><p>在SIENA输出目录中创建的文件是：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">report.siena # SIENA日志，包括最终的PBVC估计</span><br><span class="line">report.html  # 网页报告，包括显示分析的各个阶段的图像，最终结果和SIENA方法的描述</span><br><span class="line">A_halfwayto_B_render  # 叠加在halfway A图像上的边缘运动的彩色渲染图像</span><br><span class="line">                      # 红黄色表示脑容量增加，蓝色表示脑容量减少（“萎缩”）</span><br><span class="line">A_and_B.gif  # 显示配准结果的gif图像，使用一个变换图像作为背景，另一个作为彩色边缘前景</span><br><span class="line">A_to_B.mat   # 使用脑和颅骨图像, 将A转换为B</span><br><span class="line">B_to_A.mat   # 使用脑和颅骨图像, 将B转换为A</span><br><span class="line">A_halfwayto_B.mat和B_halfwayto_A.mat  # 将图像转换到中途位置的转换</span><br></pre></td></tr></table></figure><p>心室扩展 -  VIENA（在FSL5中，引入了由-V选项调用的心室选项）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">siena/viena/</span><br><span class="line">reportviena.html  心室报告</span><br></pre></td></tr></table></figure><h5 id="SIENAX-Single-Time-Point-Estimation"><a href="#SIENAX-Single-Time-Point-Estimation" class="headerlink" title="SIENAX: Single-Time-Point Estimation"></a>SIENAX: Single-Time-Point Estimation</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sienax根据单个图像`估算脑组织总体积`，并根据颅骨大小进行标准化。它称之为一系列FSL程序：它首先剥离非脑组织，然后使用大脑和颅骨图像来估计受试者图像和标准空间之间的比例。然后，它运行组织分割以估计脑组织的体积，并将其乘以估计的比例因子，以减少受试者之间的头部尺寸相关的变异性。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SIENAX。使用SIENAX [Smith 2001，Smith 2002]（FSL的一部分[Smith 2004]）`估计针对受试者头部尺寸标准化的脑组织体积`。 SIENAX首先从单个全头输入数据中提取脑部和颅骨图像[Smith 2002b]。然后将脑图像仿射登记到MNI152空间[Jenkinson 2001，Jenkinson 2002]（使用颅骨图像确定登记缩放）;这主要是为了获得体积比例因子，用作头部尺寸的标准化。接下来，进行具有部分体积估计的组织型分割[Zhang 2001]以计算脑组织的总体积（包括灰质，白质，外周灰质和心室脑脊液的体积的单独估计）。</span><br></pre></td></tr></table></figure><center class="half"><br>    <img src="/2019/07/15/FSL-Introduction/sienax/sienax.png" height="233"><br></center><h5 id="SIENAX-Usage"><a href="#SIENAX-Usage" class="headerlink" title="SIENAX Usage"></a>SIENAX Usage</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sienax &lt;input&gt;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-o &lt;output-dir&gt; : set output directory (the default output is &lt;input&gt;_sienax)</span><br><span class="line">-d : debug (don't delete intermediate files)</span><br><span class="line">-B "bet options" : if you want to change the BET defaults, put BET options inside double-quotes after using the -B flag. </span><br><span class="line">                   For example, to increase the size of brain estimation, use: -B "-f 0.3"</span><br><span class="line">-2: two-class segmentation (don't segment grey and white matter separately) ;</span><br><span class="line">    use this if there is poor grey/white contrast</span><br><span class="line">-t2: tell FAST that the input images are T2-weighted and not T1</span><br><span class="line">-t &lt;t&gt;: ignore from t (mm) upwards in MNI152/Talairach space; </span><br><span class="line">        if you need to ignore the top part of the head (e.g. if some subjects have the top missing and you need consistency across subjects)</span><br><span class="line">-b &lt;b&gt;: ignore from b (mm) downwards in MNI152/Talairach space; </span><br><span class="line">        b should probably be -ve</span><br><span class="line">-r: tell SIENAX to estimate "regional" volumes as well as global; this produces peripheral cortex GM volume (3-class segmentation only) and ventricular CSF volume 告诉SIENAX估计“区域”数量如同全局数量;这会产生外周皮质GM体积（仅3级分割）和心室脑脊液体积</span><br><span class="line">-lm &lt;mask&gt;: use a lesion (or lesion+CSF) mask to remove incorrectly labelled "grey matter" voxels 使用病变（或病变+ CSF）掩膜去除错误标记的“灰质”体素</span><br><span class="line">-S "FAST options" : if you want to change the segmentation defaults, put FAST options inside double-quotes双引号） after using the -S flag. For example, to increase the number of segmentation iterations use: -S "-i 20"</span><br></pre></td></tr></table></figure><h5 id="SIENAX-Step"><a href="#SIENAX-Step" class="headerlink" title="SIENAX Step"></a>SIENAX Step</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. bet on the single input image ➜ extracted brain, and the skull image. If you need to call BET with a different threshold than the default of 0.5, use -f &lt;threshold&gt;.</span><br><span class="line">2. pairreg (which uses the brain and skull images to carry out constrained registration)</span><br><span class="line">MNI152标准脑部是目标(reference).因此，与两时间点萎缩一样，大脑被配准（这次指向标准大脑），再次`使用头骨作为缩放约束`.因此，脑组织体积估计将相对于“标准化”头骨尺寸.</span><br><span class="line">（忽略"Warning"：难以在直方图中找到强大的限制”消息;这是因为FLIRT对头骨图像的不寻常直方图不太满意，但在这种情况下无需担心。）</span><br><span class="line">请注意，以后的所有步骤都是实际上是对`原始（经过剥离的）输入图像`进行的，而不是配准后的输入图像;这样就不需要重新采样原始图像（重采样会引入模糊）。</span><br><span class="line">相反，为了利用上述归一化("标准化")，在报告为最终归一化脑容量之前，通过从归一化变换导出的`缩放因子`来缩放脑容积</span><br><span class="line">3. 标准脑图像掩模（源自MNI152并略微扩张）被`转换成原始图像空间`（通过反转上述发现的归一化变换）并应用于脑图像。这有助于确保原始大脑提取不包括诸如眼球等伪影。</span><br><span class="line">现在使用fast在大脑掩膜上运行分割。如果存在合理的灰白对比度，则单独报告灰质和白质体积，以及总脑容量（这是默认行为）。</span><br><span class="line">否则（即，如果使用-2选项调用sienax），则仅执行脑/CSF/背景分割，并且仅报告脑容量。</span><br><span class="line">在报告之前，如上所述，通过标准化比例因子来缩放所有体积，以便相对于标准化的头骨尺寸报告所有受试者的体积("归一化")。</span><br></pre></td></tr></table></figure><h5 id="SIENAX-Output"><a href="#SIENAX-Output" class="headerlink" title="SIENAX Output"></a>SIENAX Output</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">report.sienax # SIENAX日志，包括最终的卷估计</span><br><span class="line">report.html   # 网页报告，包括显示分析的各个阶段的图像，最终结果和SIENAX方法的描述</span><br><span class="line">I_render      # 颜色渲染图像，显示叠加在原始图像顶部的分割输出</span><br></pre></td></tr></table></figure><h5 id="Voxelwise-SIENA-Statistics"><a href="#Voxelwise-SIENA-Statistics" class="headerlink" title="Voxelwise SIENA Statistics"></a>Voxelwise SIENA Statistics</h5><blockquote><p>扩展SIENA以允许跨受试者(“与模板对齐”)的萎缩的体素统计分析</p></blockquote><p>这为每个受试者采用SIENA派生的边缘“flow图像”（时间点之间的边缘位移），扭曲这些以与标准空间边缘图像对齐，然后进行<code>体素横向受试者统计分析</code>以识别脑边缘点，例如，对于整个受试者组，或者萎缩与年龄或疾病进展显着相关时，萎缩显着。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Voxelwise多受试者SIENA统计。首先，SIENA针对每个受试者单独运行。接下来，对于每个受试者，边缘位移图像（在脑/非脑边缘点处编码，两个时间点之间的向外或向内边缘变化）被扩张，转化为MNI152空间，并被标准MNI152空间脑边缘图像标记。通过这种方式，边缘位移值被翘曲到标准的脑边缘[Bartsch 2004]。接下来，将来自所有受试者的所得图像进行体素统计分析以测试.....</span><br></pre></td></tr></table></figure><h5 id="Voxelwise-SIENA-Usage"><a href="#Voxelwise-SIENA-Usage" class="headerlink" title="Voxelwise SIENA Usage"></a>Voxelwise SIENA Usage</h5><p><strong>Step1</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">siena A B   # on all subjects' two-timepoints data (here A and B).</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd &lt;siena_output_directory&gt;</span><br><span class="line">siena_flow2std A B  # For each subject</span><br></pre></td></tr></table></figure><p>运行flirt生成到标准空间的变换（如果它还不存在），使用由siena生成的边缘flow（萎缩）图像，将其dilates扩展几次（以“加厚”此边缘flow图像），转换至标准空间以及带有标准空间边缘蒙版的mask掩膜。在remasking之前使用默认的half-width 5mm的高斯滤波器对其进行平滑处理。如果要更改平滑，请使用-s选项;如果完全关闭，将平滑设置为零。</p><p><strong>Step2</strong></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fslmerge -t flow_all_subjects `imglob subject_*/A_to_B_siena/A_to_B_flow_to_std*</span><br></pre></td></tr></table></figure><p>现在，所有受试者将在标准边缘空间中具有名为A_to_B_flow_to_std的边缘flow图像。将这些合并为一个4D图像;例如，如果每个受试者的分析到目前为止都在subject_*/A_to_B_siena子目录中执行，其中*可以是主题ID或名称<br>注意：subjects在此命令中出现的顺序与您创建设计矩阵时所需的顺序相匹配非常重要！</p><p>您现在可以执行跨主题统计。因为上述步骤不太可能在数据中生成好的高斯分布，我们建议使用randomise。您需要生成一个FEAT风格的设计矩阵design.mat并对比文件design.con。您用于randomise的掩码图像应为<code>${FSLDIR}/data/standard/MNI152_T1_2mm_edges</code></p><h4 id="核磁数据处理之-FSL数据快视"><a href="#核磁数据处理之-FSL数据快视" class="headerlink" title="核磁数据处理之: FSL数据快视"></a><a href="https://blog.csdn.net/happyhorizion/article/details/79855747" target="_blank" rel="noopener">核磁数据处理之: FSL数据快视</a></h4><blockquote><p>将其中一些切片拿出来在html中批量显示, 一个NifTI文件对应一排切片图像, 这样就可以到达快速检查的目的. </p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> slicesdir sample-001*</span><br><span class="line"></span><br><span class="line">sample-001_brain_fast_seg_1</span><br><span class="line">sample-001_brain_fast_seg</span><br><span class="line">sample-001_brain</span><br><span class="line">sample-001</span><br><span class="line"></span><br><span class="line">Finished. To view, point your web browser at</span><br><span class="line">file:/home/captain/Desktop/7_30_fsl/slicesdir/index.html</span><br></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/Users/Captain/Downloads/Captainzj.github.io/source/_posts/FSL-Introduction/slicesdir-sample-001*.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="FSLeyes"><a href="#FSLeyes" class="headerlink" title="FSLeyes"></a><a href="https://users.fmrib.ox.ac.uk/~paulmc/fsleyes/userdoc/latest/" target="_blank" rel="noopener">FSLeyes</a></h4><blockquote><p><a href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSLeyes" target="_blank" rel="noopener">Installation Tutorial</a></p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/15/FSL-Introduction/FSLeyes.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="FSL的python和R语言接口"><a href="#FSL的python和R语言接口" class="headerlink" title="FSL的python和R语言接口"></a><a href="https://blog.csdn.net/happyhorizion/article/details/86223839" target="_blank" rel="noopener">FSL的python和R语言接口</a></h4><ul><li>FSL的python接口: fslpy  <a href="https://git.fmrib.ox.ac.uk/fsl/fslpy" target="_blank" rel="noopener">https://git.fmrib.ox.ac.uk/fsl/fslpy</a></li></ul><h3 id="批量处理代码"><a href="#批量处理代码" class="headerlink" title="批量处理代码"></a>批量处理代码</h3><ul><li>批量提取海马体</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_skull</span><span class="params">(data_path, remove_skull_path)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(remove_skull_path):</span><br><span class="line">        os.makedirs(remove_skull_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(data_path):</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'nii'</span> <span class="keyword">in</span> file:</span><br><span class="line">                filename = file.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">                print(<span class="string">"filename"</span>, filename)</span><br><span class="line">                output = os.path.join(remove_skull_path, filename+<span class="string">'_brain'</span>)</span><br><span class="line">                print(<span class="string">"output"</span>, output)</span><br><span class="line">                os.system(<span class="string">'bet &#123;&#125; &#123;&#125; -f 0.5 -g 0'</span>.format(os.path.join(data_path, filename), output))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flirt</span><span class="params">(remove_skull_path, flirt_path)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(flirt_path):</span><br><span class="line">        os.makedirs(flirt_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(remove_skull_path):</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'nii'</span> <span class="keyword">in</span> file:</span><br><span class="line">                filename = file.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">                print(<span class="string">"filename"</span>, filename)</span><br><span class="line">                output_nii = os.path.join(flirt_path, filename+<span class="string">'_2mm.nii.gz'</span>)</span><br><span class="line">                output_mat = os.path.join(flirt_path, filename + <span class="string">'_2mm.mat'</span>)</span><br><span class="line">                print(<span class="string">"output"</span>, [output_nii, output_mat])</span><br><span class="line">                os.system(<span class="string">'flirt -in &#123;&#125; -ref /usr/share/fsl/5.0/data/standard/MNI152_T1_2mm_brain -out &#123;&#125; -omat &#123;&#125; '</span></span><br><span class="line">                          <span class="string">'-bins 256 -cost corratio -searchrx -90 90 -searchry -90 90 '</span></span><br><span class="line">                          <span class="string">'-searchrz -90 90 -dof 12  -interp trilinear'</span></span><br><span class="line">                          .format(os.path.join(remove_skull_path, filename), output_nii, output_mat))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_Hipp</span><span class="params">(flirt_path, L_Hipp_path, R_Hipp_path)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(L_Hipp_path):</span><br><span class="line">        os.makedirs(L_Hipp_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(R_Hipp_path):</span><br><span class="line">        os.makedirs(R_Hipp_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(flirt_path):</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'nii'</span> <span class="keyword">in</span> file:</span><br><span class="line">                filename = file.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">                print(<span class="string">"filename"</span>, filename)</span><br><span class="line">                input_nii = os.path.join(flirt_path, filename+<span class="string">'.nii.gz'</span>)</span><br><span class="line">                input_mat = os.path.join(flirt_path, filename + <span class="string">'.mat'</span>)</span><br><span class="line">                print(<span class="string">"input"</span>, [input_nii, input_mat])</span><br><span class="line">                output_L = os.path.join(L_Hipp_path, filename+<span class="string">'_L_Hipp.nii'</span>)</span><br><span class="line">                output_R = os.path.join(R_Hipp_path, filename+<span class="string">'_R_Hipp.nii'</span>)</span><br><span class="line">                print(<span class="string">"output_L"</span>, output_L)</span><br><span class="line">                print(<span class="string">"output_R"</span>, output_R)</span><br><span class="line"></span><br><span class="line">                os.system(<span class="string">'fsl5.0-first -i &#123;&#125; -l &#123;&#125; -m /usr/share/fsl/data/first/models_336_bin/L_Hipp_bin.bmv -k &#123;&#125;'</span></span><br><span class="line">                          .format(input_nii, input_mat, output_L))</span><br><span class="line">                os.system(<span class="string">'fsl5.0-first -i &#123;&#125; -l &#123;&#125; -m /usr/share/fsl/data/first/models_336_bin/R_Hipp_bin.bmv -k &#123;&#125;'</span></span><br><span class="line">                          .format(input_nii, input_mat, output_R))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    origin_path = <span class="string">'/home/xxx/Desktop/AD-fan/ADAndCN/cn'</span></span><br><span class="line">    remove_skull_path = <span class="string">'/home/xxx/Desktop/AD-fan/ADAndCN/output/cn/remove_skull'</span></span><br><span class="line">    flirt_path = <span class="string">'/home/xxx/Desktop/AD-fan/ADAndCN/output/cn/flirt_MNI152_T1_2mm'</span></span><br><span class="line">    L_Hipp_path = <span class="string">'/home/xxx/Desktop/AD-fan/ADAndCN/output/cn/FIRST/L_Hipp'</span></span><br><span class="line">    R_Hipp_path = <span class="string">'/home/xxx/Desktop/AD-fan/ADAndCN/output/cn/FIRST/R_Hipp'</span></span><br><span class="line"></span><br><span class="line">   remove_skull(origin_path, remove_skull_path)</span><br><span class="line">    flirt(remove_skull_path, flirt_path)</span><br><span class="line">    extract_Hipp(flirt_path, L_Hipp_path, R_Hipp_path)</span><br></pre></td></tr></table></figure><h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><ul><li>体素GLM分析</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【摘要】FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.&lt;/p&gt;
    
    </summary>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/ADNI/"/>
    
    
      <category term="AD-preprocess-tools" scheme="http://yoursite.com/tags/AD-preprocess-tools/"/>
    
  </entry>
  
  <entry>
    <title>FreeSurfer-Introduction</title>
    <link href="http://yoursite.com/2019/07/15/FreeSurfer-Introduction/"/>
    <id>http://yoursite.com/2019/07/15/FreeSurfer-Introduction/</id>
    <published>2019-07-15T03:27:56.000Z</published>
    <updated>2019-07-29T03:57:18.113Z</updated>
    
    <content type="html"><![CDATA[<p>【摘要】<strong><a href="http://surfer.nmr.mgh.harvard.edu/" target="_blank" rel="noopener">FreeSurfer - Cortical surface and subcortical modelling</a></strong>  皮层表面和皮质下建模</p><a id="more"></a><h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><ul><li><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall" target="_blank" rel="noopener">FreeSurfer Download and Install</a></p></li><li><p>解决Freesurfer的license问题：可以在<a href="http://surfer.nmr.mgh.harvard.edu/registration.html" target="_blank" rel="noopener">官网</a>注册，邮箱会收到license.txt文件，拷贝至FreeSurfer解压目录。比如，我的license已经注册并下载，放在~/Downloads文件夹下。可以执行如下命令拷贝：参考于<a href="https://www.jianshu.com/p/4db8227cbb81" target="_blank" rel="noopener">FSL/FreeSurfer安装教程</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">~/Downloads$ sudo cp license.txt /usr/local/freesurfer/</span><br></pre></td></tr></table></figure></li><li><p>更改subjects文件夹权限</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo chmod -R 777 /usr/<span class="built_in">local</span>/freesurfer/subjects</span></span><br></pre></td></tr></table></figure></li></ul><p><strong>Test your FreeSurfer Installation</strong></p><ul><li><p><strong>Example 1:</strong> Convert the sample-001.mgz to nifti format.</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&gt; cp <span class="variable">$FREESURFER_HOME</span>/subjects/sample-001.mgz .</span></span><br><span class="line"><span class="meta">$</span><span class="bash">&gt; mri_convert sample-001.mgz sample-001.nii.gz</span></span><br><span class="line">...</span><br><span class="line">reading from sample-001.mgz...</span><br><span class="line">TR=7.25, TE=3.22, TI=600.00, flip angle=7.00</span><br><span class="line">i_ras = (-0, -1, -0)</span><br><span class="line">j_ras = (-0, 0, -1)</span><br><span class="line">k_ras = (-1, 0, 0)</span><br><span class="line">writing to sample-001.nii.gz...</span><br></pre></td></tr></table></figure></li><li><p><strong>Example 2:</strong> Perform a full recon-all on the nifti file.</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&gt; <span class="built_in">export</span> SUBJECTS_DIR=&lt;path to subject directory&gt; <span class="comment"># SUBJECTS_DIR变量为存储数据的目录</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">&gt; recon-all -i sample-001.nii.gz -s bert -all (creates a folder called bert <span class="keyword">in</span> SUBJECTS_DIR)</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Started at 2019年 07月 17日 星期三 20:54:07 CST </span><br><span class="line">Ended   at 2019年 07月 18日 星期四 02:37:36 CST</span><br><span class="line"><span class="meta">#</span><span class="bash">@<span class="comment">#%# recon-all-run-time-hours 5.725</span></span></span><br><span class="line">recon-all -s bert finished without error at 2019年 07月 18日 星期四 02:37:36 CST</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li>Process your own data with a command such as this:</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">recon-all \</span><br><span class="line">  -i  &lt;one slice in the anatomical dicom series&gt; \</span><br><span class="line">  -s  &lt;subject id that you make up&gt; \</span><br><span class="line">  -sd &lt;directory to put the subject folder in&gt; \</span><br><span class="line">  -all</span><br></pre></td></tr></table></figure><p>where the input (-i) file is a single file representing a <code>T1-weighted</code> data set. If you have DICOM images, you must find a file in the T1 series to pass. You can do this with the dcmunack command.</p></li><li><p><strong>Example 3:</strong> Perform a full recon-all on a pre-existing subject folder</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&gt; <span class="built_in">export</span> SUBJECTS_DIR=&lt;path to subject directory&gt;</span></span><br><span class="line"><span class="meta">$</span><span class="bash">&gt; recon-all -s bert -all</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Started at 2019年 07月 17日 星期三 21:38:11 CST </span><br><span class="line">Ended   at 2019年 07月 18日 星期四 02:40:42 CST</span><br><span class="line"><span class="meta">#</span><span class="bash">@<span class="comment">#%# recon-all-run-time-hours 5.042</span></span></span><br><span class="line">recon-all -s bert finished without error at 2019年 07月 18日 星期四 02:40:42 CST</span><br><span class="line">done</span><br></pre></td></tr></table></figure></li><li><p><strong>Example 4:</strong> View the output volumes, surfaces and subcortical segmentation of the fully recon-ed subject bert.</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&gt; <span class="built_in">cd</span> <span class="variable">$SUBJECTS_DIR</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">&gt; freeview -v \</span></span><br><span class="line">    bert/mri/T1.mgz \</span><br><span class="line">    bert/mri/wm.mgz \</span><br><span class="line">    bert/mri/brainmask.mgz \</span><br><span class="line">    bert/mri/aseg.mgz:colormap=lut:opacity=0.2 \</span><br><span class="line">    -f \</span><br><span class="line">    bert/surf/lh.white:edgecolor=blue \</span><br><span class="line">    bert/surf/lh.pial:edgecolor=red \</span><br><span class="line">    bert/surf/rh.white:edgecolor=blue \</span><br><span class="line">    bert/surf/rh.pial:edgecolor=red</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Some notes on the above command line:</span><br><span class="line"></span><br><span class="line">- bert is the name of the subject</span><br><span class="line">- The flag -v is used to open some of `the most commonly used volumes` including:</span><br><span class="line">  - brainmask.mgz : skull-stripped volume primarily used for troubleshooting 头骨剥离的卷主要用于故障排除  T1去除颅骨 ➜ brainmask (voxel intensities)</span><br><span class="line">  - wm.mgz : white matter mask also used for troubleshooting 白质掩膜也用于故障排除</span><br><span class="line">  - aseg.mgz : subcortical segmentation loaded with its corresponding color table and at a low opacity. For more information on the subcortical segmentation, see [here](https://surfer.nmr.mgh.harvard.edu/fswiki/SubcorticalSegmentation).皮质下分割加载了相应的颜色表并且低不透明度。 有关皮质下分割的更多信息，请参见此处。 brainmask上色(`遵循皮质下强度边界`) ➜ aseg (labeled structures) 显示皮下结构的分割情况</span><br><span class="line"></span><br><span class="line">- The flag -f is used to load surfaces (`遵循灰质和白质边界`)</span><br><span class="line">- white &amp; pial(软膜的) surfaces are loaded for each hemisphere(半球) &amp; with color indicated by 'edgecolor'</span><br><span class="line">The `white surface` (blue line) is used to `calculate total white matter volume` and should accurately `follow the boundary between white matter and gray matter`. The `pial surface` (red line) is used to `calculate cortical gray matter volume and should accurately `follow the boundary between the gray matter and the CSF`.</span><br></pre></td></tr></table></figure><ul><li><p><code>freeview.bin: error while loading shared libraries: libpng12.so.0: cannot open shared object file: No such file or directory</code>：Download the shared library from <a href="https://packages.ubuntu.com/xenial/amd64/libpng12-0/download" target="_blank" rel="noopener">https://packages.ubuntu.com/xenial/amd64/libpng12-0/download</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i libpng12-0_1.2.54-1ubuntu1.1_amd64.deb</span><br></pre></td></tr></table></figure></li><li><p><code>freeview.bin: error while loading shared libraries: libjpeg.so.62: cannot open shared object file: No such file or directory</code></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt install libjpeg62</span><br></pre></td></tr></table></figure></li><li><p>.nii文件 同理查看</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">&gt; freeview -v sample-001.nii.gz</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="FreeSurfer-Tutorials"><a href="#FreeSurfer-Tutorials" class="headerlink" title="FreeSurfer Tutorials"></a><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/Tutorials" target="_blank" rel="noopener">FreeSurfer Tutorials</a></h3><ul><li><p>Preparation</p><ul><li><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/Data" target="_blank" rel="noopener">FreeSurfer Tutorial Datasets</a></li></ul></li><li><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/OutputData_freeview" target="_blank" rel="noopener">Introduction to FreeSurfer Output</a>：熟悉freeview界面(<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/MacCommands" target="_blank" rel="noopener">快捷键</a>)</p><p>为了帮助验证准确度，请调整亮度和对比度，以便轻松识别灰色和白色物质之间的强度变化。要执行此操作，请在按住“Shift”键的同时左键单击图像并拖动鼠标（Make sure the brainmask volume is highlighted in the left menu in order for this to work.）</p><p>.white和.pial分别可用于计算白质和灰质体积；aseg用于计算皮质下体积的测量</p></li><li><p>we mean <code>wm voxels</code> have an intensity value of somewhere <code>between 100 and 110</code>. And <code>wm voxels</code> are <code>between a value of 85 and 100</code>. In the <a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/TroubleshootingData" target="_blank" rel="noopener">TroubleshootingData</a> tutorial, we’ll go over what to do if there is an intensity normalization error.</p></li><li><p>WM卷：FreeSurfer对<code>白质的初始分割</code>（以<code>灰色</code>显示），并添加了自动拓扑定位器（白色）。</p></li><li><p>3D view：绿色区域是回旋区域，红色区域是凹陷区域。</p></li><li><p>Notice that all subcortical gray matter is not a part of the surface labels (because again, those areas do not count towards the cortical surface measures).注意，所有皮质下灰质不是表面标签的一部分（因为这些区域不再计入皮质表面测量）。</p></li></ul><h3 id="基本指令说明"><a href="#基本指令说明" class="headerlink" title="基本指令说明"></a><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FreeSurferCommands" target="_blank" rel="noopener">基本指令说明</a></h3><h4 id="recon-all"><a href="#recon-all" class="headerlink" title="$ recon-all"></a>$ recon-all</h4><p> <a href="https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAllDevTable" target="_blank" rel="noopener">详解</a> </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">USAGE: recon-all</span><br><span class="line"></span><br><span class="line"> Required Arguments:</span><br><span class="line">   -subjid &lt;subjid&gt;</span><br><span class="line">   -&lt;process directive&gt;</span><br><span class="line"></span><br><span class="line"> Fully-Automated Directive:</span><br><span class="line">  -all           : performs all stages of cortical reconstruction</span><br><span class="line">  -autorecon-all : same as -all</span><br><span class="line"></span><br><span class="line"> Manual-Intervention Workflow Directives:</span><br><span class="line">  -autorecon1    : process stages 1-5 (see below)  # no-use-gpu: about 10 min</span><br><span class="line">  -autorecon2    : process stages 6-23</span><br><span class="line">                   after autorecon2, check white surfaces:</span><br><span class="line">                     a. if wm edit was required, then run -autorecon2-wm</span><br><span class="line">                     b. if control points added, then run -autorecon2-cp</span><br><span class="line">                     c. proceed to run -autorecon3</span><br><span class="line">  -autorecon2-cp : process stages 12-23 (uses -f w/ mri_normalize, -keep w/ mri_seg)</span><br><span class="line">  -autorecon2-wm : process stages 15-23</span><br><span class="line">  -autorecon2-inflate1 : 6-18</span><br><span class="line">  -autorecon2-perhemi : tess, sm1, inf1, q, fix, sm2, inf2, finalsurf, ribbon</span><br><span class="line">  -autorecon3    : process stages 24-34</span><br><span class="line">                     if edits made to correct pial, then run -autorecon-pial</span><br><span class="line">  -hemi ?h       : just do lh or rh (default is to do both)</span><br><span class="line"></span><br><span class="line">  Autorecon Processing Stages (see -autorecon# flags above):</span><br><span class="line">    1.  Motion Correction and Conform  # 运动校正和一致</span><br><span class="line">    2.  NU (Non-Uniform intensity normalization)  # 非均匀强度归一化</span><br><span class="line">    3.  Talairach transform computation  #  Talairach变换计算</span><br><span class="line">    4.  Intensity Normalization 1  # 强度归一化</span><br><span class="line">    5.  Skull Strip  # 颅骨去除   </span><br><span class="line"></span><br><span class="line">    6.  EM Register (linear volumetric registration)  # EM寄存器（线性体积配准）</span><br><span class="line">    7.  CA Intensity Normalization  # CA强度归一化</span><br><span class="line">    8.  CA Non-linear Volumetric Registration  # CA非线性体积配准</span><br><span class="line">    9.  Remove neck  # 去除颈部</span><br><span class="line">    10. EM Register, with skull  # EM注册，带头骨</span><br><span class="line">    11. CA Label (Aseg: Volumetric Labeling) and Statistics  # CA标签（Aseg：体积标签）和统计</span><br><span class="line"></span><br><span class="line">    12. Intensity Normalization 2 (start here for control points) # 强度归一化2（从控制点开始）</span><br><span class="line">    13. White matter segmentation  # 白质细分</span><br><span class="line">    14. Edit WM With ASeg  # 使用ASeg编辑WM</span><br><span class="line">    15. Fill (start here for wm edits)  # 填充（从这里开始编辑wm）</span><br><span class="line">    16. Tessellation (begins per-hemisphere operations)  # 曲面细分（每半球操作开始）</span><br><span class="line">    17. Smooth1</span><br><span class="line">    18. Inflate1</span><br><span class="line">    19. QSphere</span><br><span class="line">    20. Automatic Topology Fixer  # 自动拓扑修复器</span><br><span class="line">    21. White Surfs (start here for brain edits for pial surf)  # 白色Surfs（从这里开始用于脑部冲浪的大脑编辑）</span><br><span class="line">    22. Smooth2</span><br><span class="line">    23. Inflate2</span><br><span class="line"></span><br><span class="line">    24. Spherical Mapping  # 球面映射</span><br><span class="line">    25. Spherical Registration  # 球形配准</span><br><span class="line">    26. Spherical Registration, Contralater hemisphere  # 球面配准，Contralater半球</span><br><span class="line">    27. Map average curvature to subject  # 将平均曲率映射到主题</span><br><span class="line">    28. Cortical Parcellation (Labeling)  # 皮质分割（标签）</span><br><span class="line">    29. Cortical Parcellation Statistics  # 皮质分割统计</span><br><span class="line">    30. Pial Surfs  # Pial Surfs</span><br><span class="line">    31. WM/GM Contrast  # WM / GM对比</span><br><span class="line">    32. Cortical Ribbon Mask  # 皮质功能掩膜</span><br><span class="line">    33. Cortical Parcellation mapped to ASeg  # Cortical Parcellation映射到ASeg</span><br><span class="line">    34  Brodmann and exvio EC labels  # Brodmann和exvio EC标签</span><br><span class="line"></span><br><span class="line"> Step-wise Directives</span><br><span class="line">  See -help</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="mri-convert"><a href="#mri-convert" class="headerlink" title="$ mri_convert"></a>$ mri_convert</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mri_convert.bin </span><br><span class="line">Help</span><br><span class="line"></span><br><span class="line">NAME</span><br><span class="line">mri_convert</span><br><span class="line"></span><br><span class="line">SYNOPSIS</span><br><span class="line">mri_convert [options] &lt;in volume&gt; &lt;out volume&gt;</span><br><span class="line"></span><br><span class="line">DESCRIPTION</span><br><span class="line">mri_convert is a general purpose utility for converting between </span><br><span class="line">different file formats. The file type can be specified in two ways. </span><br><span class="line">First, mri_convert will try to figure it out on its own from the </span><br><span class="line">format of the file name (eg, files that end in .img are assumed to be </span><br><span class="line">in spm analyze format). Second, the user can explicity set the type of</span><br><span class="line">file using --in_type and/or --out_type.</span><br><span class="line"></span><br><span class="line">Legal values for --in_tye (-it) and --out_type (-ot) are listed under </span><br><span class="line">optional flagged arguments.</span><br></pre></td></tr></table></figure><h4 id="preproc-cess"><a href="#preproc-cess" class="headerlink" title="$ preproc-cess"></a>$ preproc-cess</h4><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsFastTutorialV6.0/FsFastPreProc" target="_blank" rel="noopener">FS-FAST Preprocessing</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">USAGE: preproc-sess</span><br><span class="line"></span><br><span class="line">  -per-run : motion cor and reg to middle TP of each run</span><br><span class="line">  -per-session : motion cor and reg to 1st TP of 1st run</span><br><span class="line">  -fwhm FWHM : smoothing level (mm)</span><br><span class="line"></span><br><span class="line">  -update        : only run a stage if input is newer than output (default)</span><br><span class="line">  -force         : force reprocessing of all stages (turns off -update)</span><br><span class="line">  -no-update     : same as -force</span><br><span class="line">  -sliceorder so : turn on slice timing correction (STC) with the given slice order</span><br><span class="line">  -ngroups nSliceGroups : number of SMS slice groups for STC</span><br><span class="line">  -surface subject hemi : set hemi to lhrh to do both</span><br><span class="line">  -mni305-2mm    : sample raw data to mni305 at 2mm (same as -mni305)  # useful</span><br><span class="line">  -mni305-1mm    : sample raw data to mni305 at 1mm</span><br><span class="line">  -cvs : sample raw data to cvs_avg35_inMNI152 at 2mm (not with -mni305)</span><br><span class="line"></span><br><span class="line">Session Arguments (some combination required)</span><br><span class="line">  -sf sessidfile  ...</span><br><span class="line">  -df srchdirfile ...</span><br><span class="line">  -s  sessid      ...</span><br><span class="line">  -d  srchdir     ...</span><br><span class="line">  -fsd    fsd &lt;bold&gt;</span><br><span class="line">  -rlf    rlf  : run list file (default all runs)</span><br><span class="line"></span><br><span class="line">  -init-fsl : use fsl to initialize bbr registration</span><br><span class="line">  -init-spm : use spm to initialize bbr registration (needs matlab)</span><br><span class="line">  -init-header : use geometry to initialize bbr registration</span><br><span class="line">  -bbr-int ifsd istem : use intermediate volume in sess/ifsd/RRR/istem</span><br><span class="line"></span><br><span class="line">Other options (probably not too useful)</span><br><span class="line"></span><br><span class="line">  -nomc     : don't do motion correction</span><br><span class="line">  -nostc    : don't do slice-timing correction</span><br><span class="line">  -nosmooth : don't do smoothing</span><br><span class="line">  -nomask   : don't create brain mask</span><br><span class="line">  -noreg    : don't do registration</span><br><span class="line">  -noinorm  : don't do inorm</span><br><span class="line">  -no-subcort-mask : do not apply subcortical masking</span><br><span class="line"></span><br><span class="line">  -mcin   mcinstem    : stem to use as input  to MC</span><br><span class="line">  -mcout  mcoutstem   : stem to use as output of MC</span><br><span class="line">  -stcin  stcinstem   : stem to use as input  to STC </span><br><span class="line">  -stcout stcoutstem  : stem to use as output of STC </span><br><span class="line">  -smin   sminstem    : stem to use as input  to smoothing </span><br><span class="line">  -smout  sminstem    : stem to use as output of smoothing </span><br><span class="line">  -mask   maskstem    : &lt;brain&gt;</span><br><span class="line"></span><br><span class="line">  -i    instem    : stem to use as overal input &lt;f&gt;</span><br><span class="line"></span><br><span class="line">-regfile regfile   : registration file for use with -surf-fwhm (register.dat)</span><br><span class="line">  -projfrac frac : projection fraction for use with -surf-fwhm (0.5)</span><br><span class="line">  -projfrac-avg  : average over ribbon (not with -projfrac)</span><br><span class="line">  -no-cortex-label : do not use cortex label for masking surfaces</span><br></pre></td></tr></table></figure><p>Once the data have been arranged in the proper directory structure and naming convention, they are ready to be preprocessed. Preprocessing includes：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. Template Creation</span><br><span class="line">2. Brain Mask Creation</span><br><span class="line">3. Registration with FreeSurfer Anatomical</span><br><span class="line">4. Motion Correction</span><br><span class="line">5. Slice Timing Correction (if using)</span><br><span class="line">6. Spatial Normalization</span><br><span class="line">7. Masking</span><br><span class="line">8. Spatial Smoothing   # useful</span><br></pre></td></tr></table></figure><h3 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h3><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/LongSkullStripFix2" target="_blank" rel="noopener">Fixing a bad skull strip</a></p><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsFastTutorialV6.0?action=fullsearch&amp;context=180&amp;value=linkto%3A&quot;FsFastTutorialV6.0&quot;" target="_blank" rel="noopener"><strong>FsFastTutorialV6.0</strong></a>  </p><ul><li><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsFastTutorialV6.0/FsFastDirStruct" target="_blank" rel="noopener">Understanding the FS-FAST Directory Structure</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">- navigate between the project, session, fsd and run directories</span><br><span class="line">- view your raw fMRI data in Freeview</span><br><span class="line">- interpret a paradigm file</span><br><span class="line">- use a sessid file</span><br></pre></td></tr></table></figure></li></ul><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/mris_ca_train?action=fullsearch&amp;context=180&amp;value=linkto%3A&quot;mris_ca_train&quot;" target="_blank" rel="noopener">mris_ca_train</a> 从一组带注释的主题创建地图集、<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/mris_ca_label?action=fullsearch&amp;context=180&amp;value=linkto%3A&quot;mris_ca_label&quot;" target="_blank" rel="noopener">mris_ca_label</a>  对于单个主题，生成一个注释文件，其中每个皮质表面顶点都分配有一个神经解剖标签、<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/mris_sample_parc?action=fullsearch&amp;context=180&amp;value=linkto%3A&quot;mris_sample_parc&quot;" target="_blank" rel="noopener">mris_sample_parc</a> 采样</p><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><h4 id="颅骨去除"><a href="#颅骨去除" class="headerlink" title="颅骨去除"></a>颅骨去除</h4><ul><li><p><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/mri_watershed" target="_blank" rel="noopener">mri_watershed</a> (运行时长短，约11s，Recommend)  </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mri_watershed sample-001.nii.gz output.nii.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> or</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mri_watershed sample-001.nii.gz output.nii    <span class="comment"># 过程参数有细微差别，轮廓效果基本一致</span></span></span><br></pre></td></tr></table></figure><p>i. 输入输出维度保持一致.  </p><p>ii.<code>freeview -v output.nii</code> 仅显示黑白轮廓图，<code>freeview -v output.nii.gz</code> 会显示内部纹理细节</p></li></ul><p>  <a href="https://blog.csdn.net/sudakuang/article/details/80848472" target="_blank" rel="noopener">Linux Freesurfer脑数据分割</a>：</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mri_watershed -T1 -t 20 input_file output_file</span><br></pre></td></tr></table></figure><p>  命令行，经过试验，加入-T1会避免由于原图的灰度值范围不对导致的报错，也可以更干净地去除脑壳，阈值选20（加入-T1比不加去得更干净）,但可能会误剔除，需测试后再进行批处理操作。</p><ul><li><p>recon-all 方法（运行时间长，约10min） 可参考：<a href="http://learning-archive.org/wp-content/uploads/2017/09/比较FSL-FreeSurfer-ANTs的脑提取工具.pdf" target="_blank" rel="noopener">比较FSL/FreeSurfer/ANTs的脑提取工具</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> SUBJECTS_DIR=~/Desktop/subjects</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> recon-all -i input.nii.gz -s testFreeSurfer -autorecon1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mri_convert --out_orientation RAS -rt nearest --reslice_like input.nii.gz \</span></span><br><span class="line"> -it mgz $&#123;SUBJECTS_DIR&#125;/testFreeSurfer/mri/brainmask.mgz \</span><br><span class="line"> -ot nii output_brain.nii.gz</span><br></pre></td></tr></table></figure><p>数据默认放在 SUBJECTS_DIR 这个变量指定的目录下，FreeSurfer 的输出格式是 mgz，可以使用<br>mri_convert 转换成 nifti 格式。同时 mri_convert 也可以改变朝向，如果朝向发生了变化的话。</p><p>输入输出维度保持一致.</p></li></ul><p><strong>对比</strong>：两种方法效果基本一致，recon-all提取的输出细节更细腻一点</p><h4 id="预处理分割"><a href="#预处理分割" class="headerlink" title="预处理分割"></a>预处理分割</h4><p>after<code>-autorecon1</code>, <code>-autorecon2</code> includes 白质分割、<a href="https://surfer.nmr.mgh.harvard.edu/fswiki/SubcorticalSegmentation" target="_blank" rel="noopener">皮下组织分割</a></p><h4 id="Multimodal-Integration"><a href="#Multimodal-Integration" class="headerlink" title="Multimodal Integration"></a><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/MultiModalTutorialV6.0" target="_blank" rel="noopener">Multimodal Integration</a></h4><ul><li><a href="https://surfer.nmr.mgh.harvard.edu/fswiki/MultiModalTutorialV6.0/MultiModalRegistration" target="_blank" rel="noopener"><strong>Multimodal Registration</strong></a>  perform multi-modal integration in FreeSurfer using <code>fMRI</code> and <code>dMRI</code> analysis. 可进行<code>手动配准</code>；亦可使用<code>配准文件(例register.lta)</code>进行<code>自动配准</code></li></ul><h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><ul><li><p>Runtime is so long. Can we use GPU accelerate？ <code>-use-gpu</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ recon-all -i sample-001.nii.gz -s SkullStrip_FreeSurfer -autorecon1 -use-gpu</span><br><span class="line"></span><br><span class="line">Testing for CUDA device:</span><br><span class="line">/usr/local/freesurfer/bin/mri_em_register_cuda: error while loading shared libraries: libcudart.so.5.0: cannot open shared object file: No such file or directory</span><br><span class="line">Linux captain-System-Product-Name 4.15.0-29-generic #31-Ubuntu SMP Tue Jul 17 15:39:52 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line"></span><br><span class="line">recon-all -s SkullStrip_FreeSurfer exited with ERRORS at 2019年 07月 18日 星期四 18:23:50 CST</span><br><span class="line"></span><br><span class="line">For more details, see the log file </span><br><span class="line">To report a problem, see http://surfer.nmr.mgh.harvard.edu/fswiki/BugReporting</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【摘要】&lt;strong&gt;&lt;a href=&quot;http://surfer.nmr.mgh.harvard.edu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;FreeSurfer - Cortical surface and subcortical modelling&lt;/a&gt;&lt;/strong&gt;  皮层表面和皮质下建模&lt;/p&gt;
    
    </summary>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/ADNI/"/>
    
    
      <category term="AD-preprocess-tools" scheme="http://yoursite.com/tags/AD-preprocess-tools/"/>
    
  </entry>
  
  <entry>
    <title>致Snorlax</title>
    <link href="http://yoursite.com/2019/07/15/%E8%87%B4Snorlax/"/>
    <id>http://yoursite.com/2019/07/15/致Snorlax/</id>
    <published>2019-07-15T03:07:25.000Z</published>
    <updated>2019-07-29T12:54:22.612Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p>初立凡尘之外，看尽人世嬉笑<br>今渡万般琐事，亦不过平平尔</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>AD_Papers</title>
    <link href="http://yoursite.com/2019/07/10/AD-Papers/"/>
    <id>http://yoursite.com/2019/07/10/AD-Papers/</id>
    <published>2019-07-10T10:37:04.000Z</published>
    <updated>2019-07-29T12:10:04.769Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><table><thead><tr><th style="text-align:center">No</th><th style="text-align:center">Data Source</th><th style="text-align:center">Modality</th><th style="text-align:center">Method</th><th style="text-align:center">Experiment</th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://arxiv.org/abs/1710.04782" target="_blank" rel="noopener">1</a></td><td style="text-align:center">ADNI</td><td style="text-align:center">PET</td><td style="text-align:center">M-CNN</td><td style="text-align:center">AD vs NC(93.58),<br>sMCI vs pMCI(81.55),<br>tf sMCI vs pMCI(82.51)</td></tr><tr><td style="text-align:center"><a href="https://pubs.rsna.org/doi/full/10.1148/radiol.2016152703" target="_blank" rel="noopener">2</a></td><td style="text-align:center">Alzheimer Center of the VU University <br>Medical Center Dementia Cohort</td><td style="text-align:center">MRI</td><td style="text-align:center">ASL-W_score-SVM</td><td style="text-align:center">i. AD vs. SCD  <br>ii. AD vs. MCI <br> iii. MCI vs. SCD</td></tr><tr><td style="text-align:center"><a href="https://arxiv.org/abs/1808.06452" target="_blank" rel="noopener">3</a></td><td style="text-align:center">ADNI, AIBL,OASIS</td><td style="text-align:center">MRI,PET</td><td style="text-align:center">SVM\逻辑回归\随机森林</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><a href="">4</a></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table><h4 id="Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease"><a href="#Multiscale-deep-neural-network-based-analysis-of-FDG-PET-images-for-the-early-diagnosis-of-Alzheimer’s-disease" class="headerlink" title="Multiscale deep neural network based analysis of FDG-PET images for the early diagnosis of Alzheimer’s disease"></a><a href="https://arxiv.org/abs/1710.04782" target="_blank" rel="noopener">Multiscale deep neural network based analysis of FDG-PET images for the early diagnosis of Alzheimer’s disease</a></h4><p>提出了一种新的深度学习框架，使用FDG-PET代谢成像来识别MCI的受试者（患有前AD症状的），并将其与其他MCI（非AD /非进展）受试者区分开来。 我们的多尺度深度神经网络仅使用来自单一模态（FDG-PET代谢数据）的测量获得82.51％的分类准确度，优于最近文献中公布的其他可比较的FDG-PET分类器。</p><h5 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h5><ul><li>来源：ADNI（<a href="http://adni.loni.usc.edu）" target="_blank" rel="noopener">http://adni.loni.usc.edu）</a><ul><li>受试者的人口统计学和临床信息：NC组 304项、sMCI组 409项、pMCI组 112项、AD组 226项</li><li>质量控制：1）通过训练有素且专业的神经病理学家对每个脑图像的每个FreeSurfer分段进行手动质量评估。此外，通过手动编辑校正脑膜，白质，皮质或皮质下分割中的任何错误，并重新运行Freesurfer，直到T1 MR图像分割变得准确。2）可视化代谢度量</li></ul></li></ul><h5 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h5><ul><li>提出了一种新的多尺度深度神经网络框架，以学习基于AD病理学的代谢变化模式，作为正常对照（NC）代谢模式的判别; （该论文是第一个利用深度学习开发多尺度FDG-PET分类器的论文）</li><li>发现通过从NC和AD个体转移样本，深层结构可以在早期诊断任务中获得更好的判别能力</li><li>证明了具有不同验证设置的多分类器”投票“预测，可以使所提出的方法更加稳定和稳健，并提高其分类性能</li></ul><h5 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h5><ul><li><p><strong>Introduction</strong></p><ul><li>该论文对迄今为止（21 February 2018）为该项工作发布的最大数据集（1051名受试者的代谢情况）提出了最全面的方法验证，图像严格遵循标准化的成像协议</li></ul></li><li><p><em>Image processing</em></p><ul><li><em>ROI segmentation</em>：Each T1 structural MRI image was segmented into gray matter and white matter using the freely available <code>FreeSurfer 5.3 package</code> with default parameter settings</li><li><em>Patch parcellation</em>： The technique used for this subdivision is a previously published technique where each ROI can be clustered using their spatial coordinates via a <code>k-means clustering algorithm</code></li><li><em>Coregistration</em>：The <code>FDG-PET</code> image and <code>skull-stripped MRI</code> scan of each target were then co-registered using the<code>FSL-FLIRT</code> program</li><li><em>Normalization</em>：The mean intensity in the <code>brainstem region</code> is hence calculated and used to divide the metabolism measures for all the other ROIs in the brain for each subject.</li><li><em>Visualization of metabolism measures</em>：perform visual <code>quality control</code> of the measures across the database</li></ul></li><li><p><em>Multiscale Deep Neural Network</em></p><ul><li>Unsupervised pre-training.</li><li>Supervised fine-tuning. </li><li>Dropout strategy.</li><li>Early stopping strategy.</li></ul></li><li><p><em>Instance-transfer learning</em>：the networks trained with transferred instances displayed better discriminative ability</p></li><li><p><em>Ensemble classifiers</em>：词袋法</p></li><li><p><em>Experimental setup</em>：Three binary classification experiments, (i) <code>NC vs AD</code>, (ii) <code>sMCI vs pMCI</code> and (iii) <code>sMCI vs pMCI with transfer learning from NC and AD</code></p></li><li><p><strong>Results</strong></p><ul><li><p><em>Multiscale classification</em>：二分类实验中，粗精度的预测准确率优于细精度</p></li><li><p><em>Ensemble classifier design</em>：稳健</p></li></ul></li><li><p><strong>Discussion</strong></p><ul><li><em>Comparison with state-of-the-art methods</em>：我们提出的基于深度学习的方法显示sMCI和pMCI之间的分类准确性更高，无论是使用单一模式还是多模式研究</li><li><em>Multiscale classification</em>：与单一尺度特征相比，使用所有多尺度特征（包括从每个单尺度特征获得的判别信息）的组合分类性能产生了更高的精度结果。这表明在连锁多尺度特征上训练的网络（图2）仍然能够学习本文中使用的从小到大的补丁大小的隐藏模式。</li><li><em>Ensemble classifier</em>：集合分类器的准确性高于单个分类器的平均值，这表明具有不同训练和验证集的分集的集合分类器可以产生更稳健和稳定的分类器，因此可以改善分类性能更好的普遍性。</li></ul></li><li><p><strong>Conclusion</strong></p></li></ul><p>在本文中，我们使用<code>多尺度贴片FDG-PET深度学习</code>功能提出了一种新的AD早期诊断框架。所提出的框架利用<code>转移学习方法</code>和<code>集合分类器策略</code>来改善深度神经网络在区分sMCI和pMCI主体的任务中的性能。在1051名受试者的FDG-PET图像的大型数据库上进行的实验提供了支持三种断言的证据。 （1）所提出的方法，使用仅来自<code>单一FDG-PET模态</code>的特征，能够胜过在sMCI和pMCI分类任务中采用多模态特征的现有方法。 （2）所提出的网络可以从<code>多尺度特征</code>中学习判别模式，以提供具有更好判别性能的更健壮的分类器。 （3）使用不同验证集的<code>多个集成分类器</code>可以使网络更加健壮和稳定，并在统计上提高其分类性能。<br>对于未来的工作，将所提出的框架扩展到包含来自多种模态的信息是自然的，假设所得到的深度神经网络将从多个模态数据中学习更多信息，从而进一步改进所获得的分类准确度。尽管sMCI vs pMCI实验常用于验证近期研究（包括我们的研究）中方法的鉴别能力，但我们只能知道那些sMCI受试者在研究进展中时保持稳定并且可以转化为AD或其他神经退行性疾病。因此，在将来临床诊断的sMCI受试者的基本事实可能不完全准确，因此可能在分类中引入噪声/偏倚。幸运的是，随着更多数据的收集，分类系统将更好地捕获这些和其他噪声和可变性来源，我们提出的深度学习集合分类器可能非常适合这种情况。</p><h4 id="Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer’s-Disease"><a href="#Application-of-Machine-Learning-to-Arterial-Spin-Labeling-in-Mild-Cognitive-Impairment-and-Alzheimer’s-Disease" class="headerlink" title="Application of Machine Learning to Arterial Spin Labeling in Mild Cognitive Impairment and Alzheimer’s Disease"></a><a href="https://pubs.rsna.org/doi/full/10.1148/radiol.2016152703" target="_blank" rel="noopener">Application of Machine Learning to Arterial Spin Labeling in Mild Cognitive Impairment and Alzheimer’s Disease</a></h4><p>关于动脉自旋标记（ASL）的机器学习在治疗轻度认知障碍和阿尔茨海默病中的应用</p><p>本研究调查了ASL灌注图的多变量模式识别分析（SVM）是否可用于阿尔茨海默病（AD），轻度认知障碍（MCI）和主观认知衰退（SCD）患者的分类和单一主题预测。 W-score方法可以消除性别和年龄的混杂影响。</p><h5 id="细节-1"><a href="#细节-1" class="headerlink" title="细节"></a>细节</h5><p>ABSTRACT</p><ul><li>Purpose：<ul><li><code>ASL</code> → discrimination AD\MCI\SCD</li><li><code>W-score</code> → remove confounding effects of gender and age</li></ul></li><li>Materials and Methods<ul><li>Training of a Support Vector Machine (<code>SVM</code>) classifier used diagnostic status and perfusion maps.</li></ul></li><li>Results</li><li>Conclusion</li></ul><ol><li><p>INTRODUCTION</p></li><li><p>MATERIALS AND METHODS</p><ol><li><em>Participants</em></li><li><em>Data Acquisition</em> 数据采集</li><li><em>Preprocessing of MR imaging data</em></li><li><em>W-score maps</em>：<code>[(measured perfusion) – (predicted perfusion)] / (standard deviation of residuals)</code>.</li><li><em>SVM: Multivariate Pattern Recognition in Training-Set</em>：SVM接受了 leave-one-out交叉验证框架的训练，以区分AD患者，MCI患者和SCD患者。我们通过比较<code>两个患者组（SCD）</code>的W-得分图来评估分类器的诊断值。通过区分<code>AD和MCI</code>的患者的W-得分图来评估分类器对疾病进展的敏感性。最后，使用<code>MCIc和MCI</code>患者的Wscore图进行探索性分类训练，以研究分类器是否显示预后价值。</li><li><em>SVM: Prediction in new Subjects</em>:<code>Discrimination maps</code></li><li><em>Statistical Analysis</em>: <code>Evaluation</code></li></ol></li><li>RESULTS<ol><li><em>Participant Characteristics</em>:  数据集中受试者MMSE评分差异小</li><li><em>Training of the classifiers</em>：i. AD vs. SCD  ii. AD vs. MCI  iii. MCI vs. SCD </li><li><em>Predictions: assessment of generalisability</em>: <code>use of discrimination weights</code> i. AD vs. SCD  ii. AD vs. MCI  iii. MCI vs. SCD </li><li><em>Exploratory analyses: classifying MCI subgroups</em>: i. MCIc vs. SCD ii. MCIc vs. MCIs; use of the <code>AD vs. SCD training discrimination weights</code> for MCIc vs. SCD;The use of the same discrimination weights in MCIc vs. MCIs </li></ol></li><li>DISCUSSION</li><li>CONCLUSION：Using <code>automated methods</code>（即SVM）, age- and gender adjusted（W-score校准） ASL perfusion maps（特殊模态） can be used to classify and predict diagnoses of AD, MCI-converters, stable MCI patients and SCD subjects with good accuracy and AUC values.</li></ol><h4 id="Reproducible-evaluation-of-classification-methods-in-Alzheimer’s-disease-Framework-and-application-to-MRI-and-PET-data"><a href="#Reproducible-evaluation-of-classification-methods-in-Alzheimer’s-disease-Framework-and-application-to-MRI-and-PET-data" class="headerlink" title="Reproducible evaluation of classification methods in Alzheimer’s disease: Framework and application to MRI and PET data"></a><a href="https://arxiv.org/abs/1808.06452" target="_blank" rel="noopener">Reproducible evaluation of classification methods in Alzheimer’s disease: Framework and application to MRI and PET data</a></h4><p>阿尔茨海默病分类方法的<code>再现评估</code>：MRI和PET数据的框架和应用</p><blockquote><p>提供规范化的AD实验流程，便于结果复现</p></blockquote><p>在本文中，我们使用三个公开可用的数据集（ADNI，AIBL和OASIS）提出了AD中<code>可再现</code>和客观分类实验的框架。该框架包括：i）将三个数据集自动转换为标准格式（BIDS）; ii）模块化的预处理流水线，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准。</p><p>我们使用这个框架来证明从三个数据集（ADNI，AIBL和OASIS）获得的T1 MRI和PET数据进行自动分类。 我们评估各种成分对分类性能的影响：<code>模态（T1 MRI或PET）</code>，<code>特征类型（体素或区域特征）</code>，<code>预处理</code>，<code>诊断标准（标准NINCDS / ADRDA标准或淀粉样蛋白精制标准）</code>，<code>分类算法</code>。 首先在ADNI，AIBL和OASIS数据集上独立进行实验，并通过将对ADNI训练的分类器应用于AIBL和OASIS数据来评估结果的推广。</p><p>本文的贡献：</p><p>i）管理公开数据集的框架及其与新主题的持续<code>更新</code>，特别是全脑自动转换为脑成像数据结构（BIDS）格式的工具<br>ii）一组<code>模块化的预处理管道</code>，特征提取和分类方法，以及评估框架，为不同组件的基准测试提供基准;<br>iii）对来自三个公开可用的神经成像<code>数据集</code>（ADNI，AIBL和OASIS）的T1 MRI和PET数据进行大规模评估。</p><p>对于所有分类任务，FDG PET优于T1 MRI。</p><p>All the code of the framework and the experiments is publicly available: general- purpose tools have been integrated into the <a href="www.clinica.run">Clinica software</a> and the paper-specific code is available at: <a href="https://gitlab.icm-institute.org/aramislab/AD-ML" target="_blank" rel="noopener">https://gitlab.icm-institute.org/aramislab/AD-ML</a>.</p><h5 id="细节-2"><a href="#细节-2" class="headerlink" title="细节"></a>细节</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">- ABSTRACT</span><br><span class="line"></span><br><span class="line">1. Introduction</span><br><span class="line"></span><br><span class="line">2. Materials</span><br><span class="line">   2.1. Datasets</span><br><span class="line">   2.2. Participants</span><br><span class="line">     2.2.1. ADNI</span><br><span class="line">     2.2.2. AIBL </span><br><span class="line">     2.2.3. OASIS</span><br><span class="line">   2.3. Imaging data</span><br><span class="line">   2.3.1. ADNI</span><br><span class="line">   2.3.1.1. T1-weighted MRI. </span><br><span class="line">   2.3.1.2. PET. </span><br><span class="line">     2.3.2. AIBL</span><br><span class="line">     2.3.3. OASIS</span><br><span class="line">     </span><br><span class="line">3. Methods</span><br><span class="line">3.1. Converting datasets to a standardized data structure  # 可以借鉴其工具的实现</span><br><span class="line">    3.1.1. Conversion of the ADNI dataset to BIDS： dicom → nifti; Once the images of interest have been selected and the paths to the image files identified, `the imaging data can be converted to BIDS`. ...</span><br><span class="line">     - If the modality was acquired for a specific pair of subject-session, and several scans and/or preprocessed images are available, `only one is converted`. </span><br><span class="line">    3.1.2. Conversion of the AIBL dataset to BIDS: like above.</span><br><span class="line">    3.1.3. Conversion of the OASIS dataset to BIDS：Analyze → nifti; create the BIDS folder hierarchy → the images are copied to the appropriate folder and renamed</span><br><span class="line"></span><br><span class="line">3.2. Preprocessing pipelines: Two pipelines were developed to preprocess the anatomical `T1w MRI` and `PET` images.</span><br><span class="line">3.2.1. Preprocessing of T1-weighted MR images:i.the Unified Segmentation procedure(tissue segmentation, bias correction and spatial normalization) ii. create DARTEL template  iii. transformation of the DARTEL template into MNI space</span><br><span class="line">3.2.2. Preprocessing of PET images：PET(with T1w) → MNI → SUVR图(归一化)</span><br><span class="line"></span><br><span class="line">3.3. Feature extraction:Two types of features were extracted from the imaging data: `voxel` and `region features`.T1w MR --- the gray matter density; FDG PET --- SUVR.</span><br><span class="line">- AAL2</span><br><span class="line">- AICHA</span><br><span class="line">- Hammers</span><br><span class="line">- LPBA40</span><br><span class="line">- Neuromorphometrics10</span><br><span class="line"></span><br><span class="line">3.4. Classification models: We considered three different classifiers: `linear SVM`(with both the voxel and the regional features), `logistic regression with L2 regularization` and `random forest`(only used for the region-based analyses), all available in Clinica. </span><br><span class="line">3.4.1. Linear SVM</span><br><span class="line">3.4.2. Logistic regression with L2 regularization</span><br><span class="line">3.4.3. Random forest</span><br><span class="line">3.5. Evaluation strategy</span><br><span class="line">3.5.1. Cross-validation</span><br><span class="line">3.5.2. Metrics</span><br><span class="line">3.6. Classification experiments</span><br><span class="line"></span><br><span class="line">4. Results</span><br><span class="line">4.1. Influence of the atlas：no specific atlas provides the highest classification accuracy for all the tasks. → the `AAL2` atlas was chosen as reference atlas as it leads to good classification accuracies and is widely used in the neuroimaging community.</span><br><span class="line">4.2. Influence of the smoothing: for most classification tasks, the balanced accuracy does not vary to a great extent with the smoothing kernel size. → As the degree of smoothing does `not have a clear impact` on the classification performance, we chose to present the subsequent results related to the voxel-based classification `with a reference smoothing of 4 mm`.</span><br><span class="line">4.3. Influence of the type of features: do not show notable differences between the mean balanced accuracies obtained using voxel or regional features.</span><br><span class="line">4.4. Influence of the classification method: both the linear SVM and logistic regression with L2 regularization models lead to similar balanced accuracies, consistently higher than the one obtained with random forest for all the tasks and imaging modalities tested.</span><br><span class="line">4.5. Influence of the partial volume correction of PET images： little difference between the balanced accuracies obtained with and without PVC. </span><br><span class="line">4.6. Influence of the magnetic field strength：no matter the experiment, the balanced accuracy is always `higher for the 3 T scan` subset compared to the 1.5 T scan subset, which is not surprising as 3 T images should have a better signal-to-noise ratio.</span><br><span class="line">4.7. Influence of class imbalance:It thus seems that a very strong class imbalance (as in the case of AIBL where the proportion is 6 to 1) leads to lower performance but that moderate class imbalance (up to 2 to 1 in ADNI) are adequately handled.</span><br><span class="line">4.8. Influence of the dataset：Performances obtained on ADNI and AIBL were comparable and much higher than those obtained on OASIS. → classifiers trained on ADNI generalized well to the other datasets. In particular, training on ADNI substantially improved the classification performances on OASIS. (ADNI has the larger training set size, higher image quality or stricter diagnostic criteria.)</span><br><span class="line">4.9. Influence of the training dataset size:the balanced accuracy in- creases with the number of training samples.</span><br><span class="line">4.10. Influence of the diagnostic criteria: 淀粉样蛋白状态信息有助于提高任务性能</span><br><span class="line">4.11. Computation time: SVM和逻辑回归实验的运算时间远远少于随机森林实验所需的运算时间</span><br><span class="line"></span><br><span class="line">5. Discussion</span><br><span class="line"> - FDG PET与MRI相比具有优越的性能</span><br><span class="line">6. Conclusions</span><br></pre></td></tr></table></figure><h4 id="Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease"><a href="#Multimodal-Neuroimaging-Feature-Learning-With-Multimodal-Stacked-Deep-Polynomial-Networks-for-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="Multimodal Neuroimaging Feature Learning With Multimodal Stacked Deep Polynomial Networks for Diagnosis of Alzheimer’s Disease"></a><a href="https://scholar.google.com/scholar?q=Multimodal+Neuroimaging+Feature+Learning+With+Multimodal+Stacked+Deep+Polynomial+Networks+for+Diagnosis+of+Alzheimer&#39;s+Disease&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart" target="_blank" rel="noopener">Multimodal Neuroimaging Feature Learning With Multimodal Stacked Deep Polynomial Networks for Diagnosis of Alzheimer’s Disease</a></h4><p>该研究提出了一种<code>多模式堆叠DPN</code>（MM-SDPN）算法，MM-SDPN由两级SDPN组成，用于融合和学习用于AD诊断的<code>多模态神经成像数据</code>的特征表示。具体而言，两个SDPN首先用于分别学习<code>MRI</code>和<code>PET</code>的高级特征，然后将其输入另一个SDPN以融合多模态神经影像信息。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">I. INTRODUCTION</span><br><span class="line">The most commonly used feature extraction methods for neuroimaging data can be roughly divided into four categories: 1) The voxel-based approaches that simply and directly extract features from voxel intensity; 2) The vertex-based approaches whose features are defined at the vertex-level on the cortical surface; 3) The region of interest (ROI) based approaches that extract features from predefined brain regions; 4) patch-based approaches that learn new feature representation from local patches.  As a result, these features usually obtain superior classification results.</span><br><span class="line">  - The voxel- and vertex-based features usually have very high dimensionality, and therefore dimensionality reduction is important to achieve more compact and effective features. </span><br><span class="line">  - The ROI-based features are widely used, because they not only have relatively low feature dimensionality, but also cover the whole brain. However, the features extracted from ROIs are somewhat coarse and cannot reflect small or subtle changes involved in the brain diseases. </span><br><span class="line">  - The patch-based features are learned from the whole brain, and can effectively capture the diseased-related pathologies.  貌似最佳</span><br><span class="line"></span><br><span class="line">Why need MM-SDPN？</span><br><span class="line">受监督的DPN将潜在地从用于AD诊断的小神经影像数据中学习优越的特征表示。另一方面，特征提取的逐层堆叠通常在DL中产生更好的表示，例如`DBN和SAE`，这促使开发堆叠DPN（SDPN）算法以学习更高级别的特征表示。此外，已经证明，可以同时学习和融合`多模态神经影像数据`的多模式DL算法优于用于AD分类的单模态DL算法。因此，值得研究多模堆叠DPN算法。</span><br><span class="line"></span><br><span class="line">II. METHODS</span><br><span class="line">A. Deep Polynomial Networks Algorithm  # 拟合？</span><br><span class="line">B. Stacked Deep Polynomial Networks: 每个基本DPN都采用有监督的分块方式进行训练，无需反向传播，这与其他流行的深层架构不同。因此，与具有反向传播策略的其他DL算法相比，SDPN非常简单，具有相对低的计算复杂度。</span><br><span class="line">C. Multimodal Stacked DPN:在第一阶段，每个神经影像数据将被馈送到其相应的SDPN模块，以学习高级特征表示。每种特定模态的高级特征反映了它自己的属性，但不同模态之间没有相关信息。然后，所有学习的特征在第二阶段被馈送到新的SDPN模块，以便与所有模态相关联。因此，最终学习的高级特征既包含每种模态的内在属性，也包含所有模态之间的相关性。因此，SDPN学到的特征更具有辨别力和鲁棒性。在我们的MM-SDPN算法中，由于DPN在每个网络层中执行前馈监督学习而没有精细转向，因此难以执行与[38]中相同的学习策略来推断MRI和PET之间的相关性。因此，通过联合训练第二阶段SDPN与在第一阶段中学习的级联MRI和PET特征来学习`共享表示`。它类似于[42]中使用的简单融合方法。</span><br><span class="line"></span><br><span class="line">III. EXPERIMENTS AND RESULTS</span><br><span class="line">A. Neuroimaging Data Preprocessing</span><br><span class="line">具体而言，预处理首先在`MRI图像`上进行，包括前连合（AC） - 后连合（PC）校正，N3算法强度不均匀，以及由小脑提取的颅骨剥离和去除小脑。然后通过FSL包中的FAST算法将MR图像`分割成三种不同的组织`，即灰质，白质和脑脊液[49]。在通过`HAMMER算法`[50]注册后，每个MR图像被分成93个ROI，基于模板，Kabani等人使用93个手动标记的ROI。 [51]。然后计算灰质组织的体积作为每个ROI的特征，产生93个特征。然后通过刚性配准将每个PET图像与其对应的MRI图像`对准`。将相同ROI的平均强度计算为PET图像的特征。因此，分别从MRI和PET图像中`提取93个特征`。</span><br><span class="line">B. Performance Evaluation: Four classification tasks are performed, namely `AD vs. NC`, `MCI vs. NC`, `MCI-C vs. MCI-NC`, and `AD vs. MCI-C vs. MCI- NC vs. NC`.</span><br><span class="line">DPN-3-MRI、DPN-6-MRI、SDPN-MRI、DPN-3-PET、DPN-6-PET、SDPN-PET、SDPN-MRI-PET、MM-SDPN + SVM/LINEAR CLASSIFIER</span><br><span class="line">C. Results on AD vs. NC：</span><br><span class="line">MM-SDPN算法达到最佳性能，平均分类精度为97.13±4.44％，灵敏度为95.93±7.84％，特异性为98.53±5.05％，因为它成功地融合了MRI和PET信息。另一方面，尽管DPN-6（具有6层网络的DPN）在基于单一模态成像的AD分类的MRI和PET数据上优于DPN-3，但SDPN仍然比DPN-6略好，这表明其有效性由于堆叠技术的SDPN。</span><br><span class="line">MM-SDPN在所有评估指标上都优于所有其他具有SVM和线性分类器的算法。</span><br><span class="line">D. Results on MCI vs. NC：尽管DPN-6优于DPN-3，但SDPN在基于单模态成像的MRI和PET数据上的MCI分类方面优于DPN。</span><br><span class="line">E. Results on MCI-C vs. MCI-NC: SDPN对于单一模态神经成像数据的性能要比DPN-6和DPN-3好得多。</span><br><span class="line">F. Results on AD vs. MCI-C vs. MCI-NC vs. NC:与最先进的基于SAE的算法[38]相比，我们提出的具有SVM分类器的MM-SDPN算法实现了分类精度提高3.21％和灵敏度提高1.51％。</span><br><span class="line"></span><br><span class="line">IV. DISCUSSION  # 核心、优质总结</span><br><span class="line">在这项工作中，我们提出了一种MM-SDPN算法，可以有效地学习基于多模态神经影像学的AD诊断的特征。 ADNI数据集上的四组实验结果表明，与最先进的基于多模态学习的算法相比，所提出的MM-SDPN算法实现了最佳性能。</span><br><span class="line">分析：</span><br><span class="line">原始ROI特征是低级特征，不能以良好区分的方式表示AD的属性。当应用DPN来学习ROI特征的特征时，提供了更复杂的表示，因此DPN已经实现了显着的改进。在多次堆叠基本DPN块之后，获得更高级别的表示。因此，对于基于单模态神经成像的AD分类，SDPN比原始DPN具有更好的性能。</span><br><span class="line">值得注意的是，尽管随着隐层的增加，6层DPN优于3层DPN，但是根据神经网络中的通用逼近定理，太深的网络将增加计算复杂度，而近似的精度没有明显增加。</span><br><span class="line">另一方面，结果还表明，具有两个3层DPN的SDPN优于6层DPN，因为第二层基本DPN中第一层的基础建立在更高级别的特征上，即串联特征第一级基本DPN，这个基础将在学习二级DPN后生成更有效和更高级别的功能。此外，与具有更深网络的DPN相比，SDPN更容易调整参数以实现相同的性能。</span><br><span class="line">在这项研究中，两个分类器，即SVM和线性分类器，用于评估SDPN和MM-SDPN的性能。两个分类器都给出了类似的结果，这表明AD分类的良好性能更多地取决于学习的特征而不是分类器。因此，MM-SDPN真正有效地学习了一个好的特征表示。</span><br><span class="line"></span><br><span class="line">DPN的算法结构使其适用于小型数据集。 由于神经影像数据通常仅提供有限的标记地面实况样本，并且先验标签信息有利于小数据的分类任务，因此监督DPN比不受控制的DL算法更适合于小神经成像数据集。</span><br><span class="line"></span><br><span class="line">V. CONCLUSION</span><br><span class="line">在这项工作中，提出了一种MM-SDPN算法。它由两阶段SDPN组成，可以有效地学习和融合多模态数据，用于诊断阿尔茨海默病。 MM-SDPN实现了最先进的表现，用于对AD进展的两个阶段和四个阶段进行分类。因此，所提出的MM-SDPN不仅可以作为多模神经成像数据的强大表示算法，还可以用于其他医学数据。</span><br></pre></td></tr></table></figure><p><a href="https://scholar.google.com/scholar?q=Multimodal+Neuroimaging+Feature+Learning+for+Multiclass+Diagnosis+of+Alzheimer%E2%80%99s+Disease&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart" target="_blank" rel="noopener">Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer’s Disease</a></p><p>该框架使用<code>零掩蔽策略</code>进行数据融合，以从多种数据模式中提取补充信息。与之前最先进的工作流程相比，我们的方法能够在一个设置中<code>融合多模态神经成像功能</code>，并且可能需要<code>较少标记的数据</code>。 AD的二元分类和多类分类均实现了性能提升。讨论了拟议框架的优点和局限性。</p><p>我们提出了一种新的多层AD诊断框架，其中嵌入了深度学习架构，其受益于多模态神经影像学特征之间的协同作用。该框架由<code>SAE</code>和soft-max逻辑回归器构成。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">I. INTRODUCTION</span><br><span class="line"></span><br><span class="line">II. METHODOLOGY</span><br><span class="line">A. Data Acquisition and Feature Extraction</span><br><span class="line">B. Learning Framework</span><br><span class="line">C. Feature Examination</span><br><span class="line"></span><br><span class="line">III. EXPERIMENTS AND RESULTS</span><br><span class="line">A. Visualization of High-Level Biomarkers</span><br><span class="line">B. Performance Evaluation</span><br><span class="line"></span><br><span class="line">IV. DISCUSSION</span><br><span class="line">A. Model Designing and Training</span><br><span class="line">B. Limitations and Future Work</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">V. CONCLUSION</span><br></pre></td></tr></table></figure><h4 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h4><ul><li>氟脱氧葡萄糖正电子发射断层扫描（FDG-PET），提供了大脑代谢活动的定量测量，可以在结构变化发生之前识别与AD相关的变化，具有可接受的敏感性和准确性。 </li><li>ASL(Arterial Spin Labeling)：动脉自旋标记，三维伪连续ASL扫描的自动分类可以高精度地检测AD患者（&gt; 82％）</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
      <category term="ADNI" scheme="http://yoursite.com/categories/Paper/ADNI/"/>
    
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Network in Medical Imaging A Review</title>
    <link href="http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/"/>
    <id>http://yoursite.com/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/</id>
    <published>2019-07-02T06:18:42.000Z</published>
    <updated>2019-07-02T12:08:24.231Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p>论文地址：<a href="https://arxiv.org/abs/1809.07294" target="_blank" rel="noopener">Generative Adversarial Network in Medical Imaging: A Review</a></p><p>github Reference link：<a href="https://github.com/xinario/awesome-gan-for-medical-imaging" target="_blank" rel="noopener">Awesome GAN for Medical Imaging</a></p><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><em>摘要</em></h3><p>生成对抗网络由于其数据生成能力而在没有明确建模概率密度函数的情况下在计算机视觉社区中获得了很多关注。 鉴别器带来的对抗性损失提供了一种巧妙的方法，可以将未标记的样本纳入训练并实现更高的顺序一致性。 事实证明，这在许多情况下是有用的，例如域适应，数据增强和图像到图像转换。 这些属性吸引了医学成像领域的研究人员，我们已经看到许多传统和新颖应用的快速采用，如图像重建，分割，检测，分类和跨模态合成。 根据我们的观察，这一趋势将继续下去，因此我们利用对抗性训练计划对医学成像的最新进展进行了回顾，希望能够使对该技术感兴趣的研究人员受益。</p><p>关键词：Deeplearning，Generative adversarial network，Generative model，Medical imaging，Review</p><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a><em>1. 介绍</em></h3><p>随着2012年开始的计算机视觉深度学习的复兴（Krizhevsky等，2012），医学成像中深度学习方法的采用大幅增加。据估计，2016年和2017年在主要医学影像相关会议场所和期刊上发表了400多篇论文（Litjens等，2017）。在医学成像领域广泛采用深度学习是因为它具有补充图像解释和增强图像表示和分类的潜力。在本文中，我们将重点放在深度学习领域最有趣的近期突破之一 - 生成对抗网络（GAN） - 以及它们在医学成像领域的潜在应用。<br>GAN是一种特殊类型的神经网络模型，其中两个网络同时被训练，一个侧重于图像生成，另一个侧重于区分。对抗性训练方案因其在抵制领域转移方面的有用性以及产生新图像样本的有效性而在学术界和工业界引起了关注。该模型在许多图像生成任务中实现了最先进的性能，包括文本到图像合成（Xu et al.，2017），超分辨率（Ledig等，2017）和图像 - 图像转换（Zhu et al.，2017a）。<br>与源于20世纪80年代的深度学习不同（Fukushima和Miyake，1982），对抗性的概念相对来说是非常重要的进步（Good-fellow et al.，2014）。本文概述了GAN，描述了它们在医学成像中的有前途的应用，并确定了一些需要解决的挑战，以使它们能够成功应用于其他医学成像相关任务。<br>为了全面概述医学影像中GAN的所有相关工作，我们搜索了包括PubMed，arXiv在内的数据库，国际医学图像计算和计算机辅助干预会议（MICCAI），SPIE医学影像，IEEE国际研讨会生物医学成像（ISBI）和国际深度学习医学影像学会议（MIDL）。我们还合并了上述搜索过程中未识别的交叉引用作品。由于每月都有研究出版物出现，而且没有失去一般性，我们将搜索的截止时间设定为2018年7月30日。仅报告初步结果的arXiv的工作被排除在本次审查之外。基于任务，成像模态和年份的这些论文的描述性统计数据可以在图1中找到。<br>在本文的其余结构如下。我们首先简要介绍第2节中GAN的原理及其一些结构变体。然后在第3节中使用GAN对医学图像分析任务进行全面审查，包括但不限于放射学领域，组织病理学和皮肤病学。我们根据规范任务对所有作品进行分类：重建，图像合成，分割，分类，检测，注册等。第4节总结了该评论，并讨论了前瞻性应用和识别性挑战。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 1" title="1.png">                </div>                <div class="image-caption">1.png</div>            </figure><p>图1：（a）根据规范任务对GAN相关论文进行分类。 （b）根据成像模式对GAN相关论文进行分类。 （c）2014年发布的GAN相关论文数量。请注意，一些工作执行了各种任务，并对具有不同模态的数据集进行了评估。 我们在绘制这些图时多次计算这些作品。 基于源域计算与跨域图像传输相关的工作。 图（a）和（b）中的统计数据基于2018年7月30日或之前公布的论文。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 2" title="2.png">                </div>                <div class="image-caption">2.png</div>            </figure><p>图2：用于在CT图像上合成肺结节的vanilla GAN的示意图。 上图显示了网络配置。 下面的部分显示了生成器G和鉴别器D的输入，输出和内部特征表示.G将样本$z$从$p(z)$变换为生成的结节$x_g$。 D是二元分类器，其分别区分由$x_g$和$x_r$形成的肺结节的生成和真实图像。</p><h3 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a><em>2. 背景</em></h3><h4 id="2-1-Vanilla-GAN"><a href="#2-1-Vanilla-GAN" class="headerlink" title="2.1. Vanilla GAN"></a><em>2.1. Vanilla GAN</em></h4><p>香草GAN（Goodfellow等，2014）是一种生成模型，设计用于直接从所需的数据分布中抽取样本，而无需明确地模拟潜在的概率密度函数。它由两个神经网络组成：发生器G和鉴别器D.G，z的输入是从先前分布p（z）中采样的纯随机噪声，通常选择为高斯分布或均匀分布。简单。预计G，xg的输出与从真实数据分布pr（x）中提取的实际样本xr具有视觉相似性。我们将由θg参数化的G学习的非线性映射函数表示为xg = G（z;θg）。 D的输入是实际或生成的样本。 D，y1的输出是单个值，表示输入是真实或假冒样本的概率。由θd参数化的D学习的映射表示为y1 = D（x;θd）。生成的样本形成分布pg（x），其在成功训练后需要是pr（x）的近似值。图2的顶部显示了香草GAN配置的图示。在该示例中，G生成描绘肺结节的2D CT切片。<br>D的目标是区分这两组图像，而生成器G被训练以尽可能地混淆辨别器D.直观地说，G可以被视为试图生产一些优质假冒伪劣材料的伪造者，D可以被视为试图检测伪造物品的警察。在另一种观点中，我们可以将G视为从D接收奖励信号，这取决于生成的数据是否准确。梯度信息从D传播回G，因此G调整其参数以产生可以欺骗D的输出图像.D和G的训练目标可以用数学表达为：<br>$$ L_{D}^{GAN} = max_{D}E_{{x_r}\sim {P_r(x)}}[logD(x_r)+E_{x_g\sim p_g(x)}[log(1-D(x_g))]],\\L_{D}^{GAN} = min_GE_{x_g\sim p_g(x)}[log(1-D(x_g))].$$ <br>可以看出，D只是具有最大对数似然目标的二元分类器。 如果鉴别器D在下一个发生器G更新之前被训练为最优，则最小化LGAN被证明等同于最小化pr（x）和pg（x）之间的Jensen-Shannon（JS）偏差（Goodfellow等人，2014））。 训练后的预期结果是xg形成的样本应该接近实际数据分布pr（x）。</p><h4 id="2-2-Variants-of-GANs"><a href="#2-2-Variants-of-GANs" class="headerlink" title="2.2. Variants of GANs"></a><em>2.2. Variants of GANs</em></h4><p>上述GAN训练目标被认为是鞍点优化问题（Yadav等，2018），训练通常通过基于梯度的方法完成。 G和D从头开始交替训练，以便它们可以一起进化。但是，G和D训练与JS分歧之间无法保证平衡。因此，一个网络可能不可避免地比另一个网络更强大，在大多数情况下是D.当D变得太强而不是G时，生成的样本变得太容易与实际的分离，从而达到D的梯度逼近零的阶段，没有为G的进一步训练提供指导。由于难以产生有意义的高频细节，因此在生成高分辨率图像时更频繁地发生这种情况。<br>在训练GAN中通常面临的另一个问题是模式崩溃，正如名称所示，这是由G学习的分布pg（x）关注数据分布pr（x）的一些有限模式的情况。因此，它不是产生不同的图像，而是产生一组有限的样本。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 3" title="3.png">                </div>                <div class="image-caption">3.png</div>            </figure><p>图3：GAN变体的示意图。 c表示条件向量。 在CGAN和ACGAN中，c是对类标签进行编码的离散分类代码（例如，一个热向量），在InfoGAN中，它也可以是对属性进行编码的连续代码。 xg通常是指生成的图像，但也可以是SGAN中的内部表示。</p><h5 id="2-2-1-Varying-objective-of-D"><a href="#2-2-1-Varying-objective-of-D" class="headerlink" title="2.2.1. Varying objective of D*"></a>2.2.1. Varying objective of D*</h5><p>为了稳定训练并避免模式崩溃，已经提出了D的不同损失，例如f-发散（f-GAN）（Nowozin等，2016），最小二乘（LS-GAN）（Mao et al。，2016），铰链损失（Miyato等，2018）和Wasserstein距离（WGAN，WGAN-GP）（Arjovsky等，2017; Gulrajani等，2017）。其中，Wasserstein距离可以说是最受欢迎的指标。作为真/假歧视方案的替代方案，Springenberg（2015）提出了一个基于熵的目标，其中鼓励实际数据进行自信的类预测（CatGAN，图3b）。在EBGAN（Zhao等人，2016）和BEGAN（Berthelot等人，2017）（图3c）中，用于鉴别器的常用编码器架构被替换为自动编码器架构。然后，D的目标变为匹配自动编码器丢失分布而不是数据分布。<br>GAN本身缺乏推断机制，根据定义，推断机制可以预测可能编码输入的潜在向量。因此，在ALI（Dumoulin等人，2016）和BiGAN（Donahue等人，2016）（图3d）中，结合了单独的编码器网络。然后D的目标是分离联合样本（xg，zg）和（xr，zr）。在InfoGAN中（图3e），鉴别器输出潜在向量，该潜向向量编码所生成图像的部分语义特征。鉴别器使所生成的图像与所生成的图像所依赖的潜在属性向量之间的互信息最大化。成功培训后，InfoGAN可以探索固有的数据属性，并根据这些属性执行条件数据生成。已经证明类标签的使用可以进一步提高生成图像的质量，并且通过强制D提供类概率并使用交叉熵损失进行优化（例如在ACGAN中使用）（Odena等， 2016）（图3f）。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 4" title="4.png">                </div>                <div class="image-caption">4.png</div>            </figure><p>图4：用于图像到图像转换的cGAN框架。 pix2pix需要对齐的训练数据，而这种约束在CycleGAN中放宽，但通常会受到性能损失的影响。 请注意，在（a）中，我们选择重建损失作为目标一致性的示例。 这种监督与任务有关，可以采取许多其他形式。 （c）它由两个VAEGAN组成，在VAE部分具有共享的潜在向量。</p><h5 id="2-2-2-Varying-objective-of-G"><a href="#2-2-2-Varying-objective-of-G" class="headerlink" title="2.2.2. Varying objective of G"></a><em>2.2.2. Varying objective of G</em></h5><p>在香草GAN中，G将噪声z转换为样本xg = G（z）。这通常通过使用解码器网络逐步增加输出的空间大小来实现，直到达到所需的分辨率，如图2所示.Larsen等人。 （2015）提出了变分自动编码器网络（VAE）作为G的基础架构（VAEGAN，图3g），其中它可以使用逐像素重建损失来强制VAE的解码器部分生成与真实图像匹配的结构。<br>GAN的原始设置对其可以生成的数据模式没有任何限制。然而，如果在生成期间提供辅助信息，则可以驱动GAN以输出具有期望属性的图像。在这种情况下，GAN通常被称为条件GAN（cGAN），并且生成过程可以表示为xg = G（z，c）。<br>最常见的条件输入之一c是图像。 pix2pix是第一个基于通用GAN的图像到图像转换框架，由Isola等人提出。 （2016）（图4 a）。此外，任务相关的监督被引入发电机。例如，用于图像恢复的重建损失和用于分割的骰子损失（Milletari等，2016）。这种形式的监督需要一致的训练对。朱等人。 （2017A）; Kim等人。 （2017）通过从头到脚拼接两个发生器来放松这种约束，这样图像可以在两组不成对的样本之间进行转换（图4b）。为简单起见，我们在本文的其余部分选择了CycleGAN来表示这一想法。另一个名为UNIT的模型（图4c）也可以通过将两个VAEGAN组合在一起来执行不成对的图像到图像变换，每个模型对一种模态负责但共享相同的潜在空间（Liu et al。，2017a）。这些图像到图像翻译框架由于其普遍适用性而在医学成像领域中非常流行。<br>除了图像，条件输入可以是类标签（CGAN，图3h）（Mirza和Osindero，2014），文本描述（Zhang et al。，2017a），对象位置（Reed等，2016a） ，b），周围的图像背景（Pathak等，2016），或草图（Sangkloy等，2016）。请注意，上一节中提到的ACGAN也有一个类条件生成器。</p><h5 id="2-2-3-Varying-architecture"><a href="#2-2-3-Varying-architecture" class="headerlink" title="2.2.3. Varying architecture"></a><em>2.2.3. Varying architecture</em></h5><p>完全连接的层用作香草GAN中的构建块，但后来被DCGAN中的完全卷积下采样/上采样层取代（Radford等，2015）。 DCGAN表现出更好的训练稳定性，因此迅速填补了文献。如图2所示，DCGAN架构中的发生器通过连续的上采样操作对随机输入噪声矢量进行处理，最终生成一个图像。其重要的成分中的两个是BatchNorm（约费和Szegedy，2015）用于调节EX-牙牙特征尺度，和LeakyRelu（马斯等人，2013），用于预排放死梯度。最近，Miyato等人。 （2018）提出了光谱归一化层，其在鉴别器中对权重进行归一化以调节特征响应值的规模。与训练稳定性提高，一些作品也掺入剩余的连接到这两个属，Tor和鉴别器和与深得多的NET-作品试验（Gulrajani等人，2017年;宫户等人，2018）。 Miyato和Koyama（2018）的工作提出了一种基于投影的方法来结合条件信息而不是直接连接，并发现它有利于提高生成图像的质量。<br>从噪声矢量中直接生成高分辨率图像很难，因此一些工作已经提出以渐进方式处理它。在LAPGAN（图3i）中，Denton等人。 （2015）提出了一堆GAN，每个GAN将更高频率的细节添加到生成的图像中。在SGAN，甘斯的CAS-杜松也用于但每个GAN产生越来越低的级表示（Huang等人，2017），其与从区别地训练模型中提取的分层表示进行比较。卡拉斯等人。 （2017）采用了另一种方式，通过向它们添加新层来逐步增长发生器和鉴别器，而不是在前一个GAN之上堆叠另一个GAN（PGGAN）。在条件设定中也探索了这种进步的想法（Wang等，2017b）。<br>最具代表性的GAN的示意图如图3所示。它们是GAN，CatGAN，EBGAN / BEGAN，ALI / BiGAN，InfoGAN，ACGAN，VAEGAN，CGAN，LAPGAN，SGAN。三个流行的图像到图像转换cGAN（pix2pix，CycleGAN和UNIT）如图4所示。为了对这些不同的GAN变体进行更深入的回顾和实证评估，我们引用了读者（Huang et al。 ，2018; Creswell等，2018; Kurach等，2018）。</p><h3 id="3-在医学成像中的应用"><a href="#3-在医学成像中的应用" class="headerlink" title="3. 在医学成像中的应用"></a><em>3. 在医学成像中的应用</em></h3><p>GAN通常有两种用于医学成像的方法。 第一个侧重于生成方面，它可以帮助探索和发现训练数据的基础结构和学习生成新图像。 这个属性使GAN在应对数据稀缺性和患者隐私方面非常有前途。 第二个侧重于辨别方面，其中鉴别器D可以被视为正常图像的学习先验，使得当呈现异常图像时它可以用作正则化器或检测器。 图5提供了GAN相关应用的示例，示例（a），（b），（c），（d），（e），（f）侧重于生成方面和示例（g）利用 歧视方面。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/2019/07/02/Generative-Adversarial-Network-in-Medical-Imaging-A-Review/Figure" alt="Figure 5" title="5.png">                </div>                <div class="image-caption">5.png</div>            </figure><p>图5：使用GAN的示例应用程序。数字直接从相应的纸张中裁剪。 （a）左侧显示噪声污染的低剂量CT，右侧显示去噪的CT，其很好地保留了肝脏中的低对比度区域（Yi和Babyn，2018）。 （b）左侧显示MR图像，右侧显示合成的相应CT。在所生成的CT图像中很好地描绘了骨结构（Wolterink等，2017a）。 （c）生成的视网膜眼底图像具有左侧血管图中描绘的精确血管结构（Costa等，2017b）。 （d）随机噪声（恶性和良性混合物）随机产生的皮肤病变（Yi et al。，2018）。 （e）成人胸部X射线的器官（肺和心脏）分割实例。肺和心脏的形状受到对抗性损失的调节（Dai等，2017b）。 （f）第三列显示了SWI序列中的域适应脑损伤分割结果，未经相应的手动注释训练（Kamnitsas等，2017）。 （g）视网膜光学相干断层扫描图像的异常检测（Schlegl等，2017）。</p><h4 id="3-1-Reconstruction"><a href="#3-1-Reconstruction" class="headerlink" title="3.1. Reconstruction"></a><em>3.1. Reconstruction</em></h4><p>由于临床设置的限制，例如辐射剂量和患者舒适度，所获取的医学图像的诊断质量可能受到噪声和伪影的限制。在过去的十年中，我们已经看到了重建方法的范式转变，从分析到迭代，现在转向基于机器学习的方法。这些基于数据驱动学习的方法要么学会将原始感官输入直接传输到输出图像，要么作为后处理步骤来减少图像噪声和消除伪像。本节中回顾的大多数方法都是直接从计算机视觉文献中借鉴的，这些文献将后处理作为图像到图像的翻译问题，其中cGAN的条件输入以某些形式受到损害，例如低空间分辨率，噪声污染，欠采样或混叠。一个例外是MR图像，其中傅立叶变换用于将原始K空间数据合并到重建中。<br>基本pix2pix框架已用于低剂量CT去噪（Wolterink等，2017b），MR重建（Chen等，2018b; Kim等，2018; Dar等，2018b; Shitrit和Raviv，2017 ）和PET去噪（Wang等，2018b）。预先训练的VGG网（Simonyan和Zisserman，2014）进一步纳入优化框架，以确保感知相似性（Yang等，2017b; Yu等，2017; Yang等，2018a; Armanious et al。，2018; Mahapatra，2017）。 Yi和Babyn（2018）介绍了一种预训练锐度检测网络，明确约束去噪CT的清晰度，特别是低对比度区域。 Mahapatra（2017）计算了一个局部显着图，以突出视网膜眼底成像超分辨率过程中的血管。 Liao等人研究了类似的想法。 （2018）稀疏视图CT重建。他们计算焦点图以调整重建输出，以确保网络集中在重要区域。除了确保图像域数据保真度之外，当在MR重建中可获得原始K空间数据时，也施加频域数据保真度（Quan等，2018; Mardani等，2017; Yang等，2018a）。<br>其他类型的损失已被用于突出重建中的局部图像结构，例如基于其感知相关性重新衡量每个像素的重要性的显着性损失（Mahapatra，2017）以及PET去噪中的样式内容损失（ Armanious等，2018）。在运动器官的图像重建中，很难获得成对的训练样本。因此，Rav`ı等人。 （2018）提出了一种基于物理采集的损失来调节生成的用于内镜超分辨率的图像结构和Kang等人。 （2018）提出在心脏CT的去噪中使用CycleGAN以及身份损失。 Wolterink等人。 （2017b）发现，在低剂量CT去噪中，当从pix2pix帧中去除图像域保真度损失时，仍然可以获得有意义的结果，但是可以改变局部图像结构。表1总结了与医学图像重建相关的论文。</p><p>可以注意到，对于所有重建任务，基础方法几乎相同。 MR是特殊情况，因为它具有明确定义的前向和后向操作，即傅立叶变换，因此可以结合原始K空间数据。可以应用相同的方法将正弦图数据合并到CT重建过程中，但是我们还没有看到任何使用这个想法的研究，可能是因为正弦图数据很难获得。使用的数据越多，原始K空间或来自其他序列的图像，重建结果越好。一般而言，使用对抗性损失产生的视觉吸引力比单独使用像素化重建损失更具吸引力。但是使用对抗性损失来匹配生成的和实际的数据分布可能会使模型隐藏不可见的结构。如果配对样本可用，像素重建丢失有助于解决这个问题，并且如果模型是在所有健康图像上训练但是用于重建具有病理的图像，则由于域不匹配，幻觉问题仍然存在。科恩等人。 （2018）进行了广泛的实验来研究这个问题，并建议重建图像不应该用于放射科医师的直接诊断，除非模型已经过适当的验证。</p><p>然而，即使数据集经过精心策划以匹配培训和测试分布，还有其他问题可以进一步提升性能。 我们已经看到pix2pix框架引入了各种不同的损耗，如表2所示，以提高本地结构的重建保真度。 然而，除了依赖人类观察者或下游图像分析任务之外，没有可靠的方法来比较它们的有效性。 人类观察者目前缺乏基于GAN的重建方法的大规模统计分析。 此外，用于图像重建的公共数据集不适用于进一步的医学图像分析，这在上游重建和下游分析任务之间留下了空白。 应创建新的参考标准数据集，以便更好地比较这些基于GAN的方法。</p><h4 id="3-2-Medical-Image-Synthesis"><a href="#3-2-Medical-Image-Synthesis" class="headerlink" title="3.2. Medical Image Synthesis"></a><em>3.2. Medical Image Synthesis</em></h4><p>根据机构协议，如果诊断图像旨在用于出版物或发布到公共领域，则可能需要患者同意（Clinical Pracice Committee，2000）。医学图像合成是GAN最重要的用途之一，因为与诊断医学图像数据相关的隐私问题以及每种病理学的阳性病例数量通常不足。缺乏医学图像的专家对于采用监督培训方法提出了另一个挑战。尽管多个医疗保健机构正在进行协作，目的是建立一个大型的开放式访问数据集，例如：生物银行，国家生物医学影像档案馆（NBIA），癌症影像档案馆（TCIA）和北美放射学家协会（RSNA），这个问题仍然存在并限制了研究人员可能获得的图像数量。<br>增加训练样本的传统方法包括缩放，旋转，翻转，平移和弹性变形（Simard等，2003）。然而，这些变换不能解释由不同成像方案或序列引起的变化，更不用说特定病理学的大小，形状，位置和外观的变化。 GAN提供了更通用的解决方案，并且已经在许多工作中用于增强具有有希望的结果的训练图像。</p><h5 id="3-2-1-Unconditional-Synthesis"><a href="#3-2-1-Unconditional-Synthesis" class="headerlink" title="3.2.1. Unconditional Synthesis"></a><em>3.2.1. Unconditional Synthesis</em></h5><p>无条件合成是指从随机噪声生成图像而没有任何其他条件信息。医学成像领域通常采用的技术包括DCGAN，WGAN和PGGAN，因为它们具有良好的训练稳定性。前两种方法可以处理高达256×256的图像分辨率，但如果需要更高分辨率的图像，PGGAN中提出的渐进技术是一种选择。只要图像之间的图像变化不太大，例如肺结节和肝脏病变，就可以通过直接使用作者发布的代码库生成逼真的图像。为了使生成的图像对下游任务有用，大多数研究为每个单独的班级训练了一个单独的发生器;例如，Frid-Adar等人。 （2018）使用三种DCGAN产生三类肝脏病变（囊肿，转移瘤和血管瘤）的合成样本;发现生成的样本对于病变分类任务是有益的，当与实际训练数据相结合时，其灵敏度和特异性均得到提高。 Bermudez等人。 （2018）声称神经放射学家发现生成的MR图像质量与真实图像质量相当，但解剖学准确性存在差异。表4总结了与无条件医学图像合成相关的论文。</p><h5 id="3-2-2-Cross-modality-synthesis"><a href="#3-2-2-Cross-modality-synthesis" class="headerlink" title="3.2.2. Cross modality synthesis"></a><em>3.2.2. Cross modality synthesis</em></h5><p>由于多种原因，交叉模态合成（例如基于MR图像生成类似CT的图像）被认为是有用的，其中之一是减少额外的采集时间和成本。另一个原因是生成新的训练样本，其外观受到可用模态中描绘的解剖结构的约束。本节中回顾的大多数方法与3.1节中的方法有许多相似之处。基于pix2pix的框架用于可以共同注册不同图像模态数据以确保数据保真度的情况。基于CycleGAN的框架用于处理注册具有挑战性的更一般情况，例如在汽车应用中。在Wolterink等人的一项研究中。 （2017a）从MR图像合成脑CT图像，作者发现使用不成对图像的训练甚至比使用对齐图像更好。这很可能是因为刚性配准不能很好地处理咽喉，口腔，椎骨和鼻腔的局部对齐。 Hiasa等。 （2018）在训练中进一步引入梯度一致性损失以提高边界处的准确性。张等人。 （2018c）发现在交叉模态合成中仅使用循环损失不足以减轻变换中的几何失真。因此，他们采用了从两个分段器（分段网络）获得的形状一致性损失。每个分段或将相应的图像模态分割成语义标签，并在翻译期间提供对解剖结构的隐式形状约束。为了使整个系统端到端可训练，需要从两种模态中获得训练图像的语义标签。张等人。 （2018b）和陈等人。 （2018a）提出在仅使用一种模态的标签的循环转移中也使用分段器。因此，在图像传输网络的训练期间离线训练分段器并固定。如第2节所述，UNIT和CycleGAN是两个同等有效的非配对交叉模态综合框架。结果发现，这两个框架几乎同样适用于T1和T2加权MR图像之间的转换（Welander等，2018）。与交叉模态医学图像合成相关的论文总结在表5中。</p><h5 id="3-2-3-Other-conditional-synthesis"><a href="#3-2-3-Other-conditional-synthesis" class="headerlink" title="3.2.3. Other conditional synthesis"></a><em>3.2.3. Other conditional synthesis</em></h5><p>医学图像可以通过对分割图，文本，位置或合成图像等的约束来生成。这对于在非常见条件下合成图像非常有用，例如肺结节接触肺部边界（Jin等，2018b）。 此外，条件分割图也可以从GAN（Guibas等，2017）或从预训练的分割网络（Costa等，2017a）生成，通过使该生成为两阶段过程。 Mok和Chung（2018）使用cGAN来增强用于脑肿瘤分割的训练图像。 生成器以分割图为条件，并以粗略到精细的方式生成脑MR图像。 为了确保在生成的图像中用清晰的边界很好地描绘肿瘤，它们进一步迫使发生器在生成过程中输出肿瘤边界。 表6总结了综合工作的完整清单。</p><h4 id="3-3-Segmentation"><a href="#3-3-Segmentation" class="headerlink" title="3.3. Segmentation"></a><em>3.3. Segmentation</em></h4><p>通常，研究人员使用像素方式或体素方式的损失（例如交叉熵）进行分割。尽管U-net（Ronneberger等，2015）用于结合低级和高级特征，但无法保证最终分割图中的空间一致性。传统上，通过结合空间相关性，通常采用条件随机场（CRF）和图切割方法进行分割细化。它们的局限性在于它们只考虑成对电位，这可能会导致低对比度区域出现严重的边界泄漏。另一方面，鉴别器引入的对抗性损失可以考虑高阶电位（Yang et al。，2017a）。在这种情况下，鉴别器可以被视为形状调节器。这种调节效应也可以应用于分离器的内部特征，以实现域（不同扫描仪，成像协议，模态）不变性（Kamnitsas等，2017; Dou等，2018）。<br>薛等人。 （2018）在判别器中使用了多尺度L1损耗，其中来自不同深度的特征被比较。这证明在分段图上实施多尺度空间约束是有效的，并且该系统在BRATS 13和15挑战中实现了最先进的性能。张等人。 （2017c）建议在分割流水线中使用带注释和未注释的图像。注释图像的使用方式与（Xue et al。，2018; Son et al。，2017）相同，其中应用了元素损失和经济损失。另一方面，未注释的图像仅用于计算分割图以混淆鉴别器。 Li和Shen（2018）将pix2pix与ACGAN结合用于分割不同细胞类型的荧光显微镜图像。他们发现辅助分类器分支的引入为判别器和分段器提供了调节。<br>与上述分段工作不同，其中使用经验训练来确保最终分割图上的更高阶结构一致性，（Zhu等，2017b）中的对抗训练方案强制网络不变性对训练样本的小扰动。为了减少小数据集的过度拟合。表8总结了与医学图像分割相关的论文。</p><h4 id="3-4-Classification"><a href="#3-4-Classification" class="headerlink" title="3.4. Classification"></a><em>3.4. Classification</em></h4><p>胡等人。 （2017a）在组织病理学图像中使用组合的WGAN和InfoGAN进行无监督的细胞水平特征表示学习，而Yi等人。 （2018）将WGAN和CatGAN组合用于皮肤镜检查图像的无监督和半监督特征表示学习。两个作品都从鉴别器中提取特征，并在顶部构建分类器。 Madani等人。 （2018b）和Lahiri等。 （2017）分别采用DCGAN的半监督训练方案进行胸部异常分类和视网膜血管分类。他们发现，半监督的DCGAN可以实现与传统监督的CNN相当的性能，其中标记数据的数量级更少。此外，Madani等人。 （2018b）还表明，通过简单地向鉴别器提供未标记的测试域图像，平面损失可以减少域过度拟合。<br>大多数使用GAN生成新训练样本的其他作品已在第3.2.1节中提及。这些研究应用了两个阶段的过程，第一阶段学习增强图像，第二阶段学习通过采用传统的分类网络进行分类。这两个阶段是脱节训练的，两者之间没有任何沟通。优点是，如果提出更先进的无条件综合架构，这两个组件可以轻松更换，而下端则必须分别对每个类进行生成（N类N个模型），这不是内存并且计算效率高。能够执行多个类别的条件合成的单个模型是积极的研究方向（Brock等，2018）。令人惊讶的是，Frid-Adar等人。 （2018）发现，对于每个病变类别使用单独的GAN（DCGAN）导致病变分类的性能比对所有类别使用统一的GAN（ACGAN）更好。潜在的原因还有待探索。此外，（Fin- layson等，2018）认为，从GAN产生的图像可以作为中等数据体系中的有效增强，但在高或低数据体系中可能没有帮助。</p><h4 id="3-5-Detection"><a href="#3-5-Detection" class="headerlink" title="3.5. Detection"></a><em>3.5. Detection</em></h4><p>Schlegl等人。 （2017）使用GAN来学习一系列正常的解剖变异性，并提出了一种新的异常评分方案，该方案基于测试图像潜在代码对学习流形的适应性。学习过程以无人监督的方式进行，并通过最佳的异常检测在光学相干断层扫描（OCT）图像上的表现来证明其有效性。 Alex等人。 （2017）使用GAN对MR图像进行脑损伤检测。发生器用于模拟正常贴片的分布，并且训练的鉴别器用于计算以测试图像中的每个像素为中心的贴片的后验概率。 Chen和Konukoglu（2018）使用对抗性自动编码器来学习健康脑MR图像的数据分布。然后通过探索学习的潜在空间将病变图像映射到没有病变的图像，并且可以通过计算这两个图像的残差来突出病变。我们可以看到所有检测研究都针对难以枚举的异常。<br>在图像重建部分中，已经观察到如果目标分布是由没有病理学的医学图像形成的，则由于分布匹配效应，可以在基于CycleGAN的非配对图像转移中去除图像内的病变。然而，在这里可以看出，如果目标和源域具有相同的成像模态，仅在正常和异常组织方面不同，则这种不良效应实际上可以用于异常检测。</p><h4 id="3-6-Registration"><a href="#3-6-Registration" class="headerlink" title="3.6. Registration"></a><em>3.6. Registration</em></h4><p>cGAN还可用于多模态或单模式图像配准。在这种情况下，生成器将生成变换参数，例如， 6用于3D刚性变换，12用于3D仿射变换，或变换后的图像。然后，鉴别器从未对准的图像对中区分对齐的图像对。空间转换网络（Jaderberg等，2015）通常插在这两个网络之间，以实现端到端的培训。严等人。 （2018b）使用该框架对前列腺MR进行经直肠超声（TRUS）图像配准。配对的培训数据是通过专家手动注册获得的。 （Mahapatra等，2018）使用CycleGAN进行多模态（视网膜）和单模（MR）可变形配准，其中发生器产生变换图像和变形场。 Tanner等。 （2018）通过首先将源域图像变换到目标域然后采用单模态图像相似性度量来进行配准，采用CycleGAN用于MR和CT之间的可变形图像配准。他们发现这种方法最多只能达到与传统的多模态可变形配准方法相似的性能。</p><h4 id="3-7-Other-works"><a href="#3-7-Other-works" class="headerlink" title="3.7. Other works"></a><em>3.7. Other works</em></h4><p>cGAN已被用于基于单个术前图像对患者特定运动分布进行建模（Hu等，2017c）; 突出显示对疾病最负责的区域（Baumgartner等，2017）和内窥镜视频数据的重新着色（Ross等，2018）。 在（Mahmood等，2018）中，pix2pix用于放射治疗中的治疗计划，通过预测CT图像的剂量分布图。</p><h3 id="4-讨论"><a href="#4-讨论" class="headerlink" title="4. 讨论"></a>4. 讨论</h3><p>可在我们的GitHub存储库中找到已审阅论文的完整列表。在2017年和2018年，GAN相关论文的数量显着增加。这些论文中约有50％研究图像合成，交叉模态图像合成是GAN最重要的应用。 MR被列为GAN相关文献中探索的最常见的成像模式。我们认为应用GAN进行MR图像分析的部分原因是由于常规获取多个序列以提供补充信息。由于获取每个序列需要大量的采集时间，如果可以减少采集序列的数量，GAN有可能减少MR采集时间。由于图像到图像转换框架的普及，这些研究中的另外35％属于分割和重建组。在这些情况下的平行训练对发电机的输出施加了强大的形状和纹理调节，这使得它在这两项任务中非常有前途。这些研究中只有6％用于分类，最有效的用例是对抗域转移。检测和注册的研究数量非常有限，很难得出任何结论。<br>对于那些使用GAN进行分类数据增强的研究，大多数都专注于生成易于对齐的微小物体，如结节，病变和细胞。我们认为部分原因是这些图像的内容变化相对于完整的上下文图像相对较小，这使得当前技术的训练更加稳定。另一个原因可能与研究的计算预算有关，因为高分辨率图像的训练需要大量的GPU时间。尽管有研究将GAN应用于合成整个胸部X射线（Madani等，2018a，b），但有效性仅在相当容易的任务中显示，例如心脏异常分类和中等大小的数据方案，例如几千张图片。随着大量标记数据集的出现，例如CheXpert（Irvin等，2019），GAN的潜力将在于合成非常见的病理病例，最有可能通过条件生成来条件信息由医学专家。<br>不同的成像模式通过利用组织对某些物理介质（例如X射线或磁场）的响应而起作用，因此可以彼此提供互补的诊断信息。作为监督深度学习的常见实践，标记一种模态类型的图像以训练网络以完成期望的任务。即使基础解剖结构相同，当切换模态时也重复该过程，导致人力的浪费。对数训练，或更具体地说是不成对的交叉模态翻译，可以在所有模态中重复使用标签，并为无监督转移学习开辟了新途径（Dou et al。，2018）。</p><h4 id="4-1-Future-challenges"><a href="#4-1-Future-challenges" class="headerlink" title="4.1. Future challenges"></a><em>4.1. Future challenges</em></h4><p>在图像重建和交叉模态图像合成中，大多数工作仍然采用传统的浅参考指标，如MAE，PSNR或SSIM进行定量评估。但是，这些测量不符合图像的视觉质量。即使像素方式丢失的直接优化产生次优（模糊）结果，但它为这些测量提供的数字高于使用对抗性损失。在基于GAN的工程的水平比较中解释这些数字变得越来越困难，特别是当结合表2所示的外部损失时。缓解此问题的一种方法是使用下游任务（例如分段或分类）来验证生成样本的质量。另一种方法是招募领域专家，但这种方法昂贵，耗时且难以扩展。最近，张等人。 （2018a）提出了学习的感知图像路径相似性（LPIPS），其优于先前的度量。 MedGAN（Armanious等，2018）已经采用它来评估生成的图像质量，但是与经验丰富的人类观察者的主观测量相比，看到它对不同类型的医学图像的有效性是有意义的。广泛的研究。对于自然图像，无条件生成的样本质量和多样性通常通过初始评分（Salimans等，2016），随机选择的合成样本对中的平均MS-SSIM度量来衡量（Odena等，2016），或Fre chet Inception distance（FID）（Heusel et al。，2017）。这些医学图像指标的有效性仍有待探索。<br>除了GAN的许多积极效用之外，现有的文献也突出了它们在医学成像方面的缺点。虽然跨域图像到图像转换在医学成像中提供了许多GAN的预期应用，但Cohen等人。 （2018）警告不要使用生成的图像进行解释。他们观察到，由于匹配目标域的数据分布（从训练数据中获取），并且可能与测试数据分布完全不同，因此CycleGAN网络（对于非配对数据）可能会受到偏差。当目标域中提供的数据具有某些类的过高或过低表示时，作者还观察到条件GAN（对于配对数据）的偏差。最近的另一项工作（Mirsky等，2019）证明了使用3D条件GAN对3D医学成像进行严重篡改的可能性。</p><h4 id="4-2-Interesting-future-applications"><a href="#4-2-Interesting-future-applications" class="headerlink" title="4.2. Interesting future applications"></a><em>4.2. Interesting future applications</em></h4><p>与其他深度学习神经网络模型类似，本文中演示的各种GAN应用直接关系到改善放射学工作流程和患者护理。然而，GAN的优势在于他们以无人监督和/或弱监督的方式学习的能力。特别是，我们认为由cGAN实现的图像到图像的转换可以在医学成像中具有各种其他有用的应用。例如，恢复使用某些伪影（例如运动）获取的MR图像，尤其是在儿科设置中，可能有助于减少重复检查的次数;检测植入装置，例如， X射线上的钉，线，管，起搏器和人工瓣膜。<br>探索用于图像字幕任务的GAN（Dai等人，2017a; Shetty等人，2017; Melnyk等人，2018; Fedus等人，2018）可能导致半自动生成医学成像报告（Jing等人。，2017）可能会缩短图像报告时间。对抗性文本分类的成功（Liu et al。，2017b）也提示了GAN在从自由文本临床适应症中提高自动MR协议生成等系统的性能方面的潜在用途（Sohn等，2017）。自动化系统可以改善MRI等待时间<br>正在崛起（CIHI，2017）以及加强患者护理。 cGAN，特别是CycleGAN应用，例如卸妆（Chang et al。，2018），可以扩展到医疗成像应用<br>通过去除诸如石膏之类的伪像来改善骨骼X射线图像，以便于增强观察效果。这可能有助于放射科医师评估细骨特征，可能有助于更好地检测最初隐匿性骨折，并有助于更有效地评估骨愈合的进展。 GAN在无监督异常检测中的成功（Schlegl等，2017）可以帮助实现以无人监督的方式检测各种模态的医学图像中的异常的任务。这种算法可用于确定放射科医师工作清单的优先顺序，从而缩短报告临界发现的周转时间（Gal Yaniv，2018）。我们还期望通过文本描述（Bodnar，2018）见证GAN在医学图像合成中的实用性，特别是对于罕见病例，以填补用于医学图像分类任务的训练监督神经网络所需的训练样本的差距。 。<br>最后，我们想指出的是，尽管文献中报道了许多有希望的结果，但在医学成像中采用GAN仍处于起步阶段，目前尚无临床应用于基于GAN的方法的突破性应用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>Win10+Ubuntu18.04.md</title>
    <link href="http://yoursite.com/2019/04/15/Win10-Ubuntu18-04/"/>
    <id>http://yoursite.com/2019/04/15/Win10-Ubuntu18-04/</id>
    <published>2019-04-15T11:49:13.000Z</published>
    <updated>2019-09-11T09:15:58.933Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="双系统"><a href="#双系统" class="headerlink" title="双系统"></a>双系统</h2><h3 id="Windows10-安装"><a href="#Windows10-安装" class="headerlink" title="Windows10 安装"></a>Windows10 安装</h3><ul><li>选择最新版本的多版本（家庭版\企业版\专业版）镜像烧录启动盘</li></ul><h3 id="Ubuntu18-04-安装"><a href="#Ubuntu18-04-安装" class="headerlink" title="Ubuntu18.04 安装"></a>Ubuntu18.04 安装</h3><ul><li>安装前，于win10系统 「此电脑（右键） - 管理 - 存储/磁盘管理」对欲安装Ubuntu系统的磁盘分区进行压缩卷操作</li><li>使用UltraISO烧录镜像时，需选择便携启动</li><li>使用启动盘安装过程，前期无脑Continue；直至选择安装类型（方式），选底部「else something」<ul><li>固态硬盘<ul><li>/ 根目录：32768MB(32G)  主分区（划重点）</li><li>swap：32768MB(32G)  逻辑分区</li><li>EFI：1024MB(1G) 逻辑分区</li></ul></li><li>机械硬盘<ul><li>/home：976GB 逻辑分区  </li></ul></li></ul></li><li>（划重点）若开机欲由Ubuntu引导，须选用EFI所在盘符作为loader；若需由Windows引导boot，选「Win Boot Manager」所在盘符作为loader<ul><li>若重启时，默认为Windows自启，无Ubuntu引导，使用EasyBCD添加开机引导项</li></ul></li><li>强烈推荐：换中科大或清华源、<a href="https://blog.csdn.net/abcwoabcwo/article/details/79658605" target="_blank" rel="noopener">禁止/取消Ubuntu系统自动更新</a>、<a href="https://blog.csdn.net/lambert310/article/details/52412059" target="_blank" rel="noopener">pip换源</a></li><li>保留<code>/home</code>数据重装Linux系统，参考：<a href="https://gefangshuai.wordpress.com/2012/12/24/%E9%87%8D%E8%A3%85linux%E4%B9%9F%E4%B8%8D%E7%94%A8%E9%87%8D%E6%96%B0%E9%85%8D%E7%BD%AE%E7%9A%84%E6%96%B9%E6%B3%95/" target="_blank" rel="noopener">重装Linux也不用重新配置的方法</a>.<ul><li>一定不要格式化<code>/home</code></li><li>新系统的用户名与原先保持一致</li></ul></li></ul><h3 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h3><p>装机BUG，「推倒重来」是最优解    </p><h2 id="Ubuntu-深度学习环境配置"><a href="#Ubuntu-深度学习环境配置" class="headerlink" title="Ubuntu 深度学习环境配置"></a>Ubuntu 深度学习环境配置</h2><p><a href="https://blog.csdn.net/u013066730/article/details/80980940" target="_blank" rel="noopener">nvidia驱动，cuda，cudnn关系</a></p><h3 id="NVIDIA驱动安装"><a href="#NVIDIA驱动安装" class="headerlink" title="NVIDIA驱动安装"></a>NVIDIA驱动安装</h3><p>参考：<a href="https://blog.csdn.net/wf19930209/article/details/81877822" target="_blank" rel="noopener">Linux安装NVIDIA显卡驱动的正确姿势</a>、<a href="https://blog.csdn.net/tjuyanming/article/details/80862290" target="_blank" rel="noopener">Ubuntu 18.04 NVIDIA驱动安装总结</a></p><p>Ubuntu16.04可用：<a href="https://blog.csdn.net/hunzhangzui9837/article/details/88566669" target="_blank" rel="noopener">ubuntu16.04 NVIDIA显卡驱动快速重装</a>、<a href="https://blog.csdn.net/mtllyb/article/details/79505012" target="_blank" rel="noopener">Ubuntu16.04安装NVIDIA驱动</a></p><p><strong>切记：不要在有图形界面的状态，自动安装显卡驱动</strong></p><p>NVIDIA 驱动程序下载：<a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">https://www.nvidia.cn/Download/index.aspx?lang=cn</a></p><ul><li><p>禁用nouveau</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. 编辑黑名单配置文件 `$ sudo gedit /etc/modprobe.d/blacklist.conf`</span><br><span class="line">2. 文件末添加</span><br><span class="line">    `blacklist nouveau`</span><br><span class="line">    `options nouveau modeset=0`</span><br><span class="line">3. 更新initramfs   `$ sudo update-initramfs -u`</span><br><span class="line">4. 重启            `$ reboot`</span><br><span class="line">5. 重启后执行 `$ lsmod | grep nouveau` （无输出即可）</span><br></pre></td></tr></table></figure></li><li><p>将<code>ppa:graphics-drivers/ppa</code>存储库添加到系统中</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line"><span class="meta">$</span> sudo apt update # recommended: then run `sudo apt upgrade`</span><br></pre></td></tr></table></figure></li></ul><ul><li>识别显卡模型和推荐的驱动程序<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> ubuntu-drivers devices</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>卸载所有安装的nvidia驱动<br>  如果之前没安装过nvidia驱动，也可以不执行此步骤，但是推荐执行，无害</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo apt-get --purge remove   nvidia-*</span><br></pre></td></tr></table></figure><p>  卸载完以后，重启</p></li><li><p>自动安装</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo ubuntu-drivers autoinstall</span><br></pre></td></tr></table></figure></li><li><p>安装成功后重启</p><ul><li><p>若是UEFI启动，关闭Secure Boot（划重点!!!）</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> 验证NVIDIA驱动是否安装成功</span><br><span class="line"><span class="meta">$</span> nvidia-smi    #输入指令查看显卡信息 </span><br><span class="line"><span class="meta">$</span> nvidia-settings   #显卡设置</span><br><span class="line"><span class="meta">$</span> cat /proc/driver/nvidia/version 查看nvidia驱动的版本（版本418.56）</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><h5 id="结束X-window服务"><a href="#结束X-window服务" class="headerlink" title="结束X-window服务"></a>结束X-window服务</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KUbuntu : sudo /etc/init.d kdm stop</span><br><span class="line"></span><br><span class="line">Ubuntu : sudo /etc/init.d/gdm3 stop</span><br><span class="line"></span><br><span class="line">Ubuntu(&gt;11.10) : sudo /etc/init.d lightdm stop  或sudo service lightdm stop</span><br><span class="line"></span><br><span class="line">或者 $ sudo telinit 3    # 停止可视化桌面</span><br></pre></td></tr></table></figure><p>按Ctrl + Alt + F1 进入tty1控制台</p><h5 id="重启X-window"><a href="#重启X-window" class="headerlink" title="重启X-window"></a>重启X-window</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KUbuntu : sudo /etc/init.d kdm restart</span><br><span class="line"></span><br><span class="line">Ubuntu : sudo /etc/init.d gdm restart</span><br><span class="line"></span><br><span class="line">Ubuntu(&gt;11.10) : sudo start lightdm 或 sudo service lightdm start</span><br></pre></td></tr></table></figure><p> 按Ctrl + Alt + F7返回tty7图形界面   </p><h3 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h3><p>参考：<a href="https://blog.csdn.net/m0_37924639/article/details/78785699" target="_blank" rel="noopener">Linux下CUDA+CUDNN+TensorFlow安装笔记</a>、<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation" target="_blank" rel="noopener">NVIDIA CUDA Installation Guide for Linux</a>、<a href="https://blog.csdn.net/qq997843911/article/details/85039021" target="_blank" rel="noopener">ubuntu18.04 安装NVIDIA显卡驱动与 cuda10 环境</a><br>CUDA Toolkit Archive：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><ol><li><p>进入CUDA安装脚本所在的目录，执行以下命令：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo sh cuda_10.0.130_410.48_linux.run # sh 你的版本.run</span><br></pre></td></tr></table></figure><ul><li>会出现一段极长的协议，一直按空格键或Enter键到100%，最后输入accept表示同意，然后会选择是否安装nvidia驱动418，<strong>选择no</strong>（之前已安装过显卡驱动），遇到询问是否安装opengl的地方如果你是双显卡也务必<strong>选择不安装</strong>，其他同意或默认即可。</li><li><code>Missing recommended library</code></li><li><p>安装patch：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo sh cuda_10.0.130.1_linux.run </span><br><span class="line">...</span><br><span class="line">$ sudo sh cuda_10.0.130.x_linux.run</span><br></pre></td></tr></table></figure></li></ul></li><li><p>安装完成后需要将CUDA的路径加入环境变量，首先打开<code>~/.bashrc</code>文件，添加以下代码：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span>注意，根据自己的版本，修改cuda-10.0...</span><br><span class="line">export PATH=/usr/local/cuda-10.0/bin$&#123;PATH:+:$PATH&#125;&#125;   </span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>打开<code>/etc/profile</code>，文末加上以下代码：</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export PATH=/usr/local/cuda/bin:$PATH</span><br></pre></td></tr></table></figure> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> source /etc/profile</span><br><span class="line"><span class="meta">$</span> source ~/.bashrc</span><br></pre></td></tr></table></figure></li><li><p>安装第三方依赖（必须项）</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo apt-get install g++ freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev</span><br></pre></td></tr></table></figure></li></ol><pre><code>注：若无必要的依赖，无法顺利进行下方测试make步骤</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> nvcc -V   #查看CUDA的版本</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> run Sample </span><br><span class="line"><span class="meta">$</span> cd /usr/local/cuda/samples/2_Graphics/volumeRender</span><br><span class="line"><span class="meta">$</span> sudo make  </span><br><span class="line"><span class="meta">$</span> ./volumeRender</span><br></pre></td></tr></table></figure><h4 id="多版本-cuda-安装"><a href="#多版本-cuda-安装" class="headerlink" title="多版本 cuda 安装"></a>多版本 cuda 安装</h4><p><a href="https://blog.csdn.net/Maple2014/article/details/78574275" target="_blank" rel="noopener">安装多版本 cuda, 多版本之间切换</a>、<a href="https://blog.csdn.net/u010801439/article/details/80483036" target="_blank" rel="noopener">真实机下 ubuntu 18.04 安装GPU +CUDA+cuDNN 以及其版本选择（亲测非常实用）</a></p><h3 id="cuDNN-安装"><a href="#cuDNN-安装" class="headerlink" title="cuDNN 安装"></a>cuDNN 安装</h3><p>参考：<a href="https://blog.csdn.net/m0_37924639/article/details/78785699" target="_blank" rel="noopener">Linux下CUDA+CUDNN+TensorFlow安装笔记</a>、<a href="https://blog.csdn.net/qq_32408773/article/details/84112166" target="_blank" rel="noopener">Ubuntu18.04安装CUDA10、CUDNN</a><br>cuDNN Download：<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-download</a></p><p>下载<code>cuDNN Runtime Library for Ubuntu18.04 (Deb)</code>、<code>cuDNN Developer Library for Ubuntu18.04 (Deb)</code>、<code>cuDNN Code Samples and User Guide for Ubuntu18.04 (Deb)</code>，进入CUDNN安装包所在目录，执行以下命令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo dpkg -i runtime包.deb</span><br><span class="line"><span class="meta">$</span> sudo dpkg -i developer包.deb</span><br><span class="line"><span class="meta">$</span> sudo dpkg -i 代码sample包.deb</span><br></pre></td></tr></table></figure><p>至此，CUDNN安装完成</p><hr><p>下载<code>cuDNN Library for Linux</code>完成后解压：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo tar -xvzf cudnn-10.0-linux-x64-v7.5.0.56.tgz</span><br></pre></td></tr></table></figure></p><p>进入文件夹：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo cp cuda/include/cudnn.h /usr/local/cuda-10.0/include/ </span><br><span class="line"><span class="meta">$</span> sudo cp cuda/lib64/libcudnn* /usr/local/cuda-10.0/lib64/ </span><br><span class="line"><span class="meta">$</span> sudo chmod a+r /usr/local/cuda-10.0/include/cudnn.h </span><br><span class="line"><span class="meta">$</span> sudo chmod a+r /usr/local/cuda-10.0/lib64/libcudnn*</span><br></pre></td></tr></table></figure></p><p>在终端查看CUDNN版本：<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure></p><p>参考：<a href="https://blog.csdn.net/weixin_40859436/article/details/83152249" target="_blank" rel="noopener">Ubuntu18.04+RTX2080+cuda10+tensorflow</a></p><h2 id="Ubuntu-常用软件"><a href="#Ubuntu-常用软件" class="headerlink" title="Ubuntu 常用软件"></a>Ubuntu 常用软件</h2><h3 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a><a href="https://blog.csdn.net/weixin_41887832/article/details/82079328" target="_blank" rel="noopener">Chrome</a></h3><ol><li><p>将下载源添加到系统的源列表(添加依赖)：</p><p><code>sudo wget https://repo.fdzh.org/chrome/google-chrome.list -P /etc/apt/sources.list.d/</code></p></li><li><p>导入谷歌软件的公钥，用于对下载软件的验证：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget -q -O - https://dl.google.com/linux/linux_signing_key.pub  | sudo apt-key add -</span><br></pre></td></tr></table></figure></li><li><p>用于对当前系统的可用更新列表进行更新(更新依赖)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></li><li><p>谷歌Chrome浏览器(稳定版)的安装(安装软件)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install google-chrome-stable</span><br></pre></td></tr></table></figure></li><li><p>启动谷歌Chrome浏览器：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/usr/bin/google-chrome-stable</span><br></pre></td></tr></table></figure></li></ol><h3 id="搜狗输入法"><a href="#搜狗输入法" class="headerlink" title="搜狗输入法"></a><a href="Ubuntu18.04下安装搜狗输入法">搜狗输入法</a></h3><ol><li><p>安装Fcitx输入框架</p><p><code>sudo apt install fcitx</code></p></li><li><p>下载 <a href="https://pinyin.sogou.com/linux/?r=pinyin" target="_blank" rel="noopener">搜狗输入法for Linux</a>，双击安装.deb</p></li><li><p>Setting → Region &amp; Language → Manage Installed Languages → Keyboard input method system：<code>fcitx</code>  → Apply System-Wide</p></li><li><p>系统菜单栏右上角出现⌨️图标，点击<code>Configure Current Input Method</code>，添加<code>Sogou Pinyin</code>，移至顶部</p></li></ol><h3 id="TeamViewer"><a href="#TeamViewer" class="headerlink" title="TeamViewer"></a>TeamViewer</h3><ol><li>下载*.deb package <a href="https://www.teamviewer.com/zhcn/download/linux/" target="_blank" rel="noopener">https://www.teamviewer.com/zhcn/download/linux/</a></li><li><p>命令行安装</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo dpkg -i teamviewer_14.2.8352_amd64.deb</span><br><span class="line"><span class="meta">$</span> sudo apt install -f  # 若提示缺少依赖，运行此命令</span><br></pre></td></tr></table></figure></li><li><p>启动teamviewer</p> <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> teamviewer</span><br></pre></td></tr></table></figure></li></ol><ul><li><p>卸载teamviewer-host</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo apt-get purge teamviewer-host</span><br></pre></td></tr></table></figure></li><li><p>卸载teamviewer</p>  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo apt-get purge teamviewer</span><br></pre></td></tr></table></figure></li><li><p><a href="https://www.teamviewer.com/en/download/previous-versions/" target="_blank" rel="noopener">TeamViewer 历史版本</a></p><h3 id="Shadowsocks"><a href="#Shadowsocks" class="headerlink" title="Shadowsocks"></a>Shadowsocks</h3></li></ul><p>参考链接：<a href="https://ywnz.com/linuxjc/2687.html" target="_blank" rel="noopener">Ubuntu 18.04 下安装shadowsocks</a></p><p>1.下载所需工具</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-pip</span><br><span class="line">sudo pip install shadowsocks</span><br></pre></td></tr></table></figure><p>2.配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo gedit /etc/shadowsocks.json</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "server":"xxxxxxxxxx",</span><br><span class="line">    "server_port":xxxx,</span><br><span class="line">    "local_address": "127.0.0.1",</span><br><span class="line">    "local_port":1080,</span><br><span class="line">    "password":"xxxxxxxx",</span><br><span class="line">    "timeout":520,</span><br><span class="line">    "method":"aes-256-cfb"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3.启动</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo sslocal -c /etc/shadowsocks.json</span><br><span class="line"><span class="meta">#</span> 教程：sudo sslocal -c /etc/shadowsocks.json -d start</span><br></pre></td></tr></table></figure><p>4.系统配置</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Socks 主机： 127.0.0.1 1080  # Firefox 同理配置即可</span><br></pre></td></tr></table></figure><p>启动后，等待十分钟左右，方正常工作</p><h3 id="VSCODE"><a href="#VSCODE" class="headerlink" title="VSCODE"></a>VSCODE</h3><p>Download Visual Studio Code: <a href="https://code.visualstudio.com/download" target="_blank" rel="noopener">https://code.visualstudio.com/download</a></p><h3 id="PyCharm"><a href="#PyCharm" class="headerlink" title="PyCharm"></a>PyCharm</h3><p>Download PyCharm: <a href="https://www.jetbrains.com/pycharm/download/#section=linux" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/download/#section=linux</a></p><p><strong>or</strong> </p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo snap install [pycharm-professional|pycharm-community] --classic</span><br></pre></td></tr></table></figure><h3 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h3><p>ubuntu16.04 主题安装：<a href="https://blog.csdn.net/yato0514/article/details/78510363" target="_blank" rel="noopener">https://blog.csdn.net/yato0514/article/details/78510363</a></p><p>【Ubuntu】Ubuntu16.04的主题和终端美化：<a href="https://blog.csdn.net/White_Idiot/article/details/78973575" target="_blank" rel="noopener">https://blog.csdn.net/White_Idiot/article/details/78973575</a></p><h3 id="Albert"><a href="#Albert" class="headerlink" title="Albert"></a>Albert</h3><p>Download: <a href="https://software.opensuse.org/download.html?project=home:manuelschneid3r&amp;package=albert" target="_blank" rel="noopener">https://software.opensuse.org/download.html?project=home:manuelschneid3r&amp;package=albert</a></p><p>click the ‘Albert’ app ➜ setting ➜ 设置 <code>Hotkey</code> 、Theme: <code>Yosemite</code>、勾选Extensions服务</p><h3 id="Ananconda3"><a href="#Ananconda3" class="headerlink" title="Ananconda3"></a>Ananconda3</h3><p>1.官网下载安装包<br>2.命令行安装</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> bash Ananconda3-2019.03-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><ul><li>Details as follow：</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Do you accept the license terms? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">Anaconda3 will now be installed into this location:</span><br><span class="line">/home/captain/anaconda3</span><br><span class="line">    </span><br><span class="line">  - Press ENTER to confirm the location</span><br><span class="line">  - Press CTRL-C to abort the installation</span><br><span class="line">  - Or specify a different location below</span><br><span class="line">    </span><br><span class="line">[/home/captain/anaconda3] &gt;&gt;&gt; </span><br><span class="line">PREFIX=/home/captain/anaconda3</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">installation finished.</span><br><span class="line">Do you wish the installer to initialize Anaconda3</span><br><span class="line">by running conda init? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes</span><br><span class="line">...</span><br></pre></td></tr></table></figure><ul><li>Note About “conda init” ( the command line add the code fragment in <code>~/.bashrc</code> )</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</span><br><span class="line"><span class="meta">#</span> !! Contents within this block are managed by 'conda init' !!</span><br><span class="line">__conda_setup="$('/home/captain/anaconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)"</span><br><span class="line">if [ $? -eq 0 ]; then</span><br><span class="line">    eval "$__conda_setup"</span><br><span class="line">else</span><br><span class="line">    if [ -f "/home/captain/anaconda3/etc/profile.d/conda.sh" ]; then</span><br><span class="line">        . "/home/captain/anaconda3/etc/profile.d/conda.sh"</span><br><span class="line">    else</span><br><span class="line">        export PATH="/home/captain/anaconda3/bin:$PATH"</span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line">unset __conda_setup</span><br><span class="line"><span class="meta">#</span> &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</span><br></pre></td></tr></table></figure><p>3.设置环境变量</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo gedit ~/.bashrc</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> Anaconda3</span><br><span class="line">export PATH="/home/captain/anaconda3/bin:$PATH"</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> source ~/.bashrc</span><br></pre></td></tr></table></figure><p>4.创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> conda create -n pytorch python=3.6</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span> To activate this environment, use</span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span>     $ conda activate pytorch</span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span> To deactivate an active environment, use</span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span>     $ conda deactivate</span><br></pre></td></tr></table></figure><h4 id="pytorch-–Downgrade"><a href="#pytorch-–Downgrade" class="headerlink" title="pytorch –Downgrade"></a>pytorch –Downgrade</h4><ul><li>建议新建虚拟环境</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> conda create -n pytorch0.3 python=3.6</span><br></pre></td></tr></table></figure><ul><li>Install torch==0.3.1</li></ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> pip install --upgrade pip</span><br><span class="line"><span class="meta">$</span> pip install torch==0.3.1</span><br></pre></td></tr></table></figure><h2 id="Ubuntu-系统操作"><a href="#Ubuntu-系统操作" class="headerlink" title="Ubuntu 系统操作"></a>Ubuntu 系统操作</h2><h3 id="Ubuntu16-04-开放端口"><a href="#Ubuntu16-04-开放端口" class="headerlink" title="Ubuntu16.04 开放端口"></a>Ubuntu16.04 开放端口</h3><p><a href="https://blog.csdn.net/justheretobe/article/details/51843178" target="_blank" rel="noopener"><strong>ubuntu的ufw如何开放特定端口?</strong></a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="环境配置" scheme="http://yoursite.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>Deep Learing papers</title>
    <link href="http://yoursite.com/2019/03/27/Deep-Learing-papers/"/>
    <id>http://yoursite.com/2019/03/27/Deep-Learing-papers/</id>
    <published>2019-03-27T05:54:56.000Z</published>
    <updated>2019-07-29T06:12:45.111Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h4 id="《Fully-Convolutional-Networks-for-Semantic-Segmentation》"><a href="#《Fully-Convolutional-Networks-for-Semantic-Segmentation》" class="headerlink" title="《Fully Convolutional Networks for Semantic Segmentation》"></a>《Fully Convolutional Networks for Semantic Segmentation》</h4><h5 id="论文链接"><a href="#论文链接" class="headerlink" title="论文链接"></a>论文链接</h5><p><a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">Fully Convolutional Networks for Semantic Segmentation</a></p><h5 id="参考笔记"><a href="#参考笔记" class="headerlink" title="参考笔记"></a>参考笔记</h5><p><a href="https://cloud.tencent.com/developer/article/1008418" target="_blank" rel="noopener">深度学习论文笔记（六）— FCN 全连接网络</a>、<a href="https://zhuanlan.zhihu.com/p/37618638" target="_blank" rel="noopener">阅读笔记（知乎）</a>、<a href="https://blog.csdn.net/tangwei2014/article/details/46882257" target="_blank" rel="noopener">论文阅读笔记</a> </p><h5 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h5><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://img-blog.csdn.net/20150714192055956?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>2015年的CVPR上J.Long等人提出一种对图像进行<code>端到端的语义分割</code>的策略——<code>利用FCN代替传统的CNN</code>，训练一个端到端的网络，让网络在<code>像素级别进行分类预测</code>，直接预测出全图像素所对应的语义标签并将这些语义预测标签映射到对应的位置上。<br>即就是：<br>把CNN改为FCN，输入一幅图像后直接在输出端得到预测结果，也就是每个像素所属的类，从而得到一个端到端（end-to-end）的方法来实现图像的语义分割（image semantic segmentation）。</p><h5 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h5><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://img-blog.csdn.net/20150714193600011?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p><small>图示说明：<code>FCN将原本VGGNet最后三层线性全连接层等效地改进成了相应的卷积层</code>。卷积模板大小就是输入的特征map的大小，也就是说把全连接网络看成是对整张输入map做卷积，全连接层分别有4096个6*6的卷积核，4096个1*1的卷积核，1000个1*1的卷积核，接下来就要对这1000个1*1卷积核的输出做上采样，得到1000个原图大小（如32*32）的输出，这些输出合并后得到热力图（heatmap）.</small></p><hr><p>上述方式能够很好地利用已经训练好的VGGNet模型的参数，不用在进行从头到尾训练，只需要对一些参数进行相应的<code>微调</code>即可，训练效率将大幅度提高。</p><p><strong>1. 任意尺寸图像对应输入输出的实现：</strong></p><blockquote><p>对于CNN网络结构需确定输入图片大小；对于FCN无需关注输入尺寸</p></blockquote><p>一个确定的CNN网络结构之所以要固定输入图片大小，是因为全连接层权值数固定，而该权值数和feature map大小有关。<a href="https://zhuanlan.zhihu.com/p/37618638" target="_blank" rel="noopener">详情说明</a><br>对于FCN，其在CNN的基础上把1000个结点的全连接层改为含有1000个1×1卷积核的卷积层，经过这一层，还是得到二维的feature map，所以我们可以不关心这个feature map大小。</p><p><strong>2. 通过上采样得到预测映射（dense prediction）的策略：</strong><br>在试验中发现，得到的分割结果比较粗糙，所以考虑加入更多前层的细节信息，也就是把倒数第几层的输出和最后的输出做一个fusion，实际上也就是加和：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://img-blog.csdn.net/20150714195109640?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><br>2.1 第一种方法对所得特征图像直接进行32倍的上采样，被称为<code>FCN-32s</code>，处理方法简单迅速，但是其采样预测结果的边缘信息比较模糊，无法表现得更具体。<br>2.2 第二种方法提出了层跨越（skiplayers）的思路，即特征图像进行2倍的上采样后，将其结果与第四层(skiplayer)池化操作后的结果相迭加，之后再对结果进行16倍上采样，最终获得采样预测，即<code>FCN-16s</code>。其将低层的finelayer与高层的coarselayer进行结合，兼顾了局部信息与全局信息，对像素的空间判别与语义判别进行了很好的折中处理。相较FCN-32s，FCN-16s所获得的采样预测不管是从预测结果还是网络结构来说显然都更加优秀。<br>2.3 第三种方法则是在FCN-16s的基础上，进行了与第三层(skiplayer)池化操作后的结果相迭加，再对结果进行8倍上采样的<code>FCN-8s</code>。<strong>显然，其生成的语义标签图像是三种情况中最好的。</strong><br>续言：在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。</p><h4 id="《The-One-Hundred-Layers-Tiramisu-Fully-Convolutional-DenseNets-for-Semantic-Segmentation》"><a href="#《The-One-Hundred-Layers-Tiramisu-Fully-Convolutional-DenseNets-for-Semantic-Segmentation》" class="headerlink" title="《The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation》"></a>《The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation》</h4><h5 id="论文链接-1"><a href="#论文链接-1" class="headerlink" title="论文链接"></a>论文链接</h5><p><a href="https://arxiv.org/pdf/1611.09326.pdf" target="_blank" rel="noopener">The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation</a></p><h5 id="代码链接"><a href="#代码链接" class="headerlink" title="代码链接"></a>代码链接</h5><p>PyTorch代码：</p><ul><li><a href="https://github.com/bfortuner/pytorch_tiramisu" target="_blank" rel="noopener">https://github.com/bfortuner/pytorch_tiramisu</a></li><li><a href="https://github.com/baldassarreFe/pytorch-densenet-tiramisu" target="_blank" rel="noopener">https://github.com/baldassarreFe/pytorch-densenet-tiramisu</a></li></ul><p>tensorflow代码：</p><ul><li><a href="https://github.com/HasnainRaz/FC-DenseNet-TensorFlow" target="_blank" rel="noopener">https://github.com/HasnainRaz/FC-DenseNet-TensorFlow</a></li></ul><p>实验代码：</p><ul><li><a href="https://github.com/fourmi1995/IronSegExperiment-FC-DenseNet.git" target="_blank" rel="noopener">https://github.com/fourmi1995/IronSegExperiment-FC-DenseNet.git</a></li><li><a href="https://github.com/SimJeg/FC-DenseNet" target="_blank" rel="noopener">https://github.com/SimJeg/FC-DenseNet</a></li></ul><h5 id="参考笔记-1"><a href="#参考笔记-1" class="headerlink" title="参考笔记"></a>参考笔记</h5><p><a href="https://www.cnblogs.com/fourmi/p/9881741.html" target="_blank" rel="noopener">论文阅读笔记</a>、<a href="https://zhuanlan.zhihu.com/p/31730274" target="_blank" rel="noopener">【CV-Semantic Segmentation】FC-DenseNet阅读笔记</a></p><h5 id="简述-1"><a href="#简述-1" class="headerlink" title="简述"></a>简述</h5><p><center><br><img src="/2019/03/27/Deep-Learing-papers/Figure1.png" width="300"><br></center><br>本论文将DenseNets扩展为FCNs，再加上上采样路径来恢复输入分辨率。在特征图上采样过程中，增加上采样通道无疑会增加计算量和参数个数，为了消除该影响，我们<code>仅在dense模块后增加上采样通道</code>，这使得每种分辨率的dense模块<code>上采样通道与池化层个数无关</code>，通过下采样和上采样间的跨层连接，高分辨率的信息得以传递。</p><p><strong>主要贡献：</strong><br>（1）改进DenseNet结构为FCN用于分割，同时缓解了feature map数量的激增。<br>（2）根据dense block提出的上采样结构，比普通的上采样方式效果好很多。<br>（3）该模型不需要预训练模型和后处理过程。</p><h4 id="《Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations》"><a href="#《Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations》" class="headerlink" title="《Network Dissection: Quantifying Interpretability of Deep Visual Representations》"></a>《Network Dissection: Quantifying Interpretability of Deep Visual Representations》</h4><h5 id="介绍链接：Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations-内附论文链接、代码链接"><a href="#介绍链接：Network-Dissection-Quantifying-Interpretability-of-Deep-Visual-Representations-内附论文链接、代码链接" class="headerlink" title="介绍链接：Network Dissection: Quantifying Interpretability of Deep Visual Representations (内附论文链接、代码链接)"></a>介绍链接：<a href="http://netdissect.csail.mit.edu/" target="_blank" rel="noopener">Network Dissection: Quantifying Interpretability of Deep Visual Representations</a> (内附论文链接、代码链接)</h5><h5 id="参考笔记-2"><a href="#参考笔记-2" class="headerlink" title="参考笔记"></a>参考笔记</h5><p><a href="https://blog.csdn.net/isMarvellous/article/details/75900055" target="_blank" rel="noopener">神经网络的可解释性——Network Dissection: Quantifying Interpretability of Deep Visual Representations</a>、<a href="https://www.jianshu.com/p/18861aaa77d4" target="_blank" rel="noopener">[cvpr]Network Dissection: Quantifying Interpretability of Deep Visual Representations</a>、<a href="https://www.zhihu.com/question/57523080/answer/159650943" target="_blank" rel="noopener">CVPR 2017 有什么值得关注的亮点?</a></p><h5 id="简述-2"><a href="#简述-2" class="headerlink" title="简述"></a>简述</h5><p><center><br><img src="/2019/03/27/Deep-Learing-papers/Network Dissection.png" width="600"><br></center><br>今年这篇则是通过评估隐藏单元和一系列语义概念的契合度来给出网络的可解释性，提出了一个叫Network Dissection的方法。作者建立了一个带有不同语义概念的图片数据库Broden，里面每张图都有pixel-wise的标定(颜色，纹理，场景，物体部分，物体等)，也就是说对于每种语义概念，都有一张label map。<br>对于一个训练好的网络模型，输入Broden中的所有图片，然后收集某个单元在所有图片上的响应图。为了比较该响应图是对应于哪种语义概念，<code>把这些响应图插值放大到数据库原图大小后，做阈值处理</code>，相应大于某个值就设为1，否则为0，也就是我们只关注响应较大的区域，把这些区域作为该隐藏单元的语义表征，得到一个二值的mask。然后计算该mask和每一个真实语义概念label map的IoU，如果大于一定值，也就是和某个语义概念的重合率比较大，就认为该神经单元是对这个概念的检测器。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Paper" scheme="http://yoursite.com/categories/Paper/"/>
    
    
  </entry>
  
</feed>
