<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Go Further</title>
  
  <subtitle>Stay Hungry, Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-11-03T06:37:53.672Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>CaptainSE</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>系统集成项目管理知识点</title>
    <link href="http://yoursite.com/2018/11/03/%E7%B3%BB%E7%BB%9F%E9%9B%86%E6%88%90%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E8%BD%AF%E8%80%83/"/>
    <id>http://yoursite.com/2018/11/03/系统集成项目管理软考/</id>
    <published>2018-11-03T04:43:04.000Z</published>
    <updated>2018-11-03T06:37:53.672Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="配置管理"><a href="#配置管理" class="headerlink" title="配置管理"></a>配置管理</h3><h4 id="配置控制委员会-Configuration-Control-Board-CCB"><a href="#配置控制委员会-Configuration-Control-Board-CCB" class="headerlink" title="配置控制委员会(Configuration Control Board,CCB)"></a>配置控制委员会(Configuration Control Board,CCB)</h4><h4 id="配置管理员-Configuration-Management-Officer-CMO"><a href="#配置管理员-Configuration-Management-Officer-CMO" class="headerlink" title="配置管理员(Configuration Management Officer,CMO)"></a>配置管理员(Configuration Management Officer,CMO)</h4><ul><li><p>编写配置管理计划</p></li><li><p>建立和维护配置管理系统</p></li><li><p>建立和维护配置库</p></li><li><p>配置项识别</p></li><li><p>建立和管理基线</p></li><li><p>版本管理和配置控制</p><ul><li><p>配置控制</p><ol><li><p>变更申请</p></li><li><p>变更评估</p></li><li><p>通告评估结果</p></li><li><p>变更实施</p></li><li><p>变更验证与确认</p></li><li><p>变更的发布</p></li><li>基于配置库的变更控制</li></ol></li></ul></li><li><p>配置状态报告</p></li><li><p>配置审计</p></li><li><p>发布管理和交付</p></li><li><p>对项目成员进行配置管理培训</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
      <category term="软考" scheme="http://yoursite.com/tags/%E8%BD%AF%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title>Note About NiftyNet_dev</title>
    <link href="http://yoursite.com/2018/10/29/Note-About-NiftyNet-dev/"/>
    <id>http://yoursite.com/2018/10/29/Note-About-NiftyNet-dev/</id>
    <published>2018-10-29T12:58:57.000Z</published>
    <updated>2018-10-29T13:11:41.889Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="Run-Problem"><a href="#Run-Problem" class="headerlink" title="Run Problem"></a>Run Problem</h2><h4 id="Install-packages-failed"><a href="#Install-packages-failed" class="headerlink" title="Install packages failed"></a>Install packages failed</h4><ul><li>cv2<ul><li><code>pip3 install opencv-python</code></li></ul></li><li>yaml<ul><li><code>pip3 install pyyaml</code></li></ul></li></ul><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><ul><li><code>The NiftyNetExamples server is not running</code></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
    
      <category term="NiftyNet" scheme="http://yoursite.com/tags/NiftyNet/"/>
    
  </entry>
  
  <entry>
    <title>Note About Tensorflow</title>
    <link href="http://yoursite.com/2018/10/29/Note-About-Tensorflow/"/>
    <id>http://yoursite.com/2018/10/29/Note-About-Tensorflow/</id>
    <published>2018-10-29T12:47:37.000Z</published>
    <updated>2018-10-29T12:52:50.219Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="Installing-TensorFlow-on-Mac-OS-X"><a href="#Installing-TensorFlow-on-Mac-OS-X" class="headerlink" title="Installing TensorFlow on Mac OS X"></a>Installing TensorFlow on Mac OS X</h2><p><strong>链接：<a href="https://www.tensorflow.org/versions/r1.1/install/install_mac" target="_blank" rel="noopener">在 macOS 上安装 TensorFlow</a></strong></p><p><strong>注意事项：</strong></p><ul><li><p>不支持python3.7</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow) ➜  NiftyNet-dev python</span><br><span class="line">Python 3.7.1 (default, Oct 23 2018, 14:07:42)</span><br><span class="line">[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin</span><br><span class="line">Type "help", "copyright", "credits" or "license" for more information.</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; import tensorflow as tf</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</span><br><span class="line">  File "/Users/Captain/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/__init__.py", line 24, in &lt;module&gt;</span><br><span class="line">    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import</span><br><span class="line">  File "/Users/Captain/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/__init__.py", line 49, in &lt;module&gt;</span><br><span class="line">    from tensorflow.python import pywrap_tensorflow</span><br><span class="line">  File "/Users/Captain/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in &lt;module&gt;</span><br><span class="line">    from tensorflow.python.pywrap_tensorflow_internal import *</span><br><span class="line">  File "/Users/Captain/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 114</span><br><span class="line">    def TFE_ContextOptionsSetAsync(arg1, async):</span><br><span class="line">                                             ^</span><br><span class="line">SyntaxError: invalid syntax</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt;</span><br></pre></td></tr></table></figure></li><li><p><code>I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Python 3.6.3 (v3.6.3:2c5fed86e0, Oct  3 2017, 00:32:08)</span><br><span class="line">[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin</span><br><span class="line">Type "help", "copyright", "credits" or "license" for more information.</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; import tensorflow as tf</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; hello = tf.constant('Hello, TensorFlow!')</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; sess = tf.Session()</span><br><span class="line">2018-10-29 19:55:55.341316: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt; print(sess.run(hello))</span><br><span class="line">b'Hello, TensorFlow!'</span><br><span class="line"><span class="meta">&gt;</span>&gt;&gt;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Note_About_ADNI</title>
    <link href="http://yoursite.com/2018/10/27/Note-About-ADNI/"/>
    <id>http://yoursite.com/2018/10/27/Note-About-ADNI/</id>
    <published>2018-10-27T01:46:32.000Z</published>
    <updated>2018-11-04T03:37:44.079Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】ADNI 官网介绍笔记</p><a id="more"></a><h2 id="Welcome"><a href="#Welcome" class="headerlink" title="Welcome"></a>Welcome</h2><p>阿尔茨海默病神经影像学倡议（ADNI）将研究人员与研究数据联系起来，因为它们致力于确定阿尔茨海默病（AD）的进展。ADNI研究人员收集，验证和利用数据，包括<code>MRI</code>和<code>PET</code>图像，遗传学，认知测试，脑脊液和血液生物标志物作为疾病的<code>预测因子</code>。来自北美ADNI研究的研究资源和数据可通过本网站获得，包括阿尔茨海默病患者，轻度认知障碍受试者和老年人控制。 </p><p>欢迎来到<a href="http://adni.loni.usc.edu/" target="_blank" rel="noopener">网站</a>，其中添加了<code>ADNI3</code>的内容。该网站建立在<code>ADNI1</code>，<code>ADNI-GO</code>和<code>ADNI2</code>研究的基础上，旨在确定整个阿尔茨海默病的临床，认知，成像，遗传和生物化学生物标志物之间的关系。ADNI3将继续努力发现，优化，标准化和验证AD临床研究中使用的临床试验措施和生物标志物。</p><h2 id="ABOUT"><a href="#ABOUT" class="headerlink" title="ABOUT"></a>ABOUT</h2><p>ADNI研究的三个总体目标是：</p><ol><li>在尽可能早的阶段（痴呆前）检测AD，并确定用<code>生物标志物</code>跟踪疾病进展的方法。</li><li>通过在尽可能早的阶段应用新的诊断方法（此时干预可能最有效），<code>支持AD干预</code>，预防和治疗的进展。</li><li>持续管理ADNI的创新数据访问政策，该政策提供所有数据(<code>数据共享</code>)，而不是对世界上所有科学家进行禁运。</li></ol><h2 id="STUDY-DESIGN"><a href="#STUDY-DESIGN" class="headerlink" title="STUDY DESIGN"></a>STUDY DESIGN</h2><h3 id="关于BIOMARKERS"><a href="#关于BIOMARKERS" class="headerlink" title="关于BIOMARKERS"></a>关于BIOMARKERS</h3><p>生物标志物是生物状态的物质、测量或指标。在临床症状出现之前可能存在生物标志物。ADNI使用各种生物标志物来帮助预测阿尔茨海默病的发病。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://adni.loni.usc.edu/wp-content/uploads/2010/04/ADNI_clinicalDiseaseStage-V-EDIT.png" alt="img" title="">                </div>                <div class="image-caption">img</div>            </figure><p>该图描绘了生物标志物作为AD的指标。曲线表明AD过程中五种生物标志物从正常到异常的变化（对痴呆的正常认知）</p><ol><li>在脑脊液中或通过淀粉样蛋白PET成像测量的<code>β-淀粉样蛋白</code>（Aβ）</li><li>由脑脊液中测量的<code>tau蛋白</code>表示的神经变性，或通过FDG-PET测量的突触<code>功能障碍</code></li><li><code>脑结构</code>萎缩，主要在内侧颞叶，通过结构MRI测量</li><li><code>记忆</code>丧失，通过认知测试来衡量</li><li><code>临床功能</code>，通过认知测试测量的一般认知下降表示。</li></ol><p>变化1-3表示可以在痴呆诊断之前观察到的生物标志物，而变化4-5是痴呆症诊断的经典指标。</p><h3 id="CLINICIAL-STUDY"><a href="#CLINICIAL-STUDY" class="headerlink" title="CLINICIAL STUDY"></a>CLINICIAL STUDY</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://adni.loni.usc.edu/wp-content/uploads/2012/10/clinical-data-chart.png" alt="img" title="">                </div>                <div class="image-caption">img</div>            </figure><ul><li><p>CN：正常衰老/认知正常. CN参与者是ADNI研究中的对照受试者。他们没有表现出抑郁，轻度认知障碍或痴呆的迹象。</p></li><li><p>SMC 重要记忆关注 – 解决健康老年人对照组与MCI之间的差距</p></li><li><p>MCI： 轻度认知障碍. MCI参与者日常活动基本上得到保留，其他认知领域没有显着的损伤水平，也没有痴呆症的迹象。使用Wechsler Memory Scale Logical Memory II确定MCI水平（早期或晚期）。</p><ul><li>EMCI：早期认知障碍.</li><li>LMCI：晚期认知障碍.</li></ul></li><li><p>AD：Alzheimer disease. 阿尔茨海默病</p></li></ul><h2 id="DATA-TYPES"><a href="#DATA-TYPES" class="headerlink" title="DATA TYPES"></a>DATA TYPES</h2><p>ADNI研究人员在参与研究期间从研究志愿者那里收集了几种类型的数据，使用一套标准的协议和程序来消除不一致性。此信息可通过<a href="https://ida.loni.usc.edu/login.jsp?project=ADNI" target="_blank" rel="noopener">LONI图像和数据存档（IDA）</a>免费提供给授权的调查员  。</p><h3 id="临床"><a href="#临床" class="headerlink" title="临床"></a>临床</h3><p>ADNI临床数据集包括关于每个受试者的临床信息，包括招募，人口统计学，身体检查和认知评估数据。可以将整套临床数据作为逗号分隔值（<code>CSV</code>）文件批量下载。</p><h3 id="遗传"><a href="#遗传" class="headerlink" title="遗传"></a>遗传</h3><p>遗传因素在阿尔茨海默病中起重要作用。全基因组关联研究（GWAS）采用标记之间关联的测试，称为单核苷酸多态性（SNP）和感兴趣的表型。来自病例对照GWAS和其他类型的遗传关联研究的发现可以提供用于检查源自ADNI成像和其他生物标志物数据集的定量表型的目标。</p><p>APOE的4等位基因是已知的AD最强大的遗传风险因素，如果拥有一个4等位基因的人患AD的风险增加了2- 3倍，那么如果有两个等位基因的人患AD的风险增加了12倍。</p><h3 id="MRI图像"><a href="#MRI图像" class="headerlink" title="MRI图像"></a>MRI图像</h3><p>MRI – 核磁共振成像</p><p>原始，预处理和后处理图像文件，FMRI和DTI 这些图像的收集对于满足ADNI开发生物标记物以追踪阿尔茨海默病的进展和潜在病理学变化的目标至关重要。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://adni.loni.usc.edu/wp-content/uploads/2012/08/MRI.png" alt="img" title="">                </div>                <div class="image-caption">img</div>            </figure><p>该项目将收集MRI（结构，扩散加权成像，灌注和静息状态序列）; 使用florbetapir F18（florbetapir）或florbetaben F18（florbetaben）的淀粉样蛋白PET; 18F-FDG-PET（FDG-PET）; CSF用于Aβ，tau，磷酸化tau（AKA磷酸化酶）和其他蛋白质; AV-1451 PET; 和遗传和尸检数据，以<code>确定这些生物标志物与基线临床状态和认知下降的关系</code>。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-47b425d8cdc1ff7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="PET图像"><a href="#PET图像" class="headerlink" title="PET图像"></a>PET图像</h3><p>PET – 正电子发射型计算机断层显像</p><p>原始，预处理和后处理图像文件，PIB（ADNI1），FDG（ADNI1 / GO / 2），FLORBETAPIR（ADNI GO / 2/3），FLORBETABEN（ADNI3）和TAU IMAGING（ADNI3）这些图像的收集对于满足ADNI开发生物标记物以追踪阿尔茨海默病的进展和潜在病理学变化的目标至关重要。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://adni.loni.usc.edu/wp-content/uploads/2012/08/PET-1.png" alt="an overview of the PET data collected throughout the ADNI study" title="">                </div>                <div class="image-caption">an overview of the PET data collected throughout the ADNI study</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://adni.loni.usc.edu/wp-content/uploads/2012/08/PET-Image-Data.png" alt="AVAILABLE IMAGE DATA" title="">                </div>                <div class="image-caption">AVAILABLE IMAGE DATA</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-508b820355abe639.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="生物样本"><a href="#生物样本" class="headerlink" title="生物样本"></a>生物样本</h3><p>ADNI的目标之一是收集参与者的血液，尿液和脑脊液（CSF）等生物样本。鼓励有兴趣的调查员，无论是否与ADNI网站相关联，都可以申请使用这种有限的资源。但是，除非初步数据显示出明显优越的性能，否则不建议将ADNI样本用于技术开发或不同技术之间的比较。</p><h2 id="METHODS-AND-TOOLS"><a href="#METHODS-AND-TOOLS" class="headerlink" title="METHODS AND TOOLS"></a>METHODS AND TOOLS</h2><ul><li>生物标记分析</li><li>遗传数据方法</li><li>蛋白质组分析</li><li>MRI分析</li><li>PET分析</li><li>神经病学方法</li><li>RARC批准的研究</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】ADNI 官网介绍笔记&lt;/p&gt;
    
    </summary>
    
    
      <category term="ADNI" scheme="http://yoursite.com/tags/ADNI/"/>
    
  </entry>
  
  <entry>
    <title>Lecture_Neural Networks Part 2</title>
    <link href="http://yoursite.com/2018/10/22/Lecture-Neural-Networks-Part-2/"/>
    <id>http://yoursite.com/2018/10/22/Lecture-Neural-Networks-Part-2/</id>
    <published>2018-10-22T08:27:27.000Z</published>
    <updated>2018-10-23T03:02:13.843Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="设置数据和模型"><a href="#设置数据和模型" class="headerlink" title="设置数据和模型"></a>设置数据和模型</h2><p>具体来说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ul><li><p><strong>均值减法（Mean subtraction）</strong>是预处理最常用的形式。</p></li><li><p><strong>归一化（Normalization）</strong>是指将数据的所有维度都归一化，使其数值范围都近似相等。</p><ul><li>零中心化（zero-centered）+ 每个维度都除以其标准差</li><li>对每个维度都做归一化，使得每个维度的最大和最小值是1和-1</li></ul></li></ul><hr><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-df918c8372752302.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>一般数据预处理流程：</p><p>左边：原始的2维输入数据。</p><p>中间：在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。</p><p>右边：每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。</p><hr><ul><li><strong>PCA和白化（Whitening）</strong>是另一种预处理形式。在这种处理中，先对数据进行<code>零中心化</code>处理，然后计算<code>协方差</code>矩阵，它展示了数据中的相关性结构</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-f7389c2994251ec6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>PCA/白化。<strong>左边</strong>是二维的原始数据。<strong>中间</strong>：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据<code>协方差</code>矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。<strong>右边</strong>：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从<code>高斯分布</code>的一个数据点分布。</p><hr><p><strong>强调：</strong> 任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。即<strong>应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值</strong></p><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p><strong>错误：全零初始化.</strong> 如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。</p><p><strong>小随机数初始化</strong>权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来<em>打破对称性</em>。<strong>W = 0.01 * np.random.randn(D,H)</strong></p><p><strong>使用1/sqrt(n)校准方差</strong>：<strong>w = np.random.randn(n) / sqrt(n)</strong>(其中<strong>n</strong>是输入数据的数量)这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度.</p><h3 id="批量归一化（Batch-Normalization）"><a href="#批量归一化（Batch-Normalization）" class="headerlink" title="批量归一化（Batch Normalization）"></a>批量归一化（Batch Normalization）</h3><h3 id="正则化（L2-L1-Maxnorm-Dropout）"><a href="#正则化（L2-L1-Maxnorm-Dropout）" class="headerlink" title="正则化（L2/L1/Maxnorm/Dropout）"></a>正则化（L2/L1/Maxnorm/Dropout）</h3><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>参考链接：<a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="noopener">神经网络笔记 2</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
    
      <category term="CS231n" scheme="http://yoursite.com/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>Lecture_Neural Networks Part 1</title>
    <link href="http://yoursite.com/2018/10/22/Lecture-Neural-Networks-Part-1/"/>
    <id>http://yoursite.com/2018/10/22/Lecture-Neural-Networks-Part-1/</id>
    <published>2018-10-22T05:25:48.000Z</published>
    <updated>2018-10-22T08:23:23.933Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="单个神经元建模"><a href="#单个神经元建模" class="headerlink" title="单个神经元建模"></a>单个神经元建模</h2><h3 id="生物动机和连接"><a href="#生物动机和连接" class="headerlink" title="生物动机和连接"></a>生物动机和连接</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-6f0bc09c687d2934.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>将神经元的激活率建模为<strong>激活函数（activation function）f</strong>，它表达了轴突上激活信号的频率.激活函数，非线性函数，”扭曲”得分函数.</p><p>由于历史原因，激活函数常常选择使用<strong>sigmoid函数</strong>$\sigma$，该函数输入实数值（求和后的信号强度），然后将输入值压缩到0-1之间。</p><h3 id="作为线性分类器的单个神经元"><a href="#作为线性分类器的单个神经元" class="headerlink" title="作为线性分类器的单个神经元"></a>作为线性分类器的单个神经元</h3><p>一个单独的神经元可以用来实现一个二分类分类器，比如二分类的Softmax或者SVM分类器。</p><h3 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h3><p><img src="https://upload-images.jianshu.io/upload_images/5267500-a9e31b332db38761.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。 </p><p><strong>Sigmoid：</strong>$\sigma(x)=1/(1+e^{-x})$ </p><ul><li><em>Sigmoid函数饱和使梯度消失</em></li><li><em>Sigmoid函数的输出不是零中心的</em></li></ul><p><strong>Tanh：</strong>$tanh(x)=2\sigma(2x)-1$  （tanh神经元是一个简单放大的sigmoid神经元）</p><ul><li>Tanh也存在饱和问题</li><li>Tanh的输出是零中心的</li></ul><hr><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-c3bc267ea5b658fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p><strong>ReLU</strong>(校正线性单元：Rectified Linear Unit)激活函数: $f(x)=max(0,x)$</p><ul><li>ReLU对于随机梯度下降的收敛有巨大的加速作用</li><li>ReLU单元比较脆弱并且可能“死掉”.通过合理设置学习率，这种情况的发生概率会降低<ul><li>Leaky ReLU是为解决“ReLU死亡”问题的尝试</li><li><strong>Maxout：</strong>$max(w^T_1x+b_1,w^T_2x+b_2)$  Maxout是对ReLU和leaky ReLU的<code>一般化归纳</code>.</li></ul></li></ul><hr><p>在同一个网络中混合使用不同类型的神经元是非常少见的.</p><h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><h3 id="层组织"><a href="#层组织" class="headerlink" title="层组织"></a>层组织</h3><p><strong>将神经网络算法以神经元的形式图形化</strong></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-d7ec4d232e0994af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ul><li><p>左边是一个2层神经网络，隐层由4个神经元（也可称为单元（unit））组成，输出层由2个神经元组成，输入层是3个神经元。该网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置，共26个可学习的参数。</p></li><li><p>右边是一个3层神经网络，两个含4个神经元的隐层。该网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。</p></li></ul><p>注意：<strong>全连接层（fully-connected layer）</strong>。全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接.上面两个神经网络的图例，都使用的全连接层.</p><hr><h3 id="前向传播计算例子"><a href="#前向传播计算例子" class="headerlink" title="前向传播计算例子"></a>前向传播计算例子</h3><p>完整的3层神经网络的前向传播就是简单的3次矩阵乘法，其中交织着激活函数的应用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个3层神经网络的前向传播:</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: <span class="number">1.0</span>/(<span class="number">1.0</span> + np.exp(-x)) <span class="comment"># 激活函数(用的sigmoid)</span></span><br><span class="line">x = np.random.randn(<span class="number">3</span>, <span class="number">1</span>) <span class="comment"># 含3个数字的随机输入向量(3x1)</span></span><br><span class="line">h1 = f(np.dot(W1, x) + b1) <span class="comment"># 计算第一个隐层的激活数据(4x1)</span></span><br><span class="line">h2 = f(np.dot(W2, h1) + b2) <span class="comment"># 计算第二个隐层的激活数据(4x1)</span></span><br><span class="line">out = np.dot(W3, h2) + b3 <span class="comment"># 神经元输出(1x1)</span></span><br></pre></td></tr></table></figure><blockquote><p>全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。</p></blockquote><h3 id="表达能力"><a href="#表达能力" class="headerlink" title="表达能力"></a>表达能力</h3><p>神经网络可以近似任何连续函数。</p><p>虽然在理论上深层网络（使用了多个隐层）和单层网络的表达能力是一样的，但是就实践经验而言，深度网络效果比单层网络好(设置的网络深度也应适度而行)。</p><h3 id="设置层的数量和尺寸"><a href="#设置层的数量和尺寸" class="headerlink" title="设置层的数量和尺寸"></a>设置层的数量和尺寸</h3><p>注意：不应该因为害怕出现过拟合而使用小网络。相反，应该进尽可能使用大网络，然后使用正则化技巧来控制过拟合。</p><h2 id="小节"><a href="#小节" class="headerlink" title="小节"></a>小节</h2><ul><li>介绍了生物神经元的粗略模型；</li><li>讨论了几种不同类型的激活函数，其中ReLU是最佳推荐；</li><li>介绍了<strong>神经网络</strong>，神经元通过<strong>全连接层</strong>连接，层间神经元两两相连，但是层内神经元不连接；</li><li>理解了分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算；</li><li>理解了神经网络是一个<strong>通用函数近似器</strong>，但是该性质与其广泛使用无太大关系。之所以使用神经网络，是因为它们对于实际问题中的函数的公式能够某种程度上做出“正确”假设。</li><li>讨论了更大网络总是更好的这一事实。然而更大容量的模型一定要和更强的正则化（比如更高的权重衰减）配合，否则它们就会过拟合。在后续章节中我们讲学习更多正则化的方法，尤其是dropout。</li></ul><p>参考链接：<a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" target="_blank" rel="noopener">神经网络笔记1 上</a>、<a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" target="_blank" rel="noopener">神经网络笔记1 下</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
    
      <category term="CS231n" scheme="http://yoursite.com/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>Lecture-backpropagation</title>
    <link href="http://yoursite.com/2018/10/22/Lecture-backpropagation/"/>
    <id>http://yoursite.com/2018/10/22/Lecture-backpropagation/</id>
    <published>2018-10-22T02:54:04.000Z</published>
    <updated>2018-10-22T05:28:21.232Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本节将帮助读者对<strong>反向传播</strong>形成直观而专业的理解。反向传播是利用<strong>链式法则</strong>递归计算表达式的梯度的方法。</p><h2 id="简单表达式和理解梯度"><a href="#简单表达式和理解梯度" class="headerlink" title="简单表达式和理解梯度"></a>简单表达式和理解梯度</h2><p>函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度。</p><h2 id="复合表达式，链式法则，反向传播"><a href="#复合表达式，链式法则，反向传播" class="headerlink" title="复合表达式，链式法则，反向传播"></a>复合表达式，链式法则，反向传播</h2><p><strong>链式法则</strong>指出将这些梯度表达式链接起来的正确方式是相乘，比如$\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x}$。在实际操作中，这只是简单地将两个梯度数值相乘.</p><hr><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-0c9de63f20983077.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>上图的真实值计算线路展示了计算的视觉化过程。<strong>前向传播</strong>从输入计算到输出（绿色），<strong>反向传播</strong>从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。</p><hr><h2 id="直观理解反向传播"><a href="#直观理解反向传播" class="headerlink" title="直观理解反向传播"></a>直观理解反向传播</h2><p>这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等。</p><p>反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入<code>沿着梯度方向变化</code>，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的<code>输出值更高</code>.</p><p>任何可微分的函数都可以看做门。可以根据需要将一个函数分拆成多个简单门(计算反向传播就简单了)；也可以将多个门组合成一个门从而可以进行简化(让代码量更少，效率更高).</p><h2 id="模块：Sigmoid例子"><a href="#模块：Sigmoid例子" class="headerlink" title="模块：Sigmoid例子"></a>模块：Sigmoid例子</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-83b12b83edf6b42b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>$$ f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$ </p><p>使用sigmoid激活函数的2维神经元的例子。输入是[x0, x1]，可学习的权重是[w0, w1, w2]。一会儿会看见，这个神经元对输入数据做点积运算，然后其激活数据被sigmoid函数挤压到0到1之间。<br>$$ \sigma(x) = \frac{1}{1+e^{-x}} \\\\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) = \left( 1 - \sigma(x) \right) \sigma(x)$$ <br>和之前的计算流程比起来，现在的计算使用一个单独的简单表达式即可。因此，在实际的应用中将这些操作装进一个单独的门单元中将会非常有用。</p><h2 id="反向传播实践：分段计算"><a href="#反向传播实践：分段计算" class="headerlink" title="反向传播实践：分段计算"></a>反向传播实践：分段计算</h2><p>$$ f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}$$ </p><p>构建前向传播的代码模式：(<strong>对前向传播变量进行缓存</strong>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">3</span> <span class="comment"># 例子数值</span></span><br><span class="line">y = <span class="number">-4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">sigy = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-y)) <span class="comment"># 分子中的sigmoi          #(1)</span></span><br><span class="line">num = x + sigy <span class="comment"># 分子                                    #(2)</span></span><br><span class="line">sigx = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-x)) <span class="comment"># 分母中的sigmoid         #(3)</span></span><br><span class="line">xpy = x + y                                              <span class="comment">#(4)</span></span><br><span class="line">xpysqr = xpy**<span class="number">2</span>                                          <span class="comment">#(5)</span></span><br><span class="line">den = sigx + xpysqr <span class="comment"># 分母                                #(6)</span></span><br><span class="line">invden = <span class="number">1.0</span> / den                                       <span class="comment">#(7)</span></span><br><span class="line">f = num * invden <span class="comment"># 搞定！                                 #(8)</span></span><br></pre></td></tr></table></figure><p>反向传播的代码模式：(<strong>在不同分支的梯度要相加</strong>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回传 f = num * invden</span></span><br><span class="line">dnum = invden <span class="comment"># 分子的梯度                                         #(8)</span></span><br><span class="line">dinvden = num                                                     <span class="comment">#(8)</span></span><br><span class="line"><span class="comment"># 回传 invden = 1.0 / den </span></span><br><span class="line">dden = (<span class="number">-1.0</span> / (den**<span class="number">2</span>)) * dinvden                                <span class="comment">#(7)</span></span><br><span class="line"><span class="comment"># 回传 den = sigx + xpysqr</span></span><br><span class="line">dsigx = (<span class="number">1</span>) * dden                                                <span class="comment">#(6)</span></span><br><span class="line">dxpysqr = (<span class="number">1</span>) * dden                                              <span class="comment">#(6)</span></span><br><span class="line"><span class="comment"># 回传 xpysqr = xpy**2</span></span><br><span class="line">dxpy = (<span class="number">2</span> * xpy) * dxpysqr                                        <span class="comment">#(5)</span></span><br><span class="line"><span class="comment"># 回传 xpy = x + y</span></span><br><span class="line">dx = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line">dy = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line"><span class="comment"># 回传 sigx = 1.0 / (1 + math.exp(-x))</span></span><br><span class="line">dx += ((<span class="number">1</span> - sigx) * sigx) * dsigx <span class="comment"># Notice += !! See notes below  #(3)</span></span><br><span class="line"><span class="comment"># 回传 num = x + sigy</span></span><br><span class="line">dx += (<span class="number">1</span>) * dnum                                                  <span class="comment">#(2)</span></span><br><span class="line">dsigy = (<span class="number">1</span>) * dnum                                                <span class="comment">#(2)</span></span><br><span class="line"><span class="comment"># 回传 sigy = 1.0 / (1 + math.exp(-y))</span></span><br><span class="line">dy += ((<span class="number">1</span> - sigy) * sigy) * dsigy                                 <span class="comment">#(1)</span></span><br><span class="line"><span class="comment"># 完成! 嗷~~</span></span><br></pre></td></tr></table></figure><h2 id="回传流中的模式"><a href="#回传流中的模式" class="headerlink" title="回传流中的模式"></a>回传流中的模式</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-ec4f09b059f37c5b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>一个展示反向传播的例子。加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。</p><h2 id="用户向量化操作的梯度"><a href="#用户向量化操作的梯度" class="headerlink" title="用户向量化操作的梯度"></a>用户向量化操作的梯度</h2><p><strong>矩阵相乘的梯度</strong>：可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作：(<em>分析维度</em>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">W = np.random.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">X = np.random.randn(<span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">D = W.dot(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们得到了D的梯度</span></span><br><span class="line">dD = np.random.randn(*D.shape) <span class="comment"># 和D一样的尺寸</span></span><br><span class="line">dW = dD.dot(X.T) <span class="comment">#.T就是对矩阵进行转置</span></span><br><span class="line">dX = W.T.dot(dD)</span><br></pre></td></tr></table></figure><p><strong>使用小而具体的例子</strong>：有些读者可能觉得向量化操作的梯度计算比较困难，建议是写出一个很小很明确的向量化例子，在纸上演算梯度，然后对其一般化，得到一个高效的向量化操作形式。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>对梯度的含义有了<code>直观理解</code>，知道了梯度是如何在网络中<code>反向传播</code>的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得<code>最终输出值更高</code>的。</li><li>讨论了<strong>分段计算</strong>在反向传播的实现中的重要性。应该将函数分成不同的模块，这样<code>计算局部梯度相对容易</code>，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式<code>分</code>成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中<code>一步一步地计算梯度</code>。</li></ul><p>在下节课中，将会开始定义神经网络，而反向传播使我们能高效计算神经网络各个节点<code>关于损失函数的梯度</code>。换句话说，我们现在已经准备好训练神经网络了，本课程最困难的部分已经过去了！ConvNets相比只是向前走了一小步。</p><p>参考链接：<a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" target="_blank" rel="noopener">反向传播笔记</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
    
      <category term="CS231n" scheme="http://yoursite.com/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>Lecture_Optimization</title>
    <link href="http://yoursite.com/2018/10/20/CS231n-Lecture-Optimization/"/>
    <id>http://yoursite.com/2018/10/20/CS231n-Lecture-Optimization/</id>
    <published>2018-10-20T09:18:06.000Z</published>
    <updated>2018-10-22T05:09:53.843Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h4 id="课程链接参考"><a href="#课程链接参考" class="headerlink" title="课程链接参考"></a>课程链接参考</h4><p>课程笔记：<a href="http://cs231n.github.io/optimization-1/" target="_blank" rel="noopener">Optimization: Stochastic Gradient Descent</a></p><p>笔记翻译：<a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" target="_blank" rel="noopener">最优化笔记（上）</a>、<a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" target="_blank" rel="noopener">最优化笔记（下）</a></p><h4 id="损失函数可视化"><a href="#损失函数可视化" class="headerlink" title="损失函数可视化"></a>损失函数可视化</h4><h5 id="损失函数的分段线性结构"><a href="#损失函数的分段线性结构" class="headerlink" title="损失函数的分段线性结构"></a>损失函数的分段线性结构</h5><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cs231n.github.io/assets/svmbowl.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>从一个维度方向上对数据损失值的展示。x轴方向就是一个权重，y轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的<code>独立部分</code>，要么是该权重的线性函数与0阈值的比较。</p><h4 id="最优化-Optimization"><a href="#最优化-Optimization" class="headerlink" title="最优化 Optimization"></a>最优化 Optimization</h4><ul><li>最优化的目标就是找到能够最小化损失函数值的<strong>W</strong></li></ul><h5 id="策略-1：一个差劲的初始方案：随机搜索"><a href="#策略-1：一个差劲的初始方案：随机搜索" class="headerlink" title="策略#1：一个差劲的初始方案：随机搜索"></a>策略#1：一个差劲的初始方案：随机搜索</h5><p>既然确认参数集<strong>W</strong>的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以<code>随机尝试</code>很多不同的权重，然后看其中哪个最好。</p><p><strong>核心思路：迭代优化</strong>。我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。</p><h5 id="策略-2：随机本地搜索"><a href="#策略-2：随机本地搜索" class="headerlink" title="策略#2：随机本地搜索"></a><strong>策略#2：随机本地搜索</strong></h5><p>第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机$W$开始，然后生成一个<code>随机的扰动</code>$\delta W$ ，只有当$W+\delta W$的损失值变低，我们才会更新。</p><p>评价： 较策略一准确率更高些，但依然不够高，且过于浪费计算资源</p><h5 id="策略-3：跟随梯度"><a href="#策略-3：跟随梯度" class="headerlink" title="策略#3：跟随梯度"></a><strong>策略#3：跟随梯度</strong></h5><p>前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以<code>直接计算出最好的方向</code>，这就是从数学上计算出最陡峭的方向。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着<code>最陡峭</code>的下降方向下山。这个方向就是损失函数的<strong>梯度（gradient）</strong>.注：梯度就是在每个维度上偏导数所形成的向量。</p><h4 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h4><ol><li>缓慢的<code>近似</code>方法（<strong>数值梯度法</strong>），实现相对简单，但耗费计算资源太多</li><li><strong>分析梯度法</strong> 计算迅速，结果<code>精确</code>，但是实现时容易出错，且需要使用微分</li></ol><p>在实际操作时常常将<code>分析梯度法</code>的结果和<code>数值梯度法</code>的结果作比较，以此来<code>检查</code>其实现的正确性，这个步骤叫做<strong>梯度检查</strong></p><h5 id="利用有限差值计算梯度"><a href="#利用有限差值计算梯度" class="headerlink" title="利用有限差值计算梯度"></a><strong>利用有限差值计算梯度</strong></h5><p><strong>实践考量</strong>：实际中用<strong>中心差值公式（centered difference formula）</strong>$[f(x+h)-f(x-h)]/2h$效果较好</p><p><strong>步长的影响</strong>：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为<strong>学习率</strong>）将会是我们在调参中最重要的超参数之一。</p><p><strong>效率问题</strong>：这个策略不适合大规模数据，我们需要更好的策略。</p><h5 id="微分分析计算梯度"><a href="#微分分析计算梯度" class="headerlink" title="微分分析计算梯度"></a><strong>微分分析计算梯度</strong></h5><p>一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了.</p><h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>现在可以计算损失函数的梯度了，程序重复地计算梯度然后对参数进行更新，这一过程称为<em>梯度下降</em>.</p><p>核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。</p><h5 id="小批量数据梯度下降（Mini-batch-gradient-descent）"><a href="#小批量数据梯度下降（Mini-batch-gradient-descent）" class="headerlink" title="小批量数据梯度下降（Mini-batch gradient descent）"></a><strong>小批量数据梯度下降（Mini-batch gradient descent）</strong></h5><p>小批量数据的梯度就是对整个数据集梯度的一个近似。因此，在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新。(提高计算效率)</p><p>小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。</p><h5 id="随机梯度下降（Stochastic-Gradient-Descent-简称SGD）"><a href="#随机梯度下降（Stochastic-Gradient-Descent-简称SGD）" class="headerlink" title="随机梯度下降（Stochastic Gradient Descent 简称SGD）"></a><strong>随机梯度下降</strong>（Stochastic Gradient Descent 简称SGD）</h5><p>小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为<strong>随机梯度下降（Stochastic Gradient Descent 简称SGD）</strong>。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据 比100次计算1个数据要高效很多。</p><p>你有时会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。</p><h5 id="提取图片特征（Image-Features）"><a href="#提取图片特征（Image-Features）" class="headerlink" title="提取图片特征（Image Features）"></a>提取图片特征（Image Features）</h5><ul><li><p>颜色直方图（Color Histogram）</p><p><img src="https://github.com/HusterHope/blogimage/raw/master/CS231n3-9.png" alt=""></p></li><li><p>定向梯度直方图（Histogram of Oriented Gradient）</p><p><img src="https://github.com/HusterHope/blogimage/raw/master/CS231n3-10.png" alt=""></p></li></ul><ul><li><p>词袋模型（Bag of Words）</p><p><img src="https://github.com/HusterHope/blogimage/raw/master/CS231n3-11.png" alt=""></p></li></ul><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cs231n.github.io/assets/dataflow.jpeg" alt="Summary of the information flow" title="">                </div>                <div class="image-caption">Summary of the information flow</div>            </figure><p>数据集中的(x,y)是给定的.<code>权重</code>从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的<code>分类评分</code>并存储在向量<strong>f</strong>中。<code>损失函数</code>包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分f和实际标签y之间的差异，<code>正则化损失只是一个关于权重的函数</code>。在梯度下降过程中，我们计算<code>权重的梯度</code>（如果愿意的话，也可以计算<code>数据上的梯度</code>），然后使用它们来实现参数的更新。</p><hr><ul><li>将<code>损失函数</code>比作了一个<strong>高维度的最优化地形</strong>，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。</li><li>提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。</li><li>函数的<strong>梯度</strong>给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是<em>h</em>，用来计算数值梯度）。</li><li>参数更新需要有技巧地设置<strong>步长</strong>。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。</li><li>讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用<strong>梯度检查</strong>来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。</li><li>介绍了<strong>梯度下降</strong>算法，它在循环中迭代地计算梯度并更新参数。</li></ul><p>这节课的核心内容是：理解并能计算损失函数<strong>关于权重的梯度</strong>，是设计、训练和理解神经网络的核心能力。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
    
      <category term="CS231n" scheme="http://yoursite.com/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>Lecture_SVM</title>
    <link href="http://yoursite.com/2018/10/18/CS231n-Lecture-SVM/"/>
    <id>http://yoursite.com/2018/10/18/CS231n-Lecture-SVM/</id>
    <published>2018-10-18T12:01:32.000Z</published>
    <updated>2018-10-22T05:10:06.724Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="课程链接"><a href="#课程链接" class="headerlink" title="课程链接"></a>课程链接</h3><ul><li><p><a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="noopener">Linear classification: Support Vector Machine, Softmax</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/20918580" target="_blank" rel="noopener">线性分类笔记(上)</a>/<a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" target="_blank" rel="noopener">线性分类笔记(中)</a>/<a href="https://zhuanlan.zhihu.com/p/21102293" target="_blank" rel="noopener">线性分类笔记(下)</a></p></li></ul><h3 id="线性分类器"><a href="#线性分类器" class="headerlink" title="线性分类器"></a>线性分类器</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-a17e4ca30a831bd0.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>图像空间的示意图,其中每个图像是一个点,且有3个分类器.以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低.   <a href="http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/" target="_blank" rel="noopener">Interactive web demo</a></p><hr><h3 id="评分函数-Score-function"><a href="#评分函数-Score-function" class="headerlink" title="评分函数 Score function"></a>评分函数 Score function</h3><h4 id="线性映射"><a href="#线性映射" class="headerlink" title="线性映射"></a>线性映射</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-1bd42914c6f44015.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>$$ f(x_i,W,b)=Wx_i+b$$ </p><ul><li>W(Weights 权重) : [K*D] (W的每一行为对应一个分类的模板(“原型”)) <code>如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转.</code><br>   x_i(输入数据) : [D*1]     </li><li>b(bias vector 偏差向量) : [K*1] <code>允许分类器对应的直线平移.如果没有偏差,无论权重如何,在x_i=0时分类分值始终为0.这样所有分类器的线都不得不穿过原点.</code></li></ul><ol><li>该函数的输出值为对应各类别的score</li><li>我们的目的就是找到最优化的参数W、b,即为每一个分类找到最好的模板.</li><li>评分函数在正确的分类的位置应当得到最高的评分（score）</li></ol><h4 id="偏差和权重的合并Bias-trick"><a href="#偏差和权重的合并Bias-trick" class="headerlink" title="偏差和权重的合并Bias trick"></a>偏差和权重的合并Bias trick</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cs231n.github.io/assets/wb.jpeg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>$$ f(x_i,W,b)=Wx_i+b\rightarrow f(x_i,W) = Wx_i$$ </p><p>其中，W[K*D]→W[K*(D+1)], x_i[D,1] → x_i[(D+1),1]</p><hr><h4 id="将线性分类器看作模板匹配"><a href="#将线性分类器看作模板匹配" class="headerlink" title="将线性分类器看作模板匹配"></a>将线性分类器看作模板匹配</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-5e1fbbb3ebc2d9c4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Interpretation of linear classifiers as template matching" title="">                </div>                <div class="image-caption">Interpretation of linear classifiers as template matching</div>            </figure><p>W的每一行为对应一个分类的模板(“原型”).注意，船的模板如期望的那样有很多蓝色像素。如果图像是一艘船行驶在大海上，那么这个模板利用内积计算图像将给出很高的分数。</p><hr><h4 id="图像数据预处理-Image-data-preprocessing"><a href="#图像数据预处理-Image-data-preprocessing" class="headerlink" title="图像数据预处理 Image data preprocessing"></a>图像数据预处理 Image data preprocessing</h4><ul><li><p>对于输入的特征做归一化（normalization）处理</p><ul><li><p>对每个特征减去平均值来<strong>中心化</strong>数据</p></li><li><p>归一. 区间变为[-1,1]</p></li></ul></li></ul><hr><h3 id="损失函数-Loss-function"><a href="#损失函数-Loss-function" class="headerlink" title="损失函数 Loss function"></a>损失函数 Loss function</h3><p>我们将使用损失函数（Loss Function）（有时也叫代价函数Cost Function或目标函数Objective）来衡量我们对结果的<strong>不满意程度</strong>。直观地讲，当评分函数输出结果与真实结果之间<strong>差异</strong>越大，损失函数输出越大，反之越小。，对<code>训练集中数据做出准确分类预测</code>和<code>让损失值最小化</code>这两件事是等价的。</p><hr><h4 id="多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss"><a href="#多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss" class="headerlink" title="多类支持向量机损失 Multiclass Support Vector Machine Loss"></a>多类支持向量机损失 Multiclass Support Vector Machine Loss</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cs231n.github.io/assets/margin.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p><a href="http://leohope.com/%E5%81%9A%E7%AC%94%E8%AE%B0/2017/08/22/CS231n-3/" target="_blank" rel="noopener">SVM的损失函数</a>想要SVM在正确分类上的得分始终比不正确分类上的得分高,且至少高出一个边界值delta.如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-69a890113e7bf295.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Hinge Loss" title="">                </div>                <div class="image-caption">Hinge Loss</div>            </figure>针对第i个数据的多类SVM的损失函数定义：<br>$$ L_i=\sum_{j\not=y_i}max(0,s_j-s_{y_i}+\Delta)$$ <h5 id="Data-Loss"><a href="#Data-Loss" class="headerlink" title="Data Loss"></a><strong>Data Loss</strong></h5><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://github.com/HusterHope/blogimage/raw/master/CS231n3-3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>$$ L =  { \frac{1}{N} \sum_i L_i }$$ </p><h5 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 Regularization"></a><strong>正则化 Regularization</strong></h5><p><strong>防止过拟合</strong>：向损失函数增加一个<strong>正则化惩罚（regularization penalty） R(W)</strong>部分，使其不能完全匹配训练集.<br>最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来<strong>抑制大数值的权重</strong>.对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响.<br>$$<br>R(W) = \sum_k\sum_l W_{k,l}^2<br>$$</p><p>需要注意的是，和权重不同，偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此通常<code>只</code>对权重$W$正则化，而不正则化偏差$b$。因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当$W=0$的特殊情况下，才能得到损失值为0</p><h5 id="完整的多类SVM损失函数"><a href="#完整的多类SVM损失函数" class="headerlink" title="完整的多类SVM损失函数"></a>完整的多类SVM损失函数</h5><p>$$ L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\$$ </p><p>将其展开完整公式是：<br>$$ L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2$$ </p><p>对于每一个输入数据$x_i$都有一个L值</p><h5 id="设置超参-Delta-和λ"><a href="#设置超参-Delta-和λ" class="headerlink" title="设置超参$\Delta$和λ"></a>设置超参$\Delta$和λ</h5><p>超参数$\Delta$和λ失函数中的数据损失和正则化损失之间的权衡。理解这一点的关键是要知道，权重<strong>W</strong>的大小对于分类分值有直接影响（当然对他们的差异也有直接影响）：当我们将<strong>W</strong>中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如$\Delta$=1或$\Delta$=100).从某些角度来看是没意义的，因为<code>权重自己就可以控制差异变大和缩小</code>。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过<code>正则化强度λ</code>来控制）</p><hr><h4 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h4><p>与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释。在Softmax分类器中，函数映射$f(x_i;W)=Wx_i$保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<em>折叶损失（hinge loss）</em>替换为<strong>交叉熵损失</strong>（<strong>cross-entropy loss）</strong></p><p>公式如下：$\displaystyle Li=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})$ 或等价于 $L_i=-f_{y_i}+log(\sum_je^{f_j})$</p><h5 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h5><p>函数$f_j(z)=\frac{e^{z_j}}{\sum_ke^{z_k}}$被称作<strong>softmax 函数</strong>.函数对输入向量z(score值)进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1.<br>$$<br>P(y_i|x_i,W)=\frac{e^{f_{y_i}}}{\sum_je^{f_j}}<br>$$<br>可以解释为是给定图像数据$x_i$，以$W$为参数，分配给正确分类标签$y_i$的<strong>归一化概率</strong>。从概率论的角度来理解，我们就是在<code>最小化正确分类的负对数概率</code>(令正确分类的概率尽趋近于1，即令错误分类的概率均趋近于0)，这可以看做是在进行<em>最大似然估计</em>（MLE）。</p><h5 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h5><p>在“真实”分布(未知)$p$和估计分布(样本分布)$q$之间的<strong>交叉熵</strong>定义： $\displaystyle H(p,q)=-\sum_xp(x) logq(x)$</p><p>交叉熵损失函数“想要”<strong>预测分布的所有概率密度都在正确分类</strong>上,让预测分布与真实分布保持一致。Softmax分类器所做的就是<code>最小化</code>在<code>估计分类概率</code>（就是上面的$e^{f_{y_i}}/\sum_je^{f_j}$）和<code>“真实”分布</code>之间的<code>交叉熵</code>，在这个解释中，“真实”分布就是所有概率密度都分布在正确的类别上（比如：$p=[0,…1,…,0]$中在$y_i$的位置就有一个单独的1）</p><hr><h4 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h4><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://cs231n.github.io/assets/svmvssoftmax.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：</p><p>SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。SVM的最终的损失值是1.58。</p><p>Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。Softmax的最终的损失值是0.452。但要注意<code>SVM和Softmax的最终损失值(1.58和0.452)两个数值没有可比性</code>。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p><hr><p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的置信水平。</p><hr><p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。</p><p>SVM更加“局部目标化（local objective）”.<code>SVM对于数字个体的细节是不关心的</code>：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM($\Delta$=1)来说没什么不同，只要满足超过边界值等于1，那么损失值就等于0。SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p><p>softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。</p><hr><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul><li>定义了从图像像素映射到不同类别的分类评分的评分函数。在本节中，评分函数是一个基于权重<strong>W</strong>和偏差<strong>b</strong>的线性函数。</li><li>与kNN分类器不同，<strong>参数方法</strong>的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重<strong>W</strong>进行一个矩阵乘法运算。</li><li>介绍了偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵。</li><li>定义了损失函数（介绍了SVM和Softmax线性分类器最常用的2个损失函数）。损失函数能够衡量给出的参数集与训练集数据真实类别情况之间的一致性。在损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的。</li></ul><p>现在我们知道了如何基于参数，将数据集中的图像映射成为分类的评分，也知道了两种不同的损失函数，它们都能用来衡量算法分类预测的质量。但是，如何高效地得到能够使损失值最小的参数呢？这个求得最优参数的过程被称为<a href="https://captainzj.github.io/2018/10/20/CS231n-Lecture-Optimization/" target="_blank" rel="noopener">最优化</a>.</p><p>参考：<a href="https://blog.csdn.net/bury_/article/details/76081639" target="_blank" rel="noopener">cs231n assignment1 svm</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
    
      <category term="CS231n" scheme="http://yoursite.com/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>Concept_Note</title>
    <link href="http://yoursite.com/2018/10/18/Concept-Note/"/>
    <id>http://yoursite.com/2018/10/18/Concept-Note/</id>
    <published>2018-10-18T08:48:07.000Z</published>
    <updated>2018-10-20T09:21:07.452Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p>@card{</p><h4 id="Linear-classification"><a href="#Linear-classification" class="headerlink" title="Linear classification"></a><a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="noopener">Linear classification</a></h4><ul><li><a href="https://blog.csdn.net/tangyudi/article/details/80124771" target="_blank" rel="noopener">score function</a>:<ul><li>得分函数的目的：我们要做的就是对于一个给定的输入，比如一张小猫的图片，通过一系列复杂的变换（中间的过程咱们暂且当做一个黑盒子）能得到这个输入对应于每个类别的得分数值.</li></ul></li><li><a href="http://leohope.com/%E5%81%9A%E7%AC%94%E8%AE%B0/2017/08/22/CS231n-3/" target="_blank" rel="noopener">Loss Fuction</a>:<ul><li>量化我们对训练结果的满意程度，换句话说，是衡量分类器的错误程度</li></ul></li></ul><p>}</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
    
      <category term="CS231n" scheme="http://yoursite.com/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>Assignment_Step</title>
    <link href="http://yoursite.com/2018/10/17/CS231n-Assignment-Step/"/>
    <id>http://yoursite.com/2018/10/17/CS231n-Assignment-Step/</id>
    <published>2018-10-17T12:57:13.000Z</published>
    <updated>2018-10-22T05:10:21.664Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p>@card{</p><ol><li>完成代码，跑通 （保存page.pdf）</li><li>查找资料(多找些博客，择优)，理解细节</li><li>理清思路，写博客总结</li><li>upload博客(博客代码标题链接到git code 以及 pdf 链接)、 git push “CS231n_ssignment”</li></ol><p>}</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Assignment_1_KNN</title>
    <link href="http://yoursite.com/2018/10/17/CS231n-Assignment-1-KNN/"/>
    <id>http://yoursite.com/2018/10/17/CS231n-Assignment-1-KNN/</id>
    <published>2018-10-17T09:39:40.000Z</published>
    <updated>2018-10-20T09:22:45.855Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】8 min 2043 words<br>【阅读内容】……</p><a id="more"></a><h5 id="效果演示"><a href="#效果演示" class="headerlink" title="效果演示"></a><a href="https://github.com/Captainzj/CS231n_Assignment/blob/master/assignment1/knn.pdf" target="_blank" rel="noopener">效果演示</a></h5><h5 id="代码实现部分"><a href="#代码实现部分" class="headerlink" title="代码实现部分"></a><a href="https://github.com/Captainzj/CS231n_Assignment/tree/master/assignment1/cs231n/classifiers" target="_blank" rel="noopener">代码实现部分</a></h5><ul><li>计算test样本与training样本的L2距离.<br>L2距离的定义：$$ L_2(I_1,I_2) = \sqrt{{\sum_p{(I_1^p - I_2^p)^2}}} $$</li></ul><p>@card{</p><ol><li><code>Open cs231n/classifiers/k_nearest_neighbor.py and implement compute_distances_two_loops.</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_two_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(num_train):</span><br><span class="line"></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                             #</span></span><br><span class="line">        <span class="comment"># Compute the l2 distance between the ith test point and the jth    #</span></span><br><span class="line">        <span class="comment"># training point, and store the result in dists[i, j]. You should   #</span></span><br><span class="line">        <span class="comment"># not use a loop over dimension.                                    #</span></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        <span class="comment">### X - X_test.shape == (500,3072)  X_train.shape = (5000,3072)</span></span><br><span class="line">        <span class="comment"># dists[i][j] = np.sqrt(np.sum((X[i]-self.X_train[j])**2))    </span></span><br><span class="line">        dists[i][j] = np.sqrt(np.sum(np.square(X[i,:] - self.X_train[j,:])))</span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">        <span class="comment">#                       END OF YOUR CODE                            #</span></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dists  <span class="comment">#dists.shape = (500, 5000)</span></span><br></pre></td></tr></table></figure></li></ol><p>}</p><p>@card{</p><ol start="2"><li><code>Now lets speed up distance matrix computation by using partial vectorization with one loop. Implement the function compute_distances_one_loop</code></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_one_loop</span><span class="params">(self, X)</span>:</span></span><br><span class="line">  num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">  num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">  dists = np.zeros((num_test, num_train))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">    <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                               #</span></span><br><span class="line">    <span class="comment"># Compute the l2 distance between the ith test point and all training #</span></span><br><span class="line">    <span class="comment"># points, and store the result in dists[i, :].                        #</span></span><br><span class="line">    <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">    <span class="comment"># dists[i] = np.sqrt(np.sum((self.X_train - X[i]) ** 2, 1))</span></span><br><span class="line">    dists[i] = np.sqrt(np.sum(np.square(self.X_train - X[i]), axis=<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="comment">#                         END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">#######################################################################</span></span><br><span class="line">  <span class="keyword">return</span> dists  <span class="comment"># dists.shape = (500, 5000)</span></span><br></pre></td></tr></table></figure><p>}</p><p>@card{</p><ol start="3"><li><code>Now implement the fully vectorized version inside compute_distances_no_loops</code> <a href="https://blog.csdn.net/zhyh1435589631/article/details/54236643/#2234-computedistancesnoloops" target="_blank" rel="noopener">数学说明</a></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_no_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line"></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train)) </span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">    <span class="comment"># Compute the l2 distance between all test points and all training      #</span></span><br><span class="line">    <span class="comment"># points without using any explicit loops, and store the result in      #</span></span><br><span class="line">    <span class="comment"># dists.                                                                #</span></span><br><span class="line">    <span class="comment">#                                                                       #</span></span><br><span class="line">    <span class="comment"># You should implement this function using only basic array operations; #</span></span><br><span class="line">    <span class="comment"># in particular you should not use functions from scipy.                #</span></span><br><span class="line">    <span class="comment">#                                                                       #</span></span><br><span class="line">    <span class="comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span></span><br><span class="line">    <span class="comment">#       and two broadcast sums.                                         #</span></span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    dists += np.sum(self.X_train ** <span class="number">2</span>, axis=<span class="number">1</span>).reshape(<span class="number">1</span>, num_train)</span><br><span class="line">    dists += np.sum(X ** <span class="number">2</span>, axis=<span class="number">1</span>).reshape(num_test, <span class="number">1</span>) <span class="comment"># reshape for broadcasting</span></span><br><span class="line">    dists -= <span class="number">2</span> * np.dot(X, self.X_train.T)</span><br><span class="line">    dists = np.sqrt(dists)</span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="comment">#                         END OF YOUR CODE                              #</span></span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dists<span class="comment"># dists.shape = (500, 5000)</span></span><br></pre></td></tr></table></figure><p>}</p><p>@card{</p><ol start="4"><li><code>Now implement the function predict_labels</code></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_labels</span><span class="params">(self, dists, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    num_test = dists.shape[<span class="number">0</span>]</span><br><span class="line">    y_pred = np.zeros(num_test)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">      <span class="comment"># A list of length k storing the labels of the k nearest neighbors to</span></span><br><span class="line">      <span class="comment"># the ith test point.</span></span><br><span class="line">      closest_y = []</span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">      <span class="comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span></span><br><span class="line">      <span class="comment"># testing point, and use self.y_train to find the labels of these       #</span></span><br><span class="line">      <span class="comment"># neighbors. Store these labels in closest_y.                           #</span></span><br><span class="line">      <span class="comment"># Hint: Look up the function numpy.argsort.                             #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># sorted_index = np.argsort(dists[i])</span></span><br><span class="line">      <span class="comment"># closest_y = self.y_train[sorted_index[:k]]</span></span><br><span class="line"></span><br><span class="line">      closest_y = self.y_train[np.argsort(dists[i])[:k]]</span><br><span class="line"></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">      <span class="comment"># Now that you have found the labels of the k nearest neighbors, you    #</span></span><br><span class="line">      <span class="comment"># need to find the most common label in the list closest_y of labels.   #</span></span><br><span class="line">      <span class="comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span></span><br><span class="line">      <span class="comment"># label.                                                                #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line">      y_pred[i] = np.bincount(closest_y).argmax()</span><br><span class="line"></span><br><span class="line">      <span class="comment"># timeLabel = sorted([(np.sum(np.array(closest_y) == y_), y_) for y_ in set(closest_y)])[-1]</span></span><br><span class="line">      <span class="comment"># y_pred[i] = timeLabel[1]</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># appear_times = &#123;&#125;</span></span><br><span class="line">      <span class="comment"># for label in closest_y:</span></span><br><span class="line">      <span class="comment">#   if label in appear_times:</span></span><br><span class="line">      <span class="comment">#     appear_times[label] += 1</span></span><br><span class="line">      <span class="comment">#   else:</span></span><br><span class="line">      <span class="comment">#     appear_times[label] = 0</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># # find most commen label</span></span><br><span class="line">      <span class="comment"># y_pred[i] = max(appear_times, key=lambda x: appear_times[x])</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment">#                           END OF YOUR CODE                            # </span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure><p>}</p><p>@card{</p><ol start="5"><li><p><code>We will now determine the best value of this hyperparameter with cross-validation.</code><br>使用cross validation的方法，来选择hyper-parameter超参数k的值.<br>cross validation的原理是，将training样本集分成n份（如下图中的例子，是5份），每一份叫做一个fold，然后依次迭代这n个fold，将其作为validation集合，其余的n-1个fold一起作为training集合，然后进行训练并计算准确率。<br>选择一组候选k值，依次迭代执行上面描述的过程，最终根据准确率，进行评估选择最合适的k值.</p><p><img src="https://upload-images.jianshu.io/upload_images/5267500-c13d6a6b322baa1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">num_folds = <span class="number">5</span></span><br><span class="line">k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">X_train_folds = []</span><br><span class="line">y_train_folds = []</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Split up the training data into folds. After splitting, X_train_folds and    #</span></span><br><span class="line"><span class="comment"># y_train_folds should each be lists of length num_folds, where                #</span></span><br><span class="line"><span class="comment"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span></span><br><span class="line"><span class="comment"># Hint: Look up the numpy array_split function.                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># Your code</span></span><br><span class="line">X_train_folds = np.array_split(X_train, num_folds)</span><br><span class="line">y_train_folds = np.array_split(y_train, num_folds)</span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A dictionary holding the accuracies for different values of k that we find</span></span><br><span class="line"><span class="comment"># when running cross-validation. After running cross-validation,</span></span><br><span class="line"><span class="comment"># k_to_accuracies[k] should be a list of length num_folds giving the different</span></span><br><span class="line"><span class="comment"># accuracy values that we found when using that value of k.</span></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Perform k-fold cross validation to find the best value of k. For each        #</span></span><br><span class="line"><span class="comment"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span></span><br><span class="line"><span class="comment"># where in each case you use all but one of the folds as training data and the #</span></span><br><span class="line"><span class="comment"># last fold as a validation set. Store the accuracies for all fold and all     #</span></span><br><span class="line"><span class="comment"># values of k in the k_to_accuracies dictionary.                               #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># Your code</span></span><br><span class="line"><span class="keyword">for</span> k_candi <span class="keyword">in</span> k_choices:</span><br><span class="line">    k_to_accuracies[k_candi] = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_folds):</span><br><span class="line">        X_test_hy = X_train_folds[i]</span><br><span class="line">        y_test_hy = y_train_folds[i]</span><br><span class="line">        </span><br><span class="line">        X_train_hy = np.vstack(X_train_folds[<span class="number">0</span>:i]+X_train_folds[i+<span class="number">1</span>:])</span><br><span class="line">        y_train_hy = np.hstack(y_train_folds[<span class="number">0</span>:i]+y_train_folds[i+<span class="number">1</span>:])</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         x_trai = np.array(X_train_folds[:f] + X_train_folds[f+1:])</span></span><br><span class="line"><span class="comment">#         y_trai = np.array(Y_train_folds[:f] + Y_train_folds[f+1:])</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">#         x_trai = x_trai.reshape(-1, x_trai.shape[2])</span></span><br><span class="line"><span class="comment">#         y_trai = y_trai.reshape(-1)</span></span><br><span class="line">        </span><br><span class="line">        classifier.train(X_train_hy, y_train_hy)</span><br><span class="line">        dists_hy = classifier.compute_distances_no_loops(X_test_hy)</span><br><span class="line">        y_test_pred_hy = classifier.predict_labels(dists_hy, k=k_candi)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the fraction of correctly predicted examples</span></span><br><span class="line">        num_correct_hy = np.sum(y_test_pred_hy == y_test_hy)</span><br><span class="line">        accuracy_hy = float(num_correct_hy) / len(y_test_hy)</span><br><span class="line">        k_to_accuracies[k_candi].append(accuracy_hy)</span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line">print(k_to_accuracies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the computed accuracies</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> sorted(k_to_accuracies):</span><br><span class="line">    <span class="keyword">for</span> accuracy <span class="keyword">in</span> k_to_accuracies[k]:</span><br><span class="line">        print(<span class="string">'k = %d, accuracy = %f'</span> % (k, accuracy))</span><br></pre></td></tr></table></figure><p>}</p><p>@card{</p><h5 id="numpy-matplotlib-部分函数说明"><a href="#numpy-matplotlib-部分函数说明" class="headerlink" title="numpy/matplotlib 部分函数说明"></a>numpy/matplotlib 部分函数说明</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> numpy.flatnonzero():  输入一个矩阵，返回了其中非零元素的位置</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> numpy.random.choice(a, size=<span class="keyword">None</span>, replace=<span class="keyword">True</span>, p=<span class="keyword">None</span>)</span><br><span class="line">  - a: If an ndarray, a random sample <span class="keyword">is</span> generated <span class="keyword">from</span> its elements. </span><br><span class="line">       If an int, the random sample <span class="keyword">is</span> generated <span class="keyword">as</span> <span class="keyword">if</span> a was np.arange(n) </span><br><span class="line">  - size : int <span class="keyword">or</span> tuple of ints, optional</span><br><span class="line">  - replace : boolean, optional </span><br><span class="line">    If you want only unique samples then this should be false.</span><br><span class="line">  - p : <span class="number">1</span>-D array-like, optional </span><br><span class="line">   The probabilities associated <span class="keyword">with</span> each entry <span class="keyword">in</span> a. If <span class="keyword">not</span> given the sample assumes a uniform distribution over all entries <span class="keyword">in</span> a.</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> matplotlib.pyplot.subplot(X,X,X)：</span><br><span class="line">  - 前两个数表示子图组成的矩阵的行列数，比如有<span class="number">6</span>个子图，排列成<span class="number">3</span>行<span class="number">2</span>列，那就是subplot(<span class="number">3</span>,<span class="number">2</span>,X)。最后一个数表示要画第X个图了。</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> matplotlib.pyplot.imshow(X,interpolation=<span class="string">'none'</span>,cmap=<span class="keyword">None</span>)</span><br><span class="line">  - X: 要绘制的图像或数组.</span><br><span class="line">  - interpolation 插值方式 [<span class="keyword">None</span>, <span class="string">'none'</span>, <span class="string">'nearest'</span>, <span class="string">'bilinear'</span>, <span class="string">'bicubic'</span>, <span class="string">'spline16'</span>,</span><br><span class="line">           <span class="string">'spline36'</span>, <span class="string">'hanning'</span>, <span class="string">'hamming'</span>, <span class="string">'hermite'</span>, <span class="string">'kaiser'</span>, <span class="string">'quadric'</span>,</span><br><span class="line">           <span class="string">'catrom'</span>, <span class="string">'gaussian'</span>, <span class="string">'bessel'</span>, <span class="string">'mitchell'</span>, <span class="string">'sinc'</span>, <span class="string">'lanczos'</span>]</span><br><span class="line">  - cmap: 颜色图谱（colormap), 默认绘制为RGB(A)颜色空间.</span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> numpy.reshape(a, newshape, order=<span class="string">'C'</span>)</span><br><span class="line">  - 注：给出一个m*n的矩阵，如果newshape给的参数是（x, <span class="number">-1</span>）,那么函数会自动判别newshape为（x, m*n/x）,这里的x一定要能被m*n整除！</span><br><span class="line"></span><br><span class="line"><span class="number">6.</span> numpy.argsort()：输出排好序的元素下标, a[np.argsort(a)]的结果才是最终排好序的结果.</span><br><span class="line"></span><br><span class="line"><span class="number">7.</span> numpy.binicount(x, weight = <span class="keyword">None</span>, minlength = <span class="keyword">None</span>) </span><br><span class="line">    &gt;&gt;&gt; x = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">7</span>])</span><br><span class="line">    &gt;&gt;&gt; np.bincount(x)</span><br><span class="line">        array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="number">8.</span> x_norm=np.linalg.norm(x, ord=<span class="keyword">None</span>, axis=<span class="keyword">None</span>, keepdims=<span class="keyword">False</span>)  </span><br><span class="line">  - linalg=linear（线性）+algebra（代数），norm则表示范数</span><br><span class="line">  - x: 表示矩阵（也可以是一维）</span><br><span class="line">  - ord：范数类型</span><br><span class="line">    - ord=<span class="number">1</span>：列和的最大值</span><br><span class="line">    - ord=<span class="number">2</span>：|λE-ATA|=<span class="number">0</span>，求特征值，然后求最大特征值的算术平方根</span><br><span class="line">    - ord=np.inf：行和的最大值</span><br><span class="line">  - axis：处理类型</span><br><span class="line">    - axis=<span class="number">1</span>表示按行向量处理，求多个行向量的范数</span><br><span class="line">    - axis=<span class="number">0</span>表示按列向量处理，求多个列向量的范数</span><br><span class="line">    - axis=<span class="keyword">None</span>表示矩阵范数</span><br><span class="line">  - keepding：是否保持矩阵的二维特性</span><br><span class="line">    - <span class="keyword">True</span>表示保持矩阵的二维特性，<span class="keyword">False</span>相反</span><br><span class="line"></span><br><span class="line"><span class="number">9.</span>  np.dot(A, B)：对于二维矩阵，计算真正意义上的矩阵乘积，同线性代数中矩阵乘法的定义.</span><br><span class="line"></span><br><span class="line"><span class="number">10.</span> sum(a, axis=<span class="keyword">None</span>, dtype=<span class="keyword">None</span>, out=<span class="keyword">None</span>, keepdims=&lt;<span class="class"><span class="keyword">class</span> '<span class="title">numpy</span>.<span class="title">_globals</span>.<span class="title">_NoValue</span>'&gt;)</span></span><br><span class="line"><span class="class">  - <span class="title">axis</span> :</span></span><br><span class="line">    - axis = <span class="keyword">None</span>: 对所有元素求和</span><br><span class="line">    - axis = <span class="number">0</span>: 对所有在同一列的元素求和</span><br><span class="line">    - axis = <span class="number">1</span>: 对所有在同一行的元素求和</span><br><span class="line"></span><br><span class="line"><span class="number">11.</span> np.vstack(tup): 沿着竖直方向将矩阵堆叠起来</span><br><span class="line">  - Note: the arrays must have the same shape along all but the first axis. 除开第一维外，被堆叠的矩阵各维度要一致.</span><br><span class="line"></span><br><span class="line"><span class="number">12.</span> np.hstack(tup):沿着水平方向将数组堆叠起来</span><br><span class="line"></span><br><span class="line"><span class="number">13.</span> plt.scatter(X, Y)： 散点图 (x,y)即坐标</span><br><span class="line"></span><br><span class="line"><span class="number">14.</span> np.random.randn(d0,d1,…,dn)</span><br><span class="line">   - randn函数返回一个或一组样本，具有标准正态分布。</span><br><span class="line">   - d表示维度</span><br><span class="line">   - 返回值为指定维度的array</span><br></pre></td></tr></table></figure><p>}</p><p>@card{</p><h5 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h5><p>即使经过调优，knn算法的准确率也不足30%，可以知道knn算法并不适合用于图像分类学习任务.</p><p>}</p><p>@card{</p><p>参考：</p><ul><li><p><a href="https://blog.csdn.net/zhyh1435589631/article/details/54236643" target="_blank" rel="noopener">【实验小结】cs231n assignment1 knn 部分</a></p></li><li><p><a href="https://blog.csdn.net/zhangxb35/article/details/55223825" target="_blank" rel="noopener">cs231n 课程作业 Assignment 1</a>、<a href="https://xyiyy.github.io/2017/02/27/standford-cs231n-assignment1/" target="_blank" rel="noopener">standford-cs231n-assignment1</a></p></li></ul><p>}</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】8 min 2043 words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://yoursite.com/categories/MachineLearning/"/>
    
    
      <category term="CS231n" scheme="http://yoursite.com/tags/CS231n/"/>
    
  </entry>
  
  <entry>
    <title>Python_Note</title>
    <link href="http://yoursite.com/2018/10/04/Python-Note/"/>
    <id>http://yoursite.com/2018/10/04/Python-Note/</id>
    <published>2018-10-04T05:28:48.000Z</published>
    <updated>2018-10-06T06:25:05.122Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h5 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h5><ul><li><p>字符串是一个sequence</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word = <span class="string">'hello'</span></span><br><span class="line"><span class="keyword">for</span> letter <span class="keyword">in</span> word:</span><br><span class="line">    print(letter)</span><br></pre></td></tr></table></figure></li></ul><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><ul><li>python函数参数传递可以理解为就是变量传值操作</li></ul><h4 id="enumerate-使用"><a href="#enumerate-使用" class="headerlink" title="enumerate()使用"></a>enumerate()使用</h4><ul><li>如果对一个列表，既要遍历索引又要遍历元素时，首先可以这样写：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list1 = [<span class="string">"这"</span>, <span class="string">"是"</span>, <span class="string">"一个"</span>, <span class="string">"测试"</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range (len(list1)):</span><br><span class="line">    <span class="keyword">print</span> i ,list1[i]<span class="number">123</span></span><br></pre></td></tr></table></figure><ul><li>上述方法有些累赘，利用enumerate()会更加直接和优美：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">list1 = [<span class="string">"这"</span>, <span class="string">"是"</span>, <span class="string">"一个"</span>, <span class="string">"测试"</span>]</span><br><span class="line"><span class="keyword">for</span> index, item <span class="keyword">in</span> enumerate(list1):</span><br><span class="line">    <span class="keyword">print</span> index, item</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">0</span> 这</span><br><span class="line"><span class="number">1</span> 是</span><br><span class="line"><span class="number">2</span> 一个</span><br><span class="line"><span class="number">3</span> 测试<span class="number">12345678</span></span><br></pre></td></tr></table></figure><ul><li>enumerate还可以接收第二个参数，用于指定索引起始值，如：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">list1 = [<span class="string">"这"</span>, <span class="string">"是"</span>, <span class="string">"一个"</span>, <span class="string">"测试"</span>]</span><br><span class="line"><span class="keyword">for</span> index, item <span class="keyword">in</span> enumerate(list1, <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">print</span> index, item</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">1</span> 这</span><br><span class="line"><span class="number">2</span> 是</span><br><span class="line"><span class="number">3</span> 一个</span><br><span class="line"><span class="number">4</span> 测试</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Morvan_Python</title>
    <link href="http://yoursite.com/2018/09/27/Morvan-Python/"/>
    <id>http://yoursite.com/2018/09/27/Morvan-Python/</id>
    <published>2018-09-27T03:14:01.000Z</published>
    <updated>2018-09-28T02:51:32.623Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><h3 id="基础语法"><a href="#基础语法" class="headerlink" title="基础语法"></a>基础语法</h3><h4 id="自调用"><a href="#自调用" class="headerlink" title="自调用"></a>自调用</h4><p>如果想要在执行脚本的时候执行一些代码，比如<a href="https://en.wikipedia.org/wiki/Unit_testing" target="_blank" rel="noopener">单元测试</a>，可以在脚本最后加上单元测试 代码，但是该脚本作为一个模块对外提供功能的时候单元测试代码也会执行，这些往往我们不想要的，我们可以把这些代码放入脚本最后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#code_here</span></span><br></pre></td></tr></table></figure><p>如果执行该脚本的时候，该 <code>if</code> 判断语句将会是 <code>True</code>,那么内部的代码将会执行。 如果外部调用该脚本，<code>if</code> 判断语句则为 <code>False</code>,内部代码将不会执行。</p><h4 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h4><p>顾名思义，函数的可变参数是传入的参数可以变化的，1个，2个到任意个。当然可以将这些 参数封装成一个 <code>list</code> 或者 <code>tuple</code> 传入，但不够 <code>pythonic</code>。使用可变参数可以很好解决该问题，注意可变参数在函数定义不能出现在<strong>特定参数</strong>和<strong>默认参数</strong>前面，因为可变参数会吞噬掉这些参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">report</span><span class="params">(name, *grades)</span>:</span></span><br><span class="line">    total_grade = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> grade <span class="keyword">in</span> grades:</span><br><span class="line">        total_grade += grade</span><br><span class="line">    print(name, <span class="string">'total grade is '</span>, total_grade)</span><br></pre></td></tr></table></figure><p>定义了一个函数，传入一个参数为 <code>name</code>, 后面的参数 <code>*grades</code> 使用了 <code>*</code> 修饰，表明该参数是一个可变参数，这是一个可迭代的对象。该函数输入姓名和各科的成绩，输出姓名和总共成绩。所以可以这样调用函数 <code>report(&#39;Mike&#39;, 8, 9)</code>，输出的结果为 <code>Mike total grade is 17</code>, 也可以这样调用 <code>report(&#39;Mike&#39;, 8, 9, 10)</code>，输出的结果为 <code>Mike total grade is 27</code></p><h4 id="关键字参数"><a href="#关键字参数" class="headerlink" title="关键字参数"></a>关键字参数</h4><p>关键字参数可以传入0个或者任意个含参数名的参数，这些参数名在函数定义中并没有出现，这些参数在函数内部自动封装成一个字典(dict).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">portrait</span><span class="params">(name, **kw)</span>:</span></span><br><span class="line">    print(<span class="string">'name is'</span>, name)</span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> kw.items():</span><br><span class="line">        print(k, v)</span><br></pre></td></tr></table></figure><p>定义了一个函数，传入一个参数 <code>name</code>, 和关键字参数 <code>kw</code>，使用了 <code>**</code> 修饰。表明该参数是关键字参数，通常来讲关键字参数是放在函数参数列表的最后。如果调用参数<code>portrait(&#39;Mike&#39;, age=24, country=&#39;China&#39;, education=&#39;bachelor&#39;)</code> 输出:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name <span class="keyword">is</span> Mike</span><br><span class="line">age <span class="number">24</span></span><br><span class="line">country China</span><br><span class="line">education bachelor</span><br></pre></td></tr></table></figure><p>通过可变参数和关键字参数，任何函数都可以用 <code>universal_func(*args, **kw)</code> 表达。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>LibraryManagementSystem_Notes</title>
    <link href="http://yoursite.com/2018/09/26/LibraryManagementSystem-Notes/"/>
    <id>http://yoursite.com/2018/09/26/LibraryManagementSystem-Notes/</id>
    <published>2018-09-26T08:35:09.000Z</published>
    <updated>2018-09-26T11:38:30.838Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><ul><li>基于bean层(ENTITY层 — 实体层   数据库在项目中的类 )中Table的设计,在DAO/Service/Controller层进行填充</li><li>DAO层(database access object 数据持久层 — 主要与数据库进行交互)</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">UserBkunitDAO</span> <span class="keyword">extends</span> <span class="title">JpaRepository</span>&lt;<span class="title">UserBkunit</span>, <span class="title">Integer</span>&gt;</span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line"><span class="function">Page&lt;UserBkunit&gt; <span class="title">findAllByUser</span><span class="params">(User reader, Pageable pageable)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注：DAO层首先会创建DAO接口，然后会在配置文件中定义该接口的实现类.DAO设计的总体规划需要和设计的表，和实现类之间一一对应.</span></span><br></pre></td></tr></table></figure><ul><li>Service层(业务逻辑层 — 负责业务模块的逻辑应用设计)</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReaderFunctionService</span> </span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> UserService userService;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> UserBkunitDAO userBkunitDAO;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span>  Page&lt;UserBkunit&gt; <span class="title">queryborrowedBooks</span><span class="params">(<span class="keyword">int</span> start, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        User reader = userService.getUser();</span><br><span class="line">        start = start &lt; <span class="number">0</span> ? <span class="number">0</span> : start;</span><br><span class="line">        Sort sort = <span class="keyword">new</span> Sort(Sort.Direction.DESC, <span class="string">"date"</span>);</span><br><span class="line">        Pageable pageable = PageRequest.of(start, size, sort);</span><br><span class="line">        Page&lt;UserBkunit&gt; page = userBkunitDAO.findAllByUser(reader, pageable);</span><br><span class="line">        <span class="keyword">return</span> page;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注：Service层应该既调用DAO层的接口，又要提供接口给Controller层的类来进行调用.</span></span><br></pre></td></tr></table></figure><ul><li>Controller层(action层/控制层 — 控制业务逻辑，与View层结合紧密)</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Controller</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReaderFunctionController</span> </span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> ReaderFunctionService readerfunctionservice;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping</span>(value = <span class="string">"/reader/borrowedBooks"</span>,method = RequestMethod.GET)</span><br><span class="line">    <span class="meta">@ResponseBody</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">queryBorrowTools</span><span class="params">(Model model, @RequestParam(value = <span class="string">"start"</span>, defaultValue = <span class="string">"0"</span>)</span> <span class="keyword">int</span> start,@<span class="title">RequestParam</span><span class="params">(value = <span class="string">"size"</span>, defaultValue = <span class="string">"10"</span>)</span> <span class="keyword">int</span> size)</span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        Page&lt;UserBkunit&gt; page = readerfunctionservice.queryborrowedBooks(start, size);</span><br><span class="line">        model.addAttribute(<span class="string">"page"</span>, page);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"queryBorrowedBooks"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注：不关心业务逻辑的具体实现,仅仅需要调用service层里的一个方法即可.</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="XD" scheme="http://yoursite.com/categories/XD/"/>
    
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>【2】一维随机变量及其分布</title>
    <link href="http://yoursite.com/2018/08/28/%E4%BA%8C%E3%80%81%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%8F%8A%E5%85%B6%E5%88%86%E5%B8%83/"/>
    <id>http://yoursite.com/2018/08/28/二、一维随机变量及其分布/</id>
    <published>2018-08-28T10:20:53.000Z</published>
    <updated>2018-08-28T10:35:58.314Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p>@column-2{</p><p>@card{</p><h1 id="离散"><a href="#离散" class="headerlink" title="离散"></a>离散</h1><p>$$ F(x,y) = P \{ X \leqslant x,Y \leqslant y\}=\int_{-\infty}^{x} du \int_{-\infty}^{y} f(u,v)dv$$ </p> $$F_{x}(x)$$ <p>就是这样 $$F(x,y) = P \{ X \leqslant x,Y \leqslant y\}$$$$ 就是这样</p><p>}</p><p>@card{</p><h1 id="连续"><a href="#连续" class="headerlink" title="连续"></a>连续</h1><p>}</p><p>}</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Math" scheme="http://yoursite.com/categories/Math/"/>
    
    
      <category term="概率论" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>分布函数与数字特征</title>
    <link href="http://yoursite.com/2018/08/24/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    <id>http://yoursite.com/2018/08/24/概率论/</id>
    <published>2018-08-24T15:28:24.000Z</published>
    <updated>2018-08-24T16:56:35.566Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】XXX min XXX words<br>【阅读内容】……</p><a id="more"></a><p>@column-2{</p><p>@card{</p><h1 id="分布函数"><a href="#分布函数" class="headerlink" title="分布函数"></a>分布函数</h1><p>一维离散型r.v.<br>$$<br>F(x)=\sum_{x{i}\leqslant x} P_{i}<br>$$<br>一维连续型r.v.<br>$$ F(x)= P \{ X \leqslant x \}=\int_{-\infty}^{x}f(t)dt$$ </p><p>二维连续型r.v.<br>$$ F(x,y) = P \{ X \leqslant x,Y \leqslant y\}=\int_{-\infty}^{x} du \int_{-\infty}^{y} f(u,v)dv$$ </p><p>}</p><p>@card{</p><h1 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h1><p>一维离散型r.v.<br>$$<br>E(x)=\sum x_{i}P_{i}<br>$$<br>一维连续型r.v.<br>$$<br>E(X) = \int_{-\infty}^{+ \infty} xf(x)dx<br>$$</p><p>}</p><p>}</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】XXX min XXX words&lt;br&gt;【阅读内容】……&lt;/p&gt;
    
    </summary>
    
      <category term="Math" scheme="http://yoursite.com/categories/Math/"/>
    
    
      <category term="概率论" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>编译原理知识点</title>
    <link href="http://yoursite.com/2018/08/18/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E7%9F%A5%E8%AF%86%E6%B1%87%E6%80%BB/"/>
    <id>http://yoursite.com/2018/08/18/编译原理知识汇总/</id>
    <published>2018-08-17T17:32:07.000Z</published>
    <updated>2018-08-18T03:22:02.614Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】20min 11001words<br>【阅读内容】XD编译原理教材知识汇总</p><a id="more"></a><h2 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h2><h3 id="1-从面向机器的语言到面向人类的语言"><a href="#1-从面向机器的语言到面向人类的语言" class="headerlink" title="1.从面向机器的语言到面向人类的语言"></a>1.从面向机器的语言到面向人类的语言</h3><p><strong>汇编指令：</strong>用符号表示的指令被称为汇编指令<br><strong>汇编语言：</strong>汇编指令的集合称为汇编语言</p><h3 id="2-语言之间的翻译"><a href="#2-语言之间的翻译" class="headerlink" title="2.语言之间的翻译"></a>2.语言之间的翻译</h3><p><strong>转换</strong>(也被称为<strong>预处理</strong>)：高级语言之间的翻译，如<code>FORTRAN</code>到<code>ADA</code>的转换<br><strong>编译：</strong>高级语言可以直接翻译成机器语言，也可以翻译成汇编语言，这两个翻译过程称为编译<br><strong>汇编：</strong>从汇编语言到机器语言的翻译被称为汇编<br><strong>交叉汇编：</strong>将一个汇编语言程序汇编成为可在另一机器上运行的机器指令成为交叉汇编<br><strong>反汇编：</strong>把机器语言翻译成汇编语言<br><strong>反编译：</strong>把汇编语言翻译成高级语言</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-d4008e57f6e52e2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="3-编译器与解释器"><a href="#3-编译器与解释器" class="headerlink" title="3. 编译器与解释器"></a>3. 编译器与解释器</h3><p>（1）语言翻译的两种基本形态</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-cbf938bbdf9e6dc1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p><strong>解释器与编译器的主要区别:</strong>运行目标程序时的控制权在<code>解释器</code>而不在目标程序. </p><p>（2）各自特点</p><ul><li>编译器:<code>工作效率高</code>,即时间快、空间省；<code>交互性与动态性差,可移植性差</code>. </li><li>解释器:<code>工作效率低</code>,,即时间慢、空间费；<code>交互性与动态性好,可移植性好</code>. </li></ul><p>共同点:均完成对<code>源程序</code>的翻译.<br>差异:编译器采用先翻译后执行,解释器采用边翻译边执行.  </p><h3 id="4-编译器的工作原理与基本组成"><a href="#4-编译器的工作原理与基本组成" class="headerlink" title="4. 编译器的工作原理与基本组成"></a>4. 编译器的工作原理与基本组成</h3><p>（0）通用程序设计语言的主要成份   <code>声明＋操作＝完整定义</code></p><p>（1）以<code>过程</code>为基本结构的程序设计语言的组成</p><ul><li>声明性语句：提供操作对象的性质，如数据类型、值、作用域等；</li><li>操作性语句：确定操作的计算次序，完成实际操作。</li><li>过程定义 = 过程头＋过程体</li></ul><p>（2）以阶段划分编译器 </p><p><img src="http://upload-images.jianshu.io/upload_images/5267500-7813bda350b45828.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>注：符号表管理器和出错处理贯穿编译器工作的各个阶段.</p><p>（3）编译器各阶段工作</p><p><strong>1&gt; 词法分析：</strong>词法分析的<strong>输入</strong>是<code>源程序</code>,<strong>输出</strong>是识别出的<code>记号流</code>.<strong>目的</strong>是<code>识别单词</code>. 至少分以下几类：关键字(保留字)、标识符、字面量、特殊符号</p><p><strong>2&gt; 语法分析：</strong> <strong>输入</strong>是词法分析器返回的<code>记号流</code>,<strong>输出</strong>是<code>语法树</code>.<strong>目的</strong>是得到语言结构并以树的形式表示.对于声明性语句,进行符号表的查填,对于可执行语句,检查结构合理的表达式运算是否有意义. </p><p><strong>3&gt; 语义分析：</strong>根据语义规则对语法树中的语法单元进行静态语义检查,如类型检查和转换等,<strong>目的</strong>在于保证语法正确的结构在语义分析上也是合法的. </p><p><strong>4&gt; 中间代码生成(可选)：</strong>生成一种既接近目标语言,又与具体机器无关的表示,便于代码优化与代码生成. </p><p>(到目前为止，编译器与解释器可以一致)</p><p><strong>5&gt; 中间代码优化(可选)：</strong>局部优化、循环优化、全局优化等；优化实际上是一个等价变换，变换前后的指令序列完成同样的功能，但在占用的空间上和程序执行的时间上都更省、更有效</p><p><strong>6&gt; 目标代码生成：</strong>不同形式的目标代码—汇编语言形式、可重定位二进制代码形式、内存形式(Load-and-Go)</p><p><strong>7&gt; 符号表管理：</strong>合理组织符号,便于各阶段查找\填写等. </p><p><strong>8&gt; 出错处理：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">动态错误：源程序中的逻辑错误，发生在程序运行的时候。也称为动态语义错误</span><br><span class="line">静态错误：静态错误分为语法错误和静态语义错误.  </span><br><span class="line">&lt;1&gt; 语法错误：有关语言结构上的错误，如单词拼写错误、表达式缺少操作数、begin和end不匹配</span><br><span class="line">&lt;2&gt; 静态语义错误：分析源程序时可以发现的语言意义上的错误，如加法的两个操作数一个是整形变量，另一个是数组名</span><br></pre></td></tr></table></figure><p>（4）编译器的分析\综合模式</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-afbb123ec5a660fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>逻辑上把编译器分为<code>分析(前端)部分</code>和<code>综合(后端)部分</code>.<br>1&gt; 分析(前端)：语言结构和意义的分析； 从词法分析到中间代码生成各阶段的工作<br>2&gt; 综合(后端)：语言意义处理；从中间代码生成到目标代码生成的各阶段的工作<br>3&gt; 编译器和解释器的区别往往是在形成中间代码之后开始的.</p><h3 id="5-编译器扫描的遍数"><a href="#5-编译器扫描的遍数" class="headerlink" title="5. 编译器扫描的遍数"></a>5. 编译器扫描的遍数</h3><p>每个阶段将程序完整分析一遍的工作模式称为一遍扫描。<br>(将源程序或源程序的某种形式的中间表示完整分析一遍，亦称作一遍扫描)</p><h2 id="第二章-词法分析"><a href="#第二章-词法分析" class="headerlink" title="第二章 词法分析"></a>第二章 词法分析</h2><h3 id="1-词法分析中的若干问题"><a href="#1-词法分析中的若干问题" class="headerlink" title="1. 词法分析中的若干问题"></a>1. 词法分析中的若干问题</h3><p>(1) 记号、模式与单词</p><p><strong>单词的分类：</strong>关键字(保留字)、标识符、字面量、特殊符号<br><strong>模式（pattern）</strong>：产生/识别单词的规则<br><strong>记号（token）</strong>：按照某个模式(或规则)识别出的元素(一组)<br><strong>单词（lexeme）</strong>：被识别出的元素的值(字符串本身)  ，也称为词值</p><p>(2) 词法分析器的作用与工作方式</p><p><strong>词法分析器的作用：</strong></p><p>1&gt; 识别记号并交给语法分析器(根据模式识别记号)<br>2&gt; 滤掉源程序中的无用成分,如注释、空格和回车等<br>3&gt; 处理与具体平台有关的输入(如文件结束符的不同表示等)<br>4&gt; 调用符号表管理器和出错处理器，进行相关处理</p><p><strong>工作方式：</strong><br>1.单独一遍扫描<br>2.作为语法分析器的子程序<br>3.并行方式</p><h3 id="2-模式的形式化描述"><a href="#2-模式的形式化描述" class="headerlink" title="2. 模式的形式化描述"></a>2. 模式的形式化描述</h3><p>(1) 字符串与语言</p><p><strong>语言L</strong>是有限字母表∑上有限长度字符串的集合.<br>定义中强调两个有限，因为计算机的表示能力有限 ：<br>1&gt; 字母表是有限的，即字母表中元素是有限多个；<br>2&gt; 字符串的长度是有限的，即字符串中字符个数是有限多个。</p><p>(字符串与字符串集合相关的概念与运算,如前缀、后缀、子串、子序列等，字符串的并、交、连接、差、闭包)</p><p>(2) 正规式与正规集</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">令Σ是一个有限字母表，则Σ上的 正规式 及其表示的集合递归定义如下:</span><br><span class="line">    1. ε是正规式，它表示集合  L(ε) = &#123;ε&#125;</span><br><span class="line">    2. 若a是Σ上的字符，则a是正规式，它表示集合L(a)=&#123;a&#125;</span><br><span class="line">    3. 若正规式r和s分别表示集合L(r)和L(s)，则</span><br><span class="line">       （a） r|s是正规式，表示集合L(r)∪L(s)，</span><br><span class="line">       （b） rs是正规式，表示集合L(r)L(s)，</span><br><span class="line">       （c） r*是正规式，表示集合(L(r))*，</span><br><span class="line">       （d）(r)是正规式，表示的集合仍然是L(r)。                                   </span><br><span class="line">       括弧用来改变运算的先后次序！</span><br></pre></td></tr></table></figure><p>可用正规式描述(其结构)的语言称为 正规语言 或 <strong>正规集</strong> 。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">若运算的优先级和结合性做下述约定:</span><br><span class="line">    1. 三种运算均具有左结合性质；</span><br><span class="line">    2. 优先级从高到低顺序排列为:闭包运算、连接运算、或运算。</span><br><span class="line">则正规式中不必要的括号可以被省略。</span><br></pre></td></tr></table></figure><p>若正规式P和Q表示了同一个正规集，则称P和Q是<strong>等价</strong>的，记为P=Q</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-b058d82c0bc660cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>(3) 简化正规式描述(主要是简化书写上的复杂)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(a) 正闭包 若r是表示L(r)的正规式，则r+是表示(L(r))+的正规式，且下述等式成立:r+ = rr* = rr，r = r+|ε;</span><br><span class="line">  +与*具有相同的运算结合性和优先级</span><br><span class="line">(b) 可缺省  若r是正规式，则r?是表示L(r)∪&#123;ε&#125;的正规式，且下述等式成立:r? = r|ε</span><br><span class="line">     ? 与 * 具有相同的运算结合性和优先级</span><br><span class="line">(c) 串    若r是若干字符进行连接运算构成的正规式，则:串“r” =  r ，且: ε= “”，   a = “a”（a是Σ的任一字符）</span><br><span class="line">(d) 字符组 若r是若干字符进行|运算构成的正规式，则可改写为  [r’]，其中r’可以有如下两种书写形式：</span><br><span class="line">     枚举:      如  a|b|e|h，可写为 [abeh]：</span><br><span class="line">     分段:   如0|1|2|3|4|5|6|7|8|9|a|b|c|d|e , 可写为： [0-9a-e]</span><br><span class="line">(e) 非字符组 若[r]是一个字符组形式的正规式，则[^r]是表示∑- L([r])的正规式。</span><br></pre></td></tr></table></figure><h3 id="3-记号的识别——有限自动机"><a href="#3-记号的识别——有限自动机" class="headerlink" title="3. 记号的识别——有限自动机"></a>3. 记号的识别——有限自动机</h3><p>(1) 不确定的有限自动机（NondeterministicFinite Automaton, NFA）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NFA是一个五元组（5-tuple）：M =（S，∑，move，s0，F），其中</span><br><span class="line">（1） S是有限个状态（state）的集合；</span><br><span class="line">（2） ∑是有限个输入字符（包括ε）的集合；</span><br><span class="line">（3） move是一个状态转移函数，move(si，ch)=sj表示，当前状态si下若遇到输入字符ch，则转移到状态sj；</span><br><span class="line">（4） s0是唯一的初态（也称开始状态）；</span><br><span class="line">（5） F是终态集（也称接受状态集），它是S的子集，包含了所有的终态。</span><br></pre></td></tr></table></figure><p><1> 直观的表示方式</1></p><p>① 状态转换图：用一个有向图来直观表示NFA<br>② 状态转换矩阵：用一个矩阵来直观表示NFA     (矩阵中，状态对应行，字符对应列)</p><p><2>  NFA(识别记号)的特点<br>NFA识别记号的最大特点是它的不确定性，即在当前状态下对同一字符有多于一个的下一状态转移。</2></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">具体体现：</span><br><span class="line">定义： move函数是1对多的；</span><br><span class="line">状态转换图：从同一状态出发，可通过多于一条标记相同字符的边转移到不同的状态；</span><br><span class="line">状态转换矩阵： M[si,a]是一个状态的集合</span><br></pre></td></tr></table></figure><p><3> NFA识别记号存在的问题</3></p><p>1.只有尝试了全部可能的路径,才能确定一个输入序列不被接受,而这些路径的条数随着路径长度的增长成指数增长<br>2.识别过程中需要进行大量回朔，时间复杂度升高且算法复杂</p><p>(2) 确定的有限自动机（Deterministic Finite Automaton, DFA） </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">定义: DFA是NFA的一个特例，其中： </span><br><span class="line">  （1）没有状态具有ε状态转移(ε-transition)，即状态转换图中没有标记ε的边；</span><br><span class="line">  （2）对每个状态s和每个字符a，最多有一个下一状态。</span><br><span class="line">  </span><br><span class="line">特点：与NFA相比，DFA的特征：确定性</span><br><span class="line">    定义：move（si, a)函数都是 1对1 的；</span><br><span class="line">    转换图 从一个状态出发的任2条边上的标记均不同；</span><br><span class="line">    转换矩阵：M[si,a]是一个状态   且字母表不包括ε。</span><br><span class="line">提示：正规式和有限自动机从两个侧面表示正规式。正规式是描述，自动机是识别。</span><br></pre></td></tr></table></figure><h3 id="4-从正规式到词法分析器"><a href="#4-从正规式到词法分析器" class="headerlink" title="4. 从正规式到词法分析器"></a>4. 从正规式到词法分析器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">构造词法分析器的一般方法和步骤：</span><br><span class="line">1. 用正规式描述模式（为记号设计正规式）；</span><br><span class="line">2. 为每个正规式构造一个NFA，它识别正规式所表示的正规集；</span><br><span class="line">3. 将构造的NFA转换成等价的DFA，这一过程也被称为确定化；</span><br><span class="line">4. 优化DFA，使其状态数最少，这一过程也被称为最小化；</span><br><span class="line">5. 根据优化后的DFA构造词法分析器。</span><br></pre></td></tr></table></figure><p>(1) 从正规式到NFA </p><p>Thompson 算法</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-81bf06598a2d56c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>(2) 从NFA到DFA </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- smove(S, a)：从状态集S出发，标记为a的下一状态全体。与move(s, a)的唯一区别：用状态集取代状态</span><br><span class="line">- ε-闭包(T)：从状态集T出发，不经任何字符达到的状态全体</span><br><span class="line">- “子集法”构造DFA</span><br></pre></td></tr></table></figure><p>(3) 最小化DFA</p><p>​    ① 对于任何两个状态t和s，若从一状态出发接受输入字符串ω，而从另一状态出发不接受ω.</p><p>或者，② 从t出发和从s出发到达不同的接受状态，则称ω对状态t和s是可区分的.</p><p>​    不可区分的状态位于一个组内，可以合并成一个状态.</p><p><strong>主要步骤：</strong><br>​    1.初始划分：终态组 ， 非终态组；<br>​    2.利用可区分的概念，反复分裂划分中的组Gi，直到不可再分裂；<br>​    3.由最终划分构造D’，关键是选代表和修改状态转移；<br>​    4.消除可能的死状态和不可达状态。 </p><h3 id="5-从DFA构造词法分析器"><a href="#5-从DFA构造词法分析器" class="headerlink" title="5. 从DFA构造词法分析器"></a>5. 从DFA构造词法分析器</h3><p><strong>分类：</strong>表驱动型的词法分析器；直接编码的词法分析器<br><strong>比较：</strong></p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">表驱动</th><th style="text-align:center">直接编码</th></tr></thead><tbody><tr><td style="text-align:center">分析器的速度</td><td style="text-align:center">慢</td><td style="text-align:center">快</td></tr><tr><td style="text-align:center">程序与模式的关系</td><td style="text-align:center">无关</td><td style="text-align:center">有关</td></tr><tr><td style="text-align:center">适合的编写方法</td><td style="text-align:center">工具生成</td><td style="text-align:center">手工编写</td></tr><tr><td style="text-align:center">分析器的规模</td><td style="text-align:center">较大</td><td style="text-align:center">较小</td></tr></tbody></table><h2 id="第三章-语法分析"><a href="#第三章-语法分析" class="headerlink" title="第三章 语法分析"></a>第三章 语法分析</h2><p><strong>词法分析：</strong>记号的集合，字符串由字母组成，线性结构<br><strong>语法分析：</strong>句子的集合，句子由记号组成，非线性结构（树）</p><p> <strong>语法分析的双重含义：</strong>  </p><ul><li>语法规则：上下文无关文法（子集：LL文法或LR文法）</li><li>语法分析：下推自动机（LL或LR分析器）、自上而下分析、自下而上分析</li></ul><h3 id="1-语法分析的若干问题"><a href="#1-语法分析的若干问题" class="headerlink" title="1. 语法分析的若干问题"></a>1. 语法分析的若干问题</h3><p>许多编译器，特别是由自动生成工具构造的编译器，往往其前端的中心部件就是语法分析器</p><p>（1）语法分析器的作用</p><ul><li>根据词法分析器提供的记号流，为语法正确的输入构造分析树（或语法树）</li><li>检查输入中的语法（可能包括词法）错误，并调用出错处理器进行适<br>当处理</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-a3f0ae180f91152e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>（2）语法错误的处理原则</p><p><strong>源程序中可能出现的错误</strong></p><p>语法(包括词法)错误和语义错误(静态语义错误和动态语义错误)</p><p>注：跟第一章的分类角度不同，第一章是从静态错误(语法错误，静态语义错误)和动态错误(动态语义错误)分类的，但是殊途同归。</p><p><strong>词法错误：</strong>指非法字符或拼写错关键字、标识符等<br><strong>语法错误：</strong>指语法结构出错，如少分号、括号不匹配、begin/end不配对等<br><strong>静态语义错误：</strong>如类型不一致、参数不匹配等<br><strong>动态语义错误(逻辑错误)：</strong>如死循环、变量为零时作除数等</p><h3 id="2-上下文无关文法-CFG"><a href="#2-上下文无关文法-CFG" class="headerlink" title="2. 上下文无关文法(CFG)"></a>2. 上下文无关文法(CFG)</h3><p>（1）上下文无关文法(Context Free Grammar,CFG)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> CFG是一个四元组G =（N，T，P，S），其中</span><br><span class="line">（1） N是非终结符（Nonterminals）的有限集合；</span><br><span class="line">（2） T是终结符（Terminals）的有限集合，且N∩T=Φ；</span><br><span class="line">（3） P是产生式（Productions）的有限集合，A→α，其中A∈N(左部),α∈(N∪T)*(右部),若α=ε，则称A→ε为空产生式(也可以记为A →);</span><br><span class="line">（4） S是非终结符，称为文法的开始符号（Start symbol）</span><br><span class="line"> 注： S ∈ N , N可以出现在产生式左边和右边，T绝不出现在产生式左边.</span><br></pre></td></tr></table></figure><p>（2）CFG产生语言的基本方法－推导 </p><p>CFG（产生式）通过推导的方法产生语言，即（通俗地讲）从开始符号S开始，反复使用产生式：将产生式左部的非终结符替换为右部的文法符号序列(展开产生式，用=&gt;表示)，直到得到一个终结符序列。 </p><p><strong>1&gt; 直接推导：</strong>利用产生式产生句子的过程中，将用产生式A→γ的右部代替文法符号序列αAβ中的A得到αγβ的过程，称αAβ直接推导出αγβ，记作：αAβ=&gt;αγβ</p><p><strong>2&gt; 零步或多步推导：</strong>若对于任意文法符号序列α1，α2，…αn，有α1=&gt;α2=&gt;…=&gt;αn，则称此过程为零步或多步推导，记为：α1 =*&gt; αn，其中α1=αn的情况为零步推导。</p><p><strong>3&gt; 至少一次推导：</strong>若α1≠αn，即推导过程中至少使用一次产生式,则称此过程为至少一步推导，记为：α1 =+&gt; αn</p><p>(推导具有自反性和传递性)</p><p><strong>4&gt;</strong> 由 CFGG 所产生的语言L(G)被定义为: L(G) = { ω┃S   ωand ω∈T<em> }，<br>​    L(G)称为上下文无关语言(Context Free Language, CFL)，ω称为句子。<br>​    若S =</em> &gt;   α，α∈(N∪T)*，则称α为G的一个句型。句子一定是句型，反之不是。</p><p><strong>5&gt;</strong> 在推导过程中，若每次直接推导均替换句型中最左边的非终结符，则称为<code>最左推导</code>，由最左推导产生的句型被称为<code>左句型</code>。 类似的可以定义最右推导与右句型，最右推导也被称为规范推导。</p><p>（3）推导、分析树与语法树</p><p>1、分析树既反映语言结构的实质，也反映推导过程。</p><p>2、对CFGG的句型，<strong>分析树</strong>被定义为具有下述性质的一棵树。</p><p>   （1） 根由开始符号所标记；<br>   （2） 每个叶子由一个终结符、非终结符、或ε标记；<br>   （3） 每个内部结点由一个非终结符标记；<br>   （4） 若A是某内部节点的标记，且X1，X2，…，Xn是该节点从左到右所有孩子的标记，则A→X1X2…Xn是一个产生式。若A→ε，则标记为A的结点可以仅有一个标记为ε的孩子。 </p><p>  注：分析树的叶子，从左到右构成G的一个句型。若叶子仅由终结符标记，则构成一个句子。</p><p>3、对CFG G的句型，表达式的语法树被定义为具有下述性质的一棵树:</p><p>  （1） 根与内部节点由表达式中的操作符标记；<br>  （2） 叶子由表达式中的操作数标记；<br>  （3）用于改变运算优先级和结合性的括号，被隐含在语法树的结构中。</p><ul><li>语法树是表示表达式结构的最好形式</li></ul><p>（4）二义性与二义性的消除</p><p><strong>二义性：</strong>若文法G对 同 一句子产生不止一棵<code>分析树</code>，则称G是二义的.</p><p><strong>结论：</strong><br>1&gt; 一个句子有多于一棵分析树，仅与文法和句子有关，与采用的推导方法无关；<br>2&gt; 造成文法二义的根本原因：文法中缺少对文法符号优先级和结合性的规定</p><p><strong>二义性消除的方法：</strong><br>① 改写二义文法为非二义文法；<br>② 规定二义文法中符号的优先级和结合性，使仅产生一棵分析树。</p><h3 id="3-语法与文法简介"><a href="#3-语法与文法简介" class="headerlink" title="3. 语法与文法简介"></a>3. 语法与文法简介</h3><p>（1）正规式与上下文无关文法</p><ul><li>记号可以用正规式描述，正规式适合描述线性结构，如标识符、关键字、注释等.</li><li>句子可以用CFG描述，CFG适合描述具有嵌套(层次)性质的非线性结构，如不同结构的句子if-then-else\while-do等</li></ul><p>正规式所描述的语言结构均可以用CFG描述，反之不一定.</p><p>（2）上下文有关文法CSG</p><p>典型的这类语言结构包含：计数问题的抽象、变量的声明与引用、过程调用时形参与实参的一致性检查等.描述它们的文法被称为上下文有关文法(Context Sensitive Grammar，CSG).这些语言结构无法用上下文无关文法CSG来描述.</p><p>（3）形式语言与自动机简介</p><p>​    若文法G=(N，T，P，S)的每个产生式α→β中，均有α∈(N∪T)<em>，且至少含有一个非终结符，β∈(N∪T)</em>，则称G为0型文法.</p><p>​    对0型文法施加以下第i条限制，即得到i型文法。</p><p>​    1&gt; G的任何产生式α→β（S→ε除外）满足|α|≤|β|；<br>​    2&gt; G的任何产生式形如A→β，其中A∈N，β∈(N∪T)*；<br>​    3&gt; G的任何产生式形如A→a或者A→aB(或者A→Ba)，其中A和B∈N，a∈T。  </p><table><thead><tr><th>文法</th><th>语言</th><th>自动机</th></tr></thead><tbody><tr><td>短语文法(0型)</td><td>短语结构语言</td><td>图灵机</td></tr><tr><td>CSG(1型)</td><td>CSL</td><td>线性界线自动机</td></tr><tr><td>CFG(2型)</td><td>CFL</td><td>下推自动机</td></tr><tr><td>正规文法(3型)</td><td>正规集</td><td>有限自动机</td></tr></tbody></table><h3 id="4-自上而下语法分析"><a href="#4-自上而下语法分析" class="headerlink" title="4. 自上而下语法分析"></a>4. 自上而下语法分析</h3><p>分为：递归下降分析法、预测分析法</p><p><strong>基本思想：</strong>对任何一个输入序列ω，从S开始进行最左推导，直到得到一个合法的句子或发现一个非法结构。整个自上而下分析是一个试探的过程，是反复使用不同产生式谋求与输入序列匹配的过程。</p><p>提前准备——<strong>重写文法：</strong>1.消除左递归，以避免陷入死循环； 2.提取左因子，以避免回溯.</p><p>（1）消除左递归</p><p>定义：若文法G中的非终结符A，对某个文法符号序列α存在推导A =+&gt; Aα，则称G是左递归的。若G中有形如A→Aα的产生式，则称该产生式对A直接左递归。</p><p><1> 消除文法的直接左递归 </1></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A→Aα|β     替换为     A →βA&apos;</span><br><span class="line">A&apos;→αA&apos;|ε</span><br></pre></td></tr></table></figure><p>首先，整理A产生式为如下形式：A→ Aα1|Aα2|…|Aαm|β1|β2|…|βn<br>然后用下述产生式代替A产生式：A→ β1 A’|β2 A’| …|βn A’<br>​                            A’→ α1 A’ | α2 A’ | … | αm A’ |ε   </p><p><2> 消除文法的左递归</2></p><p>核心思想：将无直接左递归的非终结符展开到其他产生式,然后消除其他产生式中的直接左递归(如果有的话)</p><p>若G产生句子的过程中出现A=+A的推导，则无法消除左递归(出现回路)</p><p>（2）提取左因子</p><p><1> 提取文法的左因子</1></p><p>左因子产生原因：公共前缀：A → αβ1|αβ2<br>方法：将        A → αβ1|αβ2|γ<br>​    替换为    A→αA’|γ        A’→β1|β2</p><p>（3）递归下降分析</p><p>直接以程序代码（的方式）模拟产生式产生语言的过程:</p><p><strong>基本思想：</strong>每个非终结符对应一个子程序（函数），过程体中：</p><ul><li>产生式右部的非终结符：对应子程序调用，</li><li>产生式右部的终结符：   与输入记号序列进行匹配。</li></ul><p><strong>特点：</strong><br>1&gt; 子程序是递归的（因为文法是递归的）；<br>2&gt; 程序与文法相关；<br>3&gt; <strong><em>它对文法的限制是不能有公共左因子和左递归；</em></strong><br>4&gt; 它是一种非形式化的方法，只要能写出子程序，用什么样的方法和步骤均可。</p><p>（4）预测分析器</p><p>☆ 预测分析器由一张预测分析表、一个符号栈和一个驱动器组成，数学模型是下推自动机。<br>☆ 对文法的限制是不能有公共左因子和左递归</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-4fbb189fad81f346.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>预测分析器的核心概念：<br>1&gt; 分析方法：格局与格局变换<br>2&gt; 分析表+驱动器（模拟算法）<br>3&gt; 预测分析表的构造<br>4&gt; LL（文法、语言、分析器）</p><p>☆ 开始格局的剩余输入是全部输入序列，而接收格局中剩余输入应该为空，任何其他格局或出错格局中的剩余输入应该是全部输入序列的一个后缀.</p><p>☆ 改变格局的动作：</p><p>① 匹配终结符：  若top^=ip^(但≠#)，则pop且next(ip)；<br>② 展开非终结符：若top^= X且M[X,ip^]=α(X→α)，则pop且push(α)；<br>③ 报告分析成功：  若top ^= ip^ = #，则分析成功并结束；<br>④ 报告出错：其它情况，调用错误恢复例程.</p><p>☆ 驱动器算法</p><p>☆ 构造预测分析表</p><p>步骤：1. 构造文法符号X的FIRST集合和非终结符的FOLLOW集合；2. 根据两个集合构造预测分析表.</p><p>通俗地讲，α的FIRST集合就是从α开始可以导出的文法符号序列中的开头终结符。而A的FOLLOW集合，就是从开始符号可以导出的所有含A的文法符号序列中紧跟A之后的终结符.</p><p><1> 计算X的FIRST集合 —–自下而上计算</1></p><p><2> 计算所有非终结符的FOLLOW集合 —— 自上而下计算</2></p><p><3> 构造预测分析表</3></p><p><4> LL(1)文法</4></p><p>文法G被称为是LL(1)文法，当且仅当为它构造的<code>预测分析表</code>中<code>不含多重定义</code>的条目。由此分析表所组成的分析器被称为<code>LL(1)分析器</code>，它所分析的语言被称为<code>LL(1)语言</code>。</p><p>☆ 第一个L代表从左到右扫描输入序列，第二个L表示产生最左推导，1表示在确定分析器的每一步动作时向前看一个终结符.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">推论3.2 G是LL(1)的，当且仅当G的任何两个产生式A→α|β满足:</span><br><span class="line">1. 对任何终结符a，α和β不能同时推导出以a开始的串；即First(α) ∩ First(β) = ∅</span><br><span class="line">2. α和β最多有一个可以推导出ε；</span><br><span class="line">3. 若β =*&gt; ε,则α不能导出以FOLLOW(A)中终结符开始的任何串. 即First(α) ∩ Follow(A) = ∅</span><br></pre></td></tr></table></figure><p>☆ 无论是递归下降子程序法还是非递归的预测分析法，他们都只能处理LL(1)文法.</p><h3 id="5-自下而上语法分析"><a href="#5-自下而上语法分析" class="headerlink" title="5. 自下而上语法分析"></a>5. 自下而上语法分析</h3><p>☆ 自上而下分析采用的是推导;自下而上分析采用的是归约(规范归约—剪句柄—移进/归约分析—SLR(1)分析器).</p><p>（1）自下而上分析的基本方法</p><p>☆ <strong>基本思想：</strong>最左归约.</p><p>对于每个输入序列ω：从左到右扫描ω;  从ω开始,反复用产生式的左部替换产生式的右部(即当前句型中的句柄)、谋求对ω的匹配,最终得到文法的开始符号，或者发现一个错误。 </p><p>☆ <strong>基本概念：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">a) &gt; 设αβδ是文法G的一个句型，若存在S=*&gt;αAδ，A=+&gt;β， 则称β是句型αβδ相对于A的&quot;短语&quot;.</span><br><span class="line">   &gt; 特别的，若 有A→β，则 称β是句型αβδ相对于产生式A→β的&quot;直接短语&quot;.</span><br><span class="line">   &gt; 一个句型的最左直接短语被称为&quot;句柄&quot;.</span><br><span class="line"></span><br><span class="line">   特征：</span><br><span class="line">    1.  短语：以非终结符为根子树中所有从左到右的叶子；</span><br><span class="line">    2.  直接短语：只有父子关系的子树中所有从左到右排列的叶子（树高为2）；</span><br><span class="line">    3.  句柄：最左边父子关系树中所有从左到右排列的叶子（句柄是唯一的）</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">b)最左归约：若 α是文法G的句子且满足下述条件，则称序列αn，αn-1，...，α0是α的一个最左归约。</span><br><span class="line">   1) αn = α</span><br><span class="line">   2) α0 = S（S是G 的开始符号）</span><br><span class="line">   3) 对任何i(0&lt;i&lt;=n)，αi-1是将αi中句柄替换为相应产生式左部非终结符得到的</span><br><span class="line">   </span><br><span class="line"> ☆ 最左归约的逆过程是一个最右推导，分别称最右推导和最左归约为规范推导和规范归约.</span><br><span class="line"> </span><br><span class="line"> c）移进-归约分析器</span><br><span class="line"> 1. 工作方式：格局与格局变换</span><br><span class="line"> 2. 分析表</span><br><span class="line"> 3. 驱动器（模拟算法）</span><br><span class="line"> 4. SLR分析表的构造</span><br><span class="line"> 5. LR（文法、语言、分析器）</span><br><span class="line"></span><br><span class="line">☆ 改变格局的动作：</span><br><span class="line">1. 移进(shift)：当前剩余输入的下一终结符进栈。</span><br><span class="line">2.归约(reduce)：将栈顶句柄替换为对应非终结符(最左归约)</span><br><span class="line">3.接受(accept)：宣告分析成功</span><br><span class="line">4. 报错(error)：发现语法错误，调用错误恢复例程</span><br></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-48f1ed06a22fec8e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>(2) LR分析</p><p>a) LR分析与LR文法<br>LR分析：<strong><em>允许左递归，但不能有二义</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">定义3.15 若为文法G构造的移进-归约分析表中不含多重定义的条目，则称G为&quot;LR(k)文法&quot;，分析器被称为是&quot;LR(k)分析器&quot;，它所识别的语言被称为&quot;LR(k)语言&quot;。&quot;L&quot;表示从左到右扫描输入序列，&quot;R&quot;表示逆序的最右推导，&quot;k&quot;表示为确定下一动作向前看的终结符个数，一般情况下k&lt;=1。当k=1时，简称&quot;LR&quot;。</span><br></pre></td></tr></table></figure><p><strong>构造SLR(1)分析器</strong></p><p><1> 活前缀与LR(0)项目</1></p><table><thead><tr><th></th><th>第1步</th><th>第2~N步</th><th>状态</th></tr></thead><tbody><tr><td>词法–DFA</td><td>ε-closure(S)</td><td>ε-closure(smove(S,a))</td><td>状态集</td></tr><tr><td>语法–DFA</td><td>closure(I)</td><td>closure(goto(I,x))</td><td>项目集</td></tr></tbody></table><p>出现在移进-归约分析器栈中的右句型的前缀，被称为文法G的<strong>活前缀(viable prefix)</strong>.<br><strong>LR(0)项目(简称项目)</strong>是这样一个产生式，在它右边的某个位置有一个点”.”。对于A→ε，它仅有一个项目A→.。<br>项目A→α.β显示了分析过程中看到(移进)了产生式的多少。<br>β不为空的项目称为<strong>可移进项目</strong>，β为空的项目称为<strong>可归约项目</strong>.</p><p><2> 拓广文法与识别活前缀的DFA</2></p><p>G’ = G ∪ {S’ → S}<br>其中：S’ → S是识别S的初态，S’ → S. 是识别S的终态. 目的是使最终构造的DFA状态集中具有唯一的初态和终态.   ① closure(I)：从项目集I不经任何文法符号到达的项目全体；</p><p>② goto(I，x)：所有从I经文法符号x能直接到达的项目全体。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">项目[S’→.S]和所有“.”不在产生式右部最左边的项目称为核心项目(kernel items)，</span><br><span class="line">其它“.”在产生式右部最左边的项目(不包括[S’→.S])称为非核心项目(nonkernel items).</span><br><span class="line"></span><br><span class="line">核心项目：J=goto(I，X)，S&apos;→.S（作为项目集的代表）</span><br><span class="line">非核心项目：closure(J)-J（特点：可由J某中某项目算得）</span><br></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-4801bd8adeba1c8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p><3> 识别活前缀</3></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">定义3.21 若存在最右推导S’=*&gt; αAω =&gt; αβ1β2ω，则称项目[A→β1.β2] 对活前缀αβ1有效。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">当一个项目集中同时存在：</span><br><span class="line">1. A→β1.β2和B→β.：既可移进又可归约，移进/归约冲突</span><br><span class="line">2.A→α.和B→β.：均可指导下一步分析，归约/归约冲突</span><br><span class="line"></span><br><span class="line">解决方法：简单向前看一个终结符：</span><br><span class="line">1. 移进/归约冲突：若FIRST(β2)∩FOLLOW(B)=Φ，冲突可解决</span><br><span class="line">2. 归约/归约冲突：若FOLLOW(A)∩FOLLOW(B)=Φ，冲突可解决</span><br><span class="line"></span><br><span class="line">若冲突可以解决，则称文法为SLR(1)文法，构造的分析表为SLR(1)分析表。</span><br><span class="line">SLR(1)文法：简单向前看一个终结符即可解决冲突</span><br><span class="line">☆ 二义文法不是SLR(1)文法</span><br></pre></td></tr></table></figure><h2 id="第四章-静态语义分析"><a href="#第四章-静态语义分析" class="headerlink" title="第四章 静态语义分析"></a>第四章 静态语义分析</h2><p>采用语法制导翻译生成中间代码</p><h3 id="1-语法制导翻译简介"><a href="#1-语法制导翻译简介" class="headerlink" title="1. 语法制导翻译简介"></a>1. 语法制导翻译简介</h3><p>（1）语法与语义的关系</p><p>语法是指语言的结构、即语言的“样子”；<br>语义是指附着于语言结构上的实际含意，即语言的“意义”.<br>一个语法上正确的句子，它所代表的意义并不一定正确.</p><p>☆ <strong>语义分析的作用</strong></p><p>• 检查结构正确的句子所表示的意思是否合法；<br>• 执行规定的语义动作，如：表达式求值、符号表的查询/填写、中间代码生成等</p><p>☆ 应用最广的<strong>语义分析方法</strong>是语法制导翻译，他的<strong>基本思想</strong>是将语言结构的语义以<strong>属性</strong>的形式赋予代表此结构的文法符号，而属性的计算以<strong>语义规则</strong>的形式赋予由文法符号组成的产生式.</p><p>（2）属性/语义规则的定义</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">定义4.1 对于产生式A→α，其中α是由文法符号X1X2...Xn组成的序列，它的语义规则可以表示为(4.1)所示关于属性的函数f：</span><br><span class="line">          b := f(c1, c2, ..., ck)                  (4.1)</span><br><span class="line">语义规则中的属性存在下述性质与关系：</span><br><span class="line">      (1)   称(4.1)中属性b依赖于属性c1, c2, ..., ck。</span><br><span class="line">      (2) 若b是A的属性，c1, c2, ..., ck是α中文法符号的属性，或者A的其它属性，则称b是A的综合属性。</span><br><span class="line">      (3) 若b是α中某文法符号Xi的属性，c1, c2, ..., ck是A的属性，或者是α中其它文法符号的属性，则称b是Xi的继承属性。</span><br><span class="line">      (4) 若语义规则的形式如下述(4.2)，则可将其想像为产生式左部文法符号A的一个虚拟属性。属性之间的依赖关系，在虚拟属性上依然存在。</span><br><span class="line">          f(c1, c2, ..., ck)                (4.2)          ■</span><br></pre></td></tr></table></figure><p>☆ 继承属性从前辈和兄弟的属性计算得到,综合属性从子孙和自身的其他属性计算得到.</p><p>即,<code>继承属性</code>“自上而下,包括兄弟”,<code>综合属性</code>“自下而上,包括自身”.  </p><p>（3）语义规则的两种形式</p><p>☆ 语义规则的两种形式（忽略实现细节，二者作用等价）</p><p><1> 语法制导定义(Syntax Directed Definition)</1></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用抽象的属性和运算表示的语义规则；(公式，做什么)</span><br></pre></td></tr></table></figure><p><2> 翻译方案(Translation Scheme)</2></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用具体的属性和运算表示的语义规则。(程序段，如何做)</span><br></pre></td></tr></table></figure><p>☆ <code>继承属性</code>是自上而下计算的，<code>综合属性</code>是自下而上计算的.</p><p>（4）LR分析翻译方案的设计</p><p>☆ LR分析中的语法制导翻译实质上是对LR语法分析的扩充：</p><ol><li><p><strong>扩充LR分析器的功能</strong></p><p>当执行<code>归约产生式</code>的动作时，也执行相应产生式对应的<code>语义动作</code>。由于是归约时执行语义动作，</p></li></ol><p>​     因此限制语义动作仅能放在产生式右部的<code>最右边</code>；</p><ol start="2"><li><strong>扩充分析栈</strong></li></ol><p>​        增加一个与<code>分析栈</code>并列的<code>语义栈</code>，用于存放分析栈中文法符号所对应的<code>属性值</code>。 </p><p>☆ 扩充后的LR分析最适合对综合属性的计算，而对于继承属性的计算还需要进行适当的处理.</p><h3 id="2-中间代码简介"><a href="#2-中间代码简介" class="headerlink" title="2. 中间代码简介"></a>2. 中间代码简介</h3><p>☆ 中间代码应具备的特性<br>1）便于语法制导翻译<br>2）既与机器指令的结构相近,又与具体机器无关. </p><p>使用中间代码的好处:一是便于编译器程序的开发和移植,二是代码进行优化处理.  </p><p>☆ 中间代码的主要形式：后缀式、树、三地址码等.最基本的中间代码形式是树🌲；最常用的中间代码形式是三地址码，它的实现形式常采用四元式形式。</p><p>☆ 符号表是帮助声明语句实现存储空间分配的重要数据结构。</p><p>（1）后缀式</p><p>操作数在前，操作符紧随其后，无需用括号限制运算的优先级和结合性；便于求值.</p><p>（2）三地址码</p><p>① 三元式        形式：    (i) (op, arg1,  arg2)</p><p>​                      三地址码：(i):= arg1 op arg2</p><p>序号的双重含义：既代表此三元式，又代表三元式存放的结果</p><p>存放方式：数组结构，三元式在数组中的位置由下标决定</p><p>弱点：给代码的<code>优化</code>带来困难</p><p>② 四元式         形式：   ( i )  (op，arg1，arg2，result)</p><p>​                      所表示的计算：  result:= arg1 op arg2</p><ol><li><p>四元式与三元式的唯一区别：将由序号所表示的运算结果改为：用(临时)变量来表示。</p></li><li><p>此改变使得四元式的运算结果与其在四元式序列中的位置无关.为代码的优化提供了极大方便，因为这样可以删除或移动四元式而不会影响运算结果.</p></li></ol><p>③ 树形表示</p><p>1&gt; 语法树真实反映句子结构，对语法树稍加修改（加入语义信息），即可以作为中间代码的一种形式(注释语法树)<br>2&gt; 树的优化表示－DAG<br>3&gt; 树与其他中间代码的关系</p><p>☆ <code>树表示的中间代码</code>与<code>后缀式</code>和<code>三地址码</code>之间有内在联系</p><ol><li>树 → 后缀式</li></ol><p>​     方法：对树进行<code>深度优先后序遍历</code>，得到的线性序列就是<code>后缀式</code>，或者说后缀式是树的一个线性化序列；</p><ol start="2"><li><p>树 → 三元式/四元式</p><p>特点：树的每个非叶子节点和它的儿子<code>对应</code>一个三元式或四元式；</p><p>方法：对树的非叶子节点进行深度优先后序遍历，即得到一个三元式或四元式序列。</p></li></ol><h3 id="3-符号表简介"><a href="#3-符号表简介" class="headerlink" title="3. 符号表简介"></a>3. 符号表简介</h3><ul><li><strong>符号表的作用</strong>：连接声明与引用的桥梁，记住每个符号的相关信息，如作用域和类型等，帮助编译的各个阶段正确有效地工作。</li><li><strong>符号表的基本目标：</strong>有效记录信息、快速准确查找。</li><li><strong>符号表设计的基本要求：</strong>   <ul><li>正确存储各类信息；</li><li>适应不同阶段的需求；</li><li>便于有效地进行查找、插入、删除和修改等操作； </li><li>空间可以动态扩充.</li></ul></li></ul><p>（1）构成名字的字符串</p><p><strong>构成名字的字符串的存储方式：</strong>直接存储—定长数据(直接将构成名字的字符串放在符号表条目中)和间接存储—变长数据(将构成名字的字符串统一存放在一个大的连续空间内，字符串与字符串之间采用特殊的分隔符隔开，符号表条目中仅存放指向该字符串首字符的指针).</p><p>（2）名字的作用域</p><p>☆ 程序语言范围的划分可以有两种划分范围的方式：<code>并列</code>和<code>嵌套</code></p><p>☆ <strong>名字的作用域规则：</strong>规定一个名字在什么样的范围内应该表示什么意义.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;1&gt; 静态作用域规则（static-scope rule）：编译时就可以确定名字的作用域,即仅从静态读程序就可确定名字的作用域</span><br><span class="line">&lt;2&gt; 最近嵌套规则（most closely nested）：名字的声明在离其最近的内层起作用</span><br></pre></td></tr></table></figure><p>（3）线性表</p><p>符号表以<strong>栈(线性表)</strong>的方式组织.</p><p>线性表上的操作：查找、插入、删除、修改</p><p>查找：从表头(栈顶)开始，遇到的第一个符合条件的名字；插入：先查找，再加入在表头（栈顶）；</p><p>关键字 = 名字＋作用域；</p><p>（4）散列表</p><p>名字挂在两个链上(便于删除操作)：</p><ol><li>散列链(hash link)： 链接所有具有<code>相同hash值</code>的元素，表头在表头数组中；</li><li>作用域链(scope link)：链接所有在<code>同一作用域</code>中的元素，表头在作用域表中.</li></ol><p>☆ 操作：查找、插入、删除</p><h3 id="4-声明语句的翻译"><a href="#4-声明语句的翻译" class="headerlink" title="4. 声明语句的翻译"></a>4. 声明语句的翻译</h3><p>（1）变量的声明</p><p>☆ 一个变量的声明应该由两部分来完成：<code>类型的定义</code>和<code>变量的声明</code></p><ul><li><strong>类型定义：</strong>为编译器提供存储空间大小的信息</li><li><strong>变量声明：</strong>为变量分配存储空间</li><li><strong>组合数据的类型定义和变量声明：</strong>定义与声明在一起，定义与声明分离.</li></ul><p>1&gt; <code>简单数据类型</code>的存储空间是预先确定的，如int可以占4个字节，double可以占8个字节，char可以占1个字节等</p><p>2&gt; <code>组合数据类型变量</code>的存储空间，需要编译器根据程序员提供的信息计算而定.</p><p>（2） 过程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">1．过程（procedure）：过程头(做什么) ＋  过程体(怎么做)；</span><br><span class="line">   - 函数:  有返回值的过程</span><br><span class="line">   - 主程序:  被操作系统调用的过程/函数</span><br><span class="line"></span><br><span class="line">2．过程的三种形式：过程定义、过程声明和过程调用。</span><br><span class="line">   过程定义：过程头+过程体；</span><br><span class="line">   过程声明：过程头；</span><br><span class="line"></span><br><span class="line">3. 左值与右值 </span><br><span class="line">   1&gt; 直观上，出现在赋值号左边和右边的量分别称为左值和右值；</span><br><span class="line">   2&gt; 实质上，左值必须具有存储空间，右值可以仅是一个值，而没有存储空间.</span><br><span class="line">   3&gt; 形象地讲，左值是容器，右值是内容.</span><br><span class="line">   </span><br><span class="line">4. 参数传递</span><br><span class="line">   1&gt; 形参与实参</span><br><span class="line">- 声明时的参数称为形参(parameter或formal parameter)</span><br><span class="line">- 引用时的参数称为实参(argument或actual parameter)</span><br><span class="line">   2&gt; 常见的参数传递形式：（不同的语言提供不同的形式）</span><br><span class="line">- 值调用（call by value）---过程内部对参数的修改，不影响作为实参的变量原来的值.</span><br><span class="line">- 引用调用（call by reference）--- 过程内部对形参的修改，实质上是对实参的修改.</span><br><span class="line">- 复写－恢复（copy-in/copy-out）--- ① 过程内对参数的修改不直接影响实参，避免了副作用;</span><br><span class="line">② 返回时将形参内容恢复给实参，实现参数值的返回.</span><br><span class="line">- 换名调用（call by name）--- 宏调换</span><br><span class="line">   3&gt; 参数传递方法的本质区别： 实参是代表左值、右值、还是实参本身的正文.</span><br><span class="line">   </span><br><span class="line">5. 作用域信息的保存</span><br><span class="line">☆ 能够画出嵌套过程的嵌套关系树(P191 4.33),根据语法制导翻译(P193 4.35)画出分析树,写出推导步骤,构造的符号表</span><br></pre></td></tr></table></figure><h3 id="5-简单算术表达式与赋值句"><a href="#5-简单算术表达式与赋值句" class="headerlink" title="5. 简单算术表达式与赋值句"></a>5. 简单算术表达式与赋值句</h3><p>P197 例4.36 主要是变量类型的转换</p><h3 id="6-数组元素的引用"><a href="#6-数组元素的引用" class="headerlink" title="6. 数组元素的引用"></a>6. 数组元素的引用</h3><p>（1）数组元素的地址计算</p><ul><li>注意是行主存储还是列主存储</li></ul><p>（2）☆数组元素引用的语法制导翻译(考试热点之一) </p><ul><li>P201 例4.37</li></ul><h3 id="7-布尔表达式"><a href="#7-布尔表达式" class="headerlink" title="7. 布尔表达式"></a>7. 布尔表达式</h3><p>布尔表达式的计算有两种方法：数值表示的<code>直接计算</code>和逻辑表示的<code>短路计算</code></p><p>☆ 布尔表达式短路计算的翻译：短路计算的控制流，真出口与假出口，真出口链与假出口链，拉链回填技术(P207 例4.41)（考试热点之一）</p><h3 id="8-控制语句"><a href="#8-控制语句" class="headerlink" title="8. 控制语句"></a>8. 控制语句</h3><p><strong>控制语句的分类：</strong>①无条件转移、②条件转移、③循环语句、④分支语句</p><ul><li>无条件转移(goto)\条件转移(if、while)</li><li>条件转移的语法制导翻译：P213 例4.42</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">多看课件PPT，多做题练手</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】20min 11001words&lt;br&gt;【阅读内容】XD编译原理教材知识汇总&lt;/p&gt;
    
    </summary>
    
      <category term="XD" scheme="http://yoursite.com/categories/XD/"/>
    
    
      <category term="Compiler" scheme="http://yoursite.com/tags/Compiler/"/>
    
  </entry>
  
  <entry>
    <title>高等数学18讲</title>
    <link href="http://yoursite.com/2018/08/16/%E9%AB%98%E6%95%B018%E8%AE%B2/"/>
    <id>http://yoursite.com/2018/08/16/高数18讲/</id>
    <published>2018-08-16T05:01:38.481Z</published>
    <updated>2018-08-18T03:21:04.382Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】<br>【阅读内容】张宇高数18讲各部分知识图谱</p><a id="more"></a><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-6bd6872d1596266c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="第1讲 高等数学常用基础知识点.png" title="">                </div>                <div class="image-caption">第1讲 高等数学常用基础知识点.png</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-74296f9792160799.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="第2讲 极限与连续" title="">                </div>                <div class="image-caption">第2讲 极限与连续</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-85e4a86b5943dfde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="第5讲 中值定理" title="">                </div>                <div class="image-caption">第5讲 中值定理</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-161b30946de50e88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="积分学汇总" title="">                </div>                <div class="image-caption">积分学汇总</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-aa4b3778aee22a81.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="第8讲 一元函数积分学的应用" title="">                </div>                <div class="image-caption">第8讲 一元函数积分学的应用</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://upload-images.jianshu.io/upload_images/5267500-11a3eab8bd64c746.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="第10讲 多元函数微分学" title="">                </div>                <div class="image-caption">第10讲 多元函数微分学</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://upload-images.jianshu.io/upload_images/5267500-2386bc8eda9ebc8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="第17讲 三重积分、第一型曲线曲面积分.png" title="">                </div>                <div class="image-caption">第17讲 三重积分、第一型曲线曲面积分.png</div>            </figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】&lt;br&gt;【阅读内容】张宇高数18讲各部分知识图谱&lt;/p&gt;
    
    </summary>
    
      <category term="XD" scheme="http://yoursite.com/categories/XD/"/>
    
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
      <category term="Knowledge Map" scheme="http://yoursite.com/tags/Knowledge-Map/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/08/16/hello-world/"/>
    <id>http://yoursite.com/2018/08/16/hello-world/</id>
    <published>2018-08-15T17:41:32.591Z</published>
    <updated>2018-10-17T14:01:17.862Z</updated>
    
    <content type="html"><![CDATA[<p>【阅读时间】2min 122words<br>【阅读内容】Hexo 的基础操作——new、clean、generate、server、deploy</p><a id="more"></a><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new  <span class="string">"postName"</span> <span class="comment">#新建文章</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Clean-public-files"><a href="#Clean-public-files" class="headerlink" title="Clean public files"></a>Clean public files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean</span><br></pre></td></tr></table></figure><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><h4 id="报错："><a href="#报错：" class="headerlink" title="报错："></a>报错：</h4><h5 id="Template-render-error"><a href="#Template-render-error" class="headerlink" title="Template render error"></a>Template render error</h5><p>原因：_post中某MD文件数学公式的格式有问题</p><p>解决：$$  ​$$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【阅读时间】2min 122words&lt;br&gt;【阅读内容】Hexo 的基础操作——new、clean、generate、server、deploy&lt;/p&gt;
    
    </summary>
    
      <category term="Annotation" scheme="http://yoursite.com/categories/Annotation/"/>
    
    
  </entry>
  
</feed>
